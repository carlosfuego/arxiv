[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.03753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v1",
                "updated": "2024-09-05T17:59:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis's utility through three\ncase studies: facilitating chatbot misuse research, visualizing and comparing\ntopic distributions across datasets, and characterizing user-specific\nconversation patterns. WildVis is open-source and designed to be extendable,\nsupporting additional datasets and customized search and visualization\nfunctionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis's utility through three\ncase studies: facilitating chatbot misuse research, visualizing and comparing\ntopic distributions across datasets, and characterizing user-specific\nconversation patterns. WildVis is open-source and designed to be extendable,\nsupporting additional datasets and customized search and visualization\nfunctionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benot Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jimnez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro Lpez-Garca"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jimnez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Bchner"
                    },
                    {
                        "name": "Leonardo Agudo Jcome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jcome"
                },
                "author": "Leonardo Agudo Jcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adri Armejach"
                    },
                    {
                        "name": "Miquel Moret"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moret"
                },
                "author": "Miquel Moret",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.03752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03752v1",
                "updated": "2024-09-05T17:59:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    12,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:59:12Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    12,
                    3,
                    249,
                    0
                ],
                "title": "Attention Heads of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Heads of Large Language Models: A Survey"
                },
                "summary": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain largely as black-box systems. Consequently, their\ndevelopment relies heavily on data-driven approaches, limiting performance\nenhancement through changes in internal architecture and reasoning pathways. As\na result, many researchers have begun exploring the potential internal\nmechanisms of LLMs, aiming to identify the essence of their reasoning\nbottlenecks, with most studies focusing on attention heads. Our survey aims to\nshed light on the internal reasoning processes of LLMs by concentrating on the\ninterpretability and underlying mechanisms of attention heads. We first distill\nthe human thought process into a four-stage framework: Knowledge Recalling,\nIn-Context Identification, Latent Reasoning, and Expression Preparation. Using\nthis framework, we systematically review existing research to identify and\ncategorize the functions of specific attention heads. Furthermore, we summarize\nthe experimental methodologies used to discover these special heads, dividing\nthem into two categories: Modeling-Free methods and Modeling-Required methods.\nAlso, we outline relevant evaluation methods and benchmarks. Finally, we\ndiscuss the limitations of current research and propose several potential\nfuture directions. Our reference list is open-sourced at\n\\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain largely as black-box systems. Consequently, their\ndevelopment relies heavily on data-driven approaches, limiting performance\nenhancement through changes in internal architecture and reasoning pathways. As\na result, many researchers have begun exploring the potential internal\nmechanisms of LLMs, aiming to identify the essence of their reasoning\nbottlenecks, with most studies focusing on attention heads. Our survey aims to\nshed light on the internal reasoning processes of LLMs by concentrating on the\ninterpretability and underlying mechanisms of attention heads. We first distill\nthe human thought process into a four-stage framework: Knowledge Recalling,\nIn-Context Identification, Latent Reasoning, and Expression Preparation. Using\nthis framework, we systematically review existing research to identify and\ncategorize the functions of specific attention heads. Furthermore, we summarize\nthe experimental methodologies used to discover these special heads, dividing\nthem into two categories: Modeling-Free methods and Modeling-Required methods.\nAlso, we outline relevant evaluation methods and benchmarks. Finally, we\ndiscuss the limitations of current research and propose several potential\nfuture directions. Our reference list is open-sourced at\n\\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}."
                },
                "authors": [
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Yuxin Huang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "20 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07543v2",
                "updated": "2024-09-05T17:58:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    58,
                    50,
                    3,
                    249,
                    0
                ],
                "published": "2023-10-11T14:47:51Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    14,
                    47,
                    51,
                    2,
                    284,
                    0
                ],
                "title": "Ordinal Characterization of Similarity Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ordinal Characterization of Similarity Judgments"
                },
                "summary": "Characterizing judgments of similarity within a perceptual or semantic\ndomain, and making inferences about the underlying structure of this domain\nfrom these judgments, has an increasingly important role in cognitive and\nsystems neuroscience. We present a new framework for this purpose that makes\nlimited assumptions about how perceptual distances are converted into\nsimilarity judgments. The approach starts from a dataset of empirical judgments\nof relative similarities: the fraction of times that a subject chooses one of\ntwo comparison stimuli to be more similar to a reference stimulus. These\nempirical judgments provide Bayesian estimates of underling choice\nprobabilities. From these estimates, we derive indices that characterize the\nset of judgments in three ways: compatibility with a symmetric dis-similarity,\ncompatibility with an ultrametric space, and compatibility with an additive\ntree. Each of the indices is derived from rank-order relationships among the\nchoice probabilities that, as we show, are necessary and sufficient for local\nconsistency with the three respective characteristics. We illustrate this\napproach with simulations and example psychophysical datasets of dis-similarity\njudgments in several visual domains and provide code that implements the\nanalyses at https://github.com/jvlab/simrank.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing judgments of similarity within a perceptual or semantic\ndomain, and making inferences about the underlying structure of this domain\nfrom these judgments, has an increasingly important role in cognitive and\nsystems neuroscience. We present a new framework for this purpose that makes\nlimited assumptions about how perceptual distances are converted into\nsimilarity judgments. The approach starts from a dataset of empirical judgments\nof relative similarities: the fraction of times that a subject chooses one of\ntwo comparison stimuli to be more similar to a reference stimulus. These\nempirical judgments provide Bayesian estimates of underling choice\nprobabilities. From these estimates, we derive indices that characterize the\nset of judgments in three ways: compatibility with a symmetric dis-similarity,\ncompatibility with an ultrametric space, and compatibility with an additive\ntree. Each of the indices is derived from rank-order relationships among the\nchoice probabilities that, as we show, are necessary and sufficient for local\nconsistency with the three respective characteristics. We illustrate this\napproach with simulations and example psychophysical datasets of dis-similarity\njudgments in several visual domains and provide code that implements the\nanalyses at https://github.com/jvlab/simrank."
                },
                "authors": [
                    {
                        "name": "Jonathan D. Victor"
                    },
                    {
                        "name": "Guillermo Aguilar"
                    },
                    {
                        "name": "Suniyya A. Waraich"
                    }
                ],
                "author_detail": {
                    "name": "Suniyya A. Waraich"
                },
                "author": "Suniyya A. Waraich",
                "arxiv_comment": "Body: 64 pages; 16 figures; 7 supplementary figures, 3 appendices.\n  This replacement is a major revision in response to peer reviews in\n  Mathematical Neuroscience and Applications (MNA). Main changes are (i)\n  reorganization to separate theory and implementation, (ii) addition of\n  analyses of synthetic datasets, (iii) expanded discussion of alternatives and\n  open issues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03741v1",
                "updated": "2024-09-05T17:54:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    54,
                    26,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:54:26Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    54,
                    26,
                    3,
                    249,
                    0
                ],
                "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable\n  Data Pose Greater Harm?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Data Importance in Machine Learning Attacks: Does Valuable\n  Data Pose Greater Harm?"
                },
                "summary": "Machine learning has revolutionized numerous domains, playing a crucial role\nin driving advancements and enabling data-centric processes. The significance\nof data in training models and shaping their performance cannot be overstated.\nRecent research has highlighted the heterogeneous impact of individual data\nsamples, particularly the presence of valuable data that significantly\ncontributes to the utility and effectiveness of machine learning models.\nHowever, a critical question remains unanswered: are these valuable data\nsamples more vulnerable to machine learning attacks? In this work, we\ninvestigate the relationship between data importance and machine learning\nattacks by analyzing five distinct attack types. Our findings reveal notable\ninsights. For example, we observe that high importance data samples exhibit\nincreased vulnerability in certain attacks, such as membership inference and\nmodel stealing. By analyzing the linkage between membership inference\nvulnerability and data importance, we demonstrate that sample characteristics\ncan be integrated into membership metrics by introducing sample-specific\ncriteria, therefore enhancing the membership inference performance. These\nfindings emphasize the urgent need for innovative defense mechanisms that\nstrike a balance between maximizing utility and safeguarding valuable data\nagainst potential exploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has revolutionized numerous domains, playing a crucial role\nin driving advancements and enabling data-centric processes. The significance\nof data in training models and shaping their performance cannot be overstated.\nRecent research has highlighted the heterogeneous impact of individual data\nsamples, particularly the presence of valuable data that significantly\ncontributes to the utility and effectiveness of machine learning models.\nHowever, a critical question remains unanswered: are these valuable data\nsamples more vulnerable to machine learning attacks? In this work, we\ninvestigate the relationship between data importance and machine learning\nattacks by analyzing five distinct attack types. Our findings reveal notable\ninsights. For example, we observe that high importance data samples exhibit\nincreased vulnerability in certain attacks, such as membership inference and\nmodel stealing. By analyzing the linkage between membership inference\nvulnerability and data importance, we demonstrate that sample characteristics\ncan be integrated into membership metrics by introducing sample-specific\ncriteria, therefore enhancing the membership inference performance. These\nfindings emphasize the urgent need for innovative defense mechanisms that\nstrike a balance between maximizing utility and safeguarding valuable data\nagainst potential exploitation."
                },
                "authors": [
                    {
                        "name": "Rui Wen"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "arxiv_comment": "To Appear in Network and Distributed System Security (NDSS) Symposium\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03735v1",
                "updated": "2024-09-05T17:50:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    50,
                    31,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:50:31Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    50,
                    31,
                    3,
                    249,
                    0
                ],
                "title": "LLM-CI: Assessing Contextual Integrity Norms in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-CI: Assessing Contextual Integrity Norms in Language Models"
                },
                "summary": "Large language models (LLMs), while memorizing parts of their training data\nscraped from the Internet, may also inadvertently encode societal preferences\nand norms. As these models are integrated into sociotechnical systems, it is\ncrucial that the norms they encode align with societal expectations. These\nnorms could vary across models, hyperparameters, optimization techniques, and\ndatasets. This is especially challenging due to prompt sensitivity$-$small\nvariations in prompts yield different responses, rendering existing assessment\nmethodologies unreliable. There is a need for a comprehensive framework\ncovering various models, optimization, and datasets, along with a reliable\nmethodology to assess encoded norms.\n  We present LLM-CI, the first open-sourced framework to assess privacy norms\nencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette\nmethodology to assess the encoded norms across different contexts and LLMs. We\npropose the multi-prompt assessment methodology to address prompt sensitivity\nby assessing the norms from only the prompts that yield consistent responses\nacross multiple variants. Using LLM-CI and our proposed methodology, we\ncomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior\nwork, examining the impact of model properties (e.g., hyperparameters,\ncapacity) and optimization strategies (e.g., alignment, quantization).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), while memorizing parts of their training data\nscraped from the Internet, may also inadvertently encode societal preferences\nand norms. As these models are integrated into sociotechnical systems, it is\ncrucial that the norms they encode align with societal expectations. These\nnorms could vary across models, hyperparameters, optimization techniques, and\ndatasets. This is especially challenging due to prompt sensitivity$-$small\nvariations in prompts yield different responses, rendering existing assessment\nmethodologies unreliable. There is a need for a comprehensive framework\ncovering various models, optimization, and datasets, along with a reliable\nmethodology to assess encoded norms.\n  We present LLM-CI, the first open-sourced framework to assess privacy norms\nencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette\nmethodology to assess the encoded norms across different contexts and LLMs. We\npropose the multi-prompt assessment methodology to address prompt sensitivity\nby assessing the norms from only the prompts that yield consistent responses\nacross multiple variants. Using LLM-CI and our proposed methodology, we\ncomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior\nwork, examining the impact of model properties (e.g., hyperparameters,\ncapacity) and optimization strategies (e.g., alignment, quantization)."
                },
                "authors": [
                    {
                        "name": "Yan Shvartzshnaider"
                    },
                    {
                        "name": "Vasisht Duddu"
                    },
                    {
                        "name": "John Lacalamita"
                    }
                ],
                "author_detail": {
                    "name": "John Lacalamita"
                },
                "author": "John Lacalamita",
                "arxiv_comment": "20 pages, 8 Figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03733v1",
                "updated": "2024-09-05T17:44:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    44,
                    49,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:44:49Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    44,
                    49,
                    3,
                    249,
                    0
                ],
                "title": "Planning In Natural Language Improves LLM Search For Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning In Natural Language Improves LLM Search For Code Generation"
                },
                "summary": "While scaling training compute has led to remarkable improvements in large\nlanguage models (LLMs), scaling inference compute has not yet yielded analogous\ngains. We hypothesize that a core missing component is a lack of diverse LLM\noutputs, leading to inefficient search due to models repeatedly sampling highly\nsimilar, yet incorrect generations. We empirically demonstrate that this lack\nof diversity can be mitigated by searching over candidate plans for solving a\nproblem in natural language. Based on this insight, we propose PLANSEARCH, a\nnovel search algorithm which shows strong results across HumanEval+, MBPP+, and\nLiveCodeBench (a contamination-free benchmark for competitive coding).\nPLANSEARCH generates a diverse set of observations about the problem and then\nuses these observations to construct plans for solving the problem. By\nsearching over plans in natural language rather than directly over code\nsolutions, PLANSEARCH explores a significantly more diverse range of potential\nsolutions compared to baseline search methods. Using PLANSEARCH on top of\nClaude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on\nLiveCodeBench, outperforming both the best score achieved without search\n(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).\nFinally, we show that, across all models, search algorithms, and benchmarks\nanalyzed, we can accurately predict performance gains due to search as a direct\nfunction of the diversity over generated ideas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While scaling training compute has led to remarkable improvements in large\nlanguage models (LLMs), scaling inference compute has not yet yielded analogous\ngains. We hypothesize that a core missing component is a lack of diverse LLM\noutputs, leading to inefficient search due to models repeatedly sampling highly\nsimilar, yet incorrect generations. We empirically demonstrate that this lack\nof diversity can be mitigated by searching over candidate plans for solving a\nproblem in natural language. Based on this insight, we propose PLANSEARCH, a\nnovel search algorithm which shows strong results across HumanEval+, MBPP+, and\nLiveCodeBench (a contamination-free benchmark for competitive coding).\nPLANSEARCH generates a diverse set of observations about the problem and then\nuses these observations to construct plans for solving the problem. By\nsearching over plans in natural language rather than directly over code\nsolutions, PLANSEARCH explores a significantly more diverse range of potential\nsolutions compared to baseline search methods. Using PLANSEARCH on top of\nClaude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on\nLiveCodeBench, outperforming both the best score achieved without search\n(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).\nFinally, we show that, across all models, search algorithms, and benchmarks\nanalyzed, we can accurately predict performance gains due to search as a direct\nfunction of the diversity over generated ideas."
                },
                "authors": [
                    {
                        "name": "Evan Wang"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Catherine Wu"
                    },
                    {
                        "name": "Yunfeng Bai"
                    },
                    {
                        "name": "Will Song"
                    },
                    {
                        "name": "Vaskar Nath"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Sean Hendryx"
                    },
                    {
                        "name": "Summer Yue"
                    },
                    {
                        "name": "Hugh Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hugh Zhang"
                },
                "author": "Hugh Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05498v2",
                "updated": "2024-09-05T17:33:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    33,
                    33,
                    3,
                    249,
                    0
                ],
                "published": "2024-06-08T15:45:31Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    15,
                    45,
                    31,
                    5,
                    160,
                    0
                ],
                "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner"
                },
                "summary": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance to concurrently protect the target LLM instance in the normal\nstack and collaborate with it for checkpoint-based access control. The\neffectiveness of SelfDefend builds upon our observation that existing LLMs\n(both target and defense LLMs) have the capability to identify harmful prompts\nor intentions in user queries, which we empirically validate using the commonly\nused GPT-3.5/4 models across all major jailbreak attacks. To further improve\nthe defense's robustness and minimize costs, we employ a data distillation\napproach to tune dedicated open-source defense models. These models outperform\nsix state-of-the-art defenses and match the performance of GPT-4-based\nSelfDefend, with significantly lower extra delays. We also empirically show\nthat the tuned models are robust to adaptive jailbreaks and prompt injections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance to concurrently protect the target LLM instance in the normal\nstack and collaborate with it for checkpoint-based access control. The\neffectiveness of SelfDefend builds upon our observation that existing LLMs\n(both target and defense LLMs) have the capability to identify harmful prompts\nor intentions in user queries, which we empirically validate using the commonly\nused GPT-3.5/4 models across all major jailbreak attacks. To further improve\nthe defense's robustness and minimize costs, we employ a data distillation\napproach to tune dedicated open-source defense models. These models outperform\nsix state-of-the-art defenses and match the performance of GPT-4-based\nSelfDefend, with significantly lower extra delays. We also empirically show\nthat the tuned models are robust to adaptive jailbreaks and prompt injections."
                },
                "authors": [
                    {
                        "name": "Xunguang Wang"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Zhenlan Ji"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yingjiu Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Juergen Rahmel"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Rahmel"
                },
                "author": "Juergen Rahmel",
                "arxiv_comment": "This paper completes its earlier vision paper, available at\n  arXiv:2402.15727. Updated to the latest analysis and results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06477v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06477v3",
                "updated": "2024-09-05T17:25:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    25,
                    1,
                    3,
                    249,
                    0
                ],
                "published": "2024-01-12T09:56:57Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    9,
                    56,
                    57,
                    4,
                    12,
                    0
                ],
                "title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation"
                },
                "summary": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun"
                },
                "authors": [
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06477v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06477v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03708v2",
                "updated": "2024-09-06T14:18:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    18,
                    20,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T17:14:23Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    14,
                    23,
                    3,
                    249,
                    0
                ],
                "title": "RAG based Question-Answering for Contextual Response Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG based Question-Answering for Contextual Response Prediction System"
                },
                "summary": "Large Language Models (LLMs) have shown versatility in various Natural\nLanguage Processing (NLP) tasks, including their potential as effective\nquestion-answering systems. However, to provide precise and relevant\ninformation in response to specific customer queries in industry settings, LLMs\nrequire access to a comprehensive knowledge base to avoid hallucinations.\nRetrieval Augmented Generation (RAG) emerges as a promising technique to\naddress this challenge. Yet, developing an accurate question-answering\nframework for real-world applications using RAG entails several challenges: 1)\ndata availability issues, 2) evaluating the quality of generated content, and\n3) the costly nature of human evaluation. In this paper, we introduce an\nend-to-end framework that employs LLMs with RAG capabilities for industry use\ncases. Given a customer query, the proposed system retrieves relevant knowledge\ndocuments and leverages them, along with previous chat history, to generate\nresponse suggestions for customer service agents in the contact centers of a\nmajor retail company. Through comprehensive automated and human evaluations, we\nshow that this solution outperforms the current BERT-based algorithms in\naccuracy and relevance. Our findings suggest that RAG-based LLMs can be an\nexcellent support to human customer service representatives by lightening their\nworkload.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown versatility in various Natural\nLanguage Processing (NLP) tasks, including their potential as effective\nquestion-answering systems. However, to provide precise and relevant\ninformation in response to specific customer queries in industry settings, LLMs\nrequire access to a comprehensive knowledge base to avoid hallucinations.\nRetrieval Augmented Generation (RAG) emerges as a promising technique to\naddress this challenge. Yet, developing an accurate question-answering\nframework for real-world applications using RAG entails several challenges: 1)\ndata availability issues, 2) evaluating the quality of generated content, and\n3) the costly nature of human evaluation. In this paper, we introduce an\nend-to-end framework that employs LLMs with RAG capabilities for industry use\ncases. Given a customer query, the proposed system retrieves relevant knowledge\ndocuments and leverages them, along with previous chat history, to generate\nresponse suggestions for customer service agents in the contact centers of a\nmajor retail company. Through comprehensive automated and human evaluations, we\nshow that this solution outperforms the current BERT-based algorithms in\naccuracy and relevance. Our findings suggest that RAG-based LLMs can be an\nexcellent support to human customer service representatives by lightening their\nworkload."
                },
                "authors": [
                    {
                        "name": "Sriram Veturi"
                    },
                    {
                        "name": "Saurabh Vaichal"
                    },
                    {
                        "name": "Reshma Lal Jagadheesh"
                    },
                    {
                        "name": "Nafis Irtiza Tripto"
                    },
                    {
                        "name": "Nian Yan"
                    }
                ],
                "author_detail": {
                    "name": "Nian Yan"
                },
                "author": "Nian Yan",
                "arxiv_comment": "Accepted at the 1st Workshop on GenAI and RAG Systems for Enterprise,\n  CIKM'24. 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03706v1",
                "updated": "2024-09-05T17:12:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    12,
                    53,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:12:53Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    12,
                    53,
                    3,
                    249,
                    0
                ],
                "title": "The Gaia Ultracool Dwarf Sample -- IV. GTC/OSIRIS optical spectra of\n  Gaia late-M and L dwarfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gaia Ultracool Dwarf Sample -- IV. GTC/OSIRIS optical spectra of\n  Gaia late-M and L dwarfs"
                },
                "summary": "As part of our comprehensive, ongoing characterisation of the low-mass end of\nthe main sequence in the Solar neighbourhood, we used the OSIRIS instrument at\nthe 10.4 m Gran Telescopio Canarias to acquire low- and mid-resolution\n(R$\\approx$300 and R$\\approx$2500) optical spectroscopy of 53 late-M and L\nultracool dwarfs. Most of these objects are known but poorly investigated and\nlacking complete kinematics. We measured spectral indices, determined spectral\ntypes (six of which are new) and inferred effective temperature and surface\ngravity from BT-Settl synthetic spectra fits for all objects. We were able to\nmeasure radial velocities via line centre fitting and cross correlation for 46\nobjects, 29 of which lacked previous radial velocity measurements. Using these\nradial velocities in combination with the latest Gaia DR3 data, we also\ncalculated Galactocentric space velocities. From their kinematics, we\nidentified two candidates outside of the thin disc and four in young stellar\nkinematic groups. Two further ultracool dwarfs are apparently young field\nobjects: 2MASSW J1246467$+$402715 (L4$\\beta$), which has a potential, weak\nlithium absorption line, and G 196$-$3B (L3$\\beta$), which was already known as\nyoung due to its well-studied primary companion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As part of our comprehensive, ongoing characterisation of the low-mass end of\nthe main sequence in the Solar neighbourhood, we used the OSIRIS instrument at\nthe 10.4 m Gran Telescopio Canarias to acquire low- and mid-resolution\n(R$\\approx$300 and R$\\approx$2500) optical spectroscopy of 53 late-M and L\nultracool dwarfs. Most of these objects are known but poorly investigated and\nlacking complete kinematics. We measured spectral indices, determined spectral\ntypes (six of which are new) and inferred effective temperature and surface\ngravity from BT-Settl synthetic spectra fits for all objects. We were able to\nmeasure radial velocities via line centre fitting and cross correlation for 46\nobjects, 29 of which lacked previous radial velocity measurements. Using these\nradial velocities in combination with the latest Gaia DR3 data, we also\ncalculated Galactocentric space velocities. From their kinematics, we\nidentified two candidates outside of the thin disc and four in young stellar\nkinematic groups. Two further ultracool dwarfs are apparently young field\nobjects: 2MASSW J1246467$+$402715 (L4$\\beta$), which has a potential, weak\nlithium absorption line, and G 196$-$3B (L3$\\beta$), which was already known as\nyoung due to its well-studied primary companion."
                },
                "authors": [
                    {
                        "name": "W. J. Cooper"
                    },
                    {
                        "name": "H. R. A. Jones"
                    },
                    {
                        "name": "R. L. Smart"
                    },
                    {
                        "name": "S. L. Folkes"
                    },
                    {
                        "name": "J. A. Caballero"
                    },
                    {
                        "name": "F. Marocco"
                    },
                    {
                        "name": "M. C. Glvez Ortiz"
                    },
                    {
                        "name": "A. J. Burgasser"
                    },
                    {
                        "name": "J. D. Kirkpatrick"
                    },
                    {
                        "name": "L. M. Sarro"
                    },
                    {
                        "name": "B. Burningham"
                    },
                    {
                        "name": "A. Cabrera-Lavers"
                    },
                    {
                        "name": "P. E. Tremblay"
                    },
                    {
                        "name": "C. Reyl"
                    },
                    {
                        "name": "N. Lodieu"
                    },
                    {
                        "name": "Z. H. Zhang"
                    },
                    {
                        "name": "N. J. Cook"
                    },
                    {
                        "name": "J. F. Faherty"
                    },
                    {
                        "name": "D. Garca-lvarez"
                    },
                    {
                        "name": "D. Montes"
                    },
                    {
                        "name": "D. J. Pinfield"
                    },
                    {
                        "name": "A. S. Rajpurohit"
                    },
                    {
                        "name": "J. Shi"
                    }
                ],
                "author_detail": {
                    "name": "J. Shi"
                },
                "author": "J. Shi",
                "arxiv_comment": "33 pages, 14 figures, Accepted by MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16607v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16607v3",
                "updated": "2024-09-05T16:39:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    39,
                    44,
                    3,
                    249,
                    0
                ],
                "published": "2024-07-23T16:13:22Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    16,
                    13,
                    22,
                    1,
                    205,
                    0
                ],
                "title": "Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?"
                },
                "summary": "The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation: byte-pair encoding (BPE) tokenizers, used by the vast majority of\nmodern language models. Our key insight is that the ordered list of merge rules\nlearned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data. Given a tokenizer's merge list along with\nexample data for each category of interest, we formulate a linear program that\nsolves for the proportion of each category in the tokenizer's training set. In\ncontrolled experiments, we show that our attack recovers mixture ratios with\nhigh precision for tokenizers trained on known mixtures of natural languages,\nprogramming languages, and data sources. We then apply our approach to\noff-the-shelf tokenizers released with recent LMs. We confirm much publicly\ndisclosed information about these models, and also make several new inferences:\nGPT-4o and Mistral NeMo's tokenizers are much more multilingual than their\npredecessors, training on 39% and 47% non-English language data, respectively;\nLlama 3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use;\nGPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). We\nhope our work sheds light on current design practices for pretraining data, and\ninspires continued research into data mixture inference for LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation: byte-pair encoding (BPE) tokenizers, used by the vast majority of\nmodern language models. Our key insight is that the ordered list of merge rules\nlearned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data. Given a tokenizer's merge list along with\nexample data for each category of interest, we formulate a linear program that\nsolves for the proportion of each category in the tokenizer's training set. In\ncontrolled experiments, we show that our attack recovers mixture ratios with\nhigh precision for tokenizers trained on known mixtures of natural languages,\nprogramming languages, and data sources. We then apply our approach to\noff-the-shelf tokenizers released with recent LMs. We confirm much publicly\ndisclosed information about these models, and also make several new inferences:\nGPT-4o and Mistral NeMo's tokenizers are much more multilingual than their\npredecessors, training on 39% and 47% non-English language data, respectively;\nLlama 3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use;\nGPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). We\nhope our work sheds light on current design practices for pretraining data, and\ninspires continued research into data mixture inference for LMs."
                },
                "authors": [
                    {
                        "name": "Jonathan Hayase"
                    },
                    {
                        "name": "Alisa Liu"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Sewoong Oh"
                    },
                    {
                        "name": "Noah A. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Noah A. Smith"
                },
                "author": "Noah A. Smith",
                "arxiv_comment": "new robustness experiments; new baselines; include Mistral,\n  Mistral-Nemo and GPT-NeoX; link to code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16607v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16607v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17399v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17399v2",
                "updated": "2024-09-05T16:34:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    34,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-04-26T13:21:30Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    13,
                    21,
                    30,
                    4,
                    117,
                    0
                ],
                "title": "Evaluations of Machine Learning Privacy Defenses are Misleading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluations of Machine Learning Privacy Defenses are Misleading"
                },
                "summary": "Empirical defenses for machine learning privacy forgo the provable guarantees\nof differential privacy in the hope of achieving higher utility while resisting\nrealistic adversaries. We identify severe pitfalls in existing empirical\nprivacy evaluations (based on membership inference attacks) that result in\nmisleading conclusions. In particular, we show that prior evaluations fail to\ncharacterize the privacy leakage of the most vulnerable samples, use weak\nattacks, and avoid comparisons with practical differential privacy baselines.\nIn 5 case studies of empirical privacy defenses, we find that prior evaluations\nunderestimate privacy leakage by an order of magnitude. Under our stronger\nevaluation, none of the empirical defenses we study are competitive with a\nproperly tuned, high-utility DP-SGD baseline (with vacuous provable\nguarantees).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical defenses for machine learning privacy forgo the provable guarantees\nof differential privacy in the hope of achieving higher utility while resisting\nrealistic adversaries. We identify severe pitfalls in existing empirical\nprivacy evaluations (based on membership inference attacks) that result in\nmisleading conclusions. In particular, we show that prior evaluations fail to\ncharacterize the privacy leakage of the most vulnerable samples, use weak\nattacks, and avoid comparisons with practical differential privacy baselines.\nIn 5 case studies of empirical privacy defenses, we find that prior evaluations\nunderestimate privacy leakage by an order of magnitude. Under our stronger\nevaluation, none of the empirical defenses we study are competitive with a\nproperly tuned, high-utility DP-SGD baseline (with vacuous provable\nguarantees)."
                },
                "authors": [
                    {
                        "name": "Michael Aerni"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Florian Tramr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramr"
                },
                "author": "Florian Tramr",
                "arxiv_doi": "10.1145/3658644.3690194",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690194",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.17399v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17399v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ACM CCS 2024",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03671v1",
                "updated": "2024-09-05T16:24:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    24,
                    42,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T16:24:42Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    24,
                    42,
                    3,
                    249,
                    0
                ],
                "title": "TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course\n  Scheduling Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course\n  Scheduling Problems"
                },
                "summary": "We present TRACE-cs, a novel hybrid system that combines symbolic reasoning\nwith large language models (LLMs) to address contrastive queries in scheduling\nproblems. TRACE-cs leverages SAT solving techniques to encode scheduling\nconstraints and generate explanations for user queries, while utilizing an LLM\nto process the user queries into logical clauses as well as refine the\nexplanations generated by the symbolic solver to natural language sentences. By\nintegrating these components, our approach demonstrates the potential of\ncombining symbolic methods with LLMs to create explainable AI agents with\ncorrectness guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TRACE-cs, a novel hybrid system that combines symbolic reasoning\nwith large language models (LLMs) to address contrastive queries in scheduling\nproblems. TRACE-cs leverages SAT solving techniques to encode scheduling\nconstraints and generate explanations for user queries, while utilizing an LLM\nto process the user queries into logical clauses as well as refine the\nexplanations generated by the symbolic solver to natural language sentences. By\nintegrating these components, our approach demonstrates the potential of\ncombining symbolic methods with LLMs to create explainable AI agents with\ncorrectness guarantees."
                },
                "authors": [
                    {
                        "name": "Stylianos Loukas Vasileiou"
                    },
                    {
                        "name": "William Yeoh"
                    }
                ],
                "author_detail": {
                    "name": "William Yeoh"
                },
                "author": "William Yeoh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06120v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06120v3",
                "updated": "2024-09-05T16:19:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    19,
                    32,
                    3,
                    249,
                    0
                ],
                "published": "2024-02-09T01:10:25Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    1,
                    10,
                    25,
                    4,
                    40,
                    0
                ],
                "title": "Exploring Group and Symmetry Principles in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Group and Symmetry Principles in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released."
                },
                "authors": [
                    {
                        "name": "Shima Imani"
                    },
                    {
                        "name": "Hamid Palangi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Palangi"
                },
                "author": "Hamid Palangi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06120v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06120v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16639v3",
                "updated": "2024-09-05T16:17:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    17,
                    20,
                    3,
                    249,
                    0
                ],
                "published": "2023-11-28T09:45:02Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    9,
                    45,
                    2,
                    1,
                    332,
                    0
                ],
                "title": "Positioning Political Texts with Large Language Models by Asking and\n  Averaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positioning Political Texts with Large Language Models by Asking and\n  Averaging"
                },
                "summary": "We use instruction-tuned Large Language Models (LLMs) like GPT-4, Llama 3,\nMiXtral, or Aya to position political texts within policy and ideological\nspaces. We ask an LLM where a tweet or a sentence of a political text stands on\nthe focal dimension and take the average of the LLM responses to position\npolitical actors such as US Senators, or longer texts such as UK party\nmanifestos or EU policy speeches given in 10 different languages. The\ncorrelations between the position estimates obtained with the best LLMs and\nbenchmarks based on text coding by experts, crowdworkers, or roll call votes\nexceed .90. This approach is generally more accurate than the positions\nobtained with supervised classifiers trained on large amounts of research data.\nUsing instruction-tuned LLMs to position texts in policy and ideological spaces\nis fast, cost-efficient, reliable, and reproducible (in the case of open LLMs)\neven if the texts are short and written in different languages. We conclude\nwith cautionary notes about the need for empirical validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use instruction-tuned Large Language Models (LLMs) like GPT-4, Llama 3,\nMiXtral, or Aya to position political texts within policy and ideological\nspaces. We ask an LLM where a tweet or a sentence of a political text stands on\nthe focal dimension and take the average of the LLM responses to position\npolitical actors such as US Senators, or longer texts such as UK party\nmanifestos or EU policy speeches given in 10 different languages. The\ncorrelations between the position estimates obtained with the best LLMs and\nbenchmarks based on text coding by experts, crowdworkers, or roll call votes\nexceed .90. This approach is generally more accurate than the positions\nobtained with supervised classifiers trained on large amounts of research data.\nUsing instruction-tuned LLMs to position texts in policy and ideological spaces\nis fast, cost-efficient, reliable, and reproducible (in the case of open LLMs)\neven if the texts are short and written in different languages. We conclude\nwith cautionary notes about the need for empirical validation."
                },
                "authors": [
                    {
                        "name": "Gal Le Mens"
                    },
                    {
                        "name": "Aina Gallego"
                    }
                ],
                "author_detail": {
                    "name": "Aina Gallego"
                },
                "author": "Aina Gallego",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03662v1",
                "updated": "2024-09-05T16:15:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    15,
                    12,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T16:15:12Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    15,
                    12,
                    3,
                    249,
                    0
                ],
                "title": "The representation landscape of few-shot learning and fine-tuning in\n  large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The representation landscape of few-shot learning and fine-tuning in\n  large language models"
                },
                "summary": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common\nstrategies for improving the performance of modern large language models (LLMs)\non specific tasks. Despite their different natures, these strategies often lead\nto comparable performance gains. However, little is known about whether they\ninduce similar representations inside LLMs. We approach this problem by\nanalyzing the probability landscape of their hidden representations in the two\ncases. More specifically, we compare how LLMs solve the same question-answering\ntask, finding that ICL and SFT create very different internal structures, in\nboth cases undergoing a sharp transition in the middle of the network. In the\nfirst half of the network, ICL shapes interpretable representations\nhierarchically organized according to their semantic content. In contrast, the\nprobability landscape obtained with SFT is fuzzier and semantically mixed. In\nthe second half of the model, the fine-tuned representations develop\nprobability modes that better encode the identity of answers, while the\nlandscape of ICL representations is characterized by less defined peaks. Our\napproach reveals the diverse computational strategies developed inside LLMs to\nsolve the same task across different conditions, allowing us to make a step\ntowards designing optimal methods to extract information from language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common\nstrategies for improving the performance of modern large language models (LLMs)\non specific tasks. Despite their different natures, these strategies often lead\nto comparable performance gains. However, little is known about whether they\ninduce similar representations inside LLMs. We approach this problem by\nanalyzing the probability landscape of their hidden representations in the two\ncases. More specifically, we compare how LLMs solve the same question-answering\ntask, finding that ICL and SFT create very different internal structures, in\nboth cases undergoing a sharp transition in the middle of the network. In the\nfirst half of the network, ICL shapes interpretable representations\nhierarchically organized according to their semantic content. In contrast, the\nprobability landscape obtained with SFT is fuzzier and semantically mixed. In\nthe second half of the model, the fine-tuned representations develop\nprobability modes that better encode the identity of answers, while the\nlandscape of ICL representations is characterized by less defined peaks. Our\napproach reveals the diverse computational strategies developed inside LLMs to\nsolve the same task across different conditions, allowing us to make a step\ntowards designing optimal methods to extract information from language models."
                },
                "authors": [
                    {
                        "name": "Diego Doimo"
                    },
                    {
                        "name": "Alessandro Serra"
                    },
                    {
                        "name": "Alessio Ansuini"
                    },
                    {
                        "name": "Alberto Cazzaniga"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Cazzaniga"
                },
                "author": "Alberto Cazzaniga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04168v2",
                "updated": "2024-09-05T16:14:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    14,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-08T02:28:43Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    2,
                    28,
                    43,
                    3,
                    221,
                    0
                ],
                "title": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City\n  Navigation without Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City\n  Navigation without Instructions"
                },
                "summary": "This paper considers a scenario in city navigation: an AI agent is provided\nwith language descriptions of the goal location with respect to some well-known\nlandmarks; By only observing the scene around, including recognizing landmarks\nand road network connections, the agent has to make decisions to navigate to\nthe goal location without instructions. This problem is very challenging,\nbecause it requires agent to establish self-position and acquire spatial\nrepresentation of complex urban environment, where landmarks are often\ninvisible. In the absence of navigation instructions, such abilities are vital\nfor the agent to make high-quality decisions in long-range city navigation.\nWith the emergent reasoning ability of large language models (LLMs), a tempting\nbaseline is to prompt LLMs to \"react\" on each observation and make decisions\naccordingly. However, this baseline has very poor performance that the agent\noften repeatedly visits same locations and make short-sighted, inconsistent\ndecisions. To address these issues, this paper introduces a novel agentic\nworkflow featured by its abilities to perceive, reflect and plan. Specifically,\nwe find LLaVA-7B can be fine-tuned to perceive the direction and distance of\nlandmarks with sufficient accuracy for city navigation. Moreover, reflection is\nachieved through a memory mechanism, where past experiences are stored and can\nbe retrieved with current perception for effective decision argumentation.\nPlanning uses reflection results to produce long-term plans, which can avoid\nshort-sighted decisions in long-range navigation. We show the designed workflow\nsignificantly improves navigation ability of the LLM agent compared with the\nstate-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers a scenario in city navigation: an AI agent is provided\nwith language descriptions of the goal location with respect to some well-known\nlandmarks; By only observing the scene around, including recognizing landmarks\nand road network connections, the agent has to make decisions to navigate to\nthe goal location without instructions. This problem is very challenging,\nbecause it requires agent to establish self-position and acquire spatial\nrepresentation of complex urban environment, where landmarks are often\ninvisible. In the absence of navigation instructions, such abilities are vital\nfor the agent to make high-quality decisions in long-range city navigation.\nWith the emergent reasoning ability of large language models (LLMs), a tempting\nbaseline is to prompt LLMs to \"react\" on each observation and make decisions\naccordingly. However, this baseline has very poor performance that the agent\noften repeatedly visits same locations and make short-sighted, inconsistent\ndecisions. To address these issues, this paper introduces a novel agentic\nworkflow featured by its abilities to perceive, reflect and plan. Specifically,\nwe find LLaVA-7B can be fine-tuned to perceive the direction and distance of\nlandmarks with sufficient accuracy for city navigation. Moreover, reflection is\nachieved through a memory mechanism, where past experiences are stored and can\nbe retrieved with current perception for effective decision argumentation.\nPlanning uses reflection results to produce long-term plans, which can avoid\nshort-sighted decisions in long-range navigation. We show the designed workflow\nsignificantly improves navigation ability of the LLM agent compared with the\nstate-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Qingbin Zeng"
                    },
                    {
                        "name": "Qinglong Yang"
                    },
                    {
                        "name": "Shunan Dong"
                    },
                    {
                        "name": "Heming Du"
                    },
                    {
                        "name": "Liang Zheng"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "The experiment and dataset are not enough, and we need more\n  experiments to verify our model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03661v1",
                "updated": "2024-09-05T16:14:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    14,
                    7,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T16:14:07Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    14,
                    7,
                    3,
                    249,
                    0
                ],
                "title": "Ensemble noise properties of the European Pulsar Timing Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble noise properties of the European Pulsar Timing Array"
                },
                "summary": "The null hypothesis in Pulsar Timing Array (PTA) analyses includes\nassumptions about ensemble properties of pulsar time-correlated noise. These\nproperties are encoded in prior probabilities for the amplitude and the\nspectral index of the power-law power spectral density of temporal correlations\nof the noise. In this work, we introduce a new procedure for numerical\nmarginalisation over the uncertainties in pulsar noise priors. The procedure\nmay be used in searches for nanohertz gravitational waves and other PTA\nanalyses to resolve prior misspecification at negligible computational cost.\nFurthermore, we infer the distribution of amplitudes and spectral indices of\nthe power spectral density of spin noise and dispersion measure variation noise\nbased on the observation of 25 millisecond pulsars by the European Pulsar\nTiming Array (EPTA). Our results may be used for the simulation of realistic\nnoise in PTAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The null hypothesis in Pulsar Timing Array (PTA) analyses includes\nassumptions about ensemble properties of pulsar time-correlated noise. These\nproperties are encoded in prior probabilities for the amplitude and the\nspectral index of the power-law power spectral density of temporal correlations\nof the noise. In this work, we introduce a new procedure for numerical\nmarginalisation over the uncertainties in pulsar noise priors. The procedure\nmay be used in searches for nanohertz gravitational waves and other PTA\nanalyses to resolve prior misspecification at negligible computational cost.\nFurthermore, we infer the distribution of amplitudes and spectral indices of\nthe power spectral density of spin noise and dispersion measure variation noise\nbased on the observation of 25 millisecond pulsars by the European Pulsar\nTiming Array (EPTA). Our results may be used for the simulation of realistic\nnoise in PTAs."
                },
                "authors": [
                    {
                        "name": "Boris Goncharov"
                    },
                    {
                        "name": "Shubhit Sardana"
                    }
                ],
                "author_detail": {
                    "name": "Shubhit Sardana"
                },
                "author": "Shubhit Sardana",
                "arxiv_comment": "8 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03659v2",
                "updated": "2024-09-06T06:50:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    50,
                    32,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T16:12:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    12,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "LLM-based multi-agent poetry generation in non-cooperative environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent poetry generation in non-cooperative environments"
                },
                "summary": "Despite substantial progress of large language models (LLMs) for automatic\npoetry generation, the generated poetry lacks diversity while the training\nprocess differs greatly from human learning. Under the rationale that the\nlearning process of the poetry generation systems should be more human-like and\ntheir output more diverse and novel, we introduce a framework based on social\nlearning where we emphasize non-cooperative interactions besides cooperative\ninteractions to encourage diversity. Our experiments are the first attempt at\nLLM-based multi-agent systems in non-cooperative environments for poetry\ngeneration employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED\nagents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows\nthat our framework benefits the poetry generation process for TRAINING-BASED\nagents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity\nand a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.\nThe generated poetry from TRAINING-BASED agents also exhibits group divergence\nin terms of lexicons, styles and semantics. PROMPTING-BASED agents in our\nframework also benefit from non-cooperative environments and a more diverse\nensemble of models with non-homogeneous agents has the potential to further\nenhance diversity, with an increase of 7.0-17.5 pp according to our\nexperiments. However, PROMPTING-BASED agents show a decrease in lexical\ndiversity over time and do not exhibit the group-based divergence intended in\nthe social network. Our paper argues for a paradigm shift in creative tasks\nsuch as automatic poetry generation to include social learning processes (via\nLLM-based agent modeling) similar to human interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial progress of large language models (LLMs) for automatic\npoetry generation, the generated poetry lacks diversity while the training\nprocess differs greatly from human learning. Under the rationale that the\nlearning process of the poetry generation systems should be more human-like and\ntheir output more diverse and novel, we introduce a framework based on social\nlearning where we emphasize non-cooperative interactions besides cooperative\ninteractions to encourage diversity. Our experiments are the first attempt at\nLLM-based multi-agent systems in non-cooperative environments for poetry\ngeneration employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED\nagents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows\nthat our framework benefits the poetry generation process for TRAINING-BASED\nagents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity\nand a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.\nThe generated poetry from TRAINING-BASED agents also exhibits group divergence\nin terms of lexicons, styles and semantics. PROMPTING-BASED agents in our\nframework also benefit from non-cooperative environments and a more diverse\nensemble of models with non-homogeneous agents has the potential to further\nenhance diversity, with an increase of 7.0-17.5 pp according to our\nexperiments. However, PROMPTING-BASED agents show a decrease in lexical\ndiversity over time and do not exhibit the group-based divergence intended in\nthe social network. Our paper argues for a paradigm shift in creative tasks\nsuch as automatic poetry generation to include social learning processes (via\nLLM-based agent modeling) similar to human interaction."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03650v1",
                "updated": "2024-09-05T16:08:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    8,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T16:08:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    8,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches."
                },
                "authors": [
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Skyler Seto"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "name": "Katherine Metcalf"
                    },
                    {
                        "name": "Barry-John Theobald"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "12 pages, 8 tables, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12547v2",
                "updated": "2024-09-05T16:07:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    7,
                    37,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-22T17:01:34Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    1,
                    34,
                    3,
                    235,
                    0
                ],
                "title": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine"
                },
                "summary": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins."
                },
                "authors": [
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Hongfei Gu"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03645v1",
                "updated": "2024-09-05T16:04:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    4,
                    41,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T16:04:41Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    4,
                    41,
                    3,
                    249,
                    0
                ],
                "title": "Mixed Source Region Signatures Inside Magnetic Switchback Patches\n  Inferred by Heavy Ion Diagnostics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Source Region Signatures Inside Magnetic Switchback Patches\n  Inferred by Heavy Ion Diagnostics"
                },
                "summary": "Since Parker Solar Probe's (Parker's) first perihelion pass at the Sun, large\namplitude Alfv\\'en waves grouped in patches have been observed near the Sun\nthroughout the mission. Several formation processes for these magnetic\nswitchback patches have been suggested with no definitive consensus. To provide\ninsight to their formation, we examine the heavy ion properties of several\nadjacent magnetic switchback patches around Parker's 11th perihelion pass\ncapitalizing on a spacecraft lineup with Solar Orbiter where each samples the\nsame solar wind streams over a large range of longitudes. Heavy ion properties\n(Fe/O, C$^{6+}$/C$^{5+}$, O$^{7+}$/O$^{6+}$) related to the wind's coronal\norigin, measured with Solar Orbiter can be linked to switchback patch\nstructures identified near the Sun with Parker. We find that switchback patches\ndo not contain distinctive ion and elemental compositional signatures different\nthan the surrounding non-switchback solar wind. Both the patches and ambient\nwind exhibit a range of fast and slow wind qualities, indicating coronal\nsources with open and closed field lines in close proximity. These observations\nand modeling indicate switchback patches form in coronal hole boundary wind and\nwith a range of source region magnetic and thermal properties. Furthermore, the\nheavy ion signatures suggest interchange reconnection and/or shear driven\nprocesses may play a role in their creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since Parker Solar Probe's (Parker's) first perihelion pass at the Sun, large\namplitude Alfv\\'en waves grouped in patches have been observed near the Sun\nthroughout the mission. Several formation processes for these magnetic\nswitchback patches have been suggested with no definitive consensus. To provide\ninsight to their formation, we examine the heavy ion properties of several\nadjacent magnetic switchback patches around Parker's 11th perihelion pass\ncapitalizing on a spacecraft lineup with Solar Orbiter where each samples the\nsame solar wind streams over a large range of longitudes. Heavy ion properties\n(Fe/O, C$^{6+}$/C$^{5+}$, O$^{7+}$/O$^{6+}$) related to the wind's coronal\norigin, measured with Solar Orbiter can be linked to switchback patch\nstructures identified near the Sun with Parker. We find that switchback patches\ndo not contain distinctive ion and elemental compositional signatures different\nthan the surrounding non-switchback solar wind. Both the patches and ambient\nwind exhibit a range of fast and slow wind qualities, indicating coronal\nsources with open and closed field lines in close proximity. These observations\nand modeling indicate switchback patches form in coronal hole boundary wind and\nwith a range of source region magnetic and thermal properties. Furthermore, the\nheavy ion signatures suggest interchange reconnection and/or shear driven\nprocesses may play a role in their creation."
                },
                "authors": [
                    {
                        "name": "Yeimy J. Rivera"
                    },
                    {
                        "name": "Samuel T. Badman"
                    },
                    {
                        "name": "Michael L. Stevens"
                    },
                    {
                        "name": "Jim M. Raines"
                    },
                    {
                        "name": "Christopher J. Owen"
                    },
                    {
                        "name": "Kristoff Paulson"
                    },
                    {
                        "name": "Tatiana Niembro"
                    },
                    {
                        "name": "Stefano A. Livi"
                    },
                    {
                        "name": "Susan T. Lepri"
                    },
                    {
                        "name": "Enrico Landi"
                    },
                    {
                        "name": "Jasper S. Halekas"
                    },
                    {
                        "name": "Tamar Ervin"
                    },
                    {
                        "name": "Ryan M. Dewey"
                    },
                    {
                        "name": "Jesse T. Coburn"
                    },
                    {
                        "name": "Stuart D. Bale"
                    },
                    {
                        "name": "B. L. Alterman"
                    }
                ],
                "author_detail": {
                    "name": "B. L. Alterman"
                },
                "author": "B. L. Alterman",
                "arxiv_comment": "Accepted for publication in ApJ on September 4th, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15488v2",
                "updated": "2024-09-05T15:50:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    50,
                    44,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-28T02:27:07Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    27,
                    7,
                    2,
                    241,
                    0
                ],
                "title": "Legilimens: Practical and Unified Content Moderation for Large Language\n  Model Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legilimens: Practical and Unified Content Moderation for Large Language\n  Model Services"
                },
                "summary": "Given the societal impact of unsafe content generated by large language\nmodels (LLMs), ensuring that LLM services comply with safety standards is a\ncrucial concern for LLM service providers. Common content moderation methods\nare limited by an effectiveness-and-efficiency dilemma, where simple models are\nfragile while sophisticated models consume excessive computational resources.\nIn this paper, we reveal for the first time that effective and efficient\ncontent moderation can be achieved by extracting conceptual features from\nchat-oriented LLMs, despite their initial fine-tuning for conversation rather\nthan content moderation. We propose a practical and unified content moderation\nframework for LLM services, named Legilimens, which features both effectiveness\nand efficiency. Our red-team model-based data augmentation enhances the\nrobustness of Legilimens against state-of-the-art jailbreaking. Additionally,\nwe develop a framework to theoretically analyze the cost-effectiveness of\nLegilimens compared to other methods. We have conducted extensive experiments\non five host LLMs, seventeen datasets, and nine jailbreaking methods to verify\nthe effectiveness, efficiency, and robustness of Legilimens against normal and\nadaptive adversaries. A comparison of Legilimens with both commercial and\nacademic baselines demonstrates the superior performance of Legilimens.\nFurthermore, we confirm that Legilimens can be applied to few-shot scenarios\nand extended to multi-label classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the societal impact of unsafe content generated by large language\nmodels (LLMs), ensuring that LLM services comply with safety standards is a\ncrucial concern for LLM service providers. Common content moderation methods\nare limited by an effectiveness-and-efficiency dilemma, where simple models are\nfragile while sophisticated models consume excessive computational resources.\nIn this paper, we reveal for the first time that effective and efficient\ncontent moderation can be achieved by extracting conceptual features from\nchat-oriented LLMs, despite their initial fine-tuning for conversation rather\nthan content moderation. We propose a practical and unified content moderation\nframework for LLM services, named Legilimens, which features both effectiveness\nand efficiency. Our red-team model-based data augmentation enhances the\nrobustness of Legilimens against state-of-the-art jailbreaking. Additionally,\nwe develop a framework to theoretically analyze the cost-effectiveness of\nLegilimens compared to other methods. We have conducted extensive experiments\non five host LLMs, seventeen datasets, and nine jailbreaking methods to verify\nthe effectiveness, efficiency, and robustness of Legilimens against normal and\nadaptive adversaries. A comparison of Legilimens with both commercial and\nacademic baselines demonstrates the superior performance of Legilimens.\nFurthermore, we confirm that Legilimens can be applied to few-shot scenarios\nand extended to multi-label classification tasks."
                },
                "authors": [
                    {
                        "name": "Jialin Wu"
                    },
                    {
                        "name": "Jiangyi Deng"
                    },
                    {
                        "name": "Shengyuan Pang"
                    },
                    {
                        "name": "Yanjiao Chen"
                    },
                    {
                        "name": "Jiayang Xu"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Wenyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyuan Xu"
                },
                "author": "Wenyuan Xu",
                "arxiv_comment": "Accepted by ACM Conference on Computer and Communications Security\n  (CCS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10468v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10468v4",
                "updated": "2024-09-05T15:47:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    47,
                    45,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-20T00:40:49Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    0,
                    40,
                    49,
                    1,
                    233,
                    0
                ],
                "title": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions"
                },
                "summary": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths."
                },
                "authors": [
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zao Yang"
                },
                "author": "Zao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10468v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10468v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03621v1",
                "updated": "2024-09-05T15:33:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T15:33:24Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "title": "Attend First, Consolidate Later: On the Importance of Attention in\n  Different LLM Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attend First, Consolidate Later: On the Importance of Attention in\n  Different LLM Layers"
                },
                "summary": "In decoder-based LLMs, the representation of a given layer serves two\npurposes: as input to the next layer during the computation of the current\ntoken; and as input to the attention mechanism of future tokens. In this work,\nwe show that the importance of the latter role might be overestimated. To show\nthat, we start by manipulating the representations of previous tokens; e.g. by\nreplacing the hidden states at some layer k with random vectors. Our\nexperimenting with four LLMs and four tasks show that this operation often\nleads to small to negligible drop in performance. Importantly, this happens if\nthe manipulation occurs in the top part of the model-k is in the final 30-50%\nof the layers. In contrast, doing the same manipulation in earlier layers might\nlead to chance level performance. We continue by switching the hidden state of\ncertain tokens with hidden states of other tokens from another prompt; e.g.,\nreplacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We\nfind that when applying this switch in the top 1/3 of the model, the model\nignores it (answering \"Rome\"). However if we apply it before, the model\nconforms to the switch (\"Paris\"). Our results hint at a two stage process in\ntransformer-based LLMs: the first part gathers input from previous tokens,\nwhile the second mainly processes that information internally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In decoder-based LLMs, the representation of a given layer serves two\npurposes: as input to the next layer during the computation of the current\ntoken; and as input to the attention mechanism of future tokens. In this work,\nwe show that the importance of the latter role might be overestimated. To show\nthat, we start by manipulating the representations of previous tokens; e.g. by\nreplacing the hidden states at some layer k with random vectors. Our\nexperimenting with four LLMs and four tasks show that this operation often\nleads to small to negligible drop in performance. Importantly, this happens if\nthe manipulation occurs in the top part of the model-k is in the final 30-50%\nof the layers. In contrast, doing the same manipulation in earlier layers might\nlead to chance level performance. We continue by switching the hidden state of\ncertain tokens with hidden states of other tokens from another prompt; e.g.,\nreplacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We\nfind that when applying this switch in the top 1/3 of the model, the model\nignores it (answering \"Rome\"). However if we apply it before, the model\nconforms to the switch (\"Paris\"). Our results hint at a two stage process in\ntransformer-based LLMs: the first part gathers input from previous tokens,\nwhile the second mainly processes that information internally."
                },
                "authors": [
                    {
                        "name": "Amit Ben Artzy"
                    },
                    {
                        "name": "Roy Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Roy Schwartz"
                },
                "author": "Roy Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05945v2",
                "updated": "2024-09-05T15:29:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    29,
                    30,
                    3,
                    249,
                    0
                ],
                "published": "2024-06-10T00:34:23Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    0,
                    34,
                    23,
                    0,
                    162,
                    0
                ],
                "title": "Machine Unlearning for Uplink Interference Cancellation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Unlearning for Uplink Interference Cancellation"
                },
                "summary": "Machine unlearning (MUL) is introduced as a means to achieve interference\ncancellation within artificial intelligence (AI)-enabled wireless systems. It\nis observed that interference cancellation with MUL demonstrates $30\\%$\nimprovement in a classification task accuracy in the presence of a corrupted AI\nmodel. Accordingly, the necessity for instantaneous channel state information\nfor existing interference source is eliminated and a corrupted latent space\nwith interference noise is cleansed with MUL algorithm, achieving this without\nthe necessity for either retraining or dataset cleansing. A Membership\nInference Attack (MIA) served as a benchmark for assessing the efficacy of MUL\nin mitigating interference within a neural network model. The advantage of the\nMUL algorithm was determined by evaluating both the probability of interference\nand the quantity of samples requiring retraining. In a simple signal-to-noise\nratio classification task, the comprehensive improvement across various test\ncases in terms of accuracy demonstrates that MUL exhibits extensive\ncapabilities and limitations, particularly in native AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning (MUL) is introduced as a means to achieve interference\ncancellation within artificial intelligence (AI)-enabled wireless systems. It\nis observed that interference cancellation with MUL demonstrates $30\\%$\nimprovement in a classification task accuracy in the presence of a corrupted AI\nmodel. Accordingly, the necessity for instantaneous channel state information\nfor existing interference source is eliminated and a corrupted latent space\nwith interference noise is cleansed with MUL algorithm, achieving this without\nthe necessity for either retraining or dataset cleansing. A Membership\nInference Attack (MIA) served as a benchmark for assessing the efficacy of MUL\nin mitigating interference within a neural network model. The advantage of the\nMUL algorithm was determined by evaluating both the probability of interference\nand the quantity of samples requiring retraining. In a simple signal-to-noise\nratio classification task, the comprehensive improvement across various test\ncases in terms of accuracy demonstrates that MUL exhibits extensive\ncapabilities and limitations, particularly in native AI applications."
                },
                "authors": [
                    {
                        "name": "Eray Guven"
                    },
                    {
                        "name": "Gunes Karabulut Kurt"
                    }
                ],
                "author_detail": {
                    "name": "Gunes Karabulut Kurt"
                },
                "author": "Gunes Karabulut Kurt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16185v2",
                "updated": "2024-09-05T15:03:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    3,
                    51,
                    3,
                    249,
                    0
                ],
                "published": "2024-01-29T14:32:27Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    14,
                    32,
                    27,
                    0,
                    29,
                    0
                ],
                "title": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including vulnerability detection. However, current efforts in\nthis area are preliminary, lacking clarity on whether LLMs' vulnerability\nreasoning capabilities stem from the models themselves or external aids such as\nknowledge retrieval and tooling support.\n  This paper aims to isolate LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and structured output generation. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conducted controlled experiments with 97 ground-truth vulnerabilities and\n97 non-vulnerable cases in Solidity and Java, testing them in a total of 9,312\nscenarios across four LLMs (GPT-4, GPT-3.5, Mixtral, and Llama 3). Our findings\nreveal the varying impacts of knowledge enhancement, context supplementation,\nprompt schemes, and models. Additionally, we identified 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in \\$3,576 in\nbounties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including vulnerability detection. However, current efforts in\nthis area are preliminary, lacking clarity on whether LLMs' vulnerability\nreasoning capabilities stem from the models themselves or external aids such as\nknowledge retrieval and tooling support.\n  This paper aims to isolate LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and structured output generation. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conducted controlled experiments with 97 ground-truth vulnerabilities and\n97 non-vulnerable cases in Solidity and Java, testing them in a total of 9,312\nscenarios across four LLMs (GPT-4, GPT-3.5, Mixtral, and Llama 3). Our findings\nreveal the varying impacts of knowledge enhancement, context supplementation,\nprompt schemes, and models. Additionally, we identified 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in \\$3,576 in\nbounties."
                },
                "authors": [
                    {
                        "name": "Yuqiang Sun"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Yue Xue"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Lyuye Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yingjiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingjiu Li"
                },
                "author": "Yingjiu Li",
                "arxiv_comment": "This is a technical report by Nanyang Technological University.\n  Updated to support both Solidity and Java",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07250v2",
                "updated": "2024-09-05T14:44:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    44,
                    46,
                    3,
                    249,
                    0
                ],
                "published": "2024-04-10T18:00:00Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    18,
                    0,
                    0,
                    2,
                    101,
                    0
                ],
                "title": "Reionization after JWST: a photon budget crisis?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reionization after JWST: a photon budget crisis?"
                },
                "summary": "New James Webb Space Telescope (JWST) observations are revealing the first\ngalaxies to be prolific producers of ionizing photons, which we argue gives\nrise to a tension between different probes of reionization. Over the last two\ndecades a consensus has emerged where star-forming galaxies are able to\ngenerate enough photons to drive reionization, given reasonable values for\ntheir number densities, ionizing efficiencies $\\xi_{\\rm ion}$ (per unit UV\nluminosity), and escape fractions $f_{\\rm esc}$. However, some new JWST\nobservations infer high values of $\\xi_{\\rm ion}$ during reionization and an\nenhanced abundance of earlier ($z\\gtrsim 9$) galaxies, dramatically increasing\nthe number of ionizing photons produced at high $z$. Simultaneously, recent\nlow-$z$ studies predict significant escape fractions for faint reionization-era\ngalaxies. Put together, we show that the galaxies we have directly observed\n($M_{\\rm UV} < -15$) not only can drive reionization, but would end it too\nearly. That is, our current galaxy observations, taken at face value, imply an\nexcess of ionizing photons and thus a process of reionization in tension with\nthe cosmic microwave background (CMB) and Lyman-$\\alpha$ forest. Considering\ngalaxies down to $M_{\\rm UV}\\approx -11$, below current observational limits,\nonly worsens this tension. We discuss possible avenues to resolve this photon\nbudget crisis, including systematics in either theory or observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New James Webb Space Telescope (JWST) observations are revealing the first\ngalaxies to be prolific producers of ionizing photons, which we argue gives\nrise to a tension between different probes of reionization. Over the last two\ndecades a consensus has emerged where star-forming galaxies are able to\ngenerate enough photons to drive reionization, given reasonable values for\ntheir number densities, ionizing efficiencies $\\xi_{\\rm ion}$ (per unit UV\nluminosity), and escape fractions $f_{\\rm esc}$. However, some new JWST\nobservations infer high values of $\\xi_{\\rm ion}$ during reionization and an\nenhanced abundance of earlier ($z\\gtrsim 9$) galaxies, dramatically increasing\nthe number of ionizing photons produced at high $z$. Simultaneously, recent\nlow-$z$ studies predict significant escape fractions for faint reionization-era\ngalaxies. Put together, we show that the galaxies we have directly observed\n($M_{\\rm UV} < -15$) not only can drive reionization, but would end it too\nearly. That is, our current galaxy observations, taken at face value, imply an\nexcess of ionizing photons and thus a process of reionization in tension with\nthe cosmic microwave background (CMB) and Lyman-$\\alpha$ forest. Considering\ngalaxies down to $M_{\\rm UV}\\approx -11$, below current observational limits,\nonly worsens this tension. We discuss possible avenues to resolve this photon\nbudget crisis, including systematics in either theory or observations."
                },
                "authors": [
                    {
                        "name": "Julian B. Muoz"
                    },
                    {
                        "name": "Jordan Mirocha"
                    },
                    {
                        "name": "John Chisholm"
                    },
                    {
                        "name": "Steven R. Furlanetto"
                    },
                    {
                        "name": "Charlotte Mason"
                    }
                ],
                "author_detail": {
                    "name": "Charlotte Mason"
                },
                "author": "Charlotte Mason",
                "arxiv_comment": "6+3 pages, 3+2 figures, 1 table, updated to match accepted version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08207v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08207v5",
                "updated": "2024-09-05T14:43:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    43,
                    55,
                    3,
                    249,
                    0
                ],
                "published": "2023-12-13T15:25:39Z",
                "published_parsed": [
                    2023,
                    12,
                    13,
                    15,
                    25,
                    39,
                    2,
                    347,
                    0
                ],
                "title": "Black-box Membership Inference Attacks against Fine-tuned Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Membership Inference Attacks against Fine-tuned Diffusion\n  Models"
                },
                "summary": "With the rapid advancement of diffusion-based image-generative models, the\nquality of generated images has become increasingly photorealistic. Moreover,\nwith the release of high-quality pre-trained image-generative models, a growing\nnumber of users are downloading these pre-trained models to fine-tune them with\ndownstream datasets for various image-generation tasks. However, employing such\npowerful pre-trained models in downstream tasks presents significant privacy\nleakage risks. In this paper, we propose the first reconstruction-based\nmembership inference attack framework, tailored for recent diffusion models,\nand in the more stringent black-box access setting. Considering four distinct\nattack scenarios and three types of attacks, this framework is capable of\ntargeting any popular conditional generator model, achieving high precision,\nevidenced by an impressive AUC of $0.95$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of diffusion-based image-generative models, the\nquality of generated images has become increasingly photorealistic. Moreover,\nwith the release of high-quality pre-trained image-generative models, a growing\nnumber of users are downloading these pre-trained models to fine-tune them with\ndownstream datasets for various image-generation tasks. However, employing such\npowerful pre-trained models in downstream tasks presents significant privacy\nleakage risks. In this paper, we propose the first reconstruction-based\nmembership inference attack framework, tailored for recent diffusion models,\nand in the more stringent black-box access setting. Considering four distinct\nattack scenarios and three types of attacks, this framework is capable of\ntargeting any popular conditional generator model, achieving high precision,\nevidenced by an impressive AUC of $0.95$."
                },
                "authors": [
                    {
                        "name": "Yan Pang"
                    },
                    {
                        "name": "Tianhao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tianhao Wang"
                },
                "author": "Tianhao Wang",
                "arxiv_doi": "10.14722/ndss.2025.23324",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14722/ndss.2025.23324",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.08207v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08207v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03588v1",
                "updated": "2024-09-05T14:43:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    43,
                    11,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:43:11Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    43,
                    11,
                    3,
                    249,
                    0
                ],
                "title": "Costs Estimation in Unit Commitment Problems using Simulation-Based\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Costs Estimation in Unit Commitment Problems using Simulation-Based\n  Inference"
                },
                "summary": "The Unit Commitment (UC) problem is a key optimization task in power systems\nto forecast the generation schedules of power units over a finite time period\nby minimizing costs while meeting demand and technical constraints. However,\nmany parameters required by the UC problem are unknown, such as the costs. In\nthis work, we estimate these unknown costs using simulation-based inference on\nan illustrative UC problem, which provides an approximated posterior\ndistribution of the parameters given observed generation schedules and demands.\nOur results highlight that the learned posterior distribution effectively\ncaptures the underlying distribution of the data, providing a range of possible\nvalues for the unknown parameters given a past observation. This posterior\nallows for the estimation of past costs using observed past generation\nschedules, enabling operators to better forecast future costs and make more\nrobust generation scheduling forecasts. We present avenues for future research\nto address overconfidence in posterior estimation, enhance the scalability of\nthe methodology and apply it to more complex UC problems modeling the network\nconstraints and renewable energy sources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unit Commitment (UC) problem is a key optimization task in power systems\nto forecast the generation schedules of power units over a finite time period\nby minimizing costs while meeting demand and technical constraints. However,\nmany parameters required by the UC problem are unknown, such as the costs. In\nthis work, we estimate these unknown costs using simulation-based inference on\nan illustrative UC problem, which provides an approximated posterior\ndistribution of the parameters given observed generation schedules and demands.\nOur results highlight that the learned posterior distribution effectively\ncaptures the underlying distribution of the data, providing a range of possible\nvalues for the unknown parameters given a past observation. This posterior\nallows for the estimation of past costs using observed past generation\nschedules, enabling operators to better forecast future costs and make more\nrobust generation scheduling forecasts. We present avenues for future research\nto address overconfidence in posterior estimation, enhance the scalability of\nthe methodology and apply it to more complex UC problems modeling the network\nconstraints and renewable energy sources."
                },
                "authors": [
                    {
                        "name": "Matthias Pirlet"
                    },
                    {
                        "name": "Adrien Bolland"
                    },
                    {
                        "name": "Gilles Louppe"
                    },
                    {
                        "name": "Damien Ernst"
                    }
                ],
                "author_detail": {
                    "name": "Damien Ernst"
                },
                "author": "Damien Ernst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07666v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07666v4",
                "updated": "2024-09-05T14:37:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    37,
                    59,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-14T16:58:48Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    58,
                    48,
                    2,
                    227,
                    0
                ],
                "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities"
                },
                "summary": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."
                },
                "authors": [
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Xingwei Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07666v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07666v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01767v2",
                "updated": "2024-09-05T14:25:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    25,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-06-03T20:16:35Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    20,
                    16,
                    35,
                    0,
                    155,
                    0
                ],
                "title": "Region-aware Grasp Framework with Normalized Grasp Space for Efficient\n  6-DoF Grasping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-aware Grasp Framework with Normalized Grasp Space for Efficient\n  6-DoF Grasping"
                },
                "summary": "A series of region-based methods succeed in extracting regional features and\nenhancing grasp detection quality. However, faced with a cluttered scene with\npotential collision, the definition of the grasp-relevant region stays\ninconsistent, and the relationship between grasps and regional spaces remains\nincompletely investigated. In this paper, we propose Normalized Grasp Space\n(NGS) from a novel region-aware viewpoint, unifying the grasp representation\nwithin a normalized regional space and benefiting the generalizability of\nmethods. Leveraging the NGS, we find that CNNs are underestimated for 3D\nfeature extraction and 6-DoF grasp detection in clutter scenes and build a\nhighly efficient Region-aware Normalized Grasp Network (RNGNet). Experiments on\nthe public benchmark show that our method achieves significant >20% performance\ngains while attaining a real-time inference speed of approximately 50 FPS.\nReal-world cluttered scene clearance experiments underscore the effectiveness\nof our method. Further, human-to-robot handover and dynamic object grasping\nexperiments demonstrate the potential of our proposed method for closed-loop\ngrasping in dynamic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A series of region-based methods succeed in extracting regional features and\nenhancing grasp detection quality. However, faced with a cluttered scene with\npotential collision, the definition of the grasp-relevant region stays\ninconsistent, and the relationship between grasps and regional spaces remains\nincompletely investigated. In this paper, we propose Normalized Grasp Space\n(NGS) from a novel region-aware viewpoint, unifying the grasp representation\nwithin a normalized regional space and benefiting the generalizability of\nmethods. Leveraging the NGS, we find that CNNs are underestimated for 3D\nfeature extraction and 6-DoF grasp detection in clutter scenes and build a\nhighly efficient Region-aware Normalized Grasp Network (RNGNet). Experiments on\nthe public benchmark show that our method achieves significant >20% performance\ngains while attaining a real-time inference speed of approximately 50 FPS.\nReal-world cluttered scene clearance experiments underscore the effectiveness\nof our method. Further, human-to-robot handover and dynamic object grasping\nexperiments demonstrate the potential of our proposed method for closed-loop\ngrasping in dynamic scenarios."
                },
                "authors": [
                    {
                        "name": "Siang Chen"
                    },
                    {
                        "name": "Pengwei Xie"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Dingchang Hu"
                    },
                    {
                        "name": "Yixiang Dai"
                    },
                    {
                        "name": "Guijin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guijin Wang"
                },
                "author": "Guijin Wang",
                "arxiv_comment": "Accepted by CoRL2024, final camera-ready version will be updated soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03563v1",
                "updated": "2024-09-05T14:19:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    19,
                    45,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:19:45Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    19,
                    45,
                    3,
                    249,
                    0
                ],
                "title": "100 instances is all you need: predicting the success of a new LLM on\n  unseen data by testing on a few instances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "100 instances is all you need: predicting the success of a new LLM on\n  unseen data by testing on a few instances"
                },
                "summary": "Predicting the performance of LLMs on individual task instances is essential\nto ensure their reliability in high-stakes applications. To do so, a\npossibility is to evaluate the considered LLM on a set of task instances and\ntrain an assessor to predict its performance based on features of the\ninstances. However, this approach requires evaluating each new LLM on a\nsufficiently large set of task instances to train an assessor specific to it.\nIn this work, we leverage the evaluation results of previously tested LLMs to\nreduce the number of evaluations required to predict the performance of a new\nLLM. In practice, we propose to test the new LLM on a small set of reference\ninstances and train a generic assessor which predicts the performance of the\nLLM on an instance based on the performance of the former on the reference set\nand features of the instance of interest. We conduct empirical studies on\nHELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets\nthat we introduce, where we evaluate all instruction-fine-tuned OpenAI models\nuntil the January 2024 version of GPT4. When predicting performance on\ninstances with the same distribution as those used to train the generic\nassessor, we find this achieves performance comparable to the LLM-specific\nassessors trained on the full set of instances. Additionally, we find that\nrandomly selecting the reference instances performs as well as some advanced\nselection methods we tested. For out of distribution, however, no clear winner\nemerges and the overall performance is worse, suggesting that the inherent\npredictability of LLMs is low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the performance of LLMs on individual task instances is essential\nto ensure their reliability in high-stakes applications. To do so, a\npossibility is to evaluate the considered LLM on a set of task instances and\ntrain an assessor to predict its performance based on features of the\ninstances. However, this approach requires evaluating each new LLM on a\nsufficiently large set of task instances to train an assessor specific to it.\nIn this work, we leverage the evaluation results of previously tested LLMs to\nreduce the number of evaluations required to predict the performance of a new\nLLM. In practice, we propose to test the new LLM on a small set of reference\ninstances and train a generic assessor which predicts the performance of the\nLLM on an instance based on the performance of the former on the reference set\nand features of the instance of interest. We conduct empirical studies on\nHELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets\nthat we introduce, where we evaluate all instruction-fine-tuned OpenAI models\nuntil the January 2024 version of GPT4. When predicting performance on\ninstances with the same distribution as those used to train the generic\nassessor, we find this achieves performance comparable to the LLM-specific\nassessors trained on the full set of instances. Additionally, we find that\nrandomly selecting the reference instances performs as well as some advanced\nselection methods we tested. For out of distribution, however, no clear winner\nemerges and the overall performance is worse, suggesting that the inherent\npredictability of LLMs is low."
                },
                "authors": [
                    {
                        "name": "Lorenzo Pacchiardi"
                    },
                    {
                        "name": "Lucy G. Cheke"
                    },
                    {
                        "name": "Jos Hernndez-Orallo"
                    }
                ],
                "author_detail": {
                    "name": "Jos Hernndez-Orallo"
                },
                "author": "Jos Hernndez-Orallo",
                "arxiv_comment": "Presented at the 2024 KDD workshop on Evaluation and Trustworthiness\n  of Generative AI Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03558v1",
                "updated": "2024-09-05T14:18:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    18,
                    16,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:18:16Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    18,
                    16,
                    3,
                    249,
                    0
                ],
                "title": "Magnetic Field Alignment Relative to Multiple Tracers in the High-mass\n  Star-forming Region RCW 36",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic Field Alignment Relative to Multiple Tracers in the High-mass\n  Star-forming Region RCW 36"
                },
                "summary": "We use polarization data from SOFIA HAWC+ to investigate the interplay\nbetween magnetic fields and stellar feedback in altering gas dynamics within\nthe high-mass star-forming region RCW 36, located in Vela C. This region is of\nparticular interest as it has a bipolar HII region powered by a massive star\ncluster which may be impacting the surrounding magnetic field. To determine if\nthis is the case, we apply the Histogram of Relative Orientations (HRO) method\nto quantify the relative alignment between the inferred magnetic field and\nelongated structures observed in several datasets such as dust emission, column\ndensity, temperature, and spectral line intensity maps. The HRO results\nindicate a bimodal alignment trend, where structures observed with dense gas\ntracers show a statistically significant preference for perpendicular alignment\nrelative to the magnetic field, while structures probed by photo-dissociation\nregion (PDR) tracers tend to align preferentially parallel relative to the\nmagnetic field. Moreover, the dense gas and PDR associated structures are found\nto be kinematically distinct such that a bimodal alignment trend is also\nobserved as a function of line-of-sight velocity. This suggests that the\nmagnetic field may have been dynamically important and set a preferred\ndirection of gas flow at the time that RCW 36 formed, resulting in a dense\nridge developing perpendicular to the magnetic field. However on\nfilament-scales near the PDR region, feedback may be energetically dominating\nthe magnetic field, warping its geometry and the associated flux-frozen gas\nstructures, causing the observed the preference for parallel relative\nalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use polarization data from SOFIA HAWC+ to investigate the interplay\nbetween magnetic fields and stellar feedback in altering gas dynamics within\nthe high-mass star-forming region RCW 36, located in Vela C. This region is of\nparticular interest as it has a bipolar HII region powered by a massive star\ncluster which may be impacting the surrounding magnetic field. To determine if\nthis is the case, we apply the Histogram of Relative Orientations (HRO) method\nto quantify the relative alignment between the inferred magnetic field and\nelongated structures observed in several datasets such as dust emission, column\ndensity, temperature, and spectral line intensity maps. The HRO results\nindicate a bimodal alignment trend, where structures observed with dense gas\ntracers show a statistically significant preference for perpendicular alignment\nrelative to the magnetic field, while structures probed by photo-dissociation\nregion (PDR) tracers tend to align preferentially parallel relative to the\nmagnetic field. Moreover, the dense gas and PDR associated structures are found\nto be kinematically distinct such that a bimodal alignment trend is also\nobserved as a function of line-of-sight velocity. This suggests that the\nmagnetic field may have been dynamically important and set a preferred\ndirection of gas flow at the time that RCW 36 formed, resulting in a dense\nridge developing perpendicular to the magnetic field. However on\nfilament-scales near the PDR region, feedback may be energetically dominating\nthe magnetic field, warping its geometry and the associated flux-frozen gas\nstructures, causing the observed the preference for parallel relative\nalignment."
                },
                "authors": [
                    {
                        "name": "Akanksha Bij"
                    },
                    {
                        "name": "Laura M. Fissel"
                    },
                    {
                        "name": "Lars Bonne"
                    },
                    {
                        "name": "Nicola Schneider"
                    },
                    {
                        "name": "Marc Berthoud"
                    },
                    {
                        "name": "Dennis Lee"
                    },
                    {
                        "name": "Giles A. Novak"
                    },
                    {
                        "name": "Sarah I. Sadavoy"
                    },
                    {
                        "name": "Thushara G. S. Pillai"
                    },
                    {
                        "name": "Maria Cunningham"
                    },
                    {
                        "name": "Paul Jones"
                    },
                    {
                        "name": "Robert Simon"
                    }
                ],
                "author_detail": {
                    "name": "Robert Simon"
                },
                "author": "Robert Simon",
                "arxiv_comment": "40 pages (25 pages main paper, 15 pages appendix), 24 figures, 5\n  tables, accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03550v1",
                "updated": "2024-09-05T14:12:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    12,
                    22,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:12:22Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    12,
                    22,
                    3,
                    249,
                    0
                ],
                "title": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any\n  Architecture"
                },
                "summary": "Diffusion models (DMs) have demonstrated exceptional generative capabilities\nacross various areas, while they are hindered by slow inference speeds and high\ncomputational demands during deployment. The most common way to accelerate DMs\ninvolves reducing the number of denoising steps during generation, achieved\nthrough faster sampling solvers or knowledge distillation (KD). In contrast to\nprior approaches, we propose a novel method that transfers the capability of\nlarge pretrained DMs to faster architectures. Specifically, we employ KD in a\ndistinct manner to compress DMs by distilling their generative ability into\nmore rapid variants. Furthermore, considering that the source data is either\nunaccessible or too enormous to store for current generative models, we\nintroduce a new paradigm for their distillation without source data, termed\nData-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our\nestablished DKDM framework comprises two main components: 1) a DKDM objective\nthat uses synthetic denoising data produced by pretrained DMs to optimize\nfaster DMs without source data, and 2) a dynamic iterative distillation method\nthat flexibly organizes the synthesis of denoising data, preventing it from\nslowing down the optimization process as the generation is slow. To our\nknowledge, this is the first attempt at using KD to distill DMs into any\narchitecture in a data-free manner. Importantly, our DKDM is orthogonal to most\nexisting acceleration methods, such as denoising step reduction, quantization\nand pruning. Experiments show that our DKDM is capable of deriving 2x faster\nDMs with performance remaining on par with the baseline. Notably, our DKDM\nenables pretrained DMs to function as \"datasets\" for training new DMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have demonstrated exceptional generative capabilities\nacross various areas, while they are hindered by slow inference speeds and high\ncomputational demands during deployment. The most common way to accelerate DMs\ninvolves reducing the number of denoising steps during generation, achieved\nthrough faster sampling solvers or knowledge distillation (KD). In contrast to\nprior approaches, we propose a novel method that transfers the capability of\nlarge pretrained DMs to faster architectures. Specifically, we employ KD in a\ndistinct manner to compress DMs by distilling their generative ability into\nmore rapid variants. Furthermore, considering that the source data is either\nunaccessible or too enormous to store for current generative models, we\nintroduce a new paradigm for their distillation without source data, termed\nData-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our\nestablished DKDM framework comprises two main components: 1) a DKDM objective\nthat uses synthetic denoising data produced by pretrained DMs to optimize\nfaster DMs without source data, and 2) a dynamic iterative distillation method\nthat flexibly organizes the synthesis of denoising data, preventing it from\nslowing down the optimization process as the generation is slow. To our\nknowledge, this is the first attempt at using KD to distill DMs into any\narchitecture in a data-free manner. Importantly, our DKDM is orthogonal to most\nexisting acceleration methods, such as denoising step reduction, quantization\nand pruning. Experiments show that our DKDM is capable of deriving 2x faster\nDMs with performance remaining on par with the baseline. Notably, our DKDM\nenables pretrained DMs to function as \"datasets\" for training new DMs."
                },
                "authors": [
                    {
                        "name": "Qianlong Xiang"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03540v1",
                "updated": "2024-09-05T14:02:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    2,
                    56,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:02:56Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    2,
                    56,
                    3,
                    249,
                    0
                ],
                "title": "Diversity in hydrogen-rich envelope mass of type II supernovae (II): SN\n  2023ixf as explosion of partially-stripped intermediate massive star",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in hydrogen-rich envelope mass of type II supernovae (II): SN\n  2023ixf as explosion of partially-stripped intermediate massive star"
                },
                "summary": "SN 2023ixf is one of the most well-observed core-collapse supernova in recent\ndecades, yet there is inconsistency in the inferred zero-age-main-sequence\n(ZAMS) mass $M_{\\rm ZAMS}$ of its progenitor. Direct observations of the pre-SN\nred supergiant (RSG) estimate $M_{\\rm ZAMS}$ spanning widely from 11 to 18\n$M_{\\rm \\odot}$. Additional constraints, including host environment and the\npulsation of its progenitor RSG, suggest a massive progenitor with $M_{\\rm\nZAMS}$ > 17 $M_{\\rm \\odot}$. However, the analysis of the properties of\nsupernova, from light curve modeling to late phase spectroscopy, favor a\nrelatively low mass scenario ($M_{\\rm ZAMS}$ < 15 $M_{\\rm \\odot}$). In this\nwork, we conduct systematic analysis of SN 2023ixf, from the RSG progenitor,\nplateau phase light curve to late phase spectroscopy. Using MESA+STELLA to\nsimulate the RSG progenitor and their explosions, we find that, despite the\nZAMS mass of the RSG models being varied from 12.0 to 17.5 $M_{\\rm \\odot}$,\nthey can produce light curves that well match with SN 2023ixf if the envelope\nmass and the explosion energy are allowed to vary. Using late phase\nspectroscopy as independent measurement, the oxygen emission line [O I]\nsuggests the ZAMS is intermediate massive (~16.0 $M_{\\rm \\odot}$), and the\nrelatively weak H$\\alpha$ emission line indicates the hydrogen envelope has\nbeen partially removed before the explosion. By incorporating the velocity\nstructure derived from the light curve modeling into an axisymmetric model, we\nsuccessfully generated [O I] line profiles that are consistent with the [O I]\nline observed in late phase spectroscopy of SN 2023ixf. Bringing these analyses\ntogether, we conclude that SN 2023ixf is the aspherical explosion of an\nintermediate massive star ($M_{\\rm ZAMS}$ = 15-16 $M_{\\rm \\odot}$) with the\nhydrogen envelope being partially stripped to 4-5 $M_{\\rm \\odot}$ prior to its\nexplosion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SN 2023ixf is one of the most well-observed core-collapse supernova in recent\ndecades, yet there is inconsistency in the inferred zero-age-main-sequence\n(ZAMS) mass $M_{\\rm ZAMS}$ of its progenitor. Direct observations of the pre-SN\nred supergiant (RSG) estimate $M_{\\rm ZAMS}$ spanning widely from 11 to 18\n$M_{\\rm \\odot}$. Additional constraints, including host environment and the\npulsation of its progenitor RSG, suggest a massive progenitor with $M_{\\rm\nZAMS}$ > 17 $M_{\\rm \\odot}$. However, the analysis of the properties of\nsupernova, from light curve modeling to late phase spectroscopy, favor a\nrelatively low mass scenario ($M_{\\rm ZAMS}$ < 15 $M_{\\rm \\odot}$). In this\nwork, we conduct systematic analysis of SN 2023ixf, from the RSG progenitor,\nplateau phase light curve to late phase spectroscopy. Using MESA+STELLA to\nsimulate the RSG progenitor and their explosions, we find that, despite the\nZAMS mass of the RSG models being varied from 12.0 to 17.5 $M_{\\rm \\odot}$,\nthey can produce light curves that well match with SN 2023ixf if the envelope\nmass and the explosion energy are allowed to vary. Using late phase\nspectroscopy as independent measurement, the oxygen emission line [O I]\nsuggests the ZAMS is intermediate massive (~16.0 $M_{\\rm \\odot}$), and the\nrelatively weak H$\\alpha$ emission line indicates the hydrogen envelope has\nbeen partially removed before the explosion. By incorporating the velocity\nstructure derived from the light curve modeling into an axisymmetric model, we\nsuccessfully generated [O I] line profiles that are consistent with the [O I]\nline observed in late phase spectroscopy of SN 2023ixf. Bringing these analyses\ntogether, we conclude that SN 2023ixf is the aspherical explosion of an\nintermediate massive star ($M_{\\rm ZAMS}$ = 15-16 $M_{\\rm \\odot}$) with the\nhydrogen envelope being partially stripped to 4-5 $M_{\\rm \\odot}$ prior to its\nexplosion."
                },
                "authors": [
                    {
                        "name": "Qiliang Fang"
                    },
                    {
                        "name": "Takashi J. Moriya"
                    },
                    {
                        "name": "Luca Ferrari"
                    },
                    {
                        "name": "Keiichi Maeda"
                    },
                    {
                        "name": "Gaston Folatelli"
                    },
                    {
                        "name": "Keila Y. Ertini"
                    },
                    {
                        "name": "Hanindyo Kuncarayakti"
                    },
                    {
                        "name": "Jennifer E. Andrews"
                    },
                    {
                        "name": "Tatsuya Matsumoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Matsumoto"
                },
                "author": "Tatsuya Matsumoto",
                "arxiv_comment": "12 pages, 4 figures. Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.09043v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.09043v4",
                "updated": "2024-09-05T14:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    0,
                    11,
                    3,
                    249,
                    0
                ],
                "published": "2023-01-22T02:59:59Z",
                "published_parsed": [
                    2023,
                    1,
                    22,
                    2,
                    59,
                    59,
                    6,
                    22,
                    0
                ],
                "title": "CodeScore: Evaluating Code Generation by Learning Code Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeScore: Evaluating Code Generation by Learning Code Execution"
                },
                "summary": "A proper code evaluation metric (CEM) profoundly impacts the evolution of\ncode generation, which is an important research field in NLP and software\nengineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU)\nsuffer from two significant drawbacks. 1. They primarily measure the surface\ndifferences between codes without considering their functional equivalence.\nHowever, functional equivalence is pivotal in evaluating the effectiveness of\ncode generation, as different codes can perform identical operations. 2. They\nare predominantly designed for the Ref-only input format. However, code\nevaluation necessitates versatility in input formats. Aside from Ref-only,\nthere are NL-only and Ref\\&NL formats, which existing match-based CEMs cannot\neffectively accommodate. In this paper, we propose CodeScore, a large language\nmodel (LLM)-based CEM, which estimates the functional correctness of generated\ncode on three input types. To acquire CodeScore, we present UniCE, a unified\ncode generation learning framework, for LLMs to learn code execution (i.e.,\nlearning PassRatio and Executability of generated code) with unified input.\nExtensive experimental results on multiple code evaluation datasets demonstrate\nthat CodeScore absolutely improves up to 58.87% correlation with functional\ncorrectness compared to other CEMs, achieves state-of-the-art performance, and\neffectively handles three input formats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A proper code evaluation metric (CEM) profoundly impacts the evolution of\ncode generation, which is an important research field in NLP and software\nengineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU)\nsuffer from two significant drawbacks. 1. They primarily measure the surface\ndifferences between codes without considering their functional equivalence.\nHowever, functional equivalence is pivotal in evaluating the effectiveness of\ncode generation, as different codes can perform identical operations. 2. They\nare predominantly designed for the Ref-only input format. However, code\nevaluation necessitates versatility in input formats. Aside from Ref-only,\nthere are NL-only and Ref\\&NL formats, which existing match-based CEMs cannot\neffectively accommodate. In this paper, we propose CodeScore, a large language\nmodel (LLM)-based CEM, which estimates the functional correctness of generated\ncode on three input types. To acquire CodeScore, we present UniCE, a unified\ncode generation learning framework, for LLMs to learn code execution (i.e.,\nlearning PassRatio and Executability of generated code) with unified input.\nExtensive experimental results on multiple code evaluation datasets demonstrate\nthat CodeScore absolutely improves up to 58.87% correlation with functional\ncorrectness compared to other CEMs, achieves state-of-the-art performance, and\neffectively handles three input formats."
                },
                "authors": [
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Jiazheng Ding"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "Accepted to TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.09043v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.09043v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12334v2",
                "updated": "2024-09-05T13:47:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    47,
                    26,
                    3,
                    249,
                    0
                ],
                "published": "2024-06-18T06:59:24Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    59,
                    24,
                    1,
                    170,
                    0
                ],
                "title": "What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to\n  Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to\n  Prompt Engineering"
                },
                "summary": "Large Language Models (LLMs) changed the way we design and interact with\nsoftware systems. Their ability to process and extract information from text\nhas drastically improved productivity in a number of routine tasks. Developers\nthat want to include these models in their software stack, however, face a\ndreadful challenge: debugging LLMs' inconsistent behavior across minor\nvariations of the prompt. We therefore introduce two metrics for classification\ntasks, namely sensitivity and consistency, which are complementary to task\nperformance. First, sensitivity measures changes of predictions across\nrephrasings of the prompt, and does not require access to ground truth labels.\nInstead, consistency measures how predictions vary across rephrasings for\nelements of the same class. We perform an empirical comparison of these metrics\non text classification tasks, using them as guideline for understanding failure\nmodes of the LLM. Our hope is that sensitivity and consistency will be helpful\nto guide prompt engineering and obtain LLMs that balance robustness with\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) changed the way we design and interact with\nsoftware systems. Their ability to process and extract information from text\nhas drastically improved productivity in a number of routine tasks. Developers\nthat want to include these models in their software stack, however, face a\ndreadful challenge: debugging LLMs' inconsistent behavior across minor\nvariations of the prompt. We therefore introduce two metrics for classification\ntasks, namely sensitivity and consistency, which are complementary to task\nperformance. First, sensitivity measures changes of predictions across\nrephrasings of the prompt, and does not require access to ground truth labels.\nInstead, consistency measures how predictions vary across rephrasings for\nelements of the same class. We perform an empirical comparison of these metrics\non text classification tasks, using them as guideline for understanding failure\nmodes of the LLM. Our hope is that sensitivity and consistency will be helpful\nto guide prompt engineering and obtain LLMs that balance robustness with\nperformance."
                },
                "authors": [
                    {
                        "name": "Federico Errica"
                    },
                    {
                        "name": "Giuseppe Siracusano"
                    },
                    {
                        "name": "Davide Sanvito"
                    },
                    {
                        "name": "Roberto Bifulco"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Bifulco"
                },
                "author": "Roberto Bifulco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03533v1",
                "updated": "2024-09-05T13:46:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    46,
                    11,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T13:46:11Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    46,
                    11,
                    3,
                    249,
                    0
                ],
                "title": "Bayesian inference of wall torques for active Brownian particles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference of wall torques for active Brownian particles"
                },
                "summary": "The motility of living things and synthetic self-propelled objects is often\ndescribed using Active Brownian particles. To capture the interaction of these\nparticles with their often complex environment, this model can be augmented\nwith empirical forces or torques, for example, to describe their alignment with\nan obstacle or wall after a collision. Here, we assess the quality of these\nempirical models by comparing their output predictions with trajectories of\nrod-shaped active particles that scatter sterically at a flat wall. We employ a\nclassical least-squares method to evaluate the instantaneous torque. In\naddition, we lay out a Bayesian inference procedure to construct the posterior\ndistribution of plausible model parameters. In contrast to the least squares\nfit, the Bayesian approach does not require orientational data of the active\nparticle and can readily be applied to experimental tracking data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The motility of living things and synthetic self-propelled objects is often\ndescribed using Active Brownian particles. To capture the interaction of these\nparticles with their often complex environment, this model can be augmented\nwith empirical forces or torques, for example, to describe their alignment with\nan obstacle or wall after a collision. Here, we assess the quality of these\nempirical models by comparing their output predictions with trajectories of\nrod-shaped active particles that scatter sterically at a flat wall. We employ a\nclassical least-squares method to evaluate the instantaneous torque. In\naddition, we lay out a Bayesian inference procedure to construct the posterior\ndistribution of plausible model parameters. In contrast to the least squares\nfit, the Bayesian approach does not require orientational data of the active\nparticle and can readily be applied to experimental tracking data."
                },
                "authors": [
                    {
                        "name": "Sascha Lambert"
                    },
                    {
                        "name": "Merle Duchene"
                    },
                    {
                        "name": "Stefan Klumpp"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Klumpp"
                },
                "author": "Stefan Klumpp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03516v1",
                "updated": "2024-09-05T13:29:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    29,
                    50,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T13:29:50Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    29,
                    50,
                    3,
                    249,
                    0
                ],
                "title": "LMLT: Low-to-high Multi-Level Vision Transformer for Image\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMLT: Low-to-high Multi-Level Vision Transformer for Image\n  Super-Resolution"
                },
                "summary": "Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have\ndemonstrated impressive performance. However, they suffer from significant\ncomplexity, resulting in high inference times and memory usage. Additionally,\nViT models using Window Self-Attention (WSA) face challenges in processing\nregions outside their windows. To address these issues, we propose the\nLow-to-high Multi-Level Transformer (LMLT), which employs attention with\nvarying feature sizes for each head. LMLT divides image features along the\nchannel dimension, gradually reduces spatial size for lower heads, and applies\nself-attention to each head. This approach effectively captures both local and\nglobal information. By integrating the results from lower heads into higher\nheads, LMLT overcomes the window boundary issues in self-attention. Extensive\nexperiments show that our model significantly reduces inference time and GPU\nmemory usage while maintaining or even surpassing the performance of\nstate-of-the-art ViT-based Image Super-Resolution methods. Our codes are\navailiable at https://github.com/jwgdmkj/LMLT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have\ndemonstrated impressive performance. However, they suffer from significant\ncomplexity, resulting in high inference times and memory usage. Additionally,\nViT models using Window Self-Attention (WSA) face challenges in processing\nregions outside their windows. To address these issues, we propose the\nLow-to-high Multi-Level Transformer (LMLT), which employs attention with\nvarying feature sizes for each head. LMLT divides image features along the\nchannel dimension, gradually reduces spatial size for lower heads, and applies\nself-attention to each head. This approach effectively captures both local and\nglobal information. By integrating the results from lower heads into higher\nheads, LMLT overcomes the window boundary issues in self-attention. Extensive\nexperiments show that our model significantly reduces inference time and GPU\nmemory usage while maintaining or even surpassing the performance of\nstate-of-the-art ViT-based Image Super-Resolution methods. Our codes are\navailiable at https://github.com/jwgdmkj/LMLT."
                },
                "authors": [
                    {
                        "name": "Jeongsoo Kim"
                    },
                    {
                        "name": "Jongho Nang"
                    },
                    {
                        "name": "Junsuk Choe"
                    }
                ],
                "author_detail": {
                    "name": "Junsuk Choe"
                },
                "author": "Junsuk Choe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03515v2",
                "updated": "2024-09-06T12:17:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    17,
                    4,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T13:29:45Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    29,
                    45,
                    3,
                    249,
                    0
                ],
                "title": "Local Measurement Scheme of Gravitational Curvature using Atom\n  Interferometers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Measurement Scheme of Gravitational Curvature using Atom\n  Interferometers"
                },
                "summary": "Light pulse atom interferometers (AIFs) are exquisite quantum probes of\nspatial inhomogeneity and gravitational curvature. Moreover, detailed\nmeasurement and calibration are necessary prerequisites for very-long-baseline\natom interferometry (VLBAI). Here we present a method in which the differential\nsignal of two co-located interferometers singles out a phase shift proportional\nto the curvature of the gravitational potential. The scale factor depends only\non well controlled quantities, namely the photon wave number, the\ninterferometer time and the atomic recoil, which allows the curvature to be\naccurately inferred from a measured phase. As a case study, we numerically\nsimulate such a co-located gradiometric interferometer in the context of the\nHannover VLBAI facility and prove the robustness of the phase shift in\ngravitational fields with complex spatial dependence. We define an estimator of\nthe gravitational curvature for non-trivial gravitational fields and calculate\nthe trade-off between signal strength and estimation accuracy with regard to\nspatial resolution. As a perspective, we discuss the case of a time-dependent\ngravitational field and corresponding measurement strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light pulse atom interferometers (AIFs) are exquisite quantum probes of\nspatial inhomogeneity and gravitational curvature. Moreover, detailed\nmeasurement and calibration are necessary prerequisites for very-long-baseline\natom interferometry (VLBAI). Here we present a method in which the differential\nsignal of two co-located interferometers singles out a phase shift proportional\nto the curvature of the gravitational potential. The scale factor depends only\non well controlled quantities, namely the photon wave number, the\ninterferometer time and the atomic recoil, which allows the curvature to be\naccurately inferred from a measured phase. As a case study, we numerically\nsimulate such a co-located gradiometric interferometer in the context of the\nHannover VLBAI facility and prove the robustness of the phase shift in\ngravitational fields with complex spatial dependence. We define an estimator of\nthe gravitational curvature for non-trivial gravitational fields and calculate\nthe trade-off between signal strength and estimation accuracy with regard to\nspatial resolution. As a perspective, we discuss the case of a time-dependent\ngravitational field and corresponding measurement strategies."
                },
                "authors": [
                    {
                        "name": "Michael Werner"
                    },
                    {
                        "name": "Ali Lezeik"
                    },
                    {
                        "name": "Dennis Schlippert"
                    },
                    {
                        "name": "Ernst Rasel"
                    },
                    {
                        "name": "Naceur Gaaloul"
                    },
                    {
                        "name": "Klemens Hammerer"
                    }
                ],
                "author_detail": {
                    "name": "Klemens Hammerer"
                },
                "author": "Klemens Hammerer",
                "arxiv_comment": "6 pages of main text, 4 pages appendix, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03512v1",
                "updated": "2024-09-05T13:22:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    22,
                    51,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T13:22:51Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    22,
                    51,
                    3,
                    249,
                    0
                ],
                "title": "From MOOC to MAIC: Reshaping Online Teaching and Learning through\n  LLM-driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From MOOC to MAIC: Reshaping Online Teaching and Learning through\n  LLM-driven Agents"
                },
                "summary": "Since the first instances of online education, where courses were uploaded to\naccessible and shared online platforms, this form of scaling the dissemination\nof human knowledge to reach a broader audience has sparked extensive discussion\nand widespread adoption. Recognizing that personalized learning still holds\nsignificant potential for improvement, new AI technologies have been\ncontinuously integrated into this learning format, resulting in a variety of\neducational AI applications such as educational recommendation and intelligent\ntutoring. The emergence of intelligence in large language models (LLMs) has\nallowed for these educational enhancements to be built upon a unified\nfoundational model, enabling deeper integration. In this context, we propose\nMAIC (Massive AI-empowered Course), a new form of online education that\nleverages LLM-driven multi-agent systems to construct an AI-augmented\nclassroom, balancing scalability with adaptivity. Beyond exploring the\nconceptual framework and technical innovations, we conduct preliminary\nexperiments at Tsinghua University, one of China's leading universities.\nDrawing from over 100,000 learning records of more than 500 students, we obtain\na series of valuable observations and initial analyses. This project will\ncontinue to evolve, ultimately aiming to establish a comprehensive open\nplatform that supports and unifies research, technology, and applications in\nexploring the possibilities of online education in the era of large model AI.\nWe envision this platform as a collaborative hub, bringing together educators,\nresearchers, and innovators to collectively explore the future of AI-driven\nonline education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the first instances of online education, where courses were uploaded to\naccessible and shared online platforms, this form of scaling the dissemination\nof human knowledge to reach a broader audience has sparked extensive discussion\nand widespread adoption. Recognizing that personalized learning still holds\nsignificant potential for improvement, new AI technologies have been\ncontinuously integrated into this learning format, resulting in a variety of\neducational AI applications such as educational recommendation and intelligent\ntutoring. The emergence of intelligence in large language models (LLMs) has\nallowed for these educational enhancements to be built upon a unified\nfoundational model, enabling deeper integration. In this context, we propose\nMAIC (Massive AI-empowered Course), a new form of online education that\nleverages LLM-driven multi-agent systems to construct an AI-augmented\nclassroom, balancing scalability with adaptivity. Beyond exploring the\nconceptual framework and technical innovations, we conduct preliminary\nexperiments at Tsinghua University, one of China's leading universities.\nDrawing from over 100,000 learning records of more than 500 students, we obtain\na series of valuable observations and initial analyses. This project will\ncontinue to evolve, ultimately aiming to establish a comprehensive open\nplatform that supports and unifies research, technology, and applications in\nexploring the possibilities of online education in the era of large model AI.\nWe envision this platform as a collaborative hub, bringing together educators,\nresearchers, and innovators to collectively explore the future of AI-driven\nonline education."
                },
                "authors": [
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Daniel Zhang-li"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Zhanxin Hao"
                    },
                    {
                        "name": "Rui Miao Li"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Yuanchun Wang"
                    },
                    {
                        "name": "Hanming Li"
                    },
                    {
                        "name": "Linlu Gong"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Jiayin Lin"
                    },
                    {
                        "name": "Jinchang Zhou"
                    },
                    {
                        "name": "Fei Qin"
                    },
                    {
                        "name": "Haohua Wang"
                    },
                    {
                        "name": "Jianxiao Jiang"
                    },
                    {
                        "name": "Lijun Deng"
                    },
                    {
                        "name": "Yisi Zhan"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xusheng Dai"
                    },
                    {
                        "name": "Xuan Yan"
                    },
                    {
                        "name": "Nianyi Lin"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Ruixin Ni"
                    },
                    {
                        "name": "Yang Dang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Manli Li"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Huiqin Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03495v1",
                "updated": "2024-09-05T13:07:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    7,
                    31,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T13:07:31Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    7,
                    31,
                    3,
                    249,
                    0
                ],
                "title": "Maximum likelihood inference for high-dimensional problems with\n  multiaffine variable relations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum likelihood inference for high-dimensional problems with\n  multiaffine variable relations"
                },
                "summary": "Maximum Likelihood Estimation of continuous variable models can be very\nchallenging in high dimensions, due to potentially complex probability\ndistributions. The existence of multiple interdependencies among variables can\nmake it very difficult to establish convergence guarantees. This leads to a\nwide use of brute-force methods, such as grid searching and Monte-Carlo\nsampling and, when applicable, complex and problem-specific algorithms. In this\npaper, we consider inference problems where the variables are related by\nmultiaffine expressions. We propose a novel Alternating and\nIteratively-Reweighted Least Squares (AIRLS) algorithm, and prove its\nconvergence for problems with Generalized Normal Distributions. We also provide\nan efficient method to compute the variance of the estimates obtained using\nAIRLS. Finally, we show how the method can be applied to graphical statistical\nmodels. We perform numerical experiments on several inference problems, showing\nsignificantly better performance than state-of-the-art approaches in terms of\nscalability, robustness to noise, and convergence speed due to an empirically\nobserved super-linear convergence rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum Likelihood Estimation of continuous variable models can be very\nchallenging in high dimensions, due to potentially complex probability\ndistributions. The existence of multiple interdependencies among variables can\nmake it very difficult to establish convergence guarantees. This leads to a\nwide use of brute-force methods, such as grid searching and Monte-Carlo\nsampling and, when applicable, complex and problem-specific algorithms. In this\npaper, we consider inference problems where the variables are related by\nmultiaffine expressions. We propose a novel Alternating and\nIteratively-Reweighted Least Squares (AIRLS) algorithm, and prove its\nconvergence for problems with Generalized Normal Distributions. We also provide\nan efficient method to compute the variance of the estimates obtained using\nAIRLS. Finally, we show how the method can be applied to graphical statistical\nmodels. We perform numerical experiments on several inference problems, showing\nsignificantly better performance than state-of-the-art approaches in terms of\nscalability, robustness to noise, and convergence speed due to an empirically\nobserved super-linear convergence rate."
                },
                "authors": [
                    {
                        "name": "Jean-Sbastien Brouillon"
                    },
                    {
                        "name": "Florian Drfler"
                    },
                    {
                        "name": "Giancarlo Ferrari-Trecate"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Ferrari-Trecate"
                },
                "author": "Giancarlo Ferrari-Trecate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03492v1",
                "updated": "2024-09-05T12:59:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    59,
                    38,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T12:59:38Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    59,
                    38,
                    3,
                    249,
                    0
                ],
                "title": "Distributionally Robust Optimisation with Bayesian Ambiguity Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributionally Robust Optimisation with Bayesian Ambiguity Sets"
                },
                "summary": "Decision making under uncertainty is challenging since the data-generating\nprocess (DGP) is often unknown. Bayesian inference proceeds by estimating the\nDGP through posterior beliefs about the model's parameters. However, minimising\nthe expected risk under these posterior beliefs can lead to sub-optimal\ndecisions due to model uncertainty or limited, noisy observations. To address\nthis, we introduce Distributionally Robust Optimisation with Bayesian Ambiguity\nSets (DRO-BAS) which hedges against uncertainty in the model by optimising the\nworst-case risk over a posterior-informed ambiguity set. We show that our\nmethod admits a closed-form dual representation for many exponential family\nmembers and showcase its improved out-of-sample robustness against existing\nBayesian DRO methodology in the Newsvendor problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision making under uncertainty is challenging since the data-generating\nprocess (DGP) is often unknown. Bayesian inference proceeds by estimating the\nDGP through posterior beliefs about the model's parameters. However, minimising\nthe expected risk under these posterior beliefs can lead to sub-optimal\ndecisions due to model uncertainty or limited, noisy observations. To address\nthis, we introduce Distributionally Robust Optimisation with Bayesian Ambiguity\nSets (DRO-BAS) which hedges against uncertainty in the model by optimising the\nworst-case risk over a posterior-informed ambiguity set. We show that our\nmethod admits a closed-form dual representation for many exponential family\nmembers and showcase its improved out-of-sample robustness against existing\nBayesian DRO methodology in the Newsvendor problem."
                },
                "authors": [
                    {
                        "name": "Charita Dellaporta"
                    },
                    {
                        "name": "Patrick O'Hara"
                    },
                    {
                        "name": "Theodoros Damoulas"
                    }
                ],
                "author_detail": {
                    "name": "Theodoros Damoulas"
                },
                "author": "Theodoros Damoulas",
                "arxiv_comment": "13 pages, 3 figures. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.08729v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.08729v3",
                "updated": "2024-09-05T12:59:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    59,
                    7,
                    3,
                    249,
                    0
                ],
                "published": "2023-05-15T15:43:45Z",
                "published_parsed": [
                    2023,
                    5,
                    15,
                    15,
                    43,
                    45,
                    0,
                    135,
                    0
                ],
                "title": "Group knowledge and individual introspection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group knowledge and individual introspection"
                },
                "summary": "We study distributed knowledge, which is what privately informed agents come\nto know by communicating freely with one another and sharing everything they\nknow. Agents are not necessarily fully rational and differ in the ability to\nform higher-order knowledge. We model the inference making process that leads\nto distributed knowledge, and we do that by introducing revision operators and\nrevision types. Since reasoning abilities can be heterogeneous, inference\nmaking turns out to be order dependent. We show that there are two\nqualitatively different cases of how distributed knowledge is attained. In the\nfirst, distributed knowledge is determined by any group member who can\nreplicate all the inferences that anyone else in the group makes. This result\nis in line with the extant literature. In the second case, no member can\nreplicate all the inferences that are made within the group. As a result,\ndistributed knowledge is determined by any two group members who can jointly\nreplicate what anyone else infers. This case can be interpreted as a form of\nwisdom of crowd effect and shows that, contrary to what is generally believed,\ndistributed knowledge cannot always be reduced to the reasoning abilities of a\nsingle group member.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study distributed knowledge, which is what privately informed agents come\nto know by communicating freely with one another and sharing everything they\nknow. Agents are not necessarily fully rational and differ in the ability to\nform higher-order knowledge. We model the inference making process that leads\nto distributed knowledge, and we do that by introducing revision operators and\nrevision types. Since reasoning abilities can be heterogeneous, inference\nmaking turns out to be order dependent. We show that there are two\nqualitatively different cases of how distributed knowledge is attained. In the\nfirst, distributed knowledge is determined by any group member who can\nreplicate all the inferences that anyone else in the group makes. This result\nis in line with the extant literature. In the second case, no member can\nreplicate all the inferences that are made within the group. As a result,\ndistributed knowledge is determined by any two group members who can jointly\nreplicate what anyone else infers. This case can be interpreted as a form of\nwisdom of crowd effect and shows that, contrary to what is generally believed,\ndistributed knowledge cannot always be reduced to the reasoning abilities of a\nsingle group member."
                },
                "authors": [
                    {
                        "name": "Michele Crescenzi"
                    }
                ],
                "author_detail": {
                    "name": "Michele Crescenzi"
                },
                "author": "Michele Crescenzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.08729v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.08729v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03478v1",
                "updated": "2024-09-05T12:38:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    38,
                    13,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T12:38:13Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    38,
                    13,
                    3,
                    249,
                    0
                ],
                "title": "LLM-based event abstraction and integration for IoT-sourced logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based event abstraction and integration for IoT-sourced logs"
                },
                "summary": "The continuous flow of data collected by Internet of Things (IoT) devices,\nhas revolutionised our ability to understand and interact with the world across\nvarious applications. However, this data must be prepared and transformed into\nevent data before analysis can begin. In this paper, we shed light on the\npotential of leveraging Large Language Models (LLMs) in event abstraction and\nintegration. Our approach aims to create event records from raw sensor readings\nand merge the logs from multiple IoT sources into a single event log suitable\nfor further Process Mining applications. We demonstrate the capabilities of\nLLMs in event abstraction considering a case study for IoT application in\nelderly care and longitudinal health monitoring. The results, showing on\naverage an accuracy of 90% in detecting high-level activities. These results\nhighlight LLMs' promising potential in addressing event abstraction and\nintegration challenges, effectively bridging the existing gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continuous flow of data collected by Internet of Things (IoT) devices,\nhas revolutionised our ability to understand and interact with the world across\nvarious applications. However, this data must be prepared and transformed into\nevent data before analysis can begin. In this paper, we shed light on the\npotential of leveraging Large Language Models (LLMs) in event abstraction and\nintegration. Our approach aims to create event records from raw sensor readings\nand merge the logs from multiple IoT sources into a single event log suitable\nfor further Process Mining applications. We demonstrate the capabilities of\nLLMs in event abstraction considering a case study for IoT application in\nelderly care and longitudinal health monitoring. The results, showing on\naverage an accuracy of 90% in detecting high-level activities. These results\nhighlight LLMs' promising potential in addressing event abstraction and\nintegration challenges, effectively bridging the existing gap."
                },
                "authors": [
                    {
                        "name": "Mohsen Shirali"
                    },
                    {
                        "name": "Mohammadreza Fani Sani"
                    },
                    {
                        "name": "Zahra Ahmadi"
                    },
                    {
                        "name": "Estefania Serral"
                    }
                ],
                "author_detail": {
                    "name": "Estefania Serral"
                },
                "author": "Estefania Serral",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M14",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03466v1",
                "updated": "2024-09-05T12:21:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    21,
                    51,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T12:21:51Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    21,
                    51,
                    3,
                    249,
                    0
                ],
                "title": "Panopticon: a novel deep learning model to detect single transit events\n  with no prior data filtering in PLATO light curves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panopticon: a novel deep learning model to detect single transit events\n  with no prior data filtering in PLATO light curves"
                },
                "summary": "To prepare for the analyses of the future PLATO light curves, we develop a\ndeep learning model, Panopticon, to detect transits in high precision\nphotometric light curves. Since PLATO's main objective is the detection of\ntemperate Earth-size planets around solar-type stars, the code is designed to\ndetect individual transit events. The filtering step, required by conventional\ndetection methods, can affect the transit, which could be an issue for long and\nshallow transits. To protect transit shape and depth, the code is also designed\nto work on unfiltered light curves. We trained the model on a set of simulated\nPLATO light curves in which we injected, at pixel level, either planetary,\neclipsing binary, or background eclipsing binary signals. We also include a\nvariety of noises in our data, such as granulation, stellar spots or cosmic\nrays. The approach is able to recover 90% of our test population, including\nmore than 25% of the Earth-analogs, even in the unfiltered light curves. The\nmodel also recovers the transits irrespective of the orbital period, and is\nable to retrieve transits on a unique event basis. These figures are obtained\nwhen accepting a false alarm rate of 1%. When keeping the false alarm rate low\n(<0.01%), it is still able to recover more than 85% of the transit signals. Any\ntransit deeper than 180ppm is essentially guaranteed to be recovered. This\nmethod is able to recover transits on a unique event basis, and does so with a\nlow false alarm rate. Thanks to light curves being one-dimensional, model\ntraining is fast, on the order of a few hours per model. This speed in training\nand inference, coupled to the recovery effectiveness and precision of the model\nmake it an ideal tool to complement, or be used ahead of, classical approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To prepare for the analyses of the future PLATO light curves, we develop a\ndeep learning model, Panopticon, to detect transits in high precision\nphotometric light curves. Since PLATO's main objective is the detection of\ntemperate Earth-size planets around solar-type stars, the code is designed to\ndetect individual transit events. The filtering step, required by conventional\ndetection methods, can affect the transit, which could be an issue for long and\nshallow transits. To protect transit shape and depth, the code is also designed\nto work on unfiltered light curves. We trained the model on a set of simulated\nPLATO light curves in which we injected, at pixel level, either planetary,\neclipsing binary, or background eclipsing binary signals. We also include a\nvariety of noises in our data, such as granulation, stellar spots or cosmic\nrays. The approach is able to recover 90% of our test population, including\nmore than 25% of the Earth-analogs, even in the unfiltered light curves. The\nmodel also recovers the transits irrespective of the orbital period, and is\nable to retrieve transits on a unique event basis. These figures are obtained\nwhen accepting a false alarm rate of 1%. When keeping the false alarm rate low\n(<0.01%), it is still able to recover more than 85% of the transit signals. Any\ntransit deeper than 180ppm is essentially guaranteed to be recovered. This\nmethod is able to recover transits on a unique event basis, and does so with a\nlow false alarm rate. Thanks to light curves being one-dimensional, model\ntraining is fast, on the order of a few hours per model. This speed in training\nand inference, coupled to the recovery effectiveness and precision of the model\nmake it an ideal tool to complement, or be used ahead of, classical approaches."
                },
                "authors": [
                    {
                        "name": "H. G. Vivien"
                    },
                    {
                        "name": "M. Deleuil"
                    },
                    {
                        "name": "N. Jannsen"
                    },
                    {
                        "name": "J. De Ridder"
                    },
                    {
                        "name": "D. Seynaeve"
                    },
                    {
                        "name": "M. -A. Carpine"
                    },
                    {
                        "name": "Y. Zerah"
                    }
                ],
                "author_detail": {
                    "name": "Y. Zerah"
                },
                "author": "Y. Zerah",
                "arxiv_comment": "Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03454v1",
                "updated": "2024-09-05T12:06:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    6,
                    38,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T12:06:38Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    6,
                    38,
                    3,
                    249,
                    0
                ],
                "title": "How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes"
                },
                "summary": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains."
                },
                "authors": [
                    {
                        "name": "Inacio Vieira"
                    },
                    {
                        "name": "Will Allred"
                    },
                    {
                        "name": "Seamus Lankford"
                    },
                    {
                        "name": "Sheila Castilho Monteiro De Sousa"
                    },
                    {
                        "name": "Andy Way"
                    }
                ],
                "author_detail": {
                    "name": "Andy Way"
                },
                "author": "Andy Way",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.14735v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.14735v5",
                "updated": "2024-09-05T12:00:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    0,
                    55,
                    3,
                    249,
                    0
                ],
                "published": "2023-10-23T09:15:18Z",
                "published_parsed": [
                    2023,
                    10,
                    23,
                    9,
                    15,
                    18,
                    0,
                    296,
                    0
                ],
                "title": "Unleashing the potential of prompt engineering in Large Language Models:\n  a comprehensive review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the potential of prompt engineering in Large Language Models:\n  a comprehensive review"
                },
                "summary": "This comprehensive review delves into the pivotal role of prompt engineering\nin unleashing the capabilities of Large Language Models (LLMs). The development\nof Artificial Intelligence (AI), from its inception in the 1950s to the\nemergence of advanced neural networks and deep learning architectures, has made\na breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in\nVision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt\nengineering is the process of structuring inputs, which has emerged as a\ncrucial technique to maximize the utility and accuracy of these models. This\npaper explores both foundational and advanced methodologies of prompt\nengineering, including techniques such as self-consistency, chain-of-thought,\nand generated knowledge, which significantly enhance model performance.\nAdditionally, it examines the prompt method of VLMs through innovative\napproaches such as Context Optimization (CoOp), Conditional Context\nOptimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this\ndiscussion is the aspect of AI security, particularly adversarial attacks that\nexploit vulnerabilities in prompt engineering. Strategies to mitigate these\nrisks and enhance model robustness are thoroughly reviewed. The evaluation of\nprompt methods is also addressed, through both subjective and objective\nmetrics, ensuring a robust analysis of their efficacy. This review also\nreflects the essential role of prompt engineering in advancing AI capabilities,\nproviding a structured framework for future research and application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review delves into the pivotal role of prompt engineering\nin unleashing the capabilities of Large Language Models (LLMs). The development\nof Artificial Intelligence (AI), from its inception in the 1950s to the\nemergence of advanced neural networks and deep learning architectures, has made\na breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in\nVision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt\nengineering is the process of structuring inputs, which has emerged as a\ncrucial technique to maximize the utility and accuracy of these models. This\npaper explores both foundational and advanced methodologies of prompt\nengineering, including techniques such as self-consistency, chain-of-thought,\nand generated knowledge, which significantly enhance model performance.\nAdditionally, it examines the prompt method of VLMs through innovative\napproaches such as Context Optimization (CoOp), Conditional Context\nOptimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this\ndiscussion is the aspect of AI security, particularly adversarial attacks that\nexploit vulnerabilities in prompt engineering. Strategies to mitigate these\nrisks and enhance model robustness are thoroughly reviewed. The evaluation of\nprompt methods is also addressed, through both subjective and objective\nmetrics, ensuring a robust analysis of their efficacy. This review also\nreflects the essential role of prompt engineering in advancing AI capabilities,\nproviding a structured framework for future research and application."
                },
                "authors": [
                    {
                        "name": "Banghao Chen"
                    },
                    {
                        "name": "Zhaofeng Zhang"
                    },
                    {
                        "name": "Nicolas Langren"
                    },
                    {
                        "name": "Shengxin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Shengxin Zhu"
                },
                "author": "Shengxin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.14735v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.14735v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02050v2",
                "updated": "2024-09-05T11:54:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    54,
                    52,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T16:53:38Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    53,
                    38,
                    1,
                    247,
                    0
                ],
                "title": "Enhancing Code-Switching Speech Recognition with LID-Based Collaborative\n  Mixture of Experts Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code-Switching Speech Recognition with LID-Based Collaborative\n  Mixture of Experts Model"
                },
                "summary": "Due to the inherent difficulty in modeling phonetic similarities across\ndifferent languages, code-switching speech recognition presents a formidable\nchallenge. This study proposes a Collaborative-MoE, a Mixture of Experts (MoE)\nmodel that leverages a collaborative mechanism among expert groups. Initially,\na preceding routing network explicitly learns Language Identification (LID)\ntasks and selects experts based on acquired LID weights. This process ensures\nrobust routing information to the MoE layer, mitigating interference from\ndiverse language domains on expert network parameter updates. The LID weights\nare also employed to facilitate inter-group collaboration, enabling the\nintegration of language-specific representations. Furthermore, within each\nlanguage expert group, a gating network operates unsupervised to foster\ncollaboration on attributes beyond language. Extensive experiments demonstrate\nthe efficacy of our approach, achieving significant performance enhancements\ncompared to alternative methods. Importantly, our method preserves the\nefficient inference capabilities characteristic of MoE models without\nnecessitating additional pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the inherent difficulty in modeling phonetic similarities across\ndifferent languages, code-switching speech recognition presents a formidable\nchallenge. This study proposes a Collaborative-MoE, a Mixture of Experts (MoE)\nmodel that leverages a collaborative mechanism among expert groups. Initially,\na preceding routing network explicitly learns Language Identification (LID)\ntasks and selects experts based on acquired LID weights. This process ensures\nrobust routing information to the MoE layer, mitigating interference from\ndiverse language domains on expert network parameter updates. The LID weights\nare also employed to facilitate inter-group collaboration, enabling the\nintegration of language-specific representations. Furthermore, within each\nlanguage expert group, a gating network operates unsupervised to foster\ncollaboration on attributes beyond language. Extensive experiments demonstrate\nthe efficacy of our approach, achieving significant performance enhancements\ncompared to alternative methods. Importantly, our method preserves the\nefficient inference capabilities characteristic of MoE models without\nnecessitating additional pre-training."
                },
                "authors": [
                    {
                        "name": "Hukai Huang"
                    },
                    {
                        "name": "Jiayan Lin"
                    },
                    {
                        "name": "Kaidi Wang"
                    },
                    {
                        "name": "Yishuang Li"
                    },
                    {
                        "name": "Wenhao Guan"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Qingyang Hong"
                    }
                ],
                "author_detail": {
                    "name": "Qingyang Hong"
                },
                "author": "Qingyang Hong",
                "arxiv_comment": "Accepted by IEEE SLT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03444v1",
                "updated": "2024-09-05T11:49:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    49,
                    53,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T11:49:53Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    49,
                    53,
                    3,
                    249,
                    0
                ],
                "title": "Fine-tuning large language models for domain adaptation: Exploration of\n  training strategies, scaling, model merging and synergistic capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models for domain adaptation: Exploration of\n  training strategies, scaling, model merging and synergistic capabilities"
                },
                "summary": "The advancement of Large Language Models (LLMs) for domain applications in\nfields such as materials science and engineering depends on the development of\nfine-tuning strategies that adapt models for specialized, technical\ncapabilities. In this work, we explore the effects of Continued Pretraining\n(CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization\napproaches, including Direct Preference Optimization (DPO) and Odds Ratio\nPreference Optimization (ORPO), on fine-tuned LLM performance. Our analysis\nshows how these strategies influence model outcomes and reveals that the\nmerging of multiple fine-tuned models can lead to the emergence of capabilities\nthat surpass the individual contributions of the parent models. We find that\nmodel merging leads to new functionalities that neither parent model could\nachieve alone, leading to improved performance in domain-specific assessments.\nExperiments with different model architectures are presented, including Llama\n3.1 8B and Mistral 7B models, where similar behaviors are observed. Exploring\nwhether the results hold also for much smaller models, we use a tiny LLM with\n1.7 billion parameters and show that very small LLMs do not necessarily feature\nemergent capabilities under model merging, suggesting that model scaling may be\na key component. In open-ended yet consistent chat conversations between a\nhuman and AI models, our assessment reveals detailed insights into how\ndifferent model variants perform and show that the smallest model achieves a\nhigh intelligence score across key criteria including reasoning depth,\ncreativity, clarity, and quantitative precision. Other experiments include the\ndevelopment of image generation prompts based on disparate biological material\ndesign concepts, to create new microstructures, architectural concepts, and\nurban design based on biological materials-inspired construction principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of Large Language Models (LLMs) for domain applications in\nfields such as materials science and engineering depends on the development of\nfine-tuning strategies that adapt models for specialized, technical\ncapabilities. In this work, we explore the effects of Continued Pretraining\n(CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization\napproaches, including Direct Preference Optimization (DPO) and Odds Ratio\nPreference Optimization (ORPO), on fine-tuned LLM performance. Our analysis\nshows how these strategies influence model outcomes and reveals that the\nmerging of multiple fine-tuned models can lead to the emergence of capabilities\nthat surpass the individual contributions of the parent models. We find that\nmodel merging leads to new functionalities that neither parent model could\nachieve alone, leading to improved performance in domain-specific assessments.\nExperiments with different model architectures are presented, including Llama\n3.1 8B and Mistral 7B models, where similar behaviors are observed. Exploring\nwhether the results hold also for much smaller models, we use a tiny LLM with\n1.7 billion parameters and show that very small LLMs do not necessarily feature\nemergent capabilities under model merging, suggesting that model scaling may be\na key component. In open-ended yet consistent chat conversations between a\nhuman and AI models, our assessment reveals detailed insights into how\ndifferent model variants perform and show that the smallest model achieves a\nhigh intelligence score across key criteria including reasoning depth,\ncreativity, clarity, and quantitative precision. Other experiments include the\ndevelopment of image generation prompts based on disparate biological material\ndesign concepts, to create new microstructures, architectural concepts, and\nurban design based on biological materials-inspired construction principles."
                },
                "authors": [
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Rachel K. Luu"
                    },
                    {
                        "name": "Markus J. Buehler"
                    }
                ],
                "author_detail": {
                    "name": "Markus J. Buehler"
                },
                "author": "Markus J. Buehler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03440v1",
                "updated": "2024-09-05T11:42:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    42,
                    26,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T11:42:26Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    42,
                    26,
                    3,
                    249,
                    0
                ],
                "title": "Rx Strategist: Prescription Verification using LLM Agents System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rx Strategist: Prescription Verification using LLM Agents System"
                },
                "summary": "To protect patient safety, modern pharmaceutical complexity demands strict\nprescription verification. We offer a new approach - Rx Strategist - that makes\nuse of knowledge graphs and different search strategies to enhance the power of\nLarge Language Models (LLMs) inside an agentic framework. This multifaceted\ntechnique allows for a multi-stage LLM pipeline and reliable information\nretrieval from a custom-built active ingredient database. Different facets of\nprescription verification, such as indication, dose, and possible drug\ninteractions, are covered in each stage of the pipeline. We alleviate the\ndrawbacks of monolithic LLM techniques by spreading reasoning over these\nstages, improving correctness and reliability while reducing memory demands.\nOur findings demonstrate that Rx Strategist surpasses many current LLMs,\nachieving performance comparable to that of a highly experienced clinical\npharmacist. In the complicated world of modern medications, this combination of\nLLMs with organized knowledge and sophisticated search methods presents a\nviable avenue for reducing prescription errors and enhancing patient outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect patient safety, modern pharmaceutical complexity demands strict\nprescription verification. We offer a new approach - Rx Strategist - that makes\nuse of knowledge graphs and different search strategies to enhance the power of\nLarge Language Models (LLMs) inside an agentic framework. This multifaceted\ntechnique allows for a multi-stage LLM pipeline and reliable information\nretrieval from a custom-built active ingredient database. Different facets of\nprescription verification, such as indication, dose, and possible drug\ninteractions, are covered in each stage of the pipeline. We alleviate the\ndrawbacks of monolithic LLM techniques by spreading reasoning over these\nstages, improving correctness and reliability while reducing memory demands.\nOur findings demonstrate that Rx Strategist surpasses many current LLMs,\nachieving performance comparable to that of a highly experienced clinical\npharmacist. In the complicated world of modern medications, this combination of\nLLMs with organized knowledge and sophisticated search methods presents a\nviable avenue for reducing prescription errors and enhancing patient outcomes."
                },
                "authors": [
                    {
                        "name": "Phuc Phan Van"
                    },
                    {
                        "name": "Dat Nguyen Minh"
                    },
                    {
                        "name": "An Dinh Ngoc"
                    },
                    {
                        "name": "Huy Phan Thanh"
                    }
                ],
                "author_detail": {
                    "name": "Huy Phan Thanh"
                },
                "author": "Huy Phan Thanh",
                "arxiv_comment": "17 Pages, 6 Figures, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03431v2",
                "updated": "2024-09-06T06:57:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    57,
                    59,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T11:23:41Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    23,
                    41,
                    3,
                    249,
                    0
                ],
                "title": "UV-Mamba: A DCN-Enhanced State Space Model for Urban Village Boundary\n  Identification in High-Resolution Remote Sensing Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UV-Mamba: A DCN-Enhanced State Space Model for Urban Village Boundary\n  Identification in High-Resolution Remote Sensing Images"
                },
                "summary": "Owing to the diverse geographical environments, intricate landscapes, and\nhigh-density settlements, the automatic identification of urban village\nboundaries using remote sensing images is a highly challenging task. This paper\nproposes a novel and efficient neural network model called UV-Mamba for\naccurate boundary detection in high-resolution remote sensing images. UV-Mamba\nmitigates the memory loss problem in long sequence modeling, which arises in\nstate space model (SSM) with increasing image size, by incorporating deformable\nconvolutions (DCN). Its architecture utilizes an encoder-decoder framework,\nincludes an encoder with four deformable state space augmentation (DSSA) blocks\nfor efficient multi-level semantic extraction and a decoder to integrate the\nextracted semantic information. We conducted experiments on the Beijing and\nXi'an datasets, and the results show that UV-Mamba achieves state-of-the-art\nperformance. Specifically, our model achieves 73.3% and 78.1% IoU on the\nBeijing and Xi'an datasets, respectively, representing improvements of 1.2% and\n3.4% IoU over the previous best model, while also being 6x faster in inference\nspeed and 40x smaller in parameter count. Source code and pre-trained models\nare available in the supplementary material.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to the diverse geographical environments, intricate landscapes, and\nhigh-density settlements, the automatic identification of urban village\nboundaries using remote sensing images is a highly challenging task. This paper\nproposes a novel and efficient neural network model called UV-Mamba for\naccurate boundary detection in high-resolution remote sensing images. UV-Mamba\nmitigates the memory loss problem in long sequence modeling, which arises in\nstate space model (SSM) with increasing image size, by incorporating deformable\nconvolutions (DCN). Its architecture utilizes an encoder-decoder framework,\nincludes an encoder with four deformable state space augmentation (DSSA) blocks\nfor efficient multi-level semantic extraction and a decoder to integrate the\nextracted semantic information. We conducted experiments on the Beijing and\nXi'an datasets, and the results show that UV-Mamba achieves state-of-the-art\nperformance. Specifically, our model achieves 73.3% and 78.1% IoU on the\nBeijing and Xi'an datasets, respectively, representing improvements of 1.2% and\n3.4% IoU over the previous best model, while also being 6x faster in inference\nspeed and 40x smaller in parameter count. Source code and pre-trained models\nare available in the supplementary material."
                },
                "authors": [
                    {
                        "name": "Lulin Li"
                    },
                    {
                        "name": "Ben Chen"
                    },
                    {
                        "name": "Xuechao Zou"
                    },
                    {
                        "name": "Junliang Xing"
                    },
                    {
                        "name": "Pin Tao"
                    }
                ],
                "author_detail": {
                    "name": "Pin Tao"
                },
                "author": "Pin Tao",
                "arxiv_comment": "5 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03430v1",
                "updated": "2024-09-05T11:22:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    22,
                    57,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T11:22:57Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    22,
                    57,
                    3,
                    249,
                    0
                ],
                "title": "Efficient prediction of potential energy surface and physical properties\n  with Kolmogorov-Arnold Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient prediction of potential energy surface and physical properties\n  with Kolmogorov-Arnold Networks"
                },
                "summary": "The application of machine learning methodologies for predicting properties\nwithin materials science has garnered significant attention. Among recent\nadvancements, Kolmogorov-Arnold Networks (KANs) have emerged as a promising\nalternative to traditional Multi-Layer Perceptrons (MLPs). This study evaluates\nthe impact of substituting MLPs with KANs within three established machine\nlearning frameworks: Allegro, Neural Equivariant Interatomic Potentials\n(NequIP), and the Edge-Based Tensor Prediction Graph Neural Network (ETGNN).\nOur results demonstrate that the integration of KANs generally yields enhanced\nprediction accuracies. Specifically, replacing MLPs with KANs in the output\nblocks leads to notable improvements in accuracy and, in certain scenarios,\nalso results in reduced training times. Furthermore, employing KANs exclusively\nin the output block facilitates faster inference and improved computational\nefficiency relative to utilizing KANs throughout the entire model. The\nselection of an optimal basis function for KANs is found to be contingent upon\nthe particular problem at hand. Our results demonstrate the strong potential of\nKANs in enhancing machine learning potentials and material property\npredictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of machine learning methodologies for predicting properties\nwithin materials science has garnered significant attention. Among recent\nadvancements, Kolmogorov-Arnold Networks (KANs) have emerged as a promising\nalternative to traditional Multi-Layer Perceptrons (MLPs). This study evaluates\nthe impact of substituting MLPs with KANs within three established machine\nlearning frameworks: Allegro, Neural Equivariant Interatomic Potentials\n(NequIP), and the Edge-Based Tensor Prediction Graph Neural Network (ETGNN).\nOur results demonstrate that the integration of KANs generally yields enhanced\nprediction accuracies. Specifically, replacing MLPs with KANs in the output\nblocks leads to notable improvements in accuracy and, in certain scenarios,\nalso results in reduced training times. Furthermore, employing KANs exclusively\nin the output block facilitates faster inference and improved computational\nefficiency relative to utilizing KANs throughout the entire model. The\nselection of an optimal basis function for KANs is found to be contingent upon\nthe particular problem at hand. Our results demonstrate the strong potential of\nKANs in enhancing machine learning potentials and material property\npredictions."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Hongyu Yu"
                    },
                    {
                        "name": "Yang Zhong"
                    },
                    {
                        "name": "Hongjun Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Hongjun Xiang"
                },
                "author": "Hongjun Xiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03420v1",
                "updated": "2024-09-05T11:09:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    9,
                    0,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T11:09:00Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    9,
                    0,
                    3,
                    249,
                    0
                ],
                "title": "mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page\n  Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page\n  Document Understanding"
                },
                "summary": "Multimodel Large Language Models(MLLMs) have achieved promising OCR-free\nDocument Understanding performance by increasing the supported resolution of\ndocument images. However, this comes at the cost of generating thousands of\nvisual tokens for a single document image, leading to excessive GPU memory and\nslower inference times, particularly in multi-page document comprehension. In\nthis work, to address these challenges, we propose a High-resolution\nDocCompressor module to compress each high-resolution document image into 324\ntokens, guided by low-resolution global visual features. With this compression\nmodule, to strengthen multi-page document comprehension ability and balance\nboth token efficiency and question-answering performance, we develop the\nDocOwl2 under a three-stage training framework: Single-image Pretraining,\nMulti-image Continue-pretraining, and Multi-task Finetuning. DocOwl2 sets a new\nstate-of-the-art across multi-page document understanding benchmarks and\nreduces first token latency by more than 50%, demonstrating advanced\ncapabilities in multi-page questioning answering, explanation with evidence\npages, and cross-page structure understanding. Additionally, compared to\nsingle-image MLLMs trained on similar data, our DocOwl2 achieves comparable\nsingle-page understanding performance with less than 20% of the visual tokens.\nOur codes, models, and data are publicly available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodel Large Language Models(MLLMs) have achieved promising OCR-free\nDocument Understanding performance by increasing the supported resolution of\ndocument images. However, this comes at the cost of generating thousands of\nvisual tokens for a single document image, leading to excessive GPU memory and\nslower inference times, particularly in multi-page document comprehension. In\nthis work, to address these challenges, we propose a High-resolution\nDocCompressor module to compress each high-resolution document image into 324\ntokens, guided by low-resolution global visual features. With this compression\nmodule, to strengthen multi-page document comprehension ability and balance\nboth token efficiency and question-answering performance, we develop the\nDocOwl2 under a three-stage training framework: Single-image Pretraining,\nMulti-image Continue-pretraining, and Multi-task Finetuning. DocOwl2 sets a new\nstate-of-the-art across multi-page document understanding benchmarks and\nreduces first token latency by more than 50%, demonstrating advanced\ncapabilities in multi-page questioning answering, explanation with evidence\npages, and cross-page structure understanding. Additionally, compared to\nsingle-image MLLMs trained on similar data, our DocOwl2 achieves comparable\nsingle-page understanding performance with less than 20% of the visual tokens.\nOur codes, models, and data are publicly available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2."
                },
                "authors": [
                    {
                        "name": "Anwen Hu"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Jiabo Ye"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Qin Jin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15778v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15778v3",
                "updated": "2024-09-05T10:30:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    10,
                    30,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-28T13:16:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    16,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities."
                },
                "authors": [
                    {
                        "name": "Jiayi Gui"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15778v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15778v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11983v2",
                "updated": "2024-09-05T10:01:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    10,
                    1,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-20T12:33:42Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    12,
                    33,
                    42,
                    0,
                    141,
                    0
                ],
                "title": "A review on the use of large language models as virtual tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A review on the use of large language models as virtual tutors"
                },
                "summary": "Transformer architectures contribute to managing long-term dependencies for\nNatural Language Processing, representing one of the most recent changes in the\nfield. These architectures are the basis of the innovative, cutting-edge Large\nLanguage Models (LLMs) that have produced a huge buzz in several fields and\nindustrial sectors, among the ones education stands out. Accordingly, these\ngenerative Artificial Intelligence-based solutions have directed the change in\ntechniques and the evolution in educational methods and contents, along with\nnetwork infrastructure, towards high-quality learning. Given the popularity of\nLLMs, this review seeks to provide a comprehensive overview of those solutions\ndesigned specifically to generate and evaluate educational materials and which\ninvolve students and teachers in their design or experimental plan. To the best\nof our knowledge, this is the first review of educational applications (e.g.,\nstudent assessment) of LLMs. As expected, the most common role of these systems\nis as virtual tutors for automatic question generation. Moreover, the most\npopular models are GTP-3 and BERT. However, due to the continuous launch of new\ngenerative models, new works are expected to be published shortly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer architectures contribute to managing long-term dependencies for\nNatural Language Processing, representing one of the most recent changes in the\nfield. These architectures are the basis of the innovative, cutting-edge Large\nLanguage Models (LLMs) that have produced a huge buzz in several fields and\nindustrial sectors, among the ones education stands out. Accordingly, these\ngenerative Artificial Intelligence-based solutions have directed the change in\ntechniques and the evolution in educational methods and contents, along with\nnetwork infrastructure, towards high-quality learning. Given the popularity of\nLLMs, this review seeks to provide a comprehensive overview of those solutions\ndesigned specifically to generate and evaluate educational materials and which\ninvolve students and teachers in their design or experimental plan. To the best\nof our knowledge, this is the first review of educational applications (e.g.,\nstudent assessment) of LLMs. As expected, the most common role of these systems\nis as virtual tutors for automatic question generation. Moreover, the most\npopular models are GTP-3 and BERT. However, due to the continuous launch of new\ngenerative models, new works are expected to be published shortly."
                },
                "authors": [
                    {
                        "name": "Silvia Garca-Mndez"
                    },
                    {
                        "name": "Francisco de Arriba-Prez"
                    },
                    {
                        "name": "Mara del Carmen Somoza-Lpez"
                    }
                ],
                "author_detail": {
                    "name": "Mara del Carmen Somoza-Lpez"
                },
                "author": "Mara del Carmen Somoza-Lpez",
                "arxiv_doi": "10.1007/s11191-024-00530-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11191-024-00530-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.11983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Science & Education (2024), 1-16",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14416v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14416v3",
                "updated": "2024-09-05T09:44:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    44,
                    23,
                    3,
                    249,
                    0
                ],
                "published": "2024-02-22T10:03:21Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    10,
                    3,
                    21,
                    3,
                    53,
                    0
                ],
                "title": "Algorithm-agnostic significance testing in supervised learning with\n  multimodal data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm-agnostic significance testing in supervised learning with\n  multimodal data"
                },
                "summary": "Valid statistical inference is crucial for decision-making but difficult to\nobtain in supervised learning with multimodal data, e.g., combinations of\nclinical features, genomic data, and medical images. Multimodal data often\nwarrants the use of black-box algorithms, for instance, random forests or\nneural networks, which impede the use of traditional variable significance\ntests. We address this problem by proposing the use of COvariance Measure Tests\n(COMETs), which are calibrated and powerful tests that can be combined with any\nsufficiently predictive supervised learning algorithm. We apply COMETs to\nseveral high-dimensional, multimodal data sets to illustrate (i) variable\nsignificance testing for finding relevant mutations modulating drug-activity,\n(ii) modality selection for predicting survival in liver cancer patients with\nmultiomics data, and (iii) modality selection with clinical features and\nmedical imaging data. In all applications, COMETs yield results consistent with\ndomain knowledge without requiring data-driven pre-processing which may\ninvalidate type I error control. These novel applications with high-dimensional\nmultimodal data corroborate prior results on the power and robustness of COMETs\nfor significance testing. COMETs are implemented in the comets R package\navailable on CRAN and pycomets Python library available on GitHub. Source code\nfor reproducing all results is available at\nhttps://github.com/LucasKook/comets. All data sets used in this work are openly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Valid statistical inference is crucial for decision-making but difficult to\nobtain in supervised learning with multimodal data, e.g., combinations of\nclinical features, genomic data, and medical images. Multimodal data often\nwarrants the use of black-box algorithms, for instance, random forests or\nneural networks, which impede the use of traditional variable significance\ntests. We address this problem by proposing the use of COvariance Measure Tests\n(COMETs), which are calibrated and powerful tests that can be combined with any\nsufficiently predictive supervised learning algorithm. We apply COMETs to\nseveral high-dimensional, multimodal data sets to illustrate (i) variable\nsignificance testing for finding relevant mutations modulating drug-activity,\n(ii) modality selection for predicting survival in liver cancer patients with\nmultiomics data, and (iii) modality selection with clinical features and\nmedical imaging data. In all applications, COMETs yield results consistent with\ndomain knowledge without requiring data-driven pre-processing which may\ninvalidate type I error control. These novel applications with high-dimensional\nmultimodal data corroborate prior results on the power and robustness of COMETs\nfor significance testing. COMETs are implemented in the comets R package\navailable on CRAN and pycomets Python library available on GitHub. Source code\nfor reproducing all results is available at\nhttps://github.com/LucasKook/comets. All data sets used in this work are openly\navailable."
                },
                "authors": [
                    {
                        "name": "Lucas Kook"
                    },
                    {
                        "name": "Anton Rask Lundborg"
                    }
                ],
                "author_detail": {
                    "name": "Anton Rask Lundborg"
                },
                "author": "Anton Rask Lundborg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14416v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14416v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03384v1",
                "updated": "2024-09-05T09:43:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    43,
                    25,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T09:43:25Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    43,
                    25,
                    3,
                    249,
                    0
                ],
                "title": "Hardware Acceleration of LLMs: A comprehensive survey and comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware Acceleration of LLMs: A comprehensive survey and comparison"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for natural\nlanguage processing tasks, revolutionizing the field with their ability to\nunderstand and generate human-like text. In this paper, we present a\ncomprehensive survey of the several research efforts that have been presented\nfor the acceleration of transformer networks for Large Language Models using\nhardware accelerators.\n  The survey presents the frameworks that have been proposed and then performs\na qualitative and quantitative comparison regarding the technology, the\nprocessing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy\nefficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each\nframework. The main challenge in comparison is that every proposed scheme is\nimplemented on a different process technology making hard a fair comparison.\nThe main contribution of this paper is that we extrapolate the results of the\nperformance and the energy efficiency on the same technology to make a fair\ncomparison; one theoretical and one more practical. We implement part of the\nLLMs on several FPGA chips to extrapolate the results to the same process\ntechnology and then we make a fair comparison of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for natural\nlanguage processing tasks, revolutionizing the field with their ability to\nunderstand and generate human-like text. In this paper, we present a\ncomprehensive survey of the several research efforts that have been presented\nfor the acceleration of transformer networks for Large Language Models using\nhardware accelerators.\n  The survey presents the frameworks that have been proposed and then performs\na qualitative and quantitative comparison regarding the technology, the\nprocessing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy\nefficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each\nframework. The main challenge in comparison is that every proposed scheme is\nimplemented on a different process technology making hard a fair comparison.\nThe main contribution of this paper is that we extrapolate the results of the\nperformance and the energy efficiency on the same technology to make a fair\ncomparison; one theoretical and one more practical. We implement part of the\nLLMs on several FPGA chips to extrapolate the results to the same process\ntechnology and then we make a fair comparison of the performance."
                },
                "authors": [
                    {
                        "name": "Nikoletta Koilia"
                    },
                    {
                        "name": "Christoforos Kachris"
                    }
                ],
                "author_detail": {
                    "name": "Christoforos Kachris"
                },
                "author": "Christoforos Kachris",
                "arxiv_comment": "https://airtable.com/appC2VwR6X4EeZ50s/shrKwchys0iktvDwk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03381v2",
                "updated": "2024-09-06T09:37:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    9,
                    37,
                    36,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T09:33:24Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "title": "CogniDual Framework: Self-Training Large Language Models within a\n  Dual-System Theoretical Framework for Improving Cognitive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogniDual Framework: Self-Training Large Language Models within a\n  Dual-System Theoretical Framework for Improving Cognitive Tasks"
                },
                "summary": "Cognitive psychology investigates perception, attention, memory, language,\nproblem-solving, decision-making, and reasoning. Kahneman's dual-system theory\nelucidates the human decision-making process, distinguishing between the rapid,\nintuitive System 1 and the deliberative, rational System 2. Recent advancements\nhave positioned large language Models (LLMs) as formidable tools nearing\nhuman-level proficiency in various cognitive tasks. Nonetheless, the presence\nof a dual-system framework analogous to human cognition in LLMs remains\nunexplored. This study introduces the \\textbf{CogniDual Framework for LLMs}\n(CFLLMs), designed to assess whether LLMs can, through self-training, evolve\nfrom deliberate deduction to intuitive responses, thereby emulating the human\nprocess of acquiring and mastering new information. Our findings reveal the\ncognitive mechanisms behind LLMs' response generation, enhancing our\nunderstanding of their capabilities in cognitive psychology. Practically,\nself-trained models can provide faster responses to certain queries, reducing\ncomputational demands during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive psychology investigates perception, attention, memory, language,\nproblem-solving, decision-making, and reasoning. Kahneman's dual-system theory\nelucidates the human decision-making process, distinguishing between the rapid,\nintuitive System 1 and the deliberative, rational System 2. Recent advancements\nhave positioned large language Models (LLMs) as formidable tools nearing\nhuman-level proficiency in various cognitive tasks. Nonetheless, the presence\nof a dual-system framework analogous to human cognition in LLMs remains\nunexplored. This study introduces the \\textbf{CogniDual Framework for LLMs}\n(CFLLMs), designed to assess whether LLMs can, through self-training, evolve\nfrom deliberate deduction to intuitive responses, thereby emulating the human\nprocess of acquiring and mastering new information. Our findings reveal the\ncognitive mechanisms behind LLMs' response generation, enhancing our\nunderstanding of their capabilities in cognitive psychology. Practically,\nself-trained models can provide faster responses to certain queries, reducing\ncomputational demands during inference."
                },
                "authors": [
                    {
                        "name": "Yongxin Deng"
                    },
                    {
                        "name": "Xihe Qiu"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Jing Pan"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Wei Chu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chu"
                },
                "arxiv_affiliation": "INF Technology",
                "author": "Wei Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03375v1",
                "updated": "2024-09-05T09:27:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    27,
                    5,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T09:27:05Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    27,
                    5,
                    3,
                    249,
                    0
                ],
                "title": "Leveraging Large Language Models through Natural Language Processing to\n  provide interpretable Machine Learning predictions of mental deterioration in\n  real time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models through Natural Language Processing to\n  provide interpretable Machine Learning predictions of mental deterioration in\n  real time"
                },
                "summary": "Based on official estimates, 50 million people worldwide are affected by\ndementia, and this number increases by 10 million new patients every year.\nWithout a cure, clinical prognostication and early intervention represent the\nmost effective ways to delay its progression. To this end, Artificial\nIntelligence and computational linguistics can be exploited for natural\nlanguage analysis, personalized assessment, monitoring, and treatment. However,\ntraditional approaches need more semantic knowledge management and\nexplicability capabilities. Moreover, using Large Language Models (LLMs) for\ncognitive decline diagnosis is still scarce, even though these models represent\nthe most advanced way for clinical-patient communication using intelligent\nsystems. Consequently, we leverage an LLM using the latest Natural Language\nProcessing (NLP) techniques in a chatbot solution to provide interpretable\nMachine Learning prediction of cognitive decline in real-time.\nLinguistic-conceptual features are exploited for appropriate natural language\nanalysis. Through explainability, we aim to fight potential biases of the\nmodels and improve their potential to help clinical workers in their diagnosis\ndecisions. More in detail, the proposed pipeline is composed of (i) data\nextraction employing NLP-based prompt engineering; (ii) stream-based data\nprocessing including feature engineering, analysis, and selection; (iii)\nreal-time classification; and (iv) the explainability dashboard to provide\nvisual and natural language descriptions of the prediction outcome.\nClassification results exceed 80 % in all evaluation metrics, with a recall\nvalue for the mental deterioration class about 85 %. To sum up, we contribute\nwith an affordable, flexible, non-invasive, personalized diagnostic system to\nthis work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on official estimates, 50 million people worldwide are affected by\ndementia, and this number increases by 10 million new patients every year.\nWithout a cure, clinical prognostication and early intervention represent the\nmost effective ways to delay its progression. To this end, Artificial\nIntelligence and computational linguistics can be exploited for natural\nlanguage analysis, personalized assessment, monitoring, and treatment. However,\ntraditional approaches need more semantic knowledge management and\nexplicability capabilities. Moreover, using Large Language Models (LLMs) for\ncognitive decline diagnosis is still scarce, even though these models represent\nthe most advanced way for clinical-patient communication using intelligent\nsystems. Consequently, we leverage an LLM using the latest Natural Language\nProcessing (NLP) techniques in a chatbot solution to provide interpretable\nMachine Learning prediction of cognitive decline in real-time.\nLinguistic-conceptual features are exploited for appropriate natural language\nanalysis. Through explainability, we aim to fight potential biases of the\nmodels and improve their potential to help clinical workers in their diagnosis\ndecisions. More in detail, the proposed pipeline is composed of (i) data\nextraction employing NLP-based prompt engineering; (ii) stream-based data\nprocessing including feature engineering, analysis, and selection; (iii)\nreal-time classification; and (iv) the explainability dashboard to provide\nvisual and natural language descriptions of the prediction outcome.\nClassification results exceed 80 % in all evaluation metrics, with a recall\nvalue for the mental deterioration class about 85 %. To sum up, we contribute\nwith an affordable, flexible, non-invasive, personalized diagnostic system to\nthis work."
                },
                "authors": [
                    {
                        "name": "Francisco de Arriba-Prez"
                    },
                    {
                        "name": "Silvia Garca-Mndez"
                    }
                ],
                "author_detail": {
                    "name": "Silvia Garca-Mndez"
                },
                "author": "Silvia Garca-Mndez",
                "arxiv_doi": "10.1007/s13369-024-09508-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s13369-024-09508-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03368v1",
                "updated": "2024-09-05T09:14:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    14,
                    44,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T09:14:44Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    14,
                    44,
                    3,
                    249,
                    0
                ],
                "title": "Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and\n  High-Performance Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and\n  High-Performance Applications"
                },
                "summary": "Spiking Neural Networks (SNNs) have emerged as a promising substitute for\nArtificial Neural Networks (ANNs) due to their advantages of fast inference and\nlow power consumption. However, the lack of efficient training algorithms has\nhindered their widespread adoption. Existing supervised learning algorithms for\nSNNs require significantly more memory and time than their ANN counterparts.\nEven commonly used ANN-SNN conversion methods necessitate re-training of ANNs\nto enhance conversion efficiency, incurring additional computational costs. To\naddress these challenges, we propose a novel training-free ANN-SNN conversion\npipeline. Our approach directly converts pre-trained ANN models into\nhigh-performance SNNs without additional training. The conversion pipeline\nincludes a local-learning-based threshold balancing algorithm, which enables\nefficient calculation of the optimal thresholds and fine-grained adjustment of\nthreshold value by channel-wise scaling. We demonstrate the scalability of our\nframework across three typical computer vision tasks: image classification,\nsemantic segmentation, and object detection. This showcases its applicability\nto both classification and regression tasks. Moreover, we have evaluated the\nenergy consumption of the converted SNNs, demonstrating their superior\nlow-power advantage compared to conventional ANNs. Our training-free algorithm\noutperforms existing methods, highlighting its practical applicability and\nefficiency. This approach simplifies the deployment of SNNs by leveraging\nopen-source pre-trained ANN models and neuromorphic hardware, enabling fast,\nlow-power inference with negligible performance reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) have emerged as a promising substitute for\nArtificial Neural Networks (ANNs) due to their advantages of fast inference and\nlow power consumption. However, the lack of efficient training algorithms has\nhindered their widespread adoption. Existing supervised learning algorithms for\nSNNs require significantly more memory and time than their ANN counterparts.\nEven commonly used ANN-SNN conversion methods necessitate re-training of ANNs\nto enhance conversion efficiency, incurring additional computational costs. To\naddress these challenges, we propose a novel training-free ANN-SNN conversion\npipeline. Our approach directly converts pre-trained ANN models into\nhigh-performance SNNs without additional training. The conversion pipeline\nincludes a local-learning-based threshold balancing algorithm, which enables\nefficient calculation of the optimal thresholds and fine-grained adjustment of\nthreshold value by channel-wise scaling. We demonstrate the scalability of our\nframework across three typical computer vision tasks: image classification,\nsemantic segmentation, and object detection. This showcases its applicability\nto both classification and regression tasks. Moreover, we have evaluated the\nenergy consumption of the converted SNNs, demonstrating their superior\nlow-power advantage compared to conventional ANNs. Our training-free algorithm\noutperforms existing methods, highlighting its practical applicability and\nefficiency. This approach simplifies the deployment of SNNs by leveraging\nopen-source pre-trained ANN models and neuromorphic hardware, enabling fast,\nlow-power inference with negligible performance reduction."
                },
                "authors": [
                    {
                        "name": "Tong Bu"
                    },
                    {
                        "name": "Maohua Li"
                    },
                    {
                        "name": "Zhaofei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaofei Yu"
                },
                "author": "Zhaofei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02634v2",
                "updated": "2024-09-05T09:11:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    11,
                    25,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T11:55:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    55,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion\n  Dependency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion\n  Dependency"
                },
                "summary": "With the introduction of diffusion-based video generation techniques,\naudio-conditioned human video generation has recently achieved significant\nbreakthroughs in both the naturalness of motion and the synthesis of portrait\ndetails. Due to the limited control of audio signals in driving human motion,\nexisting methods often add auxiliary spatial signals to stabilize movements,\nwhich may compromise the naturalness and freedom of motion. In this paper, we\npropose an end-to-end audio-only conditioned video diffusion model named Loopy.\nSpecifically, we designed an inter- and intra-clip temporal module and an\naudio-to-latents module, enabling the model to leverage long-term motion\ninformation from the data to learn natural motion patterns and improving\naudio-portrait movement correlation. This method removes the need for manually\nspecified spatial motion templates used in existing methods to constrain motion\nduring inference. Extensive experiments show that Loopy outperforms recent\naudio-driven portrait diffusion models, delivering more lifelike and\nhigh-quality results across various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the introduction of diffusion-based video generation techniques,\naudio-conditioned human video generation has recently achieved significant\nbreakthroughs in both the naturalness of motion and the synthesis of portrait\ndetails. Due to the limited control of audio signals in driving human motion,\nexisting methods often add auxiliary spatial signals to stabilize movements,\nwhich may compromise the naturalness and freedom of motion. In this paper, we\npropose an end-to-end audio-only conditioned video diffusion model named Loopy.\nSpecifically, we designed an inter- and intra-clip temporal module and an\naudio-to-latents module, enabling the model to leverage long-term motion\ninformation from the data to learn natural motion patterns and improving\naudio-portrait movement correlation. This method removes the need for manually\nspecified spatial motion templates used in existing methods to constrain motion\nduring inference. Extensive experiments show that Loopy outperforms recent\naudio-driven portrait diffusion models, delivering more lifelike and\nhigh-quality results across various scenarios."
                },
                "authors": [
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Chao Liang"
                    },
                    {
                        "name": "Jiaqi Yang"
                    },
                    {
                        "name": "Gaojie Lin"
                    },
                    {
                        "name": "Tianyun Zhong"
                    },
                    {
                        "name": "Yanbo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yanbo Zheng"
                },
                "author": "Yanbo Zheng",
                "arxiv_comment": "Homepage: https://loopyavatar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03363v1",
                "updated": "2024-09-05T09:10:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    10,
                    38,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T09:10:38Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    10,
                    38,
                    3,
                    249,
                    0
                ],
                "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding"
                },
                "summary": "The training data in large language models is key to their success, but it\nalso presents privacy and security risks, as it may contain sensitive\ninformation. Detecting pre-training data is crucial for mitigating these\nconcerns. Existing methods typically analyze target text in isolation or solely\nwith non-member contexts, overlooking potential insights from simultaneously\nconsidering both member and non-member contexts. While previous work suggested\nthat member contexts provide little information due to the minor distributional\nshift they induce, our analysis reveals that these subtle shifts can be\neffectively leveraged when contrasted with non-member contexts. In this paper,\nwe propose Con-ReCall, a novel approach that leverages the asymmetric\ndistributional shifts induced by member and non-member contexts through\ncontrastive decoding, amplifying subtle differences to enhance membership\ninference. Extensive empirical evaluations demonstrate that Con-ReCall achieves\nstate-of-the-art performance on the WikiMIA benchmark and is robust against\nvarious text manipulation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training data in large language models is key to their success, but it\nalso presents privacy and security risks, as it may contain sensitive\ninformation. Detecting pre-training data is crucial for mitigating these\nconcerns. Existing methods typically analyze target text in isolation or solely\nwith non-member contexts, overlooking potential insights from simultaneously\nconsidering both member and non-member contexts. While previous work suggested\nthat member contexts provide little information due to the minor distributional\nshift they induce, our analysis reveals that these subtle shifts can be\neffectively leveraged when contrasted with non-member contexts. In this paper,\nwe propose Con-ReCall, a novel approach that leverages the asymmetric\ndistributional shifts induced by member and non-member contexts through\ncontrastive decoding, amplifying subtle differences to enhance membership\ninference. Extensive empirical evaluations demonstrate that Con-ReCall achieves\nstate-of-the-art performance on the WikiMIA benchmark and is robust against\nvarious text manipulation techniques."
                },
                "authors": [
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03346v1",
                "updated": "2024-09-05T08:45:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    8,
                    45,
                    44,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T08:45:44Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    8,
                    45,
                    44,
                    3,
                    249,
                    0
                ],
                "title": "Sketch: A Toolkit for Streamlining LLM Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sketch: A Toolkit for Streamlining LLM Operations"
                },
                "summary": "Large language models (LLMs) represented by GPT family have achieved\nremarkable success. The characteristics of LLMs lie in their ability to\naccommodate a wide range of tasks through a generative approach. However, the\nflexibility of their output format poses challenges in controlling and\nharnessing the model's outputs, thereby constraining the application of LLMs in\nvarious domains. In this work, we present Sketch, an innovative toolkit\ndesigned to streamline LLM operations across diverse fields. Sketch comprises\nthe following components: (1) a suite of task description schemas and prompt\ntemplates encompassing various NLP tasks; (2) a user-friendly, interactive\nprocess for building structured output LLM services tailored to various NLP\ntasks; (3) an open-source dataset for output format control, along with tools\nfor dataset construction; and (4) an open-source model based on\nLLaMA3-8B-Instruct that adeptly comprehends and adheres to output formatting\ninstructions. We anticipate this initiative to bring considerable convenience\nto LLM users, achieving the goal of ''plug-and-play'' for various applications.\nThe components of Sketch will be progressively open-sourced at\nhttps://github.com/cofe-ai/Sketch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represented by GPT family have achieved\nremarkable success. The characteristics of LLMs lie in their ability to\naccommodate a wide range of tasks through a generative approach. However, the\nflexibility of their output format poses challenges in controlling and\nharnessing the model's outputs, thereby constraining the application of LLMs in\nvarious domains. In this work, we present Sketch, an innovative toolkit\ndesigned to streamline LLM operations across diverse fields. Sketch comprises\nthe following components: (1) a suite of task description schemas and prompt\ntemplates encompassing various NLP tasks; (2) a user-friendly, interactive\nprocess for building structured output LLM services tailored to various NLP\ntasks; (3) an open-source dataset for output format control, along with tools\nfor dataset construction; and (4) an open-source model based on\nLLaMA3-8B-Instruct that adeptly comprehends and adheres to output formatting\ninstructions. We anticipate this initiative to bring considerable convenience\nto LLM users, achieving the goal of ''plug-and-play'' for various applications.\nThe components of Sketch will be progressively open-sourced at\nhttps://github.com/cofe-ai/Sketch."
                },
                "authors": [
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Wenjia Ma"
                    },
                    {
                        "name": "Xuezhi Fang"
                    },
                    {
                        "name": "Yiqun Yao"
                    },
                    {
                        "name": "Naitong Yu"
                    },
                    {
                        "name": "Xuying Meng"
                    },
                    {
                        "name": "Peng Han"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Aixin Sun"
                    },
                    {
                        "name": "Yequan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yequan Wang"
                },
                "author": "Yequan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03320v1",
                "updated": "2024-09-05T07:49:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    49,
                    21,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T07:49:21Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    49,
                    21,
                    3,
                    249,
                    0
                ],
                "title": "YOLO-PPA based Efficient Traffic Sign Detection for Cruise Control in\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLO-PPA based Efficient Traffic Sign Detection for Cruise Control in\n  Autonomous Driving"
                },
                "summary": "It is very important to detect traffic signs efficiently and accurately in\nautonomous driving systems. However, the farther the distance, the smaller the\ntraffic signs. Existing object detection algorithms can hardly detect these\nsmall scaled signs.In addition, the performance of embedded devices on vehicles\nlimits the scale of detection models.To address these challenges, a YOLO PPA\nbased traffic sign detection algorithm is proposed in this paper.The\nexperimental results on the GTSDB dataset show that compared to the original\nYOLO, the proposed method improves inference efficiency by 11.2%. The mAP 50 is\nalso improved by 93.2%, which demonstrates the effectiveness of the proposed\nYOLO PPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is very important to detect traffic signs efficiently and accurately in\nautonomous driving systems. However, the farther the distance, the smaller the\ntraffic signs. Existing object detection algorithms can hardly detect these\nsmall scaled signs.In addition, the performance of embedded devices on vehicles\nlimits the scale of detection models.To address these challenges, a YOLO PPA\nbased traffic sign detection algorithm is proposed in this paper.The\nexperimental results on the GTSDB dataset show that compared to the original\nYOLO, the proposed method improves inference efficiency by 11.2%. The mAP 50 is\nalso improved by 93.2%, which demonstrates the effectiveness of the proposed\nYOLO PPA."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Wenqing Zhang"
                    },
                    {
                        "name": "Chaoyi Tan"
                    },
                    {
                        "name": "Xiangtian Li"
                    },
                    {
                        "name": "Qianyi Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qianyi Sun"
                },
                "author": "Qianyi Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10598v2",
                "updated": "2024-09-05T07:45:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    45,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2023-08-21T09:56:37Z",
                "published_parsed": [
                    2023,
                    8,
                    21,
                    9,
                    56,
                    37,
                    0,
                    233,
                    0
                ],
                "title": "Inferring Power Grid Information with Power Line Communications: Review\n  and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Power Grid Information with Power Line Communications: Review\n  and Insights"
                },
                "summary": "High-frequency signals were widely studied in the last decade to identify\ngrid and channel conditions in power lines. PLMs operating on the grid's\nphysical layer are capable of transmitting such signals to infer information\nabout the medium. When applied to the electrical grid, one of the key\nadvantages of PLC is its capacity to use signals to provide information about\nthe grid itself. This makes PLC an ideal communication technology for smart\ngrid applications, particularly in the realms of grid monitoring and\nsurveillance. In this paper, we focus on PLC grid information inference and\nprovide several contributions: a classification of PLC-based applications, a\nreview of the relevant literature, and insights to further advance the field.\nOur research identified contributions addressing PLMs for three main grid\ninformation inference applications: topology inference, anomaly detection, and\ngrid cybersecurity. We utilize the outcome of our review to shed light on the\ncurrent limitations of the research contributions and suggest future research\ndirections in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-frequency signals were widely studied in the last decade to identify\ngrid and channel conditions in power lines. PLMs operating on the grid's\nphysical layer are capable of transmitting such signals to infer information\nabout the medium. When applied to the electrical grid, one of the key\nadvantages of PLC is its capacity to use signals to provide information about\nthe grid itself. This makes PLC an ideal communication technology for smart\ngrid applications, particularly in the realms of grid monitoring and\nsurveillance. In this paper, we focus on PLC grid information inference and\nprovide several contributions: a classification of PLC-based applications, a\nreview of the relevant literature, and insights to further advance the field.\nOur research identified contributions addressing PLMs for three main grid\ninformation inference applications: topology inference, anomaly detection, and\ngrid cybersecurity. We utilize the outcome of our review to shed light on the\ncurrent limitations of the research contributions and suggest future research\ndirections in this field."
                },
                "authors": [
                    {
                        "name": "Javier Hernandez Fernandez"
                    },
                    {
                        "name": "Abdulah Jarouf"
                    },
                    {
                        "name": "Aymen Omri"
                    },
                    {
                        "name": "Roberto Di Pietro"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Di Pietro"
                },
                "author": "Roberto Di Pietro",
                "arxiv_comment": "Computer Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02727v2",
                "updated": "2024-09-05T07:17:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    17,
                    59,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T14:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    1,
                    48,
                    2,
                    248,
                    0
                ],
                "title": "Pooling And Attention: What Are Effective Designs For LLM-Based\n  Embedding Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pooling And Attention: What Are Effective Designs For LLM-Based\n  Embedding Models?"
                },
                "summary": "The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "https://github.com/yixuantt/PoolingAndAttn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03296v1",
                "updated": "2024-09-05T07:06:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    6,
                    14,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T07:06:14Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    6,
                    14,
                    3,
                    249,
                    0
                ],
                "title": "An Efficient Two-Dimensional Functional Mixed-Effect Model Framework for\n  Repeatedly Measured Functional Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Two-Dimensional Functional Mixed-Effect Model Framework for\n  Repeatedly Measured Functional Data"
                },
                "summary": "With the rapid development of wearable device technologies, accelerometers\ncan record minute-by-minute physical activity for consecutive days, which\nprovides important insight into a dynamic association between the intensity of\nphysical activity and mental health outcomes for large-scale population\nstudies. Using Shanghai school adolescent cohort we estimate the effect of\nhealth assessment results on physical activity profiles recorded by\naccelerometers throughout a week, which is recognized as repeatedly measured\nfunctional data. To achieve this goal, we propose an innovative two-dimensional\nfunctional mixed-effect model (2dFMM) for the specialized data, which smoothly\nvaries over longitudinal day observations with covariate-dependent mean and\ncovariance functions. The modeling framework characterizes the longitudinal and\nfunctional structures while incorporating two-dimensional fixed effects for\ncovariates of interest. We also develop a fast three-stage estimation procedure\nto provide accurate fixed-effect inference for model interpretability and\nimprove computational efficiency when encountering large datasets. We find\nstrong evidence of intraday and interday varying significant associations\nbetween physical activity and mental health assessments among our cohort\npopulation, which shed light on possible intervention strategies targeting\ndaily physical activity patterns to improve school adolescent mental health.\nOur method is also used in environmental data to illustrate the wide\napplicability. Supplementary materials for this article are available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of wearable device technologies, accelerometers\ncan record minute-by-minute physical activity for consecutive days, which\nprovides important insight into a dynamic association between the intensity of\nphysical activity and mental health outcomes for large-scale population\nstudies. Using Shanghai school adolescent cohort we estimate the effect of\nhealth assessment results on physical activity profiles recorded by\naccelerometers throughout a week, which is recognized as repeatedly measured\nfunctional data. To achieve this goal, we propose an innovative two-dimensional\nfunctional mixed-effect model (2dFMM) for the specialized data, which smoothly\nvaries over longitudinal day observations with covariate-dependent mean and\ncovariance functions. The modeling framework characterizes the longitudinal and\nfunctional structures while incorporating two-dimensional fixed effects for\ncovariates of interest. We also develop a fast three-stage estimation procedure\nto provide accurate fixed-effect inference for model interpretability and\nimprove computational efficiency when encountering large datasets. We find\nstrong evidence of intraday and interday varying significant associations\nbetween physical activity and mental health assessments among our cohort\npopulation, which shed light on possible intervention strategies targeting\ndaily physical activity patterns to improve school adolescent mental health.\nOur method is also used in environmental data to illustrate the wide\napplicability. Supplementary materials for this article are available online."
                },
                "authors": [
                    {
                        "name": "Cheng Cao"
                    },
                    {
                        "name": "Jiguo Cao"
                    },
                    {
                        "name": "Hao Pan"
                    },
                    {
                        "name": "Yunting Zhang"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Xinyue Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinyue Li"
                },
                "author": "Xinyue Li",
                "arxiv_comment": "50 pages, 8 figures in main, 6 figures in supp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03295v1",
                "updated": "2024-09-05T07:03:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    3,
                    23,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T07:03:23Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    3,
                    23,
                    3,
                    249,
                    0
                ],
                "title": "N-gram Prediction and Word Difference Representations for Language\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-gram Prediction and Word Difference Representations for Language\n  Modeling"
                },
                "summary": "Causal language modeling (CLM) serves as the foundational framework\nunderpinning remarkable successes of recent large language models (LLMs).\nDespite its success, the training approach for next word prediction poses a\npotential risk of causing the model to overly focus on local dependencies\nwithin a sentence. While prior studies have been introduced to predict future N\nwords simultaneously, they were primarily applied to tasks such as masked\nlanguage modeling (MLM) and neural machine translation (NMT). In this study, we\nintroduce a simple N-gram prediction framework for the CLM task. Moreover, we\nintroduce word difference representation (WDR) as a surrogate and\ncontextualized target representation during model training on the basis of\nN-gram prediction framework. To further enhance the quality of next word\nprediction, we propose an ensemble method that incorporates the future N words'\nprediction results. Empirical evaluations across multiple benchmark datasets\nencompassing CLM and NMT tasks demonstrate the significant advantages of our\nproposed methods over the conventional CLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal language modeling (CLM) serves as the foundational framework\nunderpinning remarkable successes of recent large language models (LLMs).\nDespite its success, the training approach for next word prediction poses a\npotential risk of causing the model to overly focus on local dependencies\nwithin a sentence. While prior studies have been introduced to predict future N\nwords simultaneously, they were primarily applied to tasks such as masked\nlanguage modeling (MLM) and neural machine translation (NMT). In this study, we\nintroduce a simple N-gram prediction framework for the CLM task. Moreover, we\nintroduce word difference representation (WDR) as a surrogate and\ncontextualized target representation during model training on the basis of\nN-gram prediction framework. To further enhance the quality of next word\nprediction, we propose an ensemble method that incorporates the future N words'\nprediction results. Empirical evaluations across multiple benchmark datasets\nencompassing CLM and NMT tasks demonstrate the significant advantages of our\nproposed methods over the conventional CLM."
                },
                "authors": [
                    {
                        "name": "DongNyeong Heo"
                    },
                    {
                        "name": "Daniela Noemi Rim"
                    },
                    {
                        "name": "Heeyoul Choi"
                    }
                ],
                "author_detail": {
                    "name": "Heeyoul Choi"
                },
                "author": "Heeyoul Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03291v1",
                "updated": "2024-09-05T06:55:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    55,
                    13,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:55:13Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    55,
                    13,
                    3,
                    249,
                    0
                ],
                "title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated\n  Short News-Like Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated\n  Short News-Like Posts"
                },
                "summary": "With the emergence of widely available powerful LLMs, disinformation\ngenerated by large Language Models (LLMs) has become a major concern.\nHistorically, LLM detectors have been touted as a solution, but their\neffectiveness in the real world is still to be proven. In this paper, we focus\non an important setting in information operations -- short news-like posts\ngenerated by moderately sophisticated attackers.\n  We demonstrate that existing LLM detectors, whether zero-shot or\npurpose-trained, are not ready for real-world use in that setting. All tested\nzero-shot detectors perform inconsistently with prior benchmarks and are highly\nvulnerable to sampling temperature increase, a trivial attack absent from\nrecent benchmarks. A purpose-trained detector generalizing across LLMs and\nunseen attacks can be developed, but it fails to generalize to new\nhuman-written texts.\n  We argue that the former indicates domain-specific benchmarking is needed,\nwhile the latter suggests a trade-off between the adversarial evasion\nresilience and overfitting to the reference human text, with both needing\nevaluation in benchmarks and currently absent. We believe this suggests a\nre-consideration of current LLM detector benchmarking approaches and provides a\ndynamically extensible benchmark to allow it\n(https://github.com/Reliable-Information-Lab-HEVS/dynamic_llm_detector_benchmark).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of widely available powerful LLMs, disinformation\ngenerated by large Language Models (LLMs) has become a major concern.\nHistorically, LLM detectors have been touted as a solution, but their\neffectiveness in the real world is still to be proven. In this paper, we focus\non an important setting in information operations -- short news-like posts\ngenerated by moderately sophisticated attackers.\n  We demonstrate that existing LLM detectors, whether zero-shot or\npurpose-trained, are not ready for real-world use in that setting. All tested\nzero-shot detectors perform inconsistently with prior benchmarks and are highly\nvulnerable to sampling temperature increase, a trivial attack absent from\nrecent benchmarks. A purpose-trained detector generalizing across LLMs and\nunseen attacks can be developed, but it fails to generalize to new\nhuman-written texts.\n  We argue that the former indicates domain-specific benchmarking is needed,\nwhile the latter suggests a trade-off between the adversarial evasion\nresilience and overfitting to the reference human text, with both needing\nevaluation in benchmarks and currently absent. We believe this suggests a\nre-consideration of current LLM detector benchmarking approaches and provides a\ndynamically extensible benchmark to allow it\n(https://github.com/Reliable-Information-Lab-HEVS/dynamic_llm_detector_benchmark)."
                },
                "authors": [
                    {
                        "name": "Henrique Da Silva Gameiro"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    }
                ],
                "author_detail": {
                    "name": "Ljiljana Dolamic"
                },
                "author": "Ljiljana Dolamic",
                "arxiv_comment": "20 pages, 7 tables, 13 figures, under consideration for EMNLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03284v1",
                "updated": "2024-09-05T06:49:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    49,
                    14,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:49:14Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    49,
                    14,
                    3,
                    249,
                    0
                ],
                "title": "iText2KG: Incremental Knowledge Graphs Construction Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iText2KG: Incremental Knowledge Graphs Construction Using Large Language\n  Models"
                },
                "summary": "Most available data is unstructured, making it challenging to access valuable\ninformation. Automatically building Knowledge Graphs (KGs) is crucial for\nstructuring data and making it accessible, allowing users to search for\ninformation effectively. KGs also facilitate insights, inference, and\nreasoning. Traditional NLP methods, such as named entity recognition and\nrelation extraction, are key in information retrieval but face limitations,\nincluding the use of predefined entity types and the need for supervised\nlearning. Current research leverages large language models' capabilities, such\nas zero- or few-shot learning. However, unresolved and semantically duplicated\nentities and relations still pose challenges, leading to inconsistent graphs\nand requiring extensive post-processing. Additionally, most approaches are\ntopic-dependent. In this paper, we propose iText2KG, a method for incremental,\ntopic-independent KG construction without post-processing. This plug-and-play,\nzero-shot method is applicable across a wide range of KG construction scenarios\nand comprises four modules: Document Distiller, Incremental Entity Extractor,\nIncremental Relation Extractor, and Graph Integrator and Visualization. Our\nmethod demonstrates superior performance compared to baseline methods across\nthree scenarios: converting scientific papers to graphs, websites to graphs,\nand CVs to graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most available data is unstructured, making it challenging to access valuable\ninformation. Automatically building Knowledge Graphs (KGs) is crucial for\nstructuring data and making it accessible, allowing users to search for\ninformation effectively. KGs also facilitate insights, inference, and\nreasoning. Traditional NLP methods, such as named entity recognition and\nrelation extraction, are key in information retrieval but face limitations,\nincluding the use of predefined entity types and the need for supervised\nlearning. Current research leverages large language models' capabilities, such\nas zero- or few-shot learning. However, unresolved and semantically duplicated\nentities and relations still pose challenges, leading to inconsistent graphs\nand requiring extensive post-processing. Additionally, most approaches are\ntopic-dependent. In this paper, we propose iText2KG, a method for incremental,\ntopic-independent KG construction without post-processing. This plug-and-play,\nzero-shot method is applicable across a wide range of KG construction scenarios\nand comprises four modules: Document Distiller, Incremental Entity Extractor,\nIncremental Relation Extractor, and Graph Integrator and Visualization. Our\nmethod demonstrates superior performance compared to baseline methods across\nthree scenarios: converting scientific papers to graphs, websites to graphs,\nand CVs to graphs."
                },
                "authors": [
                    {
                        "name": "Yassir Lairgi"
                    },
                    {
                        "name": "Ludovic Moncla"
                    },
                    {
                        "name": "Rmy Cazabet"
                    },
                    {
                        "name": "Khalid Benabdeslem"
                    },
                    {
                        "name": "Pierre Clau"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Clau"
                },
                "author": "Pierre Clau",
                "arxiv_comment": "Accepted at The International Web Information Systems Engineering\n  conference (the WISE conference) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16570v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16570v5",
                "updated": "2024-09-05T06:47:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    47,
                    18,
                    3,
                    249,
                    0
                ],
                "published": "2023-11-28T07:28:08Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    7,
                    28,
                    8,
                    1,
                    332,
                    0
                ],
                "title": "Epistemic Limits of Empirical Finance: Causal Reductionism and\n  Self-Reference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epistemic Limits of Empirical Finance: Causal Reductionism and\n  Self-Reference"
                },
                "summary": "The clarion call for causal reduction in the study of capital markets is\nintensifying. However, in self-referencing and open systems such as capital\nmarkets, the idea of unidirectional causation (if applicable) may be limiting\nat best, and unstable or fallacious at worst. In this work, we critically\nassess the use of scientific deduction and causal inference within the study of\nempirical finance and financial econometrics. We then demonstrate the idea of\ncompeting causal chains using a toy model adapted from ecological predator/prey\nrelationships. From this, we develop the alternative view that the study of\nempirical finance, and the risks contained therein, may be better appreciated\nonce we admit that our current arsenal of quantitative finance tools may be\nlimited to ex post causal inference under popular assumptions. Where these\nassumptions are challenged, for example in a recognizable reflexive context,\nthe prescription of unidirectional causation proves deeply problematic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The clarion call for causal reduction in the study of capital markets is\nintensifying. However, in self-referencing and open systems such as capital\nmarkets, the idea of unidirectional causation (if applicable) may be limiting\nat best, and unstable or fallacious at worst. In this work, we critically\nassess the use of scientific deduction and causal inference within the study of\nempirical finance and financial econometrics. We then demonstrate the idea of\ncompeting causal chains using a toy model adapted from ecological predator/prey\nrelationships. From this, we develop the alternative view that the study of\nempirical finance, and the risks contained therein, may be better appreciated\nonce we admit that our current arsenal of quantitative finance tools may be\nlimited to ex post causal inference under popular assumptions. Where these\nassumptions are challenged, for example in a recognizable reflexive context,\nthe prescription of unidirectional causation proves deeply problematic."
                },
                "authors": [
                    {
                        "name": "Daniel Polakow"
                    },
                    {
                        "name": "Tim Gebbie"
                    },
                    {
                        "name": "Emlyn Flint"
                    }
                ],
                "author_detail": {
                    "name": "Emlyn Flint"
                },
                "author": "Emlyn Flint",
                "arxiv_comment": "9 pages, 2 figures, streamlined discuss and additional references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16570v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16570v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91B28",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03277v1",
                "updated": "2024-09-05T06:41:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    41,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:41:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    41,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding"
                },
                "summary": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, the application of alignment training\nwithin the chart domain is still underexplored. To address this, we propose\nChartMoE, which employs the mixture of expert (MoE) architecture to replace the\ntraditional linear projector to bridge the modality gap. Specifically, we train\nmultiple linear connectors through distinct alignment tasks, which are utilized\nas the foundational initialization parameters for different experts.\nAdditionally, we introduce ChartMoE-Align, a dataset with over 900K\nchart-table-JSON-code quadruples to conduct three alignment tasks\n(chart-table/JSON/code). Combined with the vanilla connector, we initialize\ndifferent experts in four distinct ways and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, the application of alignment training\nwithin the chart domain is still underexplored. To address this, we propose\nChartMoE, which employs the mixture of expert (MoE) architecture to replace the\ntraditional linear projector to bridge the modality gap. Specifically, we train\nmultiple linear connectors through distinct alignment tasks, which are utilized\nas the foundational initialization parameters for different experts.\nAdditionally, we introduce ChartMoE-Align, a dataset with over 900K\nchart-table-JSON-code quadruples to conduct three alignment tasks\n(chart-table/JSON/code). Combined with the vanilla connector, we initialize\ndifferent experts in four distinct ways and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark."
                },
                "authors": [
                    {
                        "name": "Zhengzhuo Xu"
                    },
                    {
                        "name": "Bowen Qu"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Sinan Du"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16567v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16567v3",
                "updated": "2024-09-05T06:34:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    34,
                    11,
                    3,
                    249,
                    0
                ],
                "published": "2024-02-26T13:46:51Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    13,
                    46,
                    51,
                    0,
                    57,
                    0
                ],
                "title": "Aligning Large Language Models to a Domain-specific Graph Database for\n  NL2GQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models to a Domain-specific Graph Database for\n  NL2GQL"
                },
                "summary": "Graph Databases (Graph DB) find extensive application across diverse domains\nsuch as finance, social networks, and medicine. Yet, the translation of Natural\nLanguage (NL) into the Graph Query Language (GQL), referred to as NL2GQL, poses\nsignificant challenges owing to its intricate and specialized nature. Some\napproaches have sought to utilize Large Language Models (LLMs) to address\nanalogous tasks like text2SQL. Nonetheless, in the realm of NL2GQL tasks\ntailored to a particular domain, the absence of domain-specific NL-GQL data\npairs adds complexity to aligning LLMs with the graph DB. To tackle this\nchallenge, we present a well-defined pipeline. Initially, we utilize ChatGPT to\ngenerate NL-GQL data pairs, leveraging the provided graph DB with\nself-instruction. Subsequently, we employ the generated data to fine-tune LLMs,\nensuring alignment between LLMs and the graph DB. Moreover, we find the\nimportance of relevant schema in efficiently generating accurate GQLs. Thus, we\nintroduce a method to extract relevant schema as the input context. We evaluate\nour method using two carefully constructed datasets derived from graph DBs in\nthe finance and medicine domains, named FinGQL and MediGQL. Experimental\nresults reveal that our approach significantly outperforms a set of baseline\nmethods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and\n7.09 absolute points on EX for FinGQL and MediGQL, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Databases (Graph DB) find extensive application across diverse domains\nsuch as finance, social networks, and medicine. Yet, the translation of Natural\nLanguage (NL) into the Graph Query Language (GQL), referred to as NL2GQL, poses\nsignificant challenges owing to its intricate and specialized nature. Some\napproaches have sought to utilize Large Language Models (LLMs) to address\nanalogous tasks like text2SQL. Nonetheless, in the realm of NL2GQL tasks\ntailored to a particular domain, the absence of domain-specific NL-GQL data\npairs adds complexity to aligning LLMs with the graph DB. To tackle this\nchallenge, we present a well-defined pipeline. Initially, we utilize ChatGPT to\ngenerate NL-GQL data pairs, leveraging the provided graph DB with\nself-instruction. Subsequently, we employ the generated data to fine-tune LLMs,\nensuring alignment between LLMs and the graph DB. Moreover, we find the\nimportance of relevant schema in efficiently generating accurate GQLs. Thus, we\nintroduce a method to extract relevant schema as the input context. We evaluate\nour method using two carefully constructed datasets derived from graph DBs in\nthe finance and medicine domains, named FinGQL and MediGQL. Experimental\nresults reveal that our approach significantly outperforms a set of baseline\nmethods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and\n7.09 absolute points on EX for FinGQL and MediGQL, respectively."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Liang"
                    },
                    {
                        "name": "Keren Tan"
                    },
                    {
                        "name": "Tingyu Xie"
                    },
                    {
                        "name": "Wenbiao Tao"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Yunshi Lan"
                    },
                    {
                        "name": "Weining Qian"
                    }
                ],
                "author_detail": {
                    "name": "Weining Qian"
                },
                "author": "Weining Qian",
                "arxiv_comment": "13 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16567v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16567v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15966v2",
                "updated": "2024-09-05T06:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    33,
                    31,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-28T17:38:44Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    44,
                    2,
                    241,
                    0
                ],
                "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding"
                },
                "summary": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM."
                },
                "authors": [
                    {
                        "name": "Yuan Tang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Qiao Yu"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Yixue Hao"
                    },
                    {
                        "name": "Long Hu"
                    },
                    {
                        "name": "Min Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min Chen"
                },
                "author": "Min Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03274v2",
                "updated": "2024-09-06T10:31:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    31,
                    7,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T06:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    31,
                    37,
                    3,
                    249,
                    0
                ],
                "title": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures."
                },
                "authors": [
                    {
                        "name": "Jing Cui"
                    },
                    {
                        "name": "Yishi Xu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Shuchang Zhou"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03272v1",
                "updated": "2024-09-05T06:30:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    30,
                    1,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:30:01Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    30,
                    1,
                    3,
                    249,
                    0
                ],
                "title": "OccLLaMA: An Occupancy-Language-Action Generative World Model for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OccLLaMA: An Occupancy-Language-Action Generative World Model for\n  Autonomous Driving"
                },
                "summary": "The rise of multi-modal large language models(MLLMs) has spurred their\napplications in autonomous driving. Recent MLLM-based methods perform action by\nlearning a direct mapping from perception to action, neglecting the dynamics of\nthe world and the relations between action and world dynamics. In contrast,\nhuman beings possess world model that enables them to simulate the future\nstates based on 3D internal visual representation and plan actions accordingly.\nTo this end, we propose OccLLaMA, an occupancy-language-action generative world\nmodel, which uses semantic occupancy as a general visual representation and\nunifies vision-language-action(VLA) modalities through an autoregressive model.\nSpecifically, we introduce a novel VQVAE-like scene tokenizer to efficiently\ndiscretize and reconstruct semantic occupancy scenes, considering its sparsity\nand classes imbalance. Then, we build a unified multi-modal vocabulary for\nvision, language and action. Furthermore, we enhance LLM, specifically LLaMA,\nto perform the next token/scene prediction on the unified vocabulary to\ncomplete multiple tasks in autonomous driving. Extensive experiments\ndemonstrate that OccLLaMA achieves competitive performance across multiple\ntasks, including 4D occupancy forecasting, motion planning, and visual question\nanswering, showcasing its potential as a foundation model in autonomous\ndriving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of multi-modal large language models(MLLMs) has spurred their\napplications in autonomous driving. Recent MLLM-based methods perform action by\nlearning a direct mapping from perception to action, neglecting the dynamics of\nthe world and the relations between action and world dynamics. In contrast,\nhuman beings possess world model that enables them to simulate the future\nstates based on 3D internal visual representation and plan actions accordingly.\nTo this end, we propose OccLLaMA, an occupancy-language-action generative world\nmodel, which uses semantic occupancy as a general visual representation and\nunifies vision-language-action(VLA) modalities through an autoregressive model.\nSpecifically, we introduce a novel VQVAE-like scene tokenizer to efficiently\ndiscretize and reconstruct semantic occupancy scenes, considering its sparsity\nand classes imbalance. Then, we build a unified multi-modal vocabulary for\nvision, language and action. Furthermore, we enhance LLM, specifically LLaMA,\nto perform the next token/scene prediction on the unified vocabulary to\ncomplete multiple tasks in autonomous driving. Extensive experiments\ndemonstrate that OccLLaMA achieves competitive performance across multiple\ntasks, including 4D occupancy forecasting, motion planning, and visual question\nanswering, showcasing its potential as a foundation model in autonomous\ndriving."
                },
                "authors": [
                    {
                        "name": "Julong Wei"
                    },
                    {
                        "name": "Shanshuai Yuan"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Qingda Hu"
                    },
                    {
                        "name": "Zhongxue Gan"
                    },
                    {
                        "name": "Wenchao Ding"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Ding"
                },
                "author": "Wenchao Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03271v1",
                "updated": "2024-09-05T06:28:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    28,
                    5,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:28:05Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    28,
                    5,
                    3,
                    249,
                    0
                ],
                "title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through\n  Strategy Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through\n  Strategy Elicitation"
                },
                "summary": "The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for\nenhancing the reasoning capabilities of large language models (LLMs). However,\ndespite their widespread adoption and success, CoT methods often exhibit\ninstability due to their inability to consistently ensure the quality of\ngenerated reasoning paths, leading to sub-optimal reasoning performance. To\naddress this challenge, we propose the \\textbf{Strategic Chain-of-Thought}\n(SCoT), a novel methodology designed to refine LLM performance by integrating\nstrategic knowledge prior to generating intermediate reasoning steps. SCoT\nemploys a two-stage approach within a single prompt: first eliciting an\neffective problem-solving strategy, which is then used to guide the generation\nof high-quality CoT paths and final answers. Our experiments across eight\nchallenging reasoning datasets demonstrate significant improvements, including\na 21.05\\% increase on the GSM8K dataset and 24.13\\% on the Tracking\\_Objects\ndataset, respectively, using the Llama3-8b model. Additionally, we extend the\nSCoT framework to develop a few-shot method with automatically matched\ndemonstrations, yielding even stronger results. These findings underscore the\nefficacy of SCoT, highlighting its potential to substantially enhance LLM\nperformance in complex reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for\nenhancing the reasoning capabilities of large language models (LLMs). However,\ndespite their widespread adoption and success, CoT methods often exhibit\ninstability due to their inability to consistently ensure the quality of\ngenerated reasoning paths, leading to sub-optimal reasoning performance. To\naddress this challenge, we propose the \\textbf{Strategic Chain-of-Thought}\n(SCoT), a novel methodology designed to refine LLM performance by integrating\nstrategic knowledge prior to generating intermediate reasoning steps. SCoT\nemploys a two-stage approach within a single prompt: first eliciting an\neffective problem-solving strategy, which is then used to guide the generation\nof high-quality CoT paths and final answers. Our experiments across eight\nchallenging reasoning datasets demonstrate significant improvements, including\na 21.05\\% increase on the GSM8K dataset and 24.13\\% on the Tracking\\_Objects\ndataset, respectively, using the Llama3-8b model. Additionally, we extend the\nSCoT framework to develop a few-shot method with automatically matched\ndemonstrations, yielding even stronger results. These findings underscore the\nefficacy of SCoT, highlighting its potential to substantially enhance LLM\nperformance in complex reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Zhihu Wang"
                    },
                    {
                        "name": "Heyuan Huang"
                    },
                    {
                        "name": "Ming Fan"
                    },
                    {
                        "name": "Yubo Zhang"
                    },
                    {
                        "name": "Zhixing Wang"
                    },
                    {
                        "name": "Haijun Wang"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03267v1",
                "updated": "2024-09-05T06:24:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    24,
                    29,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:24:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    24,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "No Man is an Island: Towards Fully Automatic Programming by Code Search,\n  Code Generation and Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Man is an Island: Towards Fully Automatic Programming by Code Search,\n  Code Generation and Program Repair"
                },
                "summary": "Automatic programming attempts to minimize human intervention in the\ngeneration of executable code, and has been a long-standing challenge in the\nsoftware engineering community. To advance automatic programming, researchers\nare focusing on three primary directions: (1) code search that reuses existing\ncode snippets from external databases; (2) code generation that produces new\ncode snippets from natural language; and (3) program repair that refines\nexisting code snippets by fixing detected bugs. Despite significant\nadvancements, the effectiveness of state-of-the-art techniques is still\nlimited, such as the usability of searched code and the correctness of\ngenerated code.\n  Motivated by the real-world programming process, where developers usually use\nvarious external tools to aid their coding processes, such as code search\nengines and code testing tools, in this work, we propose \\toolname{}, an\nautomatic programming framework that leverages recent large language models\n(LLMs) to integrate the three research areas to address their inherent\nlimitations. In particular, our framework first leverages different code search\nstrategies to retrieve similar code snippets, which are then used to further\nguide the code generation process of LLMs. Our framework further validates the\nquality of generated code by compilers and test cases, and constructs repair\nprompts to query LLMs for generating correct patches. We conduct preliminary\nexperiments to demonstrate the potential of our framework, \\eg helping\nCodeLlama solve 267 programming problems with an improvement of 62.53\\%. As a\ngeneric framework, \\toolname{} can integrate various code search, generation,\nand repair tools, combining these three research areas together for the first\ntime. More importantly, it demonstrates the potential of using traditional SE\ntools to enhance the usability of LLMs in automatic programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic programming attempts to minimize human intervention in the\ngeneration of executable code, and has been a long-standing challenge in the\nsoftware engineering community. To advance automatic programming, researchers\nare focusing on three primary directions: (1) code search that reuses existing\ncode snippets from external databases; (2) code generation that produces new\ncode snippets from natural language; and (3) program repair that refines\nexisting code snippets by fixing detected bugs. Despite significant\nadvancements, the effectiveness of state-of-the-art techniques is still\nlimited, such as the usability of searched code and the correctness of\ngenerated code.\n  Motivated by the real-world programming process, where developers usually use\nvarious external tools to aid their coding processes, such as code search\nengines and code testing tools, in this work, we propose \\toolname{}, an\nautomatic programming framework that leverages recent large language models\n(LLMs) to integrate the three research areas to address their inherent\nlimitations. In particular, our framework first leverages different code search\nstrategies to retrieve similar code snippets, which are then used to further\nguide the code generation process of LLMs. Our framework further validates the\nquality of generated code by compilers and test cases, and constructs repair\nprompts to query LLMs for generating correct patches. We conduct preliminary\nexperiments to demonstrate the potential of our framework, \\eg helping\nCodeLlama solve 267 programming problems with an improvement of 62.53\\%. As a\ngeneric framework, \\toolname{} can integrate various code search, generation,\nand repair tools, combining these three research areas together for the first\ntime. More importantly, it demonstrates the potential of using traditional SE\ntools to enhance the usability of LLMs in automatic programming."
                },
                "authors": [
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Ye Shang"
                    },
                    {
                        "name": "Tongke Zhang"
                    },
                    {
                        "name": "Shengcheng Yu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02387v2",
                "updated": "2024-09-05T05:36:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    36,
                    10,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T02:30:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    30,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges"
                },
                "summary": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Keyu Chen"
                },
                "author": "Keyu Chen",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03258v1",
                "updated": "2024-09-05T05:34:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    34,
                    16,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T05:34:16Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    34,
                    16,
                    3,
                    249,
                    0
                ],
                "title": "GraphInsight: Unlocking Insights in Large Language Models for Graph\n  Structure Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphInsight: Unlocking Insights in Large Language Models for Graph\n  Structure Understanding"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated potential in\nprocessing graphs, they struggle with comprehending graphical structure\ninformation through prompts of graph description sequences, especially as the\ngraph size increases. We attribute this challenge to the uneven memory\nperformance of LLMs across different positions in graph description sequences,\nknown as ''positional biases''. To address this, we propose GraphInsight, a\nnovel framework aimed at improving LLMs' comprehension of both macro- and\nmicro-level graphical information. GraphInsight is grounded in two key\nstrategies: 1) placing critical graphical information in positions where LLMs\nexhibit stronger memory performance, and 2) investigating a lightweight\nexternal knowledge base for regions with weaker memory performance, inspired by\nretrieval-augmented generation (RAG). Moreover, GraphInsight explores\nintegrating these two strategies into LLM agent processes for composite graph\ntasks that require multi-step reasoning. Extensive empirical studies on\nbenchmarks with a wide range of evaluation tasks show that GraphInsight\nsignificantly outperforms all other graph description methods (e.g., prompting\ntechniques and reordering strategies) in understanding graph structures of\nvarying sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated potential in\nprocessing graphs, they struggle with comprehending graphical structure\ninformation through prompts of graph description sequences, especially as the\ngraph size increases. We attribute this challenge to the uneven memory\nperformance of LLMs across different positions in graph description sequences,\nknown as ''positional biases''. To address this, we propose GraphInsight, a\nnovel framework aimed at improving LLMs' comprehension of both macro- and\nmicro-level graphical information. GraphInsight is grounded in two key\nstrategies: 1) placing critical graphical information in positions where LLMs\nexhibit stronger memory performance, and 2) investigating a lightweight\nexternal knowledge base for regions with weaker memory performance, inspired by\nretrieval-augmented generation (RAG). Moreover, GraphInsight explores\nintegrating these two strategies into LLM agent processes for composite graph\ntasks that require multi-step reasoning. Extensive empirical studies on\nbenchmarks with a wide range of evaluation tasks show that GraphInsight\nsignificantly outperforms all other graph description methods (e.g., prompting\ntechniques and reordering strategies) in understanding graph structures of\nvarying sizes."
                },
                "authors": [
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Shuo Han"
                    },
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Zezhong Ding"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03257v1",
                "updated": "2024-09-05T05:31:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    31,
                    29,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T05:31:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    31,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "Understanding LLM Development Through Longitudinal Study: Insights from\n  the Open Ko-LLM Leaderboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Development Through Longitudinal Study: Insights from\n  the Open Ko-LLM Leaderboard"
                },
                "summary": "This paper conducts a longitudinal study over eleven months to address the\nlimitations of prior research on the Open Ko-LLM Leaderboard, which have relied\non empirical studies with restricted observation periods of only five months.\nBy extending the analysis duration, we aim to provide a more comprehensive\nunderstanding of the progression in developing Korean large language models\n(LLMs). Our study is guided by three primary research questions: (1) What are\nthe specific challenges in improving LLM performance across diverse tasks on\nthe Open Ko-LLM Leaderboard over time? (2) How does model size impact task\nperformance correlations across various benchmarks? (3) How have the patterns\nin leaderboard rankings shifted over time on the Open Ko-LLM Leaderboard?. By\nanalyzing 1,769 models over this period, our research offers a comprehensive\nexamination of the ongoing advancements in LLMs and the evolving nature of\nevaluation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper conducts a longitudinal study over eleven months to address the\nlimitations of prior research on the Open Ko-LLM Leaderboard, which have relied\non empirical studies with restricted observation periods of only five months.\nBy extending the analysis duration, we aim to provide a more comprehensive\nunderstanding of the progression in developing Korean large language models\n(LLMs). Our study is guided by three primary research questions: (1) What are\nthe specific challenges in improving LLM performance across diverse tasks on\nthe Open Ko-LLM Leaderboard over time? (2) How does model size impact task\nperformance correlations across various benchmarks? (3) How have the patterns\nin leaderboard rankings shifted over time on the Open Ko-LLM Leaderboard?. By\nanalyzing 1,769 models over this period, our research offers a comprehensive\nexamination of the ongoing advancements in LLMs and the evolving nature of\nevaluation frameworks."
                },
                "authors": [
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Hyeonwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeonwoo Kim"
                },
                "author": "Hyeonwoo Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02097v2",
                "updated": "2024-09-05T04:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    4,
                    53,
                    37,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:54:39Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    54,
                    39,
                    1,
                    247,
                    0
                ],
                "title": "LinFusion: 1 GPU, 1 Minute, 16K Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinFusion: 1 GPU, 1 Minute, 16K Image"
                },
                "summary": "Modern diffusion models, particularly those utilizing a Transformer-based\nUNet for denoising, rely heavily on self-attention operations to manage complex\nspatial relationships, thus achieving impressive generation performance.\nHowever, this existing paradigm faces significant challenges in generating\nhigh-resolution visual content due to its quadratic time and memory complexity\nwith respect to the number of spatial tokens. To address this limitation, we\naim at a novel linear attention mechanism as an alternative in this paper.\nSpecifically, we begin our exploration from recently introduced models with\nlinear complexity, e.g., Mamba2, RWKV6, Gated Linear Attention, etc, and\nidentify two key features-attention normalization and non-causal inference-that\nenhance high-resolution visual generation performance. Building on these\ninsights, we introduce a generalized linear attention paradigm, which serves as\na low-rank approximation of a wide spectrum of popular linear token mixers. To\nsave the training cost and better leverage pre-trained models, we initialize\nour models and distill the knowledge from pre-trained StableDiffusion (SD). We\nfind that the distilled model, termed LinFusion, achieves performance on par\nwith or superior to the original SD after only modest training, while\nsignificantly reducing time and memory complexity. Extensive experiments on\nSD-v1.5, SD-v2.1, and SD-XL demonstrate that LinFusion delivers satisfactory\nzero-shot cross-resolution generation performance, generating high-resolution\nimages like 16K resolution. Moreover, it is highly compatible with pre-trained\nSD components, such as ControlNet and IP-Adapter, requiring no adaptation\nefforts. Codes are available at https://github.com/Huage001/LinFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern diffusion models, particularly those utilizing a Transformer-based\nUNet for denoising, rely heavily on self-attention operations to manage complex\nspatial relationships, thus achieving impressive generation performance.\nHowever, this existing paradigm faces significant challenges in generating\nhigh-resolution visual content due to its quadratic time and memory complexity\nwith respect to the number of spatial tokens. To address this limitation, we\naim at a novel linear attention mechanism as an alternative in this paper.\nSpecifically, we begin our exploration from recently introduced models with\nlinear complexity, e.g., Mamba2, RWKV6, Gated Linear Attention, etc, and\nidentify two key features-attention normalization and non-causal inference-that\nenhance high-resolution visual generation performance. Building on these\ninsights, we introduce a generalized linear attention paradigm, which serves as\na low-rank approximation of a wide spectrum of popular linear token mixers. To\nsave the training cost and better leverage pre-trained models, we initialize\nour models and distill the knowledge from pre-trained StableDiffusion (SD). We\nfind that the distilled model, termed LinFusion, achieves performance on par\nwith or superior to the original SD after only modest training, while\nsignificantly reducing time and memory complexity. Extensive experiments on\nSD-v1.5, SD-v2.1, and SD-XL demonstrate that LinFusion delivers satisfactory\nzero-shot cross-resolution generation performance, generating high-resolution\nimages like 16K resolution. Moreover, it is highly compatible with pre-trained\nSD components, such as ControlNet and IP-Adapter, requiring no adaptation\nefforts. Codes are available at https://github.com/Huage001/LinFusion."
                },
                "authors": [
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Work in Progress. Codes are available at\n  https://github.com/Huage001/LinFusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03248v1",
                "updated": "2024-09-05T04:51:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    4,
                    51,
                    58,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T04:51:58Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    4,
                    51,
                    58,
                    3,
                    249,
                    0
                ],
                "title": "A Stochastic Approach to Reconstructing the Speed of Light in Cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stochastic Approach to Reconstructing the Speed of Light in Cosmology"
                },
                "summary": "The Varying Speed of Light (VSL) model describes how the speed of light in a\nvacuum changes with cosmological redshift. Despite numerous models, there is\nlittle observational evidence for this variation. While the speed of light can\nbe accurately measured by physical means, cosmological methods are rarely used.\nPrevious studies quantified the speed of light at specific redshifts using\nGaussian processes and reconstructed the redshift-dependent function $c(z)$. It\nis crucial to quantify the speed of light across varying redshifts. We use the\nlatest data on angular diameter distances $D_A(z)$ and Hubble parameters $H(z)$\nfrom baryon acoustic oscillation (BAO) and cosmic chronometer measurements in\nthe redshift interval $z\\in[0.07,1.965]$. The speed of light $c(z)$ is\ndetermined using Gaussian and deep Gaussian processes to reconstruct $H(z)$,\n$D_A(z)$, and $D^{\\prime}_A(z)$. Furthermore, we conduct comparisons across\nthree distinct models, encompassing two renowned VSL models. We get the result\nof the parameters constraints in the models (1) for the ``$c$-c\" model,\n$c_0=29492.6 \\pm^{6.2}_{5.3} \\mathrm{~km} \\mathrm{~s}^{-1}$. (2) For the\n``$c$-cl\" model, $c_0=29665.5 \\pm^{11.2}_{11.4}\\mathrm{~km} \\mathrm{~s}^{-1}$\nand $n=0.05535 \\pm^{0.00008}_{0.00007}$. (3) For the ``$c$-CPL\" model,\n$c_0=29555.7 \\pm^{13.3}_{13.2} \\mathrm{~km} \\mathrm{~s}^{-1}$ and $n=-0.0607\n\\pm 0.0001$. Based on our findings, it may be inferred that Barrow's classical\nVSL model is not a suitable fit for our data. In contrast, the widely\nrecognized Chevallier-Polarski-Linder (CPL) VSL model, under some\ncircumstances, as well as the universal ``c is constant\" model, demonstrate a\nsatisfactory ability to account for our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Varying Speed of Light (VSL) model describes how the speed of light in a\nvacuum changes with cosmological redshift. Despite numerous models, there is\nlittle observational evidence for this variation. While the speed of light can\nbe accurately measured by physical means, cosmological methods are rarely used.\nPrevious studies quantified the speed of light at specific redshifts using\nGaussian processes and reconstructed the redshift-dependent function $c(z)$. It\nis crucial to quantify the speed of light across varying redshifts. We use the\nlatest data on angular diameter distances $D_A(z)$ and Hubble parameters $H(z)$\nfrom baryon acoustic oscillation (BAO) and cosmic chronometer measurements in\nthe redshift interval $z\\in[0.07,1.965]$. The speed of light $c(z)$ is\ndetermined using Gaussian and deep Gaussian processes to reconstruct $H(z)$,\n$D_A(z)$, and $D^{\\prime}_A(z)$. Furthermore, we conduct comparisons across\nthree distinct models, encompassing two renowned VSL models. We get the result\nof the parameters constraints in the models (1) for the ``$c$-c\" model,\n$c_0=29492.6 \\pm^{6.2}_{5.3} \\mathrm{~km} \\mathrm{~s}^{-1}$. (2) For the\n``$c$-cl\" model, $c_0=29665.5 \\pm^{11.2}_{11.4}\\mathrm{~km} \\mathrm{~s}^{-1}$\nand $n=0.05535 \\pm^{0.00008}_{0.00007}$. (3) For the ``$c$-CPL\" model,\n$c_0=29555.7 \\pm^{13.3}_{13.2} \\mathrm{~km} \\mathrm{~s}^{-1}$ and $n=-0.0607\n\\pm 0.0001$. Based on our findings, it may be inferred that Barrow's classical\nVSL model is not a suitable fit for our data. In contrast, the widely\nrecognized Chevallier-Polarski-Linder (CPL) VSL model, under some\ncircumstances, as well as the universal ``c is constant\" model, demonstrate a\nsatisfactory ability to account for our findings."
                },
                "authors": [
                    {
                        "name": "Cheng-Yu Zhang"
                    },
                    {
                        "name": "Wei Hong"
                    },
                    {
                        "name": "Yu-Chen Wang"
                    },
                    {
                        "name": "Tong-Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong-Jie Zhang"
                },
                "author": "Tong-Jie Zhang",
                "arxiv_comment": "15 pages, 9 figures; Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03247v1",
                "updated": "2024-09-05T04:51:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    4,
                    51,
                    18,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T04:51:18Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    4,
                    51,
                    18,
                    3,
                    249,
                    0
                ],
                "title": "End User Authoring of Personalized Content Classifiers: Comparing\n  Example Labeling, Rule Writing, and LLM Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End User Authoring of Personalized Content Classifiers: Comparing\n  Example Labeling, Rule Writing, and LLM Prompting"
                },
                "summary": "Existing tools for laypeople to create personal classifiers often assume a\nmotivated user working uninterrupted in a single, lengthy session. However,\nusers tend to engage with social media casually, with many short sessions on an\nongoing, daily basis. To make creating personal classifiers for content\ncuration easier for such users, tools should support rapid initialization and\niterative refinement. In this work, we compare three strategies -- (1) example\nlabeling, (2) rule writing, and (3) large language model (LLM) prompting -- for\nend users to build personal content classifiers. From an experiment with 37\nnon-programmers tasked with creating personalized comment moderation filters,\nwe found that with LLM prompting, participants reached 95\\% of peak performance\nin 5 minutes, beating other strategies due to higher recall, but all strategies\nstruggled with iterative refinement. Despite LLM prompting's better\nperformance, participants preferred different strategies in different contexts\nand, even when prompting, provided examples or wrote rule-like prompts,\nsuggesting hybrid approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing tools for laypeople to create personal classifiers often assume a\nmotivated user working uninterrupted in a single, lengthy session. However,\nusers tend to engage with social media casually, with many short sessions on an\nongoing, daily basis. To make creating personal classifiers for content\ncuration easier for such users, tools should support rapid initialization and\niterative refinement. In this work, we compare three strategies -- (1) example\nlabeling, (2) rule writing, and (3) large language model (LLM) prompting -- for\nend users to build personal content classifiers. From an experiment with 37\nnon-programmers tasked with creating personalized comment moderation filters,\nwe found that with LLM prompting, participants reached 95\\% of peak performance\nin 5 minutes, beating other strategies due to higher recall, but all strategies\nstruggled with iterative refinement. Despite LLM prompting's better\nperformance, participants preferred different strategies in different contexts\nand, even when prompting, provided examples or wrote rule-like prompts,\nsuggesting hybrid approaches."
                },
                "authors": [
                    {
                        "name": "Leijie Wang"
                    },
                    {
                        "name": "Kathryn Yurechko"
                    },
                    {
                        "name": "Pranati Dani"
                    },
                    {
                        "name": "Quan Ze Chen"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02897v2",
                "updated": "2024-09-05T03:53:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    53,
                    13,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T17:41:19Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    41,
                    19,
                    2,
                    248,
                    0
                ],
                "title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA"
                },
                "summary": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o."
                },
                "authors": [
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Wanjun Gu"
                    },
                    {
                        "name": "Danqing Liu"
                    },
                    {
                        "name": "Minhao Zou"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Ling Feng"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03225v1",
                "updated": "2024-09-05T03:45:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    45,
                    35,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T03:45:35Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    45,
                    35,
                    3,
                    249,
                    0
                ],
                "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration"
                },
                "summary": "Black-box large language models (LLMs) are increasingly deployed in various\nenvironments, making it essential for these models to effectively convey their\nconfidence and uncertainty, especially in high-stakes settings. However, these\nmodels often exhibit overconfidence, leading to potential risks and\nmisjudgments. Existing techniques for eliciting and calibrating LLM confidence\nhave primarily focused on general reasoning datasets, yielding only modest\nimprovements. Accurate calibration is crucial for informed decision-making and\npreventing adverse outcomes but remains challenging due to the complexity and\nvariability of tasks these models perform. In this work, we investigate the\nmiscalibration behavior of black-box LLMs within the healthcare setting. We\npropose a novel method, \\textit{Atypical Presentations Recalibration}, which\nleverages atypical presentations to adjust the model's confidence estimates.\nOur approach significantly improves calibration, reducing calibration errors by\napproximately 60\\% on three medical question answering datasets and\noutperforming existing methods such as vanilla verbalized confidence, CoT\nverbalized confidence and others. Additionally, we provide an in-depth analysis\nof the role of atypicality within the recalibration framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box large language models (LLMs) are increasingly deployed in various\nenvironments, making it essential for these models to effectively convey their\nconfidence and uncertainty, especially in high-stakes settings. However, these\nmodels often exhibit overconfidence, leading to potential risks and\nmisjudgments. Existing techniques for eliciting and calibrating LLM confidence\nhave primarily focused on general reasoning datasets, yielding only modest\nimprovements. Accurate calibration is crucial for informed decision-making and\npreventing adverse outcomes but remains challenging due to the complexity and\nvariability of tasks these models perform. In this work, we investigate the\nmiscalibration behavior of black-box LLMs within the healthcare setting. We\npropose a novel method, \\textit{Atypical Presentations Recalibration}, which\nleverages atypical presentations to adjust the model's confidence estimates.\nOur approach significantly improves calibration, reducing calibration errors by\napproximately 60\\% on three medical question answering datasets and\noutperforming existing methods such as vanilla verbalized confidence, CoT\nverbalized confidence and others. Additionally, we provide an in-depth analysis\nof the role of atypicality within the recalibration framework."
                },
                "authors": [
                    {
                        "name": "Jeremy Qin"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Quoc Dinh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Quoc Dinh Nguyen"
                },
                "author": "Quoc Dinh Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03223v1",
                "updated": "2024-09-05T03:42:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    42,
                    11,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T03:42:11Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    42,
                    11,
                    3,
                    249,
                    0
                ],
                "title": "Why mamba is effective? Exploit Linear Transformer-Mamba Network for\n  Multi-Modality Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why mamba is effective? Exploit Linear Transformer-Mamba Network for\n  Multi-Modality Image Fusion"
                },
                "summary": "Multi-modality image fusion aims to integrate the merits of images from\ndifferent sources and render high-quality fusion images. However, existing\nfeature extraction and fusion methods are either constrained by inherent local\nreduction bias and static parameters during inference (CNN) or limited by\nquadratic computational complexity (Transformers), and cannot effectively\nextract and fuse features. To solve this problem, we propose a dual-branch\nimage fusion network called Tmamba. It consists of linear Transformer and\nMamba, which has global modeling capabilities while maintaining linear\ncomplexity. Due to the difference between the Transformer and Mamba structures,\nthe features extracted by the two branches carry channel and position\ninformation respectively. T-M interaction structure is designed between the two\nbranches, using global learnable parameters and convolutional layers to\ntransfer position and channel information respectively. We further propose\ncross-modal interaction at the attention level to obtain cross-modal attention.\nExperiments show that our Tmamba achieves promising results in multiple fusion\ntasks, including infrared-visible image fusion and medical image fusion. Code\nwith checkpoints will be available after the peer-review process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modality image fusion aims to integrate the merits of images from\ndifferent sources and render high-quality fusion images. However, existing\nfeature extraction and fusion methods are either constrained by inherent local\nreduction bias and static parameters during inference (CNN) or limited by\nquadratic computational complexity (Transformers), and cannot effectively\nextract and fuse features. To solve this problem, we propose a dual-branch\nimage fusion network called Tmamba. It consists of linear Transformer and\nMamba, which has global modeling capabilities while maintaining linear\ncomplexity. Due to the difference between the Transformer and Mamba structures,\nthe features extracted by the two branches carry channel and position\ninformation respectively. T-M interaction structure is designed between the two\nbranches, using global learnable parameters and convolutional layers to\ntransfer position and channel information respectively. We further propose\ncross-modal interaction at the attention level to obtain cross-modal attention.\nExperiments show that our Tmamba achieves promising results in multiple fusion\ntasks, including infrared-visible image fusion and medical image fusion. Code\nwith checkpoints will be available after the peer-review process."
                },
                "authors": [
                    {
                        "name": "Chenguang Zhu"
                    },
                    {
                        "name": "Shan Gao"
                    },
                    {
                        "name": "Huafeng Chen"
                    },
                    {
                        "name": "Guangqian Guo"
                    },
                    {
                        "name": "Chaowei Wang"
                    },
                    {
                        "name": "Yaoxing Wang"
                    },
                    {
                        "name": "Chen Shu Lei"
                    },
                    {
                        "name": "Quanjiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Quanjiang Fan"
                },
                "author": "Quanjiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03219v1",
                "updated": "2024-09-05T03:33:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    33,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T03:33:54Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    33,
                    54,
                    3,
                    249,
                    0
                ],
                "title": "Content Moderation by LLM: From Accuracy to Legitimacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Moderation by LLM: From Accuracy to Legitimacy"
                },
                "summary": "One trending application of LLM (large language model) is to use it for\ncontent moderation in online platforms. Most current studies on this\napplication have focused on the metric of accuracy - the extent to which LLM\nmakes correct decisions about content. This article argues that accuracy is\ninsufficient and misleading, because it fails to grasp the distinction between\neasy cases and hard cases as well as the inevitable trade-offs in achieving\nhigher accuracy. Closer examination reveals that content moderation is a\nconstitutive part of platform governance, the key of which is to gain and\nenhance legitimacy. Instead of making moderation decisions correct, the chief\ngoal of LLM is to make them legitimate. In this regard, this article proposes a\nparadigm shift from the single benchmark of accuracy towards a legitimacy-based\nframework of evaluating the performance of LLM moderators. The framework\nsuggests that for easy cases, the key is to ensure accuracy, speed and\ntransparency, while for hard cases, what matters is reasoned justification and\nuser participation. Examined under this framework, LLM's real potential in\nmoderation is not accuracy improvement. Rather, LLM can better contribute in\nfour other aspects: to conduct screening of hard cases from easy cases, to\nprovide quality explanations for moderation decisions, to assist human\nreviewers in getting more contextual information, and to facilitate user\nparticipation in a more interactive way. Using normative theories from law and\nsocial sciences to critically assess the new technological application, this\narticle seeks to redefine LLM's role in content moderation and redirect\nrelevant research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One trending application of LLM (large language model) is to use it for\ncontent moderation in online platforms. Most current studies on this\napplication have focused on the metric of accuracy - the extent to which LLM\nmakes correct decisions about content. This article argues that accuracy is\ninsufficient and misleading, because it fails to grasp the distinction between\neasy cases and hard cases as well as the inevitable trade-offs in achieving\nhigher accuracy. Closer examination reveals that content moderation is a\nconstitutive part of platform governance, the key of which is to gain and\nenhance legitimacy. Instead of making moderation decisions correct, the chief\ngoal of LLM is to make them legitimate. In this regard, this article proposes a\nparadigm shift from the single benchmark of accuracy towards a legitimacy-based\nframework of evaluating the performance of LLM moderators. The framework\nsuggests that for easy cases, the key is to ensure accuracy, speed and\ntransparency, while for hard cases, what matters is reasoned justification and\nuser participation. Examined under this framework, LLM's real potential in\nmoderation is not accuracy improvement. Rather, LLM can better contribute in\nfour other aspects: to conduct screening of hard cases from easy cases, to\nprovide quality explanations for moderation decisions, to assist human\nreviewers in getting more contextual information, and to facilitate user\nparticipation in a more interactive way. Using normative theories from law and\nsocial sciences to critically assess the new technological application, this\narticle seeks to redefine LLM's role in content moderation and redirect\nrelevant research in this field."
                },
                "authors": [
                    {
                        "name": "Tao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Huang"
                },
                "author": "Tao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03215v1",
                "updated": "2024-09-05T03:22:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    22,
                    22,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T03:22:22Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    22,
                    22,
                    3,
                    249,
                    0
                ],
                "title": "xLAM: A Family of Large Action Models to Empower AI Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLAM: A Family of Large Action Models to Empower AI Agent Systems"
                },
                "summary": "Autonomous agents powered by large language models (LLMs) have attracted\nsignificant research interest. However, the open-source community faces many\nchallenges in developing specialized models for agent tasks, driven by the\nscarcity of high-quality agent datasets and the absence of standard protocols\nin this area. We introduce and publicly release xLAM, a series of large action\nmodels designed for AI agent tasks. The xLAM series includes five models with\nboth dense and mixture-of-expert architectures, ranging from 1B to 8x22B\nparameters, trained using a scalable, flexible pipeline that unifies, augments,\nand synthesizes diverse datasets to enhance AI agents' generalizability and\nperformance across varied environments. Our experimental results demonstrate\nthat xLAM consistently delivers exceptional performance across multiple agent\nability benchmarks, notably securing the 1st position on the Berkeley\nFunction-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other\nmodels in terms of tool use. By releasing the xLAM series, we aim to advance\nthe performance of open-source LLMs for autonomous AI agents, potentially\naccelerating progress and democratizing access to high-performance models for\nagent tasks. Models are available at\nhttps://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by large language models (LLMs) have attracted\nsignificant research interest. However, the open-source community faces many\nchallenges in developing specialized models for agent tasks, driven by the\nscarcity of high-quality agent datasets and the absence of standard protocols\nin this area. We introduce and publicly release xLAM, a series of large action\nmodels designed for AI agent tasks. The xLAM series includes five models with\nboth dense and mixture-of-expert architectures, ranging from 1B to 8x22B\nparameters, trained using a scalable, flexible pipeline that unifies, augments,\nand synthesizes diverse datasets to enhance AI agents' generalizability and\nperformance across varied environments. Our experimental results demonstrate\nthat xLAM consistently delivers exceptional performance across multiple agent\nability benchmarks, notably securing the 1st position on the Berkeley\nFunction-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other\nmodels in terms of tool use. By releasing the xLAM series, we aim to advance\nthe performance of open-source LLMs for autonomous AI agents, potentially\naccelerating progress and democratizing access to high-performance models for\nagent tasks. Models are available at\nhttps://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4"
                },
                "authors": [
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Shirley Kokane"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Tulika Awalgaonkar"
                    },
                    {
                        "name": "Rithesh Murthy"
                    },
                    {
                        "name": "Eric Hu"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "Technical report for the Salesforce xLAM model series",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03206v1",
                "updated": "2024-09-05T02:54:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    54,
                    17,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T02:54:17Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    54,
                    17,
                    3,
                    249,
                    0
                ],
                "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with\n  Temporal Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with\n  Temporal Considerations"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have significantly improved\nperformance across various image-language applications. Recently, there has\nbeen a growing interest in adapting image pre-trained MLLMs for video-related\ntasks. However, most efforts concentrate on enhancing the vision encoder and\nprojector components, while the core part, Large Language Models (LLMs),\nremains comparatively under-explored. In this paper, we propose two strategies\nto enhance the model's capability in video understanding tasks by improving\ninter-layer attention computation in LLMs. Specifically, the first approach\nfocuses on the enhancement of Rotary Position Embedding (RoPE) with\nTemporal-Aware Dual RoPE, which introduces temporal position information to\nstrengthen the MLLM's temporal modeling capabilities while preserving the\nrelative position relationships of both visual and text tokens. The second\napproach involves enhancing the Attention Mask with the Frame-wise Block Causal\nAttention Mask, a simple yet effective method that broadens visual token\ninteractions within and across video frames while maintaining the causal\ninference mechanism. Based on these proposed methods, we adapt LLaVA for video\nunderstanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our\nTC-LLaVA achieves new state-of-the-art performance across various video\nunderstanding benchmarks with only supervised fine-tuning (SFT) on\nvideo-related datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have significantly improved\nperformance across various image-language applications. Recently, there has\nbeen a growing interest in adapting image pre-trained MLLMs for video-related\ntasks. However, most efforts concentrate on enhancing the vision encoder and\nprojector components, while the core part, Large Language Models (LLMs),\nremains comparatively under-explored. In this paper, we propose two strategies\nto enhance the model's capability in video understanding tasks by improving\ninter-layer attention computation in LLMs. Specifically, the first approach\nfocuses on the enhancement of Rotary Position Embedding (RoPE) with\nTemporal-Aware Dual RoPE, which introduces temporal position information to\nstrengthen the MLLM's temporal modeling capabilities while preserving the\nrelative position relationships of both visual and text tokens. The second\napproach involves enhancing the Attention Mask with the Frame-wise Block Causal\nAttention Mask, a simple yet effective method that broadens visual token\ninteractions within and across video frames while maintaining the causal\ninference mechanism. Based on these proposed methods, we adapt LLaVA for video\nunderstanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our\nTC-LLaVA achieves new state-of-the-art performance across various video\nunderstanding benchmarks with only supervised fine-tuning (SFT) on\nvideo-related datasets."
                },
                "authors": [
                    {
                        "name": "Mingze Gao"
                    },
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Mingda Li"
                    },
                    {
                        "name": "Jiangtao Xie"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03197v1",
                "updated": "2024-09-05T02:39:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    39,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T02:39:39Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    39,
                    39,
                    3,
                    249,
                    0
                ],
                "title": "Active Galactic Nuclei in the Green Valley at z$\\sim$0.7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Galactic Nuclei in the Green Valley at z$\\sim$0.7"
                },
                "summary": "We present NIR spectroscopy using MMT/MMIRS for a sample of twenty-nine\nmassive galaxies ($\\mathrm{log\\ M_* / M_{\\odot} \\gtrsim10}$) at\n$\\mathrm{z\\sim0.7}$ with optical spectroscopy from the LEGA-C survey. Having\nboth optical and NIR spectroscopy at this redshift allows us to measure the\nfull suite of rest-optical strong emission lines, enabling the study of\nionization sources and the rest-optical selection of active galactic nuclei\n(AGN), as well as the measurement of dust-corrected $\\mathrm{H\\alpha}$-based\nSFRs. We find that eleven out of twenty-nine galaxies host AGN. We infer the\nnonparametric star formation histories with the SED fitting code\n\\texttt{Prospector} and classify galaxies as star-forming, green valley, or\nquiescent based on their most recent sSFRs. We explore the connection between\nAGN activity and suppressed star formation and find that $89\\pm15\\%$ of\ngalaxies in the green valley or below host AGN, while only $15\\%\\pm8\\%$ of\ngalaxies above the green valley host AGN. We construct the star-forming main\nsequence (SFMS) and find that the AGN host galaxies are 0.37 dex below the SFMS\nwhile galaxies without detectable AGN are consistent with being on the SFMS.\nHowever, when compared to a bootstrapped mass-matched sample, the SFRs of our\nsample of AGN host galaxies are consistent with the full LEGA-C sample. Based\non this mass-matched analysis, we cannot rule out that this suppression of star\nformation is driven by other processes associated with the higher mass of the\nAGN sample. We therefore cannot link the presence of AGN activity to the\nquenching of star formation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NIR spectroscopy using MMT/MMIRS for a sample of twenty-nine\nmassive galaxies ($\\mathrm{log\\ M_* / M_{\\odot} \\gtrsim10}$) at\n$\\mathrm{z\\sim0.7}$ with optical spectroscopy from the LEGA-C survey. Having\nboth optical and NIR spectroscopy at this redshift allows us to measure the\nfull suite of rest-optical strong emission lines, enabling the study of\nionization sources and the rest-optical selection of active galactic nuclei\n(AGN), as well as the measurement of dust-corrected $\\mathrm{H\\alpha}$-based\nSFRs. We find that eleven out of twenty-nine galaxies host AGN. We infer the\nnonparametric star formation histories with the SED fitting code\n\\texttt{Prospector} and classify galaxies as star-forming, green valley, or\nquiescent based on their most recent sSFRs. We explore the connection between\nAGN activity and suppressed star formation and find that $89\\pm15\\%$ of\ngalaxies in the green valley or below host AGN, while only $15\\%\\pm8\\%$ of\ngalaxies above the green valley host AGN. We construct the star-forming main\nsequence (SFMS) and find that the AGN host galaxies are 0.37 dex below the SFMS\nwhile galaxies without detectable AGN are consistent with being on the SFMS.\nHowever, when compared to a bootstrapped mass-matched sample, the SFRs of our\nsample of AGN host galaxies are consistent with the full LEGA-C sample. Based\non this mass-matched analysis, we cannot rule out that this suppression of star\nformation is driven by other processes associated with the higher mass of the\nAGN sample. We therefore cannot link the presence of AGN activity to the\nquenching of star formation."
                },
                "authors": [
                    {
                        "name": "Charity Woodrum"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Marcia Rieke"
                    },
                    {
                        "name": "Kevin N. Hainline"
                    },
                    {
                        "name": "Raphael E. Hviding"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Robert Kennicutt"
                    },
                    {
                        "name": "Christopher N. A. Willmer"
                    }
                ],
                "author_detail": {
                    "name": "Christopher N. A. Willmer"
                },
                "author": "Christopher N. A. Willmer",
                "arxiv_doi": "10.3847/1538-4357/ad74f1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad74f1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, 6 figures, 3 tables. Accepted for publication in ApJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00935v2",
                "updated": "2024-09-05T02:39:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    39,
                    23,
                    3,
                    249,
                    0
                ],
                "published": "2023-10-02T06:57:45Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    6,
                    57,
                    45,
                    0,
                    275,
                    0
                ],
                "title": "Resolving Knowledge Conflicts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving Knowledge Conflicts in Large Language Models"
                },
                "summary": "Large language models (LLMs) often encounter knowledge conflicts, scenarios\nwhere discrepancy arises between the internal parametric knowledge of LLMs and\nnon-parametric information provided in the prompt context. In this work we ask\nwhat are the desiderata for LLMs when a knowledge conflict arises and whether\nexisting LLMs fulfill them. We posit that LLMs should 1) identify knowledge\nconflicts, 2) pinpoint conflicting information segments, and 3) provide\ndistinct answers or viewpoints in conflicting scenarios. To this end, we\nintroduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual\nknowledge conflicts and quantitatively evaluating to what extent LLMs achieve\nthese goals. KNOWLEDGE CONFLICT includes diverse and complex situations of\nknowledge conflict, knowledge from diverse entities and domains, two synthetic\nconflict creation methods, and settings with progressively increasing\ndifficulty to reflect realistic knowledge conflicts. Extensive experiments with\nthe KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in\nidentifying the existence of knowledge conflicts, they struggle to determine\nthe specific conflicting knowledge and produce a response with distinct answers\namidst conflicting information. To address these challenges, we propose new\ninstruction-based approaches that augment LLMs to better achieve the three\ngoals. Further analysis shows that abilities to tackle knowledge conflicts are\ngreatly impacted by factors such as knowledge domain and prompt text, while\ngenerating robust responses to knowledge conflict scenarios remains an open\nresearch question.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often encounter knowledge conflicts, scenarios\nwhere discrepancy arises between the internal parametric knowledge of LLMs and\nnon-parametric information provided in the prompt context. In this work we ask\nwhat are the desiderata for LLMs when a knowledge conflict arises and whether\nexisting LLMs fulfill them. We posit that LLMs should 1) identify knowledge\nconflicts, 2) pinpoint conflicting information segments, and 3) provide\ndistinct answers or viewpoints in conflicting scenarios. To this end, we\nintroduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual\nknowledge conflicts and quantitatively evaluating to what extent LLMs achieve\nthese goals. KNOWLEDGE CONFLICT includes diverse and complex situations of\nknowledge conflict, knowledge from diverse entities and domains, two synthetic\nconflict creation methods, and settings with progressively increasing\ndifficulty to reflect realistic knowledge conflicts. Extensive experiments with\nthe KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in\nidentifying the existence of knowledge conflicts, they struggle to determine\nthe specific conflicting knowledge and produce a response with distinct answers\namidst conflicting information. To address these challenges, we propose new\ninstruction-based approaches that augment LLMs to better achieve the three\ngoals. Further analysis shows that abilities to tackle knowledge conflicts are\ngreatly impacted by factors such as knowledge domain and prompt text, while\ngenerating robust responses to knowledge conflict scenarios remains an open\nresearch question."
                },
                "authors": [
                    {
                        "name": "Yike Wang"
                    },
                    {
                        "name": "Shangbin Feng"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Vidhisha Balachandran"
                    },
                    {
                        "name": "Tianxing He"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    }
                ],
                "author_detail": {
                    "name": "Yulia Tsvetkov"
                },
                "author": "Yulia Tsvetkov",
                "arxiv_comment": "Published at COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13993v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13993v4",
                "updated": "2024-09-05T02:21:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    21,
                    42,
                    3,
                    249,
                    0
                ],
                "published": "2024-04-22T08:59:35Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    8,
                    59,
                    35,
                    0,
                    113,
                    0
                ],
                "title": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion"
                },
                "summary": "Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series."
                },
                "authors": [
                    {
                        "name": "Yingxuan Li"
                    },
                    {
                        "name": "Ryota Hinami"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    },
                    {
                        "name": "Yusuke Matsui"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Matsui"
                },
                "author": "Yusuke Matsui",
                "arxiv_comment": "Accepted to ACM Multimedia 2024. Project page:\n  https://liyingxuan1012.github.io/zeroshot-speaker-prediction ; Github repo:\n  https://github.com/liyingxuan1012/zeroshot-speaker-prediction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13993v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13993v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03183v1",
                "updated": "2024-09-05T02:19:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    19,
                    34,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T02:19:34Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    19,
                    34,
                    3,
                    249,
                    0
                ],
                "title": "Bypassing DARCY Defense: Indistinguishable Universal Adversarial\n  Triggers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bypassing DARCY Defense: Indistinguishable Universal Adversarial\n  Triggers"
                },
                "summary": "Neural networks (NN) classification models for Natural Language Processing\n(NLP) are vulnerable to the Universal Adversarial Triggers (UAT) attack that\ntriggers a model to produce a specific prediction for any input. DARCY borrows\nthe \"honeypot\" concept to bait multiple trapdoors, effectively detecting the\nadversarial examples generated by UAT. Unfortunately, we find a new UAT\ngeneration method, called IndisUAT, which produces triggers (i.e., tokens) and\nuses them to craft adversarial examples whose feature distribution is\nindistinguishable from that of the benign examples in a randomly-chosen\ncategory at the detection layer of DARCY. The produced adversarial examples\nincur the maximal loss of predicting results in the DARCY-protected models.\nMeanwhile, the produced triggers are effective in black-box models for text\ngeneration, text inference, and reading comprehension. Finally, the evaluation\nresults under NN models for NLP tasks indicate that the IndisUAT method can\neffectively circumvent DARCY and penetrate other defenses. For example,\nIndisUAT can reduce the true positive rate of DARCY's detection by at least\n40.8% and 90.6%, and drop the accuracy by at least 33.3% and 51.6% in the RNN\nand CNN models, respectively. IndisUAT reduces the accuracy of the BERT's\nadversarial defense model by at least 34.0%, and makes the GPT-2 language model\nspew racist outputs even when conditioned on non-racial context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks (NN) classification models for Natural Language Processing\n(NLP) are vulnerable to the Universal Adversarial Triggers (UAT) attack that\ntriggers a model to produce a specific prediction for any input. DARCY borrows\nthe \"honeypot\" concept to bait multiple trapdoors, effectively detecting the\nadversarial examples generated by UAT. Unfortunately, we find a new UAT\ngeneration method, called IndisUAT, which produces triggers (i.e., tokens) and\nuses them to craft adversarial examples whose feature distribution is\nindistinguishable from that of the benign examples in a randomly-chosen\ncategory at the detection layer of DARCY. The produced adversarial examples\nincur the maximal loss of predicting results in the DARCY-protected models.\nMeanwhile, the produced triggers are effective in black-box models for text\ngeneration, text inference, and reading comprehension. Finally, the evaluation\nresults under NN models for NLP tasks indicate that the IndisUAT method can\neffectively circumvent DARCY and penetrate other defenses. For example,\nIndisUAT can reduce the true positive rate of DARCY's detection by at least\n40.8% and 90.6%, and drop the accuracy by at least 33.3% and 51.6% in the RNN\nand CNN models, respectively. IndisUAT reduces the accuracy of the BERT's\nadversarial defense model by at least 34.0%, and makes the GPT-2 language model\nspew racist outputs even when conditioned on non-racial context."
                },
                "authors": [
                    {
                        "name": "Zuquan Peng"
                    },
                    {
                        "name": "Yuanyuan He"
                    },
                    {
                        "name": "Jianbing Ni"
                    },
                    {
                        "name": "Ben Niu"
                    }
                ],
                "author_detail": {
                    "name": "Ben Niu"
                },
                "author": "Ben Niu",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03171v1",
                "updated": "2024-09-05T01:58:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    58,
                    29,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T01:58:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    58,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented\n  Generation Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented\n  Generation Question Answering"
                },
                "summary": "In this paper we present a multi-adapter retrieval augmented generation\nsystem (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP\n2024. CRAG is a question answering dataset contains 3 different subtasks aimed\nat realistic question and answering RAG related tasks, with a diverse set of\nquestion topics, question types, time dynamic answers, and questions featuring\nentities of varying popularity.\n  Our system follows a standard setup for web based RAG, which uses processed\nweb pages to provide context for an LLM to produce generations, while also\nquerying API endpoints for additional information. MARAGS also utilizes\nmultiple different adapters to solve the various requirements for these tasks\nwith a standard cross-encoder model for ranking candidate passages relevant for\nanswering the question. Our system achieved 2nd place for Task 1 as well as 3rd\nplace on Task 2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present a multi-adapter retrieval augmented generation\nsystem (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP\n2024. CRAG is a question answering dataset contains 3 different subtasks aimed\nat realistic question and answering RAG related tasks, with a diverse set of\nquestion topics, question types, time dynamic answers, and questions featuring\nentities of varying popularity.\n  Our system follows a standard setup for web based RAG, which uses processed\nweb pages to provide context for an LLM to produce generations, while also\nquerying API endpoints for additional information. MARAGS also utilizes\nmultiple different adapters to solve the various requirements for these tasks\nwith a standard cross-encoder model for ranking candidate passages relevant for\nanswering the question. Our system achieved 2nd place for Task 1 as well as 3rd\nplace on Task 2."
                },
                "authors": [
                    {
                        "name": "Mitchell DeHaven"
                    }
                ],
                "author_detail": {
                    "name": "Mitchell DeHaven"
                },
                "author": "Mitchell DeHaven",
                "arxiv_comment": "Accepted to CRAG KDD Cup 24 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03166v1",
                "updated": "2024-09-05T01:51:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    51,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T01:51:54Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    51,
                    54,
                    3,
                    249,
                    0
                ],
                "title": "Continual Skill and Task Learning via Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Skill and Task Learning via Dialogue"
                },
                "summary": "Continual and interactive robot learning is a challenging problem as the\nrobot is present with human users who expect the robot to learn novel skills to\nsolve novel tasks perpetually with sample efficiency. In this work we present a\nframework for robots to query and learn visuo-motor robot skills and task\nrelevant information via natural language dialog interactions with human users.\nPrevious approaches either focus on improving the performance of instruction\nfollowing agents, or passively learn novel skills or concepts. Instead, we used\ndialog combined with a language-skill grounding embedding to query or confirm\nskills and/or tasks requested by a user. To achieve this goal, we developed and\nintegrated three different components for our agent. Firstly, we propose a\nnovel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA),\nwhich enables the existing SoTA ACT model to perform few-shot continual\nlearning. Secondly, we develop an alignment model that projects demonstrations\nacross skill embodiments into a shared embedding allowing us to know when to\nask questions and/or demonstrations from users. Finally, we integrated an\nexisting LLM to interact with a human user to perform grounded interactive\ncontinual skill learning to solve a task. Our ACT-LoRA model learns novel\nfine-tuned skills with a 100% accuracy when trained with only five\ndemonstrations for a novel skill while still maintaining a 74.75% accuracy on\npre-trained skills in the RLBench dataset where other models fall significantly\nshort. We also performed a human-subjects study with 8 subjects to demonstrate\nthe continual learning capabilities of our combined framework. We achieve a\nsuccess rate of 75% in the task of sandwich making with the real robot learning\nfrom participant data demonstrating that robots can learn novel skills or task\nknowledge from dialogue with non-expert users using our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual and interactive robot learning is a challenging problem as the\nrobot is present with human users who expect the robot to learn novel skills to\nsolve novel tasks perpetually with sample efficiency. In this work we present a\nframework for robots to query and learn visuo-motor robot skills and task\nrelevant information via natural language dialog interactions with human users.\nPrevious approaches either focus on improving the performance of instruction\nfollowing agents, or passively learn novel skills or concepts. Instead, we used\ndialog combined with a language-skill grounding embedding to query or confirm\nskills and/or tasks requested by a user. To achieve this goal, we developed and\nintegrated three different components for our agent. Firstly, we propose a\nnovel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA),\nwhich enables the existing SoTA ACT model to perform few-shot continual\nlearning. Secondly, we develop an alignment model that projects demonstrations\nacross skill embodiments into a shared embedding allowing us to know when to\nask questions and/or demonstrations from users. Finally, we integrated an\nexisting LLM to interact with a human user to perform grounded interactive\ncontinual skill learning to solve a task. Our ACT-LoRA model learns novel\nfine-tuned skills with a 100% accuracy when trained with only five\ndemonstrations for a novel skill while still maintaining a 74.75% accuracy on\npre-trained skills in the RLBench dataset where other models fall significantly\nshort. We also performed a human-subjects study with 8 subjects to demonstrate\nthe continual learning capabilities of our combined framework. We achieve a\nsuccess rate of 75% in the task of sandwich making with the real robot learning\nfrom participant data demonstrating that robots can learn novel skills or task\nknowledge from dialogue with non-expert users using our approach."
                },
                "authors": [
                    {
                        "name": "Weiwei Gu"
                    },
                    {
                        "name": "Suresh Kondepudi"
                    },
                    {
                        "name": "Lixiao Huang"
                    },
                    {
                        "name": "Nakul Gopalan"
                    }
                ],
                "author_detail": {
                    "name": "Nakul Gopalan"
                },
                "author": "Nakul Gopalan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03161v1",
                "updated": "2024-09-05T01:36:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    36,
                    0,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T01:36:00Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    36,
                    0,
                    3,
                    249,
                    0
                ],
                "title": "MaterialBENCH: Evaluating College-Level Materials Science\n  Problem-Solving Abilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaterialBENCH: Evaluating College-Level Materials Science\n  Problem-Solving Abilities of Large Language Models"
                },
                "summary": "A college-level benchmark dataset for large language models (LLMs) in the\nmaterials science field, MaterialBENCH, is constructed. This dataset consists\nof problem-answer pairs, based on university textbooks. There are two types of\nproblems: one is the free-response answer type, and the other is the\nmultiple-choice type. Multiple-choice problems are constructed by adding three\nincorrect answers as choices to a correct answer, so that LLMs can choose one\nof the four as a response. Most of the problems for free-response answer and\nmultiple-choice types overlap except for the format of the answers. We also\nconduct experiments using the MaterialBENCH on LLMs, including ChatGPT-3.5,\nChatGPT-4, Bard (at the time of the experiments), and GPT-3.5 and GPT-4 with\nthe OpenAI API. The differences and similarities in the performance of LLMs\nmeasured by the MaterialBENCH are analyzed and discussed. Performance\ndifferences between the free-response type and multiple-choice type in the same\nmodels and the influence of using system massages on multiple-choice problems\nare also studied. We anticipate that MaterialBENCH will encourage further\ndevelopments of LLMs in reasoning abilities to solve more complicated problems\nand eventually contribute to materials research and discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A college-level benchmark dataset for large language models (LLMs) in the\nmaterials science field, MaterialBENCH, is constructed. This dataset consists\nof problem-answer pairs, based on university textbooks. There are two types of\nproblems: one is the free-response answer type, and the other is the\nmultiple-choice type. Multiple-choice problems are constructed by adding three\nincorrect answers as choices to a correct answer, so that LLMs can choose one\nof the four as a response. Most of the problems for free-response answer and\nmultiple-choice types overlap except for the format of the answers. We also\nconduct experiments using the MaterialBENCH on LLMs, including ChatGPT-3.5,\nChatGPT-4, Bard (at the time of the experiments), and GPT-3.5 and GPT-4 with\nthe OpenAI API. The differences and similarities in the performance of LLMs\nmeasured by the MaterialBENCH are analyzed and discussed. Performance\ndifferences between the free-response type and multiple-choice type in the same\nmodels and the influence of using system massages on multiple-choice problems\nare also studied. We anticipate that MaterialBENCH will encourage further\ndevelopments of LLMs in reasoning abilities to solve more complicated problems\nand eventually contribute to materials research and discovery."
                },
                "authors": [
                    {
                        "name": "Michiko Yoshitake"
                    },
                    {
                        "name": "Yuta Suzuki"
                    },
                    {
                        "name": "Ryo Igarashi"
                    },
                    {
                        "name": "Yoshitaka Ushiku"
                    },
                    {
                        "name": "Keisuke Nagato"
                    }
                ],
                "author_detail": {
                    "name": "Keisuke Nagato"
                },
                "arxiv_affiliation": "Univ. Tokyo",
                "author": "Keisuke Nagato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01722v2",
                "updated": "2024-09-05T01:13:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    13,
                    24,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T09:03:38Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    9,
                    3,
                    38,
                    1,
                    247,
                    0
                ],
                "title": "ACCESS-FL: Agile Communication and Computation for Efficient Secure\n  Aggregation in Stable Federated Learning Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACCESS-FL: Agile Communication and Computation for Efficient Secure\n  Aggregation in Stable Federated Learning Networks"
                },
                "summary": "Federated Learning (FL) is a promising distributed learning framework\ndesigned for privacy-aware applications. FL trains models on client devices\nwithout sharing the client's data and generates a global model on a server by\naggregating model updates. Traditional FL approaches risk exposing sensitive\nclient data when plain model updates are transmitted to the server, making them\nvulnerable to security threats such as model inversion attacks where the server\ncan infer the client's original training data from monitoring the changes of\nthe trained model in different rounds. Google's Secure Aggregation (SecAgg)\nprotocol addresses this threat by employing a double-masking technique, secret\nsharing, and cryptography computations in honest-but-curious and adversarial\nscenarios with client dropouts. However, in scenarios without the presence of\nan active adversary, the computational and communication cost of SecAgg\nsignificantly increases by growing the number of clients. To address this\nissue, in this paper, we propose ACCESS-FL, a\ncommunication-and-computation-efficient secure aggregation method designed for\nhonest-but-curious scenarios in stable FL networks with a limited rate of\nclient dropout. ACCESS-FL reduces the computation/communication cost to a\nconstant level (independent of the network size) by generating shared secrets\nbetween only two clients and eliminating the need for double masking, secret\nsharing, and cryptography computations. To evaluate the performance of\nACCESS-FL, we conduct experiments using the MNIST, FMNIST, and CIFAR datasets\nto verify the performance of our proposed method. The evaluation results\ndemonstrate that our proposed method significantly reduces computation and\ncommunication overhead compared to state-of-the-art methods, SecAgg and\nSecAgg+.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a promising distributed learning framework\ndesigned for privacy-aware applications. FL trains models on client devices\nwithout sharing the client's data and generates a global model on a server by\naggregating model updates. Traditional FL approaches risk exposing sensitive\nclient data when plain model updates are transmitted to the server, making them\nvulnerable to security threats such as model inversion attacks where the server\ncan infer the client's original training data from monitoring the changes of\nthe trained model in different rounds. Google's Secure Aggregation (SecAgg)\nprotocol addresses this threat by employing a double-masking technique, secret\nsharing, and cryptography computations in honest-but-curious and adversarial\nscenarios with client dropouts. However, in scenarios without the presence of\nan active adversary, the computational and communication cost of SecAgg\nsignificantly increases by growing the number of clients. To address this\nissue, in this paper, we propose ACCESS-FL, a\ncommunication-and-computation-efficient secure aggregation method designed for\nhonest-but-curious scenarios in stable FL networks with a limited rate of\nclient dropout. ACCESS-FL reduces the computation/communication cost to a\nconstant level (independent of the network size) by generating shared secrets\nbetween only two clients and eliminating the need for double masking, secret\nsharing, and cryptography computations. To evaluate the performance of\nACCESS-FL, we conduct experiments using the MNIST, FMNIST, and CIFAR datasets\nto verify the performance of our proposed method. The evaluation results\ndemonstrate that our proposed method significantly reduces computation and\ncommunication overhead compared to state-of-the-art methods, SecAgg and\nSecAgg+."
                },
                "authors": [
                    {
                        "name": "Niousha Nazemi"
                    },
                    {
                        "name": "Omid Tavallaie"
                    },
                    {
                        "name": "Shuaijun Chen"
                    },
                    {
                        "name": "Anna Maria Mandalari"
                    },
                    {
                        "name": "Kanchana Thilakarathna"
                    },
                    {
                        "name": "Ralph Holz"
                    },
                    {
                        "name": "Hamed Haddadi"
                    },
                    {
                        "name": "Albert Y. Zomaya"
                    }
                ],
                "author_detail": {
                    "name": "Albert Y. Zomaya"
                },
                "author": "Albert Y. Zomaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03155v1",
                "updated": "2024-09-05T01:11:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    11,
                    58,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T01:11:58Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    11,
                    58,
                    3,
                    249,
                    0
                ],
                "title": "Debate on Graph: a Flexible and Reliable Reasoning Framework for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debate on Graph: a Flexible and Reliable Reasoning Framework for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) may suffer from hallucinations in real-world\napplications due to the lack of relevant knowledge. In contrast, knowledge\ngraphs encompass extensive, multi-relational structures that store a vast array\nof symbolic facts. Consequently, integrating LLMs with knowledge graphs has\nbeen extensively explored, with Knowledge Graph Question Answering (KGQA)\nserving as a critical touchstone for the integration. This task requires LLMs\nto answer natural language questions by retrieving relevant triples from\nknowledge graphs. However, existing methods face two significant challenges:\n\\textit{excessively long reasoning paths distracting from the answer\ngeneration}, and \\textit{false-positive relations hindering the path\nrefinement}. In this paper, we propose an iterative interactive KGQA framework\nthat leverages the interactive learning capabilities of LLMs to perform\nreasoning and Debating over Graphs (DoG). Specifically, DoG employs a\nsubgraph-focusing mechanism, allowing LLMs to perform answer trying after each\nreasoning step, thereby mitigating the impact of lengthy reasoning paths. On\nthe other hand, DoG utilizes a multi-role debate team to gradually simplify\ncomplex questions, reducing the influence of false-positive relations. This\ndebate mechanism ensures the reliability of the reasoning process. Experimental\nresults on five public datasets demonstrate the effectiveness and superiority\nof our architecture. Notably, DoG outperforms the state-of-the-art method ToG\nby 23.7\\% and 9.1\\% in accuracy on WebQuestions and GrailQA, respectively.\nFurthermore, the integration experiments with various LLMs on the mentioned\ndatasets highlight the flexibility of DoG. Code is available at\n\\url{https://github.com/reml-group/DoG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) may suffer from hallucinations in real-world\napplications due to the lack of relevant knowledge. In contrast, knowledge\ngraphs encompass extensive, multi-relational structures that store a vast array\nof symbolic facts. Consequently, integrating LLMs with knowledge graphs has\nbeen extensively explored, with Knowledge Graph Question Answering (KGQA)\nserving as a critical touchstone for the integration. This task requires LLMs\nto answer natural language questions by retrieving relevant triples from\nknowledge graphs. However, existing methods face two significant challenges:\n\\textit{excessively long reasoning paths distracting from the answer\ngeneration}, and \\textit{false-positive relations hindering the path\nrefinement}. In this paper, we propose an iterative interactive KGQA framework\nthat leverages the interactive learning capabilities of LLMs to perform\nreasoning and Debating over Graphs (DoG). Specifically, DoG employs a\nsubgraph-focusing mechanism, allowing LLMs to perform answer trying after each\nreasoning step, thereby mitigating the impact of lengthy reasoning paths. On\nthe other hand, DoG utilizes a multi-role debate team to gradually simplify\ncomplex questions, reducing the influence of false-positive relations. This\ndebate mechanism ensures the reliability of the reasoning process. Experimental\nresults on five public datasets demonstrate the effectiveness and superiority\nof our architecture. Notably, DoG outperforms the state-of-the-art method ToG\nby 23.7\\% and 9.1\\% in accuracy on WebQuestions and GrailQA, respectively.\nFurthermore, the integration experiments with various LLMs on the mentioned\ndatasets highlight the flexibility of DoG. Code is available at\n\\url{https://github.com/reml-group/DoG}."
                },
                "authors": [
                    {
                        "name": "Jie Ma"
                    },
                    {
                        "name": "Zhitao Gao"
                    },
                    {
                        "name": "Qi Chai"
                    },
                    {
                        "name": "Wangchun Sun"
                    },
                    {
                        "name": "Pinghui Wang"
                    },
                    {
                        "name": "Hongbin Pei"
                    },
                    {
                        "name": "Jing Tao"
                    },
                    {
                        "name": "Lingyun Song"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Lizhen Cui"
                    }
                ],
                "author_detail": {
                    "name": "Lizhen Cui"
                },
                "author": "Lizhen Cui",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.03752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03752v1",
                "updated": "2024-09-05T17:59:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    12,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:59:12Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    12,
                    3,
                    249,
                    0
                ],
                "title": "Attention Heads of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Heads of Large Language Models: A Survey"
                },
                "summary": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain largely as black-box systems. Consequently, their\ndevelopment relies heavily on data-driven approaches, limiting performance\nenhancement through changes in internal architecture and reasoning pathways. As\na result, many researchers have begun exploring the potential internal\nmechanisms of LLMs, aiming to identify the essence of their reasoning\nbottlenecks, with most studies focusing on attention heads. Our survey aims to\nshed light on the internal reasoning processes of LLMs by concentrating on the\ninterpretability and underlying mechanisms of attention heads. We first distill\nthe human thought process into a four-stage framework: Knowledge Recalling,\nIn-Context Identification, Latent Reasoning, and Expression Preparation. Using\nthis framework, we systematically review existing research to identify and\ncategorize the functions of specific attention heads. Furthermore, we summarize\nthe experimental methodologies used to discover these special heads, dividing\nthem into two categories: Modeling-Free methods and Modeling-Required methods.\nAlso, we outline relevant evaluation methods and benchmarks. Finally, we\ndiscuss the limitations of current research and propose several potential\nfuture directions. Our reference list is open-sourced at\n\\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in\nvarious tasks but remain largely as black-box systems. Consequently, their\ndevelopment relies heavily on data-driven approaches, limiting performance\nenhancement through changes in internal architecture and reasoning pathways. As\na result, many researchers have begun exploring the potential internal\nmechanisms of LLMs, aiming to identify the essence of their reasoning\nbottlenecks, with most studies focusing on attention heads. Our survey aims to\nshed light on the internal reasoning processes of LLMs by concentrating on the\ninterpretability and underlying mechanisms of attention heads. We first distill\nthe human thought process into a four-stage framework: Knowledge Recalling,\nIn-Context Identification, Latent Reasoning, and Expression Preparation. Using\nthis framework, we systematically review existing research to identify and\ncategorize the functions of specific attention heads. Furthermore, we summarize\nthe experimental methodologies used to discover these special heads, dividing\nthem into two categories: Modeling-Free methods and Modeling-Required methods.\nAlso, we outline relevant evaluation methods and benchmarks. Finally, we\ndiscuss the limitations of current research and propose several potential\nfuture directions. Our reference list is open-sourced at\n\\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}."
                },
                "authors": [
                    {
                        "name": "Zifan Zheng"
                    },
                    {
                        "name": "Yezhaohui Wang"
                    },
                    {
                        "name": "Yuxin Huang"
                    },
                    {
                        "name": "Shichao Song"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "20 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03735v1",
                "updated": "2024-09-05T17:50:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    50,
                    31,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:50:31Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    50,
                    31,
                    3,
                    249,
                    0
                ],
                "title": "LLM-CI: Assessing Contextual Integrity Norms in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-CI: Assessing Contextual Integrity Norms in Language Models"
                },
                "summary": "Large language models (LLMs), while memorizing parts of their training data\nscraped from the Internet, may also inadvertently encode societal preferences\nand norms. As these models are integrated into sociotechnical systems, it is\ncrucial that the norms they encode align with societal expectations. These\nnorms could vary across models, hyperparameters, optimization techniques, and\ndatasets. This is especially challenging due to prompt sensitivity$-$small\nvariations in prompts yield different responses, rendering existing assessment\nmethodologies unreliable. There is a need for a comprehensive framework\ncovering various models, optimization, and datasets, along with a reliable\nmethodology to assess encoded norms.\n  We present LLM-CI, the first open-sourced framework to assess privacy norms\nencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette\nmethodology to assess the encoded norms across different contexts and LLMs. We\npropose the multi-prompt assessment methodology to address prompt sensitivity\nby assessing the norms from only the prompts that yield consistent responses\nacross multiple variants. Using LLM-CI and our proposed methodology, we\ncomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior\nwork, examining the impact of model properties (e.g., hyperparameters,\ncapacity) and optimization strategies (e.g., alignment, quantization).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), while memorizing parts of their training data\nscraped from the Internet, may also inadvertently encode societal preferences\nand norms. As these models are integrated into sociotechnical systems, it is\ncrucial that the norms they encode align with societal expectations. These\nnorms could vary across models, hyperparameters, optimization techniques, and\ndatasets. This is especially challenging due to prompt sensitivity$-$small\nvariations in prompts yield different responses, rendering existing assessment\nmethodologies unreliable. There is a need for a comprehensive framework\ncovering various models, optimization, and datasets, along with a reliable\nmethodology to assess encoded norms.\n  We present LLM-CI, the first open-sourced framework to assess privacy norms\nencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette\nmethodology to assess the encoded norms across different contexts and LLMs. We\npropose the multi-prompt assessment methodology to address prompt sensitivity\nby assessing the norms from only the prompts that yield consistent responses\nacross multiple variants. Using LLM-CI and our proposed methodology, we\ncomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior\nwork, examining the impact of model properties (e.g., hyperparameters,\ncapacity) and optimization strategies (e.g., alignment, quantization)."
                },
                "authors": [
                    {
                        "name": "Yan Shvartzshnaider"
                    },
                    {
                        "name": "Vasisht Duddu"
                    },
                    {
                        "name": "John Lacalamita"
                    }
                ],
                "author_detail": {
                    "name": "John Lacalamita"
                },
                "author": "John Lacalamita",
                "arxiv_comment": "20 pages, 8 Figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03733v1",
                "updated": "2024-09-05T17:44:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    44,
                    49,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:44:49Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    44,
                    49,
                    3,
                    249,
                    0
                ],
                "title": "Planning In Natural Language Improves LLM Search For Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning In Natural Language Improves LLM Search For Code Generation"
                },
                "summary": "While scaling training compute has led to remarkable improvements in large\nlanguage models (LLMs), scaling inference compute has not yet yielded analogous\ngains. We hypothesize that a core missing component is a lack of diverse LLM\noutputs, leading to inefficient search due to models repeatedly sampling highly\nsimilar, yet incorrect generations. We empirically demonstrate that this lack\nof diversity can be mitigated by searching over candidate plans for solving a\nproblem in natural language. Based on this insight, we propose PLANSEARCH, a\nnovel search algorithm which shows strong results across HumanEval+, MBPP+, and\nLiveCodeBench (a contamination-free benchmark for competitive coding).\nPLANSEARCH generates a diverse set of observations about the problem and then\nuses these observations to construct plans for solving the problem. By\nsearching over plans in natural language rather than directly over code\nsolutions, PLANSEARCH explores a significantly more diverse range of potential\nsolutions compared to baseline search methods. Using PLANSEARCH on top of\nClaude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on\nLiveCodeBench, outperforming both the best score achieved without search\n(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).\nFinally, we show that, across all models, search algorithms, and benchmarks\nanalyzed, we can accurately predict performance gains due to search as a direct\nfunction of the diversity over generated ideas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While scaling training compute has led to remarkable improvements in large\nlanguage models (LLMs), scaling inference compute has not yet yielded analogous\ngains. We hypothesize that a core missing component is a lack of diverse LLM\noutputs, leading to inefficient search due to models repeatedly sampling highly\nsimilar, yet incorrect generations. We empirically demonstrate that this lack\nof diversity can be mitigated by searching over candidate plans for solving a\nproblem in natural language. Based on this insight, we propose PLANSEARCH, a\nnovel search algorithm which shows strong results across HumanEval+, MBPP+, and\nLiveCodeBench (a contamination-free benchmark for competitive coding).\nPLANSEARCH generates a diverse set of observations about the problem and then\nuses these observations to construct plans for solving the problem. By\nsearching over plans in natural language rather than directly over code\nsolutions, PLANSEARCH explores a significantly more diverse range of potential\nsolutions compared to baseline search methods. Using PLANSEARCH on top of\nClaude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on\nLiveCodeBench, outperforming both the best score achieved without search\n(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).\nFinally, we show that, across all models, search algorithms, and benchmarks\nanalyzed, we can accurately predict performance gains due to search as a direct\nfunction of the diversity over generated ideas."
                },
                "authors": [
                    {
                        "name": "Evan Wang"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Catherine Wu"
                    },
                    {
                        "name": "Yunfeng Bai"
                    },
                    {
                        "name": "Will Song"
                    },
                    {
                        "name": "Vaskar Nath"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Sean Hendryx"
                    },
                    {
                        "name": "Summer Yue"
                    },
                    {
                        "name": "Hugh Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hugh Zhang"
                },
                "author": "Hugh Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05498v2",
                "updated": "2024-09-05T17:33:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    33,
                    33,
                    3,
                    249,
                    0
                ],
                "published": "2024-06-08T15:45:31Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    15,
                    45,
                    31,
                    5,
                    160,
                    0
                ],
                "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner"
                },
                "summary": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance to concurrently protect the target LLM instance in the normal\nstack and collaborate with it for checkpoint-based access control. The\neffectiveness of SelfDefend builds upon our observation that existing LLMs\n(both target and defense LLMs) have the capability to identify harmful prompts\nor intentions in user queries, which we empirically validate using the commonly\nused GPT-3.5/4 models across all major jailbreak attacks. To further improve\nthe defense's robustness and minimize costs, we employ a data distillation\napproach to tune dedicated open-source defense models. These models outperform\nsix state-of-the-art defenses and match the performance of GPT-4-based\nSelfDefend, with significantly lower extra delays. We also empirically show\nthat the tuned models are robust to adaptive jailbreaks and prompt injections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance to concurrently protect the target LLM instance in the normal\nstack and collaborate with it for checkpoint-based access control. The\neffectiveness of SelfDefend builds upon our observation that existing LLMs\n(both target and defense LLMs) have the capability to identify harmful prompts\nor intentions in user queries, which we empirically validate using the commonly\nused GPT-3.5/4 models across all major jailbreak attacks. To further improve\nthe defense's robustness and minimize costs, we employ a data distillation\napproach to tune dedicated open-source defense models. These models outperform\nsix state-of-the-art defenses and match the performance of GPT-4-based\nSelfDefend, with significantly lower extra delays. We also empirically show\nthat the tuned models are robust to adaptive jailbreaks and prompt injections."
                },
                "authors": [
                    {
                        "name": "Xunguang Wang"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Zhenlan Ji"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yingjiu Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Juergen Rahmel"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Rahmel"
                },
                "author": "Juergen Rahmel",
                "arxiv_comment": "This paper completes its earlier vision paper, available at\n  arXiv:2402.15727. Updated to the latest analysis and results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06477v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06477v3",
                "updated": "2024-09-05T17:25:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    25,
                    1,
                    3,
                    249,
                    0
                ],
                "published": "2024-01-12T09:56:57Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    9,
                    56,
                    57,
                    4,
                    12,
                    0
                ],
                "title": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kun: Answer Polishment for Chinese Self-Alignment with Instruction\n  Back-Translation"
                },
                "summary": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun"
                },
                "authors": [
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Jiawei Guo"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06477v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06477v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03708v2",
                "updated": "2024-09-06T14:18:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    18,
                    20,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T17:14:23Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    14,
                    23,
                    3,
                    249,
                    0
                ],
                "title": "RAG based Question-Answering for Contextual Response Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG based Question-Answering for Contextual Response Prediction System"
                },
                "summary": "Large Language Models (LLMs) have shown versatility in various Natural\nLanguage Processing (NLP) tasks, including their potential as effective\nquestion-answering systems. However, to provide precise and relevant\ninformation in response to specific customer queries in industry settings, LLMs\nrequire access to a comprehensive knowledge base to avoid hallucinations.\nRetrieval Augmented Generation (RAG) emerges as a promising technique to\naddress this challenge. Yet, developing an accurate question-answering\nframework for real-world applications using RAG entails several challenges: 1)\ndata availability issues, 2) evaluating the quality of generated content, and\n3) the costly nature of human evaluation. In this paper, we introduce an\nend-to-end framework that employs LLMs with RAG capabilities for industry use\ncases. Given a customer query, the proposed system retrieves relevant knowledge\ndocuments and leverages them, along with previous chat history, to generate\nresponse suggestions for customer service agents in the contact centers of a\nmajor retail company. Through comprehensive automated and human evaluations, we\nshow that this solution outperforms the current BERT-based algorithms in\naccuracy and relevance. Our findings suggest that RAG-based LLMs can be an\nexcellent support to human customer service representatives by lightening their\nworkload.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown versatility in various Natural\nLanguage Processing (NLP) tasks, including their potential as effective\nquestion-answering systems. However, to provide precise and relevant\ninformation in response to specific customer queries in industry settings, LLMs\nrequire access to a comprehensive knowledge base to avoid hallucinations.\nRetrieval Augmented Generation (RAG) emerges as a promising technique to\naddress this challenge. Yet, developing an accurate question-answering\nframework for real-world applications using RAG entails several challenges: 1)\ndata availability issues, 2) evaluating the quality of generated content, and\n3) the costly nature of human evaluation. In this paper, we introduce an\nend-to-end framework that employs LLMs with RAG capabilities for industry use\ncases. Given a customer query, the proposed system retrieves relevant knowledge\ndocuments and leverages them, along with previous chat history, to generate\nresponse suggestions for customer service agents in the contact centers of a\nmajor retail company. Through comprehensive automated and human evaluations, we\nshow that this solution outperforms the current BERT-based algorithms in\naccuracy and relevance. Our findings suggest that RAG-based LLMs can be an\nexcellent support to human customer service representatives by lightening their\nworkload."
                },
                "authors": [
                    {
                        "name": "Sriram Veturi"
                    },
                    {
                        "name": "Saurabh Vaichal"
                    },
                    {
                        "name": "Reshma Lal Jagadheesh"
                    },
                    {
                        "name": "Nafis Irtiza Tripto"
                    },
                    {
                        "name": "Nian Yan"
                    }
                ],
                "author_detail": {
                    "name": "Nian Yan"
                },
                "author": "Nian Yan",
                "arxiv_comment": "Accepted at the 1st Workshop on GenAI and RAG Systems for Enterprise,\n  CIKM'24. 6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03671v1",
                "updated": "2024-09-05T16:24:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    24,
                    42,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T16:24:42Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    24,
                    42,
                    3,
                    249,
                    0
                ],
                "title": "TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course\n  Scheduling Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course\n  Scheduling Problems"
                },
                "summary": "We present TRACE-cs, a novel hybrid system that combines symbolic reasoning\nwith large language models (LLMs) to address contrastive queries in scheduling\nproblems. TRACE-cs leverages SAT solving techniques to encode scheduling\nconstraints and generate explanations for user queries, while utilizing an LLM\nto process the user queries into logical clauses as well as refine the\nexplanations generated by the symbolic solver to natural language sentences. By\nintegrating these components, our approach demonstrates the potential of\ncombining symbolic methods with LLMs to create explainable AI agents with\ncorrectness guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TRACE-cs, a novel hybrid system that combines symbolic reasoning\nwith large language models (LLMs) to address contrastive queries in scheduling\nproblems. TRACE-cs leverages SAT solving techniques to encode scheduling\nconstraints and generate explanations for user queries, while utilizing an LLM\nto process the user queries into logical clauses as well as refine the\nexplanations generated by the symbolic solver to natural language sentences. By\nintegrating these components, our approach demonstrates the potential of\ncombining symbolic methods with LLMs to create explainable AI agents with\ncorrectness guarantees."
                },
                "authors": [
                    {
                        "name": "Stylianos Loukas Vasileiou"
                    },
                    {
                        "name": "William Yeoh"
                    }
                ],
                "author_detail": {
                    "name": "William Yeoh"
                },
                "author": "William Yeoh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06120v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06120v3",
                "updated": "2024-09-05T16:19:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    19,
                    32,
                    3,
                    249,
                    0
                ],
                "published": "2024-02-09T01:10:25Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    1,
                    10,
                    25,
                    4,
                    40,
                    0
                ],
                "title": "Exploring Group and Symmetry Principles in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Group and Symmetry Principles in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of applications; however, assessing their reasoning capabilities\nremains a significant challenge. In this paper, we introduce a framework\ngrounded in group and symmetry principles, which have played a crucial role in\nfields such as physics and mathematics, and offer another way to evaluate their\ncapabilities. While the proposed framework is general, to showcase the benefits\nof employing these properties, we focus on arithmetic reasoning and investigate\nthe performance of these models on four group properties: closure, identity,\ninverse, and associativity. Our findings reveal that LLMs studied in this work\nstruggle to preserve group properties across different test regimes. In the\nclosure test, we observe biases towards specific outputs and an abrupt\ndegradation in their performance from 100% to 0% after a specific sequence\nlength. They also perform poorly in the identity test, which represents adding\nirrelevant information in the context, and show sensitivity when subjected to\ninverse test, which examines the robustness of the model with respect to\nnegation. In addition, we demonstrate that breaking down problems into smaller\nsteps helps LLMs in the associativity test that we have conducted. To support\nthese tests we have developed a synthetic dataset which will be released."
                },
                "authors": [
                    {
                        "name": "Shima Imani"
                    },
                    {
                        "name": "Hamid Palangi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Palangi"
                },
                "author": "Hamid Palangi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06120v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06120v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16639v3",
                "updated": "2024-09-05T16:17:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    17,
                    20,
                    3,
                    249,
                    0
                ],
                "published": "2023-11-28T09:45:02Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    9,
                    45,
                    2,
                    1,
                    332,
                    0
                ],
                "title": "Positioning Political Texts with Large Language Models by Asking and\n  Averaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positioning Political Texts with Large Language Models by Asking and\n  Averaging"
                },
                "summary": "We use instruction-tuned Large Language Models (LLMs) like GPT-4, Llama 3,\nMiXtral, or Aya to position political texts within policy and ideological\nspaces. We ask an LLM where a tweet or a sentence of a political text stands on\nthe focal dimension and take the average of the LLM responses to position\npolitical actors such as US Senators, or longer texts such as UK party\nmanifestos or EU policy speeches given in 10 different languages. The\ncorrelations between the position estimates obtained with the best LLMs and\nbenchmarks based on text coding by experts, crowdworkers, or roll call votes\nexceed .90. This approach is generally more accurate than the positions\nobtained with supervised classifiers trained on large amounts of research data.\nUsing instruction-tuned LLMs to position texts in policy and ideological spaces\nis fast, cost-efficient, reliable, and reproducible (in the case of open LLMs)\neven if the texts are short and written in different languages. We conclude\nwith cautionary notes about the need for empirical validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use instruction-tuned Large Language Models (LLMs) like GPT-4, Llama 3,\nMiXtral, or Aya to position political texts within policy and ideological\nspaces. We ask an LLM where a tweet or a sentence of a political text stands on\nthe focal dimension and take the average of the LLM responses to position\npolitical actors such as US Senators, or longer texts such as UK party\nmanifestos or EU policy speeches given in 10 different languages. The\ncorrelations between the position estimates obtained with the best LLMs and\nbenchmarks based on text coding by experts, crowdworkers, or roll call votes\nexceed .90. This approach is generally more accurate than the positions\nobtained with supervised classifiers trained on large amounts of research data.\nUsing instruction-tuned LLMs to position texts in policy and ideological spaces\nis fast, cost-efficient, reliable, and reproducible (in the case of open LLMs)\neven if the texts are short and written in different languages. We conclude\nwith cautionary notes about the need for empirical validation."
                },
                "authors": [
                    {
                        "name": "Gal Le Mens"
                    },
                    {
                        "name": "Aina Gallego"
                    }
                ],
                "author_detail": {
                    "name": "Aina Gallego"
                },
                "author": "Aina Gallego",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03662v1",
                "updated": "2024-09-05T16:15:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    15,
                    12,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T16:15:12Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    15,
                    12,
                    3,
                    249,
                    0
                ],
                "title": "The representation landscape of few-shot learning and fine-tuning in\n  large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The representation landscape of few-shot learning and fine-tuning in\n  large language models"
                },
                "summary": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common\nstrategies for improving the performance of modern large language models (LLMs)\non specific tasks. Despite their different natures, these strategies often lead\nto comparable performance gains. However, little is known about whether they\ninduce similar representations inside LLMs. We approach this problem by\nanalyzing the probability landscape of their hidden representations in the two\ncases. More specifically, we compare how LLMs solve the same question-answering\ntask, finding that ICL and SFT create very different internal structures, in\nboth cases undergoing a sharp transition in the middle of the network. In the\nfirst half of the network, ICL shapes interpretable representations\nhierarchically organized according to their semantic content. In contrast, the\nprobability landscape obtained with SFT is fuzzier and semantically mixed. In\nthe second half of the model, the fine-tuned representations develop\nprobability modes that better encode the identity of answers, while the\nlandscape of ICL representations is characterized by less defined peaks. Our\napproach reveals the diverse computational strategies developed inside LLMs to\nsolve the same task across different conditions, allowing us to make a step\ntowards designing optimal methods to extract information from language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) and supervised fine-tuning (SFT) are two common\nstrategies for improving the performance of modern large language models (LLMs)\non specific tasks. Despite their different natures, these strategies often lead\nto comparable performance gains. However, little is known about whether they\ninduce similar representations inside LLMs. We approach this problem by\nanalyzing the probability landscape of their hidden representations in the two\ncases. More specifically, we compare how LLMs solve the same question-answering\ntask, finding that ICL and SFT create very different internal structures, in\nboth cases undergoing a sharp transition in the middle of the network. In the\nfirst half of the network, ICL shapes interpretable representations\nhierarchically organized according to their semantic content. In contrast, the\nprobability landscape obtained with SFT is fuzzier and semantically mixed. In\nthe second half of the model, the fine-tuned representations develop\nprobability modes that better encode the identity of answers, while the\nlandscape of ICL representations is characterized by less defined peaks. Our\napproach reveals the diverse computational strategies developed inside LLMs to\nsolve the same task across different conditions, allowing us to make a step\ntowards designing optimal methods to extract information from language models."
                },
                "authors": [
                    {
                        "name": "Diego Doimo"
                    },
                    {
                        "name": "Alessandro Serra"
                    },
                    {
                        "name": "Alessio Ansuini"
                    },
                    {
                        "name": "Alberto Cazzaniga"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Cazzaniga"
                },
                "author": "Alberto Cazzaniga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04168v2",
                "updated": "2024-09-05T16:14:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    14,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-08T02:28:43Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    2,
                    28,
                    43,
                    3,
                    221,
                    0
                ],
                "title": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City\n  Navigation without Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City\n  Navigation without Instructions"
                },
                "summary": "This paper considers a scenario in city navigation: an AI agent is provided\nwith language descriptions of the goal location with respect to some well-known\nlandmarks; By only observing the scene around, including recognizing landmarks\nand road network connections, the agent has to make decisions to navigate to\nthe goal location without instructions. This problem is very challenging,\nbecause it requires agent to establish self-position and acquire spatial\nrepresentation of complex urban environment, where landmarks are often\ninvisible. In the absence of navigation instructions, such abilities are vital\nfor the agent to make high-quality decisions in long-range city navigation.\nWith the emergent reasoning ability of large language models (LLMs), a tempting\nbaseline is to prompt LLMs to \"react\" on each observation and make decisions\naccordingly. However, this baseline has very poor performance that the agent\noften repeatedly visits same locations and make short-sighted, inconsistent\ndecisions. To address these issues, this paper introduces a novel agentic\nworkflow featured by its abilities to perceive, reflect and plan. Specifically,\nwe find LLaVA-7B can be fine-tuned to perceive the direction and distance of\nlandmarks with sufficient accuracy for city navigation. Moreover, reflection is\nachieved through a memory mechanism, where past experiences are stored and can\nbe retrieved with current perception for effective decision argumentation.\nPlanning uses reflection results to produce long-term plans, which can avoid\nshort-sighted decisions in long-range navigation. We show the designed workflow\nsignificantly improves navigation ability of the LLM agent compared with the\nstate-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers a scenario in city navigation: an AI agent is provided\nwith language descriptions of the goal location with respect to some well-known\nlandmarks; By only observing the scene around, including recognizing landmarks\nand road network connections, the agent has to make decisions to navigate to\nthe goal location without instructions. This problem is very challenging,\nbecause it requires agent to establish self-position and acquire spatial\nrepresentation of complex urban environment, where landmarks are often\ninvisible. In the absence of navigation instructions, such abilities are vital\nfor the agent to make high-quality decisions in long-range city navigation.\nWith the emergent reasoning ability of large language models (LLMs), a tempting\nbaseline is to prompt LLMs to \"react\" on each observation and make decisions\naccordingly. However, this baseline has very poor performance that the agent\noften repeatedly visits same locations and make short-sighted, inconsistent\ndecisions. To address these issues, this paper introduces a novel agentic\nworkflow featured by its abilities to perceive, reflect and plan. Specifically,\nwe find LLaVA-7B can be fine-tuned to perceive the direction and distance of\nlandmarks with sufficient accuracy for city navigation. Moreover, reflection is\nachieved through a memory mechanism, where past experiences are stored and can\nbe retrieved with current perception for effective decision argumentation.\nPlanning uses reflection results to produce long-term plans, which can avoid\nshort-sighted decisions in long-range navigation. We show the designed workflow\nsignificantly improves navigation ability of the LLM agent compared with the\nstate-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Qingbin Zeng"
                    },
                    {
                        "name": "Qinglong Yang"
                    },
                    {
                        "name": "Shunan Dong"
                    },
                    {
                        "name": "Heming Du"
                    },
                    {
                        "name": "Liang Zheng"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "The experiment and dataset are not enough, and we need more\n  experiments to verify our model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03659v2",
                "updated": "2024-09-06T06:50:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    50,
                    32,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T16:12:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    12,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "LLM-based multi-agent poetry generation in non-cooperative environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent poetry generation in non-cooperative environments"
                },
                "summary": "Despite substantial progress of large language models (LLMs) for automatic\npoetry generation, the generated poetry lacks diversity while the training\nprocess differs greatly from human learning. Under the rationale that the\nlearning process of the poetry generation systems should be more human-like and\ntheir output more diverse and novel, we introduce a framework based on social\nlearning where we emphasize non-cooperative interactions besides cooperative\ninteractions to encourage diversity. Our experiments are the first attempt at\nLLM-based multi-agent systems in non-cooperative environments for poetry\ngeneration employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED\nagents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows\nthat our framework benefits the poetry generation process for TRAINING-BASED\nagents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity\nand a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.\nThe generated poetry from TRAINING-BASED agents also exhibits group divergence\nin terms of lexicons, styles and semantics. PROMPTING-BASED agents in our\nframework also benefit from non-cooperative environments and a more diverse\nensemble of models with non-homogeneous agents has the potential to further\nenhance diversity, with an increase of 7.0-17.5 pp according to our\nexperiments. However, PROMPTING-BASED agents show a decrease in lexical\ndiversity over time and do not exhibit the group-based divergence intended in\nthe social network. Our paper argues for a paradigm shift in creative tasks\nsuch as automatic poetry generation to include social learning processes (via\nLLM-based agent modeling) similar to human interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial progress of large language models (LLMs) for automatic\npoetry generation, the generated poetry lacks diversity while the training\nprocess differs greatly from human learning. Under the rationale that the\nlearning process of the poetry generation systems should be more human-like and\ntheir output more diverse and novel, we introduce a framework based on social\nlearning where we emphasize non-cooperative interactions besides cooperative\ninteractions to encourage diversity. Our experiments are the first attempt at\nLLM-based multi-agent systems in non-cooperative environments for poetry\ngeneration employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED\nagents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows\nthat our framework benefits the poetry generation process for TRAINING-BASED\nagents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity\nand a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.\nThe generated poetry from TRAINING-BASED agents also exhibits group divergence\nin terms of lexicons, styles and semantics. PROMPTING-BASED agents in our\nframework also benefit from non-cooperative environments and a more diverse\nensemble of models with non-homogeneous agents has the potential to further\nenhance diversity, with an increase of 7.0-17.5 pp according to our\nexperiments. However, PROMPTING-BASED agents show a decrease in lexical\ndiversity over time and do not exhibit the group-based divergence intended in\nthe social network. Our paper argues for a paradigm shift in creative tasks\nsuch as automatic poetry generation to include social learning processes (via\nLLM-based agent modeling) similar to human interaction."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03650v1",
                "updated": "2024-09-05T16:08:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    8,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T16:08:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    8,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches."
                },
                "authors": [
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Skyler Seto"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "name": "Katherine Metcalf"
                    },
                    {
                        "name": "Barry-John Theobald"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "12 pages, 8 tables, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12547v2",
                "updated": "2024-09-05T16:07:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    7,
                    37,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-22T17:01:34Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    1,
                    34,
                    3,
                    235,
                    0
                ],
                "title": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluating and Building Versatile Large Language Models for\n  Medicine"
                },
                "summary": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present MedS-Bench, a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs) in clinical contexts.\nUnlike existing benchmarks that focus on multiple-choice question answering,\nMedS-Bench spans 11 high-level clinical tasks, including clinical report\nsummarization, treatment recommendations, diagnosis, named entity recognition,\nand medical concept explanation, among others. We evaluated six leading LLMs,\ne.g., MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5 using\nfew-shot prompting, and found that even the most sophisticated models struggle\nwith these complex tasks. To address these limitations, we developed MedS-Ins,\na large-scale instruction tuning dataset for medicine. MedS-Ins comprises 58\nmedically oriented language corpora, totaling 13.5 million samples across 122\ntasks. To demonstrate the dataset's utility, we conducted a proof-of-concept\nexperiment by performing instruction tuning on a lightweight, open-source\nmedical language model. The resulting model, MMedIns-Llama 3, significantly\noutperformed existing models across nearly all clinical tasks. To promote\nfurther advancements in the application of LLMs to clinical challenges, we have\nmade the MedS-Ins dataset fully accessible and invite the research community to\ncontribute to its expansion.Additionally, we have launched a dynamic\nleaderboard for MedS-Bench, which we plan to regularly update the test set to\ntrack progress and enhance the adaptation of general LLMs to the medical\ndomain. Leaderboard: https://henrychur.github.io/MedS-Bench/. Github:\nhttps://github.com/MAGIC-AI4Med/MedS-Ins."
                },
                "authors": [
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Hongfei Gu"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15488v2",
                "updated": "2024-09-05T15:50:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    50,
                    44,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-28T02:27:07Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    2,
                    27,
                    7,
                    2,
                    241,
                    0
                ],
                "title": "Legilimens: Practical and Unified Content Moderation for Large Language\n  Model Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legilimens: Practical and Unified Content Moderation for Large Language\n  Model Services"
                },
                "summary": "Given the societal impact of unsafe content generated by large language\nmodels (LLMs), ensuring that LLM services comply with safety standards is a\ncrucial concern for LLM service providers. Common content moderation methods\nare limited by an effectiveness-and-efficiency dilemma, where simple models are\nfragile while sophisticated models consume excessive computational resources.\nIn this paper, we reveal for the first time that effective and efficient\ncontent moderation can be achieved by extracting conceptual features from\nchat-oriented LLMs, despite their initial fine-tuning for conversation rather\nthan content moderation. We propose a practical and unified content moderation\nframework for LLM services, named Legilimens, which features both effectiveness\nand efficiency. Our red-team model-based data augmentation enhances the\nrobustness of Legilimens against state-of-the-art jailbreaking. Additionally,\nwe develop a framework to theoretically analyze the cost-effectiveness of\nLegilimens compared to other methods. We have conducted extensive experiments\non five host LLMs, seventeen datasets, and nine jailbreaking methods to verify\nthe effectiveness, efficiency, and robustness of Legilimens against normal and\nadaptive adversaries. A comparison of Legilimens with both commercial and\nacademic baselines demonstrates the superior performance of Legilimens.\nFurthermore, we confirm that Legilimens can be applied to few-shot scenarios\nand extended to multi-label classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the societal impact of unsafe content generated by large language\nmodels (LLMs), ensuring that LLM services comply with safety standards is a\ncrucial concern for LLM service providers. Common content moderation methods\nare limited by an effectiveness-and-efficiency dilemma, where simple models are\nfragile while sophisticated models consume excessive computational resources.\nIn this paper, we reveal for the first time that effective and efficient\ncontent moderation can be achieved by extracting conceptual features from\nchat-oriented LLMs, despite their initial fine-tuning for conversation rather\nthan content moderation. We propose a practical and unified content moderation\nframework for LLM services, named Legilimens, which features both effectiveness\nand efficiency. Our red-team model-based data augmentation enhances the\nrobustness of Legilimens against state-of-the-art jailbreaking. Additionally,\nwe develop a framework to theoretically analyze the cost-effectiveness of\nLegilimens compared to other methods. We have conducted extensive experiments\non five host LLMs, seventeen datasets, and nine jailbreaking methods to verify\nthe effectiveness, efficiency, and robustness of Legilimens against normal and\nadaptive adversaries. A comparison of Legilimens with both commercial and\nacademic baselines demonstrates the superior performance of Legilimens.\nFurthermore, we confirm that Legilimens can be applied to few-shot scenarios\nand extended to multi-label classification tasks."
                },
                "authors": [
                    {
                        "name": "Jialin Wu"
                    },
                    {
                        "name": "Jiangyi Deng"
                    },
                    {
                        "name": "Shengyuan Pang"
                    },
                    {
                        "name": "Yanjiao Chen"
                    },
                    {
                        "name": "Jiayang Xu"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Wenyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyuan Xu"
                },
                "author": "Wenyuan Xu",
                "arxiv_comment": "Accepted by ACM Conference on Computer and Communications Security\n  (CCS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10468v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10468v4",
                "updated": "2024-09-05T15:47:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    47,
                    45,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-20T00:40:49Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    0,
                    40,
                    49,
                    1,
                    233,
                    0
                ],
                "title": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Privacy Leakage of Language Models to Training Data via Adjusted\n  Influence Functions"
                },
                "summary": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The responses generated by Large Language Models (LLMs) can include sensitive\ninformation from individuals and organizations, leading to potential privacy\nleakage. This work implements Influence Functions (IFs) to trace privacy\nleakage back to the training data, thereby mitigating privacy concerns of\nLanguage Models (LMs). However, we notice that current IFs struggle to\naccurately estimate the influence of tokens with large gradient norms,\npotentially overestimating their influence. When tracing the most influential\nsamples, this leads to frequently tracing back to samples with large gradient\nnorm tokens, overshadowing the actual most influential samples even if their\ninfluences are well estimated. To address this issue, we propose Heuristically\nAdjusted IF (HAIF), which reduces the weight of tokens with large gradient\nnorms, thereby significantly improving the accuracy of tracing the most\ninfluential samples. To establish easily obtained groundtruth for tracing\nprivacy leakage, we construct two datasets, PII-E and PII-CR, representing two\ndistinct scenarios: one with identical text in the model outputs and\npre-training data, and the other where models leverage their reasoning\nabilities to generate text divergent from pre-training data. HAIF significantly\nimproves tracing accuracy, enhancing it by 20.96% to 73.71% on the PII-E\ndataset and 3.21% to 45.93% on the PII-CR dataset, compared to the best SOTA\nIFs against various GPT-2 and QWen-1.5 models. HAIF also outperforms SOTA IFs\non real-world pretraining data CLUECorpus2020, demonstrating strong robustness\nregardless prompt and response lengths."
                },
                "authors": [
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Zao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zao Yang"
                },
                "author": "Zao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10468v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10468v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03621v1",
                "updated": "2024-09-05T15:33:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T15:33:24Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "title": "Attend First, Consolidate Later: On the Importance of Attention in\n  Different LLM Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attend First, Consolidate Later: On the Importance of Attention in\n  Different LLM Layers"
                },
                "summary": "In decoder-based LLMs, the representation of a given layer serves two\npurposes: as input to the next layer during the computation of the current\ntoken; and as input to the attention mechanism of future tokens. In this work,\nwe show that the importance of the latter role might be overestimated. To show\nthat, we start by manipulating the representations of previous tokens; e.g. by\nreplacing the hidden states at some layer k with random vectors. Our\nexperimenting with four LLMs and four tasks show that this operation often\nleads to small to negligible drop in performance. Importantly, this happens if\nthe manipulation occurs in the top part of the model-k is in the final 30-50%\nof the layers. In contrast, doing the same manipulation in earlier layers might\nlead to chance level performance. We continue by switching the hidden state of\ncertain tokens with hidden states of other tokens from another prompt; e.g.,\nreplacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We\nfind that when applying this switch in the top 1/3 of the model, the model\nignores it (answering \"Rome\"). However if we apply it before, the model\nconforms to the switch (\"Paris\"). Our results hint at a two stage process in\ntransformer-based LLMs: the first part gathers input from previous tokens,\nwhile the second mainly processes that information internally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In decoder-based LLMs, the representation of a given layer serves two\npurposes: as input to the next layer during the computation of the current\ntoken; and as input to the attention mechanism of future tokens. In this work,\nwe show that the importance of the latter role might be overestimated. To show\nthat, we start by manipulating the representations of previous tokens; e.g. by\nreplacing the hidden states at some layer k with random vectors. Our\nexperimenting with four LLMs and four tasks show that this operation often\nleads to small to negligible drop in performance. Importantly, this happens if\nthe manipulation occurs in the top part of the model-k is in the final 30-50%\nof the layers. In contrast, doing the same manipulation in earlier layers might\nlead to chance level performance. We continue by switching the hidden state of\ncertain tokens with hidden states of other tokens from another prompt; e.g.,\nreplacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We\nfind that when applying this switch in the top 1/3 of the model, the model\nignores it (answering \"Rome\"). However if we apply it before, the model\nconforms to the switch (\"Paris\"). Our results hint at a two stage process in\ntransformer-based LLMs: the first part gathers input from previous tokens,\nwhile the second mainly processes that information internally."
                },
                "authors": [
                    {
                        "name": "Amit Ben Artzy"
                    },
                    {
                        "name": "Roy Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Roy Schwartz"
                },
                "author": "Roy Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16185v2",
                "updated": "2024-09-05T15:03:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    3,
                    51,
                    3,
                    249,
                    0
                ],
                "published": "2024-01-29T14:32:27Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    14,
                    32,
                    27,
                    0,
                    29,
                    0
                ],
                "title": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including vulnerability detection. However, current efforts in\nthis area are preliminary, lacking clarity on whether LLMs' vulnerability\nreasoning capabilities stem from the models themselves or external aids such as\nknowledge retrieval and tooling support.\n  This paper aims to isolate LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and structured output generation. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conducted controlled experiments with 97 ground-truth vulnerabilities and\n97 non-vulnerable cases in Solidity and Java, testing them in a total of 9,312\nscenarios across four LLMs (GPT-4, GPT-3.5, Mixtral, and Llama 3). Our findings\nreveal the varying impacts of knowledge enhancement, context supplementation,\nprompt schemes, and models. Additionally, we identified 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in \\$3,576 in\nbounties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including vulnerability detection. However, current efforts in\nthis area are preliminary, lacking clarity on whether LLMs' vulnerability\nreasoning capabilities stem from the models themselves or external aids such as\nknowledge retrieval and tooling support.\n  This paper aims to isolate LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and structured output generation. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conducted controlled experiments with 97 ground-truth vulnerabilities and\n97 non-vulnerable cases in Solidity and Java, testing them in a total of 9,312\nscenarios across four LLMs (GPT-4, GPT-3.5, Mixtral, and Llama 3). Our findings\nreveal the varying impacts of knowledge enhancement, context supplementation,\nprompt schemes, and models. Additionally, we identified 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in \\$3,576 in\nbounties."
                },
                "authors": [
                    {
                        "name": "Yuqiang Sun"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Yue Xue"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Lyuye Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yingjiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingjiu Li"
                },
                "author": "Yingjiu Li",
                "arxiv_comment": "This is a technical report by Nanyang Technological University.\n  Updated to support both Solidity and Java",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07666v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07666v4",
                "updated": "2024-09-05T14:37:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    37,
                    59,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-14T16:58:48Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    58,
                    48,
                    2,
                    227,
                    0
                ],
                "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities"
                },
                "summary": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."
                },
                "authors": [
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Xingwei Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07666v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07666v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03577v1",
                "updated": "2024-09-05T14:31:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    31,
                    5,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:31:05Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    31,
                    5,
                    3,
                    249,
                    0
                ],
                "title": "CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement\n  Learning"
                },
                "summary": "Reinforcement learning agents can achieve superhuman performance in static\ntasks but are costly to train and fragile to task changes. This limits their\ndeployment in real-world scenarios where training experience is expensive or\nthe context changes through factors like sensor degradation, environmental\nprocesses or changing mission priorities. Lifelong reinforcement learning aims\nto improve sample efficiency and adaptability by studying how agents perform in\nevolving problems. The difficulty that these changes pose to an agent is rarely\nmeasured directly, however. Agent performances can be compared across a change,\nbut this is often prohibitively expensive. We propose Change-Induced Regret\nProxy (CHIRP) metrics, a class of metrics for approximating a change's\ndifficulty while avoiding the high costs of using trained agents. A\nrelationship between a CHIRP metric and agent performance is identified in two\nenvironments, a simple grid world and MetaWorld's suite of robotic arm tasks.\nWe demonstrate two uses for these metrics: for learning, an agent that clusters\nMDPs based on a CHIRP metric achieves $17\\%$ higher average returns than three\nexisting agents in a sequence of MetaWorld tasks. We also show how a CHIRP can\nbe calibrated to compare the difficulty of changes across distinctly different\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning agents can achieve superhuman performance in static\ntasks but are costly to train and fragile to task changes. This limits their\ndeployment in real-world scenarios where training experience is expensive or\nthe context changes through factors like sensor degradation, environmental\nprocesses or changing mission priorities. Lifelong reinforcement learning aims\nto improve sample efficiency and adaptability by studying how agents perform in\nevolving problems. The difficulty that these changes pose to an agent is rarely\nmeasured directly, however. Agent performances can be compared across a change,\nbut this is often prohibitively expensive. We propose Change-Induced Regret\nProxy (CHIRP) metrics, a class of metrics for approximating a change's\ndifficulty while avoiding the high costs of using trained agents. A\nrelationship between a CHIRP metric and agent performance is identified in two\nenvironments, a simple grid world and MetaWorld's suite of robotic arm tasks.\nWe demonstrate two uses for these metrics: for learning, an agent that clusters\nMDPs based on a CHIRP metric achieves $17\\%$ higher average returns than three\nexisting agents in a sequence of MetaWorld tasks. We also show how a CHIRP can\nbe calibrated to compare the difficulty of changes across distinctly different\nenvironments."
                },
                "authors": [
                    {
                        "name": "John Birkbeck"
                    },
                    {
                        "name": "Adam Sobey"
                    },
                    {
                        "name": "Federico Cerutti"
                    },
                    {
                        "name": "Katherine Heseltine Hurley Flynn"
                    },
                    {
                        "name": "Timothy J. Norman"
                    }
                ],
                "author_detail": {
                    "name": "Timothy J. Norman"
                },
                "author": "Timothy J. Norman",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03563v1",
                "updated": "2024-09-05T14:19:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    19,
                    45,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:19:45Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    19,
                    45,
                    3,
                    249,
                    0
                ],
                "title": "100 instances is all you need: predicting the success of a new LLM on\n  unseen data by testing on a few instances",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "100 instances is all you need: predicting the success of a new LLM on\n  unseen data by testing on a few instances"
                },
                "summary": "Predicting the performance of LLMs on individual task instances is essential\nto ensure their reliability in high-stakes applications. To do so, a\npossibility is to evaluate the considered LLM on a set of task instances and\ntrain an assessor to predict its performance based on features of the\ninstances. However, this approach requires evaluating each new LLM on a\nsufficiently large set of task instances to train an assessor specific to it.\nIn this work, we leverage the evaluation results of previously tested LLMs to\nreduce the number of evaluations required to predict the performance of a new\nLLM. In practice, we propose to test the new LLM on a small set of reference\ninstances and train a generic assessor which predicts the performance of the\nLLM on an instance based on the performance of the former on the reference set\nand features of the instance of interest. We conduct empirical studies on\nHELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets\nthat we introduce, where we evaluate all instruction-fine-tuned OpenAI models\nuntil the January 2024 version of GPT4. When predicting performance on\ninstances with the same distribution as those used to train the generic\nassessor, we find this achieves performance comparable to the LLM-specific\nassessors trained on the full set of instances. Additionally, we find that\nrandomly selecting the reference instances performs as well as some advanced\nselection methods we tested. For out of distribution, however, no clear winner\nemerges and the overall performance is worse, suggesting that the inherent\npredictability of LLMs is low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the performance of LLMs on individual task instances is essential\nto ensure their reliability in high-stakes applications. To do so, a\npossibility is to evaluate the considered LLM on a set of task instances and\ntrain an assessor to predict its performance based on features of the\ninstances. However, this approach requires evaluating each new LLM on a\nsufficiently large set of task instances to train an assessor specific to it.\nIn this work, we leverage the evaluation results of previously tested LLMs to\nreduce the number of evaluations required to predict the performance of a new\nLLM. In practice, we propose to test the new LLM on a small set of reference\ninstances and train a generic assessor which predicts the performance of the\nLLM on an instance based on the performance of the former on the reference set\nand features of the instance of interest. We conduct empirical studies on\nHELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets\nthat we introduce, where we evaluate all instruction-fine-tuned OpenAI models\nuntil the January 2024 version of GPT4. When predicting performance on\ninstances with the same distribution as those used to train the generic\nassessor, we find this achieves performance comparable to the LLM-specific\nassessors trained on the full set of instances. Additionally, we find that\nrandomly selecting the reference instances performs as well as some advanced\nselection methods we tested. For out of distribution, however, no clear winner\nemerges and the overall performance is worse, suggesting that the inherent\npredictability of LLMs is low."
                },
                "authors": [
                    {
                        "name": "Lorenzo Pacchiardi"
                    },
                    {
                        "name": "Lucy G. Cheke"
                    },
                    {
                        "name": "Jos Hernndez-Orallo"
                    }
                ],
                "author_detail": {
                    "name": "Jos Hernndez-Orallo"
                },
                "author": "Jos Hernndez-Orallo",
                "arxiv_comment": "Presented at the 2024 KDD workshop on Evaluation and Trustworthiness\n  of Generative AI Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03555v1",
                "updated": "2024-09-05T14:15:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    15,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:15:54Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    15,
                    54,
                    3,
                    249,
                    0
                ],
                "title": "Unified Framework for Neural Network Compression via Decomposition and\n  Optimal Rank Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Framework for Neural Network Compression via Decomposition and\n  Optimal Rank Selection"
                },
                "summary": "Despite their high accuracy, complex neural networks demand significant\ncomputational resources, posing challenges for deployment on\nresource-constrained devices such as mobile phones and embedded systems.\nCompression algorithms have been developed to address these challenges by\nreducing model size and computational demands while maintaining accuracy. Among\nthese approaches, factorization methods based on tensor decomposition are\ntheoretically sound and effective. However, they face difficulties in selecting\nthe appropriate rank for decomposition. This paper tackles this issue by\npresenting a unified framework that simultaneously applies decomposition and\noptimal rank selection, employing a composite compression loss within defined\nrank constraints. Our approach includes an automatic rank search in a\ncontinuous space, efficiently identifying optimal rank configurations without\nthe use of training data, making it computationally efficient. Combined with a\nsubsequent fine-tuning step, our approach maintains the performance of highly\ncompressed models on par with their original counterparts. Using various\nbenchmark datasets, we demonstrate the efficacy of our method through a\ncomprehensive analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their high accuracy, complex neural networks demand significant\ncomputational resources, posing challenges for deployment on\nresource-constrained devices such as mobile phones and embedded systems.\nCompression algorithms have been developed to address these challenges by\nreducing model size and computational demands while maintaining accuracy. Among\nthese approaches, factorization methods based on tensor decomposition are\ntheoretically sound and effective. However, they face difficulties in selecting\nthe appropriate rank for decomposition. This paper tackles this issue by\npresenting a unified framework that simultaneously applies decomposition and\noptimal rank selection, employing a composite compression loss within defined\nrank constraints. Our approach includes an automatic rank search in a\ncontinuous space, efficiently identifying optimal rank configurations without\nthe use of training data, making it computationally efficient. Combined with a\nsubsequent fine-tuning step, our approach maintains the performance of highly\ncompressed models on par with their original counterparts. Using various\nbenchmark datasets, we demonstrate the efficacy of our method through a\ncomprehensive analysis."
                },
                "authors": [
                    {
                        "name": "Ali Aghababaei-Harandi"
                    },
                    {
                        "name": "Massih-Reza Amini"
                    }
                ],
                "author_detail": {
                    "name": "Massih-Reza Amini"
                },
                "author": "Massih-Reza Amini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03550v1",
                "updated": "2024-09-05T14:12:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    12,
                    22,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:12:22Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    12,
                    22,
                    3,
                    249,
                    0
                ],
                "title": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any\n  Architecture"
                },
                "summary": "Diffusion models (DMs) have demonstrated exceptional generative capabilities\nacross various areas, while they are hindered by slow inference speeds and high\ncomputational demands during deployment. The most common way to accelerate DMs\ninvolves reducing the number of denoising steps during generation, achieved\nthrough faster sampling solvers or knowledge distillation (KD). In contrast to\nprior approaches, we propose a novel method that transfers the capability of\nlarge pretrained DMs to faster architectures. Specifically, we employ KD in a\ndistinct manner to compress DMs by distilling their generative ability into\nmore rapid variants. Furthermore, considering that the source data is either\nunaccessible or too enormous to store for current generative models, we\nintroduce a new paradigm for their distillation without source data, termed\nData-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our\nestablished DKDM framework comprises two main components: 1) a DKDM objective\nthat uses synthetic denoising data produced by pretrained DMs to optimize\nfaster DMs without source data, and 2) a dynamic iterative distillation method\nthat flexibly organizes the synthesis of denoising data, preventing it from\nslowing down the optimization process as the generation is slow. To our\nknowledge, this is the first attempt at using KD to distill DMs into any\narchitecture in a data-free manner. Importantly, our DKDM is orthogonal to most\nexisting acceleration methods, such as denoising step reduction, quantization\nand pruning. Experiments show that our DKDM is capable of deriving 2x faster\nDMs with performance remaining on par with the baseline. Notably, our DKDM\nenables pretrained DMs to function as \"datasets\" for training new DMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have demonstrated exceptional generative capabilities\nacross various areas, while they are hindered by slow inference speeds and high\ncomputational demands during deployment. The most common way to accelerate DMs\ninvolves reducing the number of denoising steps during generation, achieved\nthrough faster sampling solvers or knowledge distillation (KD). In contrast to\nprior approaches, we propose a novel method that transfers the capability of\nlarge pretrained DMs to faster architectures. Specifically, we employ KD in a\ndistinct manner to compress DMs by distilling their generative ability into\nmore rapid variants. Furthermore, considering that the source data is either\nunaccessible or too enormous to store for current generative models, we\nintroduce a new paradigm for their distillation without source data, termed\nData-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our\nestablished DKDM framework comprises two main components: 1) a DKDM objective\nthat uses synthetic denoising data produced by pretrained DMs to optimize\nfaster DMs without source data, and 2) a dynamic iterative distillation method\nthat flexibly organizes the synthesis of denoising data, preventing it from\nslowing down the optimization process as the generation is slow. To our\nknowledge, this is the first attempt at using KD to distill DMs into any\narchitecture in a data-free manner. Importantly, our DKDM is orthogonal to most\nexisting acceleration methods, such as denoising step reduction, quantization\nand pruning. Experiments show that our DKDM is capable of deriving 2x faster\nDMs with performance remaining on par with the baseline. Notably, our DKDM\nenables pretrained DMs to function as \"datasets\" for training new DMs."
                },
                "authors": [
                    {
                        "name": "Qianlong Xiang"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.09043v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.09043v4",
                "updated": "2024-09-05T14:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    0,
                    11,
                    3,
                    249,
                    0
                ],
                "published": "2023-01-22T02:59:59Z",
                "published_parsed": [
                    2023,
                    1,
                    22,
                    2,
                    59,
                    59,
                    6,
                    22,
                    0
                ],
                "title": "CodeScore: Evaluating Code Generation by Learning Code Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeScore: Evaluating Code Generation by Learning Code Execution"
                },
                "summary": "A proper code evaluation metric (CEM) profoundly impacts the evolution of\ncode generation, which is an important research field in NLP and software\nengineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU)\nsuffer from two significant drawbacks. 1. They primarily measure the surface\ndifferences between codes without considering their functional equivalence.\nHowever, functional equivalence is pivotal in evaluating the effectiveness of\ncode generation, as different codes can perform identical operations. 2. They\nare predominantly designed for the Ref-only input format. However, code\nevaluation necessitates versatility in input formats. Aside from Ref-only,\nthere are NL-only and Ref\\&NL formats, which existing match-based CEMs cannot\neffectively accommodate. In this paper, we propose CodeScore, a large language\nmodel (LLM)-based CEM, which estimates the functional correctness of generated\ncode on three input types. To acquire CodeScore, we present UniCE, a unified\ncode generation learning framework, for LLMs to learn code execution (i.e.,\nlearning PassRatio and Executability of generated code) with unified input.\nExtensive experimental results on multiple code evaluation datasets demonstrate\nthat CodeScore absolutely improves up to 58.87% correlation with functional\ncorrectness compared to other CEMs, achieves state-of-the-art performance, and\neffectively handles three input formats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A proper code evaluation metric (CEM) profoundly impacts the evolution of\ncode generation, which is an important research field in NLP and software\nengineering. Prevailing match-based CEMs (e.g., BLEU, Accuracy, and CodeBLEU)\nsuffer from two significant drawbacks. 1. They primarily measure the surface\ndifferences between codes without considering their functional equivalence.\nHowever, functional equivalence is pivotal in evaluating the effectiveness of\ncode generation, as different codes can perform identical operations. 2. They\nare predominantly designed for the Ref-only input format. However, code\nevaluation necessitates versatility in input formats. Aside from Ref-only,\nthere are NL-only and Ref\\&NL formats, which existing match-based CEMs cannot\neffectively accommodate. In this paper, we propose CodeScore, a large language\nmodel (LLM)-based CEM, which estimates the functional correctness of generated\ncode on three input types. To acquire CodeScore, we present UniCE, a unified\ncode generation learning framework, for LLMs to learn code execution (i.e.,\nlearning PassRatio and Executability of generated code) with unified input.\nExtensive experimental results on multiple code evaluation datasets demonstrate\nthat CodeScore absolutely improves up to 58.87% correlation with functional\ncorrectness compared to other CEMs, achieves state-of-the-art performance, and\neffectively handles three input formats."
                },
                "authors": [
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Jiazheng Ding"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Zhuo Li"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "Accepted to TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.09043v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.09043v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12334v2",
                "updated": "2024-09-05T13:47:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    47,
                    26,
                    3,
                    249,
                    0
                ],
                "published": "2024-06-18T06:59:24Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    59,
                    24,
                    1,
                    170,
                    0
                ],
                "title": "What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to\n  Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Did I Do Wrong? Quantifying LLMs' Sensitivity and Consistency to\n  Prompt Engineering"
                },
                "summary": "Large Language Models (LLMs) changed the way we design and interact with\nsoftware systems. Their ability to process and extract information from text\nhas drastically improved productivity in a number of routine tasks. Developers\nthat want to include these models in their software stack, however, face a\ndreadful challenge: debugging LLMs' inconsistent behavior across minor\nvariations of the prompt. We therefore introduce two metrics for classification\ntasks, namely sensitivity and consistency, which are complementary to task\nperformance. First, sensitivity measures changes of predictions across\nrephrasings of the prompt, and does not require access to ground truth labels.\nInstead, consistency measures how predictions vary across rephrasings for\nelements of the same class. We perform an empirical comparison of these metrics\non text classification tasks, using them as guideline for understanding failure\nmodes of the LLM. Our hope is that sensitivity and consistency will be helpful\nto guide prompt engineering and obtain LLMs that balance robustness with\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) changed the way we design and interact with\nsoftware systems. Their ability to process and extract information from text\nhas drastically improved productivity in a number of routine tasks. Developers\nthat want to include these models in their software stack, however, face a\ndreadful challenge: debugging LLMs' inconsistent behavior across minor\nvariations of the prompt. We therefore introduce two metrics for classification\ntasks, namely sensitivity and consistency, which are complementary to task\nperformance. First, sensitivity measures changes of predictions across\nrephrasings of the prompt, and does not require access to ground truth labels.\nInstead, consistency measures how predictions vary across rephrasings for\nelements of the same class. We perform an empirical comparison of these metrics\non text classification tasks, using them as guideline for understanding failure\nmodes of the LLM. Our hope is that sensitivity and consistency will be helpful\nto guide prompt engineering and obtain LLMs that balance robustness with\nperformance."
                },
                "authors": [
                    {
                        "name": "Federico Errica"
                    },
                    {
                        "name": "Giuseppe Siracusano"
                    },
                    {
                        "name": "Davide Sanvito"
                    },
                    {
                        "name": "Roberto Bifulco"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Bifulco"
                },
                "author": "Roberto Bifulco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03512v1",
                "updated": "2024-09-05T13:22:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    22,
                    51,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T13:22:51Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    13,
                    22,
                    51,
                    3,
                    249,
                    0
                ],
                "title": "From MOOC to MAIC: Reshaping Online Teaching and Learning through\n  LLM-driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From MOOC to MAIC: Reshaping Online Teaching and Learning through\n  LLM-driven Agents"
                },
                "summary": "Since the first instances of online education, where courses were uploaded to\naccessible and shared online platforms, this form of scaling the dissemination\nof human knowledge to reach a broader audience has sparked extensive discussion\nand widespread adoption. Recognizing that personalized learning still holds\nsignificant potential for improvement, new AI technologies have been\ncontinuously integrated into this learning format, resulting in a variety of\neducational AI applications such as educational recommendation and intelligent\ntutoring. The emergence of intelligence in large language models (LLMs) has\nallowed for these educational enhancements to be built upon a unified\nfoundational model, enabling deeper integration. In this context, we propose\nMAIC (Massive AI-empowered Course), a new form of online education that\nleverages LLM-driven multi-agent systems to construct an AI-augmented\nclassroom, balancing scalability with adaptivity. Beyond exploring the\nconceptual framework and technical innovations, we conduct preliminary\nexperiments at Tsinghua University, one of China's leading universities.\nDrawing from over 100,000 learning records of more than 500 students, we obtain\na series of valuable observations and initial analyses. This project will\ncontinue to evolve, ultimately aiming to establish a comprehensive open\nplatform that supports and unifies research, technology, and applications in\nexploring the possibilities of online education in the era of large model AI.\nWe envision this platform as a collaborative hub, bringing together educators,\nresearchers, and innovators to collectively explore the future of AI-driven\nonline education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the first instances of online education, where courses were uploaded to\naccessible and shared online platforms, this form of scaling the dissemination\nof human knowledge to reach a broader audience has sparked extensive discussion\nand widespread adoption. Recognizing that personalized learning still holds\nsignificant potential for improvement, new AI technologies have been\ncontinuously integrated into this learning format, resulting in a variety of\neducational AI applications such as educational recommendation and intelligent\ntutoring. The emergence of intelligence in large language models (LLMs) has\nallowed for these educational enhancements to be built upon a unified\nfoundational model, enabling deeper integration. In this context, we propose\nMAIC (Massive AI-empowered Course), a new form of online education that\nleverages LLM-driven multi-agent systems to construct an AI-augmented\nclassroom, balancing scalability with adaptivity. Beyond exploring the\nconceptual framework and technical innovations, we conduct preliminary\nexperiments at Tsinghua University, one of China's leading universities.\nDrawing from over 100,000 learning records of more than 500 students, we obtain\na series of valuable observations and initial analyses. This project will\ncontinue to evolve, ultimately aiming to establish a comprehensive open\nplatform that supports and unifies research, technology, and applications in\nexploring the possibilities of online education in the era of large model AI.\nWe envision this platform as a collaborative hub, bringing together educators,\nresearchers, and innovators to collectively explore the future of AI-driven\nonline education."
                },
                "authors": [
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Daniel Zhang-li"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Zhanxin Hao"
                    },
                    {
                        "name": "Rui Miao Li"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Yuanchun Wang"
                    },
                    {
                        "name": "Hanming Li"
                    },
                    {
                        "name": "Linlu Gong"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Jiayin Lin"
                    },
                    {
                        "name": "Jinchang Zhou"
                    },
                    {
                        "name": "Fei Qin"
                    },
                    {
                        "name": "Haohua Wang"
                    },
                    {
                        "name": "Jianxiao Jiang"
                    },
                    {
                        "name": "Lijun Deng"
                    },
                    {
                        "name": "Yisi Zhan"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xusheng Dai"
                    },
                    {
                        "name": "Xuan Yan"
                    },
                    {
                        "name": "Nianyi Lin"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Ruixin Ni"
                    },
                    {
                        "name": "Yang Dang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Manli Li"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Huiqin Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03478v1",
                "updated": "2024-09-05T12:38:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    38,
                    13,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T12:38:13Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    38,
                    13,
                    3,
                    249,
                    0
                ],
                "title": "LLM-based event abstraction and integration for IoT-sourced logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based event abstraction and integration for IoT-sourced logs"
                },
                "summary": "The continuous flow of data collected by Internet of Things (IoT) devices,\nhas revolutionised our ability to understand and interact with the world across\nvarious applications. However, this data must be prepared and transformed into\nevent data before analysis can begin. In this paper, we shed light on the\npotential of leveraging Large Language Models (LLMs) in event abstraction and\nintegration. Our approach aims to create event records from raw sensor readings\nand merge the logs from multiple IoT sources into a single event log suitable\nfor further Process Mining applications. We demonstrate the capabilities of\nLLMs in event abstraction considering a case study for IoT application in\nelderly care and longitudinal health monitoring. The results, showing on\naverage an accuracy of 90% in detecting high-level activities. These results\nhighlight LLMs' promising potential in addressing event abstraction and\nintegration challenges, effectively bridging the existing gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continuous flow of data collected by Internet of Things (IoT) devices,\nhas revolutionised our ability to understand and interact with the world across\nvarious applications. However, this data must be prepared and transformed into\nevent data before analysis can begin. In this paper, we shed light on the\npotential of leveraging Large Language Models (LLMs) in event abstraction and\nintegration. Our approach aims to create event records from raw sensor readings\nand merge the logs from multiple IoT sources into a single event log suitable\nfor further Process Mining applications. We demonstrate the capabilities of\nLLMs in event abstraction considering a case study for IoT application in\nelderly care and longitudinal health monitoring. The results, showing on\naverage an accuracy of 90% in detecting high-level activities. These results\nhighlight LLMs' promising potential in addressing event abstraction and\nintegration challenges, effectively bridging the existing gap."
                },
                "authors": [
                    {
                        "name": "Mohsen Shirali"
                    },
                    {
                        "name": "Mohammadreza Fani Sani"
                    },
                    {
                        "name": "Zahra Ahmadi"
                    },
                    {
                        "name": "Estefania Serral"
                    }
                ],
                "author_detail": {
                    "name": "Estefania Serral"
                },
                "author": "Estefania Serral",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M14",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03454v1",
                "updated": "2024-09-05T12:06:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    6,
                    38,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T12:06:38Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    6,
                    38,
                    3,
                    249,
                    0
                ],
                "title": "How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes"
                },
                "summary": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains."
                },
                "authors": [
                    {
                        "name": "Inacio Vieira"
                    },
                    {
                        "name": "Will Allred"
                    },
                    {
                        "name": "Seamus Lankford"
                    },
                    {
                        "name": "Sheila Castilho Monteiro De Sousa"
                    },
                    {
                        "name": "Andy Way"
                    }
                ],
                "author_detail": {
                    "name": "Andy Way"
                },
                "author": "Andy Way",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.14735v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.14735v5",
                "updated": "2024-09-05T12:00:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    12,
                    0,
                    55,
                    3,
                    249,
                    0
                ],
                "published": "2023-10-23T09:15:18Z",
                "published_parsed": [
                    2023,
                    10,
                    23,
                    9,
                    15,
                    18,
                    0,
                    296,
                    0
                ],
                "title": "Unleashing the potential of prompt engineering in Large Language Models:\n  a comprehensive review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the potential of prompt engineering in Large Language Models:\n  a comprehensive review"
                },
                "summary": "This comprehensive review delves into the pivotal role of prompt engineering\nin unleashing the capabilities of Large Language Models (LLMs). The development\nof Artificial Intelligence (AI), from its inception in the 1950s to the\nemergence of advanced neural networks and deep learning architectures, has made\na breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in\nVision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt\nengineering is the process of structuring inputs, which has emerged as a\ncrucial technique to maximize the utility and accuracy of these models. This\npaper explores both foundational and advanced methodologies of prompt\nengineering, including techniques such as self-consistency, chain-of-thought,\nand generated knowledge, which significantly enhance model performance.\nAdditionally, it examines the prompt method of VLMs through innovative\napproaches such as Context Optimization (CoOp), Conditional Context\nOptimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this\ndiscussion is the aspect of AI security, particularly adversarial attacks that\nexploit vulnerabilities in prompt engineering. Strategies to mitigate these\nrisks and enhance model robustness are thoroughly reviewed. The evaluation of\nprompt methods is also addressed, through both subjective and objective\nmetrics, ensuring a robust analysis of their efficacy. This review also\nreflects the essential role of prompt engineering in advancing AI capabilities,\nproviding a structured framework for future research and application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review delves into the pivotal role of prompt engineering\nin unleashing the capabilities of Large Language Models (LLMs). The development\nof Artificial Intelligence (AI), from its inception in the 1950s to the\nemergence of advanced neural networks and deep learning architectures, has made\na breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in\nVision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt\nengineering is the process of structuring inputs, which has emerged as a\ncrucial technique to maximize the utility and accuracy of these models. This\npaper explores both foundational and advanced methodologies of prompt\nengineering, including techniques such as self-consistency, chain-of-thought,\nand generated knowledge, which significantly enhance model performance.\nAdditionally, it examines the prompt method of VLMs through innovative\napproaches such as Context Optimization (CoOp), Conditional Context\nOptimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this\ndiscussion is the aspect of AI security, particularly adversarial attacks that\nexploit vulnerabilities in prompt engineering. Strategies to mitigate these\nrisks and enhance model robustness are thoroughly reviewed. The evaluation of\nprompt methods is also addressed, through both subjective and objective\nmetrics, ensuring a robust analysis of their efficacy. This review also\nreflects the essential role of prompt engineering in advancing AI capabilities,\nproviding a structured framework for future research and application."
                },
                "authors": [
                    {
                        "name": "Banghao Chen"
                    },
                    {
                        "name": "Zhaofeng Zhang"
                    },
                    {
                        "name": "Nicolas Langren"
                    },
                    {
                        "name": "Shengxin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Shengxin Zhu"
                },
                "author": "Shengxin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.14735v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.14735v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03444v1",
                "updated": "2024-09-05T11:49:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    49,
                    53,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T11:49:53Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    49,
                    53,
                    3,
                    249,
                    0
                ],
                "title": "Fine-tuning large language models for domain adaptation: Exploration of\n  training strategies, scaling, model merging and synergistic capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models for domain adaptation: Exploration of\n  training strategies, scaling, model merging and synergistic capabilities"
                },
                "summary": "The advancement of Large Language Models (LLMs) for domain applications in\nfields such as materials science and engineering depends on the development of\nfine-tuning strategies that adapt models for specialized, technical\ncapabilities. In this work, we explore the effects of Continued Pretraining\n(CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization\napproaches, including Direct Preference Optimization (DPO) and Odds Ratio\nPreference Optimization (ORPO), on fine-tuned LLM performance. Our analysis\nshows how these strategies influence model outcomes and reveals that the\nmerging of multiple fine-tuned models can lead to the emergence of capabilities\nthat surpass the individual contributions of the parent models. We find that\nmodel merging leads to new functionalities that neither parent model could\nachieve alone, leading to improved performance in domain-specific assessments.\nExperiments with different model architectures are presented, including Llama\n3.1 8B and Mistral 7B models, where similar behaviors are observed. Exploring\nwhether the results hold also for much smaller models, we use a tiny LLM with\n1.7 billion parameters and show that very small LLMs do not necessarily feature\nemergent capabilities under model merging, suggesting that model scaling may be\na key component. In open-ended yet consistent chat conversations between a\nhuman and AI models, our assessment reveals detailed insights into how\ndifferent model variants perform and show that the smallest model achieves a\nhigh intelligence score across key criteria including reasoning depth,\ncreativity, clarity, and quantitative precision. Other experiments include the\ndevelopment of image generation prompts based on disparate biological material\ndesign concepts, to create new microstructures, architectural concepts, and\nurban design based on biological materials-inspired construction principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of Large Language Models (LLMs) for domain applications in\nfields such as materials science and engineering depends on the development of\nfine-tuning strategies that adapt models for specialized, technical\ncapabilities. In this work, we explore the effects of Continued Pretraining\n(CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization\napproaches, including Direct Preference Optimization (DPO) and Odds Ratio\nPreference Optimization (ORPO), on fine-tuned LLM performance. Our analysis\nshows how these strategies influence model outcomes and reveals that the\nmerging of multiple fine-tuned models can lead to the emergence of capabilities\nthat surpass the individual contributions of the parent models. We find that\nmodel merging leads to new functionalities that neither parent model could\nachieve alone, leading to improved performance in domain-specific assessments.\nExperiments with different model architectures are presented, including Llama\n3.1 8B and Mistral 7B models, where similar behaviors are observed. Exploring\nwhether the results hold also for much smaller models, we use a tiny LLM with\n1.7 billion parameters and show that very small LLMs do not necessarily feature\nemergent capabilities under model merging, suggesting that model scaling may be\na key component. In open-ended yet consistent chat conversations between a\nhuman and AI models, our assessment reveals detailed insights into how\ndifferent model variants perform and show that the smallest model achieves a\nhigh intelligence score across key criteria including reasoning depth,\ncreativity, clarity, and quantitative precision. Other experiments include the\ndevelopment of image generation prompts based on disparate biological material\ndesign concepts, to create new microstructures, architectural concepts, and\nurban design based on biological materials-inspired construction principles."
                },
                "authors": [
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Rachel K. Luu"
                    },
                    {
                        "name": "Markus J. Buehler"
                    }
                ],
                "author_detail": {
                    "name": "Markus J. Buehler"
                },
                "author": "Markus J. Buehler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03440v1",
                "updated": "2024-09-05T11:42:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    42,
                    26,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T11:42:26Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    42,
                    26,
                    3,
                    249,
                    0
                ],
                "title": "Rx Strategist: Prescription Verification using LLM Agents System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rx Strategist: Prescription Verification using LLM Agents System"
                },
                "summary": "To protect patient safety, modern pharmaceutical complexity demands strict\nprescription verification. We offer a new approach - Rx Strategist - that makes\nuse of knowledge graphs and different search strategies to enhance the power of\nLarge Language Models (LLMs) inside an agentic framework. This multifaceted\ntechnique allows for a multi-stage LLM pipeline and reliable information\nretrieval from a custom-built active ingredient database. Different facets of\nprescription verification, such as indication, dose, and possible drug\ninteractions, are covered in each stage of the pipeline. We alleviate the\ndrawbacks of monolithic LLM techniques by spreading reasoning over these\nstages, improving correctness and reliability while reducing memory demands.\nOur findings demonstrate that Rx Strategist surpasses many current LLMs,\nachieving performance comparable to that of a highly experienced clinical\npharmacist. In the complicated world of modern medications, this combination of\nLLMs with organized knowledge and sophisticated search methods presents a\nviable avenue for reducing prescription errors and enhancing patient outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect patient safety, modern pharmaceutical complexity demands strict\nprescription verification. We offer a new approach - Rx Strategist - that makes\nuse of knowledge graphs and different search strategies to enhance the power of\nLarge Language Models (LLMs) inside an agentic framework. This multifaceted\ntechnique allows for a multi-stage LLM pipeline and reliable information\nretrieval from a custom-built active ingredient database. Different facets of\nprescription verification, such as indication, dose, and possible drug\ninteractions, are covered in each stage of the pipeline. We alleviate the\ndrawbacks of monolithic LLM techniques by spreading reasoning over these\nstages, improving correctness and reliability while reducing memory demands.\nOur findings demonstrate that Rx Strategist surpasses many current LLMs,\nachieving performance comparable to that of a highly experienced clinical\npharmacist. In the complicated world of modern medications, this combination of\nLLMs with organized knowledge and sophisticated search methods presents a\nviable avenue for reducing prescription errors and enhancing patient outcomes."
                },
                "authors": [
                    {
                        "name": "Phuc Phan Van"
                    },
                    {
                        "name": "Dat Nguyen Minh"
                    },
                    {
                        "name": "An Dinh Ngoc"
                    },
                    {
                        "name": "Huy Phan Thanh"
                    }
                ],
                "author_detail": {
                    "name": "Huy Phan Thanh"
                },
                "author": "Huy Phan Thanh",
                "arxiv_comment": "17 Pages, 6 Figures, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03439v1",
                "updated": "2024-09-05T11:42:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    42,
                    8,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T11:42:08Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    42,
                    8,
                    3,
                    249,
                    0
                ],
                "title": "KiloBot: A Programming Language for Deploying Perception-Guided\n  Industrial Manipulators at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KiloBot: A Programming Language for Deploying Perception-Guided\n  Industrial Manipulators at Scale"
                },
                "summary": "We would like industrial robots to handle unstructured environments with\ncameras and perception pipelines. In contrast to traditional industrial robots\nthat replay offline-crafted trajectories, online behavior planning is required\nfor these perception-guided industrial applications. Aside from perception and\nplanning algorithms, deploying perception-guided manipulators also requires\nsubstantial effort in integration. One approach is writing scripts in a\ntraditional language (such as Python) to construct the planning problem and\nperform integration with other algorithmic modules & external devices. While\nscripting in Python is feasible for a handful of robots and applications,\ndeploying perception-guided manipulation at scale (e.g., more than 10000 robot\nworkstations in over 2000 customer sites) becomes intractable. To resolve this\nchallenge, we propose a Domain-Specific Language (DSL) for perception-guided\nmanipulation applications. To scale up the deployment,our DSL provides: 1) an\neasily accessible interface to construct & solve a sub-class of Task and Motion\nPlanning (TAMP) problems that are important in practical applications; and 2) a\nmechanism to implement flexible control flow to perform integration and address\ncustomized requirements of distinct industrial application. Combined with an\nintuitive graphical programming frontend, our DSL is mainly used by machine\noperators without coding experience in traditional programming languages.\nWithin hours of training, operators are capable of orchestrating interesting\nsophisticated manipulation behaviors with our DSL. Extensive practical\ndeployments demonstrate the efficacy of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We would like industrial robots to handle unstructured environments with\ncameras and perception pipelines. In contrast to traditional industrial robots\nthat replay offline-crafted trajectories, online behavior planning is required\nfor these perception-guided industrial applications. Aside from perception and\nplanning algorithms, deploying perception-guided manipulators also requires\nsubstantial effort in integration. One approach is writing scripts in a\ntraditional language (such as Python) to construct the planning problem and\nperform integration with other algorithmic modules & external devices. While\nscripting in Python is feasible for a handful of robots and applications,\ndeploying perception-guided manipulation at scale (e.g., more than 10000 robot\nworkstations in over 2000 customer sites) becomes intractable. To resolve this\nchallenge, we propose a Domain-Specific Language (DSL) for perception-guided\nmanipulation applications. To scale up the deployment,our DSL provides: 1) an\neasily accessible interface to construct & solve a sub-class of Task and Motion\nPlanning (TAMP) problems that are important in practical applications; and 2) a\nmechanism to implement flexible control flow to perform integration and address\ncustomized requirements of distinct industrial application. Combined with an\nintuitive graphical programming frontend, our DSL is mainly used by machine\noperators without coding experience in traditional programming languages.\nWithin hours of training, operators are capable of orchestrating interesting\nsophisticated manipulation behaviors with our DSL. Extensive practical\ndeployments demonstrate the efficacy of our method."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Jingqiang Wang"
                    },
                    {
                        "name": "Xinv Zhu"
                    },
                    {
                        "name": "Jun Zhong"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Youshuang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Youshuang Ding"
                },
                "author": "Youshuang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.07357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.07357v2",
                "updated": "2024-09-05T11:20:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    20,
                    8,
                    3,
                    249,
                    0
                ],
                "published": "2023-04-14T19:33:44Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    19,
                    33,
                    44,
                    4,
                    104,
                    0
                ],
                "title": "Efficient Incremental Penetration Depth Estimation between Convex\n  Geometries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Incremental Penetration Depth Estimation between Convex\n  Geometries"
                },
                "summary": "Penetration depth (PD) is essential for robotics due to its extensive\napplications in dynamic simulation, motion planning, haptic rendering, etc. The\nExpanding Polytope Algorithm (EPA) is the de facto standard for this problem,\nwhich estimates PD by expanding an inner polyhedral approximation of an\nimplicit set. In this paper, we propose a novel optimization-based algorithm\nthat incrementally estimates minimum penetration depth and its direction. One\nmajor advantage of our method is that it can be warm-started by exploiting the\nspatial and temporal coherence, which emerges naturally in many robotic\napplications (e.g., the temporal coherence between adjacent simulation time\nknots). As a result, our algorithm achieves substantial speedup -- we\ndemonstrate it is 5-30x faster than EPA on several benchmarks. Moreover, our\napproach is built upon the same implicit geometry representation as EPA, which\nenables easy integration and deployment into existing software stacks. We also\nprovide an open-source implementation on: https://github.com/weigao95/mind-fcl",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration depth (PD) is essential for robotics due to its extensive\napplications in dynamic simulation, motion planning, haptic rendering, etc. The\nExpanding Polytope Algorithm (EPA) is the de facto standard for this problem,\nwhich estimates PD by expanding an inner polyhedral approximation of an\nimplicit set. In this paper, we propose a novel optimization-based algorithm\nthat incrementally estimates minimum penetration depth and its direction. One\nmajor advantage of our method is that it can be warm-started by exploiting the\nspatial and temporal coherence, which emerges naturally in many robotic\napplications (e.g., the temporal coherence between adjacent simulation time\nknots). As a result, our algorithm achieves substantial speedup -- we\ndemonstrate it is 5-30x faster than EPA on several benchmarks. Moreover, our\napproach is built upon the same implicit geometry representation as EPA, which\nenables easy integration and deployment into existing software stacks. We also\nprovide an open-source implementation on: https://github.com/weigao95/mind-fcl"
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_comment": "IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.07357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.07357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15778v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15778v3",
                "updated": "2024-09-05T10:30:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    10,
                    30,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-28T13:16:41Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    13,
                    16,
                    41,
                    2,
                    241,
                    0
                ],
                "title": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated notable capabilities across\nvarious tasks, showcasing complex problem-solving abilities. Understanding and\nexecuting complex rules, along with multi-step planning, are fundamental to\nlogical reasoning and critical for practical LLM agents and decision-making\nsystems. However, evaluating LLMs as effective rule-based executors and\nplanners remains underexplored. In this paper, we introduce LogicGame, a novel\nbenchmark designed to evaluate the comprehensive rule understanding, execution,\nand planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame\nprovides diverse games that contain a series of rules with an initial state,\nrequiring models to comprehend and apply predefined regulations to solve\nproblems. We create simulated scenarios in which models execute or plan\noperations to achieve specific outcomes. These game scenarios are specifically\ndesigned to distinguish logical reasoning from mere knowledge by relying\nexclusively on predefined rules. This separation allows for a pure assessment\nof rule-based reasoning capabilities. The evaluation considers not only final\noutcomes but also intermediate steps, providing a comprehensive assessment of\nmodel performance. Moreover, these intermediate steps are deterministic and can\nbe automatically verified. LogicGame defines game scenarios with varying\ndifficulty levels, from simple rule applications to complex reasoning chains,\nin order to offer a precise evaluation of model performance on rule\nunderstanding and multi-step execution. Utilizing LogicGame, we test various\nLLMs and identify notable shortcomings in their rule-based logical reasoning\nabilities."
                },
                "authors": [
                    {
                        "name": "Jiayi Gui"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15778v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15778v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16607v2",
                "updated": "2024-09-05T10:11:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    10,
                    11,
                    36,
                    3,
                    249,
                    0
                ],
                "published": "2024-04-25T13:47:37Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    13,
                    47,
                    37,
                    3,
                    116,
                    0
                ],
                "title": "A Comprehensive Design Framework for UE-side and BS-Side RIS Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Design Framework for UE-side and BS-Side RIS Deployments"
                },
                "summary": "Integrating reconfigurable intelligent surfaces (RISs) in emerging\ncommunication systems is a fast-growing research field that has recently earned\nmuch attention. While implementing RISs near the base station (BS), i.e.,\nBS-side RIS, or user equipment (UE), i.e., UE-side RIS, exhibits optimum\nperformance, understanding the differences between these two deployments in\nterms of the system design perspective needs to be clarified. Critical design\nparameters, such as RIS size, phase shift adjustment, control link, and element\ntype (passive/active), require greater clarity across these scenarios.\nOverlooking the intricacies of such critical design parameters in light of 6G\ndemands endangers practical implementation, widening the gap between\ntheoretical insights and practical applications. In this regard, our study\ninvestigates the impact of each RIS deployment strategy on the anticipated 6G\nrequirements and offers tailored RIS design recommendations to fulfill these\nforward-looking requirements. Through this, we clarify the practical\ndistinctions and propose a comprehensive framework for differentiating between\nBS-side and UE-side RIS scenarios in terms of their design parameters.\nHighlighting the unique needs of each and the potential challenges ahead, we\naim to fuse the theoretical underpinnings of RIS with tangible implementation\nconsiderations, propelling progress in both the academic sphere and the\nindustry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating reconfigurable intelligent surfaces (RISs) in emerging\ncommunication systems is a fast-growing research field that has recently earned\nmuch attention. While implementing RISs near the base station (BS), i.e.,\nBS-side RIS, or user equipment (UE), i.e., UE-side RIS, exhibits optimum\nperformance, understanding the differences between these two deployments in\nterms of the system design perspective needs to be clarified. Critical design\nparameters, such as RIS size, phase shift adjustment, control link, and element\ntype (passive/active), require greater clarity across these scenarios.\nOverlooking the intricacies of such critical design parameters in light of 6G\ndemands endangers practical implementation, widening the gap between\ntheoretical insights and practical applications. In this regard, our study\ninvestigates the impact of each RIS deployment strategy on the anticipated 6G\nrequirements and offers tailored RIS design recommendations to fulfill these\nforward-looking requirements. Through this, we clarify the practical\ndistinctions and propose a comprehensive framework for differentiating between\nBS-side and UE-side RIS scenarios in terms of their design parameters.\nHighlighting the unique needs of each and the potential challenges ahead, we\naim to fuse the theoretical underpinnings of RIS with tangible implementation\nconsiderations, propelling progress in both the academic sphere and the\nindustry."
                },
                "authors": [
                    {
                        "name": "Mahmoud Raeisi"
                    },
                    {
                        "name": "Aymen Khaleel"
                    },
                    {
                        "name": "Mehmet Cagri Ilter"
                    },
                    {
                        "name": "Majid Gerami"
                    },
                    {
                        "name": "Ertugrul Basar"
                    }
                ],
                "author_detail": {
                    "name": "Ertugrul Basar"
                },
                "author": "Ertugrul Basar",
                "arxiv_comment": "Submitted in IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11983v2",
                "updated": "2024-09-05T10:01:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    10,
                    1,
                    39,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-20T12:33:42Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    12,
                    33,
                    42,
                    0,
                    141,
                    0
                ],
                "title": "A review on the use of large language models as virtual tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A review on the use of large language models as virtual tutors"
                },
                "summary": "Transformer architectures contribute to managing long-term dependencies for\nNatural Language Processing, representing one of the most recent changes in the\nfield. These architectures are the basis of the innovative, cutting-edge Large\nLanguage Models (LLMs) that have produced a huge buzz in several fields and\nindustrial sectors, among the ones education stands out. Accordingly, these\ngenerative Artificial Intelligence-based solutions have directed the change in\ntechniques and the evolution in educational methods and contents, along with\nnetwork infrastructure, towards high-quality learning. Given the popularity of\nLLMs, this review seeks to provide a comprehensive overview of those solutions\ndesigned specifically to generate and evaluate educational materials and which\ninvolve students and teachers in their design or experimental plan. To the best\nof our knowledge, this is the first review of educational applications (e.g.,\nstudent assessment) of LLMs. As expected, the most common role of these systems\nis as virtual tutors for automatic question generation. Moreover, the most\npopular models are GTP-3 and BERT. However, due to the continuous launch of new\ngenerative models, new works are expected to be published shortly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer architectures contribute to managing long-term dependencies for\nNatural Language Processing, representing one of the most recent changes in the\nfield. These architectures are the basis of the innovative, cutting-edge Large\nLanguage Models (LLMs) that have produced a huge buzz in several fields and\nindustrial sectors, among the ones education stands out. Accordingly, these\ngenerative Artificial Intelligence-based solutions have directed the change in\ntechniques and the evolution in educational methods and contents, along with\nnetwork infrastructure, towards high-quality learning. Given the popularity of\nLLMs, this review seeks to provide a comprehensive overview of those solutions\ndesigned specifically to generate and evaluate educational materials and which\ninvolve students and teachers in their design or experimental plan. To the best\nof our knowledge, this is the first review of educational applications (e.g.,\nstudent assessment) of LLMs. As expected, the most common role of these systems\nis as virtual tutors for automatic question generation. Moreover, the most\npopular models are GTP-3 and BERT. However, due to the continuous launch of new\ngenerative models, new works are expected to be published shortly."
                },
                "authors": [
                    {
                        "name": "Silvia Garca-Mndez"
                    },
                    {
                        "name": "Francisco de Arriba-Prez"
                    },
                    {
                        "name": "Mara del Carmen Somoza-Lpez"
                    }
                ],
                "author_detail": {
                    "name": "Mara del Carmen Somoza-Lpez"
                },
                "author": "Mara del Carmen Somoza-Lpez",
                "arxiv_doi": "10.1007/s11191-024-00530-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11191-024-00530-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.11983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Science & Education (2024), 1-16",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03384v1",
                "updated": "2024-09-05T09:43:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    43,
                    25,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T09:43:25Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    43,
                    25,
                    3,
                    249,
                    0
                ],
                "title": "Hardware Acceleration of LLMs: A comprehensive survey and comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware Acceleration of LLMs: A comprehensive survey and comparison"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for natural\nlanguage processing tasks, revolutionizing the field with their ability to\nunderstand and generate human-like text. In this paper, we present a\ncomprehensive survey of the several research efforts that have been presented\nfor the acceleration of transformer networks for Large Language Models using\nhardware accelerators.\n  The survey presents the frameworks that have been proposed and then performs\na qualitative and quantitative comparison regarding the technology, the\nprocessing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy\nefficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each\nframework. The main challenge in comparison is that every proposed scheme is\nimplemented on a different process technology making hard a fair comparison.\nThe main contribution of this paper is that we extrapolate the results of the\nperformance and the energy efficiency on the same technology to make a fair\ncomparison; one theoretical and one more practical. We implement part of the\nLLMs on several FPGA chips to extrapolate the results to the same process\ntechnology and then we make a fair comparison of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for natural\nlanguage processing tasks, revolutionizing the field with their ability to\nunderstand and generate human-like text. In this paper, we present a\ncomprehensive survey of the several research efforts that have been presented\nfor the acceleration of transformer networks for Large Language Models using\nhardware accelerators.\n  The survey presents the frameworks that have been proposed and then performs\na qualitative and quantitative comparison regarding the technology, the\nprocessing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy\nefficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each\nframework. The main challenge in comparison is that every proposed scheme is\nimplemented on a different process technology making hard a fair comparison.\nThe main contribution of this paper is that we extrapolate the results of the\nperformance and the energy efficiency on the same technology to make a fair\ncomparison; one theoretical and one more practical. We implement part of the\nLLMs on several FPGA chips to extrapolate the results to the same process\ntechnology and then we make a fair comparison of the performance."
                },
                "authors": [
                    {
                        "name": "Nikoletta Koilia"
                    },
                    {
                        "name": "Christoforos Kachris"
                    }
                ],
                "author_detail": {
                    "name": "Christoforos Kachris"
                },
                "author": "Christoforos Kachris",
                "arxiv_comment": "https://airtable.com/appC2VwR6X4EeZ50s/shrKwchys0iktvDwk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03381v2",
                "updated": "2024-09-06T09:37:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    9,
                    37,
                    36,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T09:33:24Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    33,
                    24,
                    3,
                    249,
                    0
                ],
                "title": "CogniDual Framework: Self-Training Large Language Models within a\n  Dual-System Theoretical Framework for Improving Cognitive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogniDual Framework: Self-Training Large Language Models within a\n  Dual-System Theoretical Framework for Improving Cognitive Tasks"
                },
                "summary": "Cognitive psychology investigates perception, attention, memory, language,\nproblem-solving, decision-making, and reasoning. Kahneman's dual-system theory\nelucidates the human decision-making process, distinguishing between the rapid,\nintuitive System 1 and the deliberative, rational System 2. Recent advancements\nhave positioned large language Models (LLMs) as formidable tools nearing\nhuman-level proficiency in various cognitive tasks. Nonetheless, the presence\nof a dual-system framework analogous to human cognition in LLMs remains\nunexplored. This study introduces the \\textbf{CogniDual Framework for LLMs}\n(CFLLMs), designed to assess whether LLMs can, through self-training, evolve\nfrom deliberate deduction to intuitive responses, thereby emulating the human\nprocess of acquiring and mastering new information. Our findings reveal the\ncognitive mechanisms behind LLMs' response generation, enhancing our\nunderstanding of their capabilities in cognitive psychology. Practically,\nself-trained models can provide faster responses to certain queries, reducing\ncomputational demands during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive psychology investigates perception, attention, memory, language,\nproblem-solving, decision-making, and reasoning. Kahneman's dual-system theory\nelucidates the human decision-making process, distinguishing between the rapid,\nintuitive System 1 and the deliberative, rational System 2. Recent advancements\nhave positioned large language Models (LLMs) as formidable tools nearing\nhuman-level proficiency in various cognitive tasks. Nonetheless, the presence\nof a dual-system framework analogous to human cognition in LLMs remains\nunexplored. This study introduces the \\textbf{CogniDual Framework for LLMs}\n(CFLLMs), designed to assess whether LLMs can, through self-training, evolve\nfrom deliberate deduction to intuitive responses, thereby emulating the human\nprocess of acquiring and mastering new information. Our findings reveal the\ncognitive mechanisms behind LLMs' response generation, enhancing our\nunderstanding of their capabilities in cognitive psychology. Practically,\nself-trained models can provide faster responses to certain queries, reducing\ncomputational demands during inference."
                },
                "authors": [
                    {
                        "name": "Yongxin Deng"
                    },
                    {
                        "name": "Xihe Qiu"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Jing Pan"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Yinghui Xu"
                    },
                    {
                        "name": "Wei Chu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chu"
                },
                "arxiv_affiliation": "INF Technology",
                "author": "Wei Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03375v1",
                "updated": "2024-09-05T09:27:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    27,
                    5,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T09:27:05Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    27,
                    5,
                    3,
                    249,
                    0
                ],
                "title": "Leveraging Large Language Models through Natural Language Processing to\n  provide interpretable Machine Learning predictions of mental deterioration in\n  real time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models through Natural Language Processing to\n  provide interpretable Machine Learning predictions of mental deterioration in\n  real time"
                },
                "summary": "Based on official estimates, 50 million people worldwide are affected by\ndementia, and this number increases by 10 million new patients every year.\nWithout a cure, clinical prognostication and early intervention represent the\nmost effective ways to delay its progression. To this end, Artificial\nIntelligence and computational linguistics can be exploited for natural\nlanguage analysis, personalized assessment, monitoring, and treatment. However,\ntraditional approaches need more semantic knowledge management and\nexplicability capabilities. Moreover, using Large Language Models (LLMs) for\ncognitive decline diagnosis is still scarce, even though these models represent\nthe most advanced way for clinical-patient communication using intelligent\nsystems. Consequently, we leverage an LLM using the latest Natural Language\nProcessing (NLP) techniques in a chatbot solution to provide interpretable\nMachine Learning prediction of cognitive decline in real-time.\nLinguistic-conceptual features are exploited for appropriate natural language\nanalysis. Through explainability, we aim to fight potential biases of the\nmodels and improve their potential to help clinical workers in their diagnosis\ndecisions. More in detail, the proposed pipeline is composed of (i) data\nextraction employing NLP-based prompt engineering; (ii) stream-based data\nprocessing including feature engineering, analysis, and selection; (iii)\nreal-time classification; and (iv) the explainability dashboard to provide\nvisual and natural language descriptions of the prediction outcome.\nClassification results exceed 80 % in all evaluation metrics, with a recall\nvalue for the mental deterioration class about 85 %. To sum up, we contribute\nwith an affordable, flexible, non-invasive, personalized diagnostic system to\nthis work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on official estimates, 50 million people worldwide are affected by\ndementia, and this number increases by 10 million new patients every year.\nWithout a cure, clinical prognostication and early intervention represent the\nmost effective ways to delay its progression. To this end, Artificial\nIntelligence and computational linguistics can be exploited for natural\nlanguage analysis, personalized assessment, monitoring, and treatment. However,\ntraditional approaches need more semantic knowledge management and\nexplicability capabilities. Moreover, using Large Language Models (LLMs) for\ncognitive decline diagnosis is still scarce, even though these models represent\nthe most advanced way for clinical-patient communication using intelligent\nsystems. Consequently, we leverage an LLM using the latest Natural Language\nProcessing (NLP) techniques in a chatbot solution to provide interpretable\nMachine Learning prediction of cognitive decline in real-time.\nLinguistic-conceptual features are exploited for appropriate natural language\nanalysis. Through explainability, we aim to fight potential biases of the\nmodels and improve their potential to help clinical workers in their diagnosis\ndecisions. More in detail, the proposed pipeline is composed of (i) data\nextraction employing NLP-based prompt engineering; (ii) stream-based data\nprocessing including feature engineering, analysis, and selection; (iii)\nreal-time classification; and (iv) the explainability dashboard to provide\nvisual and natural language descriptions of the prediction outcome.\nClassification results exceed 80 % in all evaluation metrics, with a recall\nvalue for the mental deterioration class about 85 %. To sum up, we contribute\nwith an affordable, flexible, non-invasive, personalized diagnostic system to\nthis work."
                },
                "authors": [
                    {
                        "name": "Francisco de Arriba-Prez"
                    },
                    {
                        "name": "Silvia Garca-Mndez"
                    }
                ],
                "author_detail": {
                    "name": "Silvia Garca-Mndez"
                },
                "author": "Silvia Garca-Mndez",
                "arxiv_doi": "10.1007/s13369-024-09508-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s13369-024-09508-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03368v1",
                "updated": "2024-09-05T09:14:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    14,
                    44,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T09:14:44Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    14,
                    44,
                    3,
                    249,
                    0
                ],
                "title": "Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and\n  High-Performance Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Conversion of Pretrained ANNs to SNNs for Low-Power and\n  High-Performance Applications"
                },
                "summary": "Spiking Neural Networks (SNNs) have emerged as a promising substitute for\nArtificial Neural Networks (ANNs) due to their advantages of fast inference and\nlow power consumption. However, the lack of efficient training algorithms has\nhindered their widespread adoption. Existing supervised learning algorithms for\nSNNs require significantly more memory and time than their ANN counterparts.\nEven commonly used ANN-SNN conversion methods necessitate re-training of ANNs\nto enhance conversion efficiency, incurring additional computational costs. To\naddress these challenges, we propose a novel training-free ANN-SNN conversion\npipeline. Our approach directly converts pre-trained ANN models into\nhigh-performance SNNs without additional training. The conversion pipeline\nincludes a local-learning-based threshold balancing algorithm, which enables\nefficient calculation of the optimal thresholds and fine-grained adjustment of\nthreshold value by channel-wise scaling. We demonstrate the scalability of our\nframework across three typical computer vision tasks: image classification,\nsemantic segmentation, and object detection. This showcases its applicability\nto both classification and regression tasks. Moreover, we have evaluated the\nenergy consumption of the converted SNNs, demonstrating their superior\nlow-power advantage compared to conventional ANNs. Our training-free algorithm\noutperforms existing methods, highlighting its practical applicability and\nefficiency. This approach simplifies the deployment of SNNs by leveraging\nopen-source pre-trained ANN models and neuromorphic hardware, enabling fast,\nlow-power inference with negligible performance reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) have emerged as a promising substitute for\nArtificial Neural Networks (ANNs) due to their advantages of fast inference and\nlow power consumption. However, the lack of efficient training algorithms has\nhindered their widespread adoption. Existing supervised learning algorithms for\nSNNs require significantly more memory and time than their ANN counterparts.\nEven commonly used ANN-SNN conversion methods necessitate re-training of ANNs\nto enhance conversion efficiency, incurring additional computational costs. To\naddress these challenges, we propose a novel training-free ANN-SNN conversion\npipeline. Our approach directly converts pre-trained ANN models into\nhigh-performance SNNs without additional training. The conversion pipeline\nincludes a local-learning-based threshold balancing algorithm, which enables\nefficient calculation of the optimal thresholds and fine-grained adjustment of\nthreshold value by channel-wise scaling. We demonstrate the scalability of our\nframework across three typical computer vision tasks: image classification,\nsemantic segmentation, and object detection. This showcases its applicability\nto both classification and regression tasks. Moreover, we have evaluated the\nenergy consumption of the converted SNNs, demonstrating their superior\nlow-power advantage compared to conventional ANNs. Our training-free algorithm\noutperforms existing methods, highlighting its practical applicability and\nefficiency. This approach simplifies the deployment of SNNs by leveraging\nopen-source pre-trained ANN models and neuromorphic hardware, enabling fast,\nlow-power inference with negligible performance reduction."
                },
                "authors": [
                    {
                        "name": "Tong Bu"
                    },
                    {
                        "name": "Maohua Li"
                    },
                    {
                        "name": "Zhaofei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaofei Yu"
                },
                "author": "Zhaofei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03363v1",
                "updated": "2024-09-05T09:10:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    10,
                    38,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T09:10:38Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    9,
                    10,
                    38,
                    3,
                    249,
                    0
                ],
                "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding"
                },
                "summary": "The training data in large language models is key to their success, but it\nalso presents privacy and security risks, as it may contain sensitive\ninformation. Detecting pre-training data is crucial for mitigating these\nconcerns. Existing methods typically analyze target text in isolation or solely\nwith non-member contexts, overlooking potential insights from simultaneously\nconsidering both member and non-member contexts. While previous work suggested\nthat member contexts provide little information due to the minor distributional\nshift they induce, our analysis reveals that these subtle shifts can be\neffectively leveraged when contrasted with non-member contexts. In this paper,\nwe propose Con-ReCall, a novel approach that leverages the asymmetric\ndistributional shifts induced by member and non-member contexts through\ncontrastive decoding, amplifying subtle differences to enhance membership\ninference. Extensive empirical evaluations demonstrate that Con-ReCall achieves\nstate-of-the-art performance on the WikiMIA benchmark and is robust against\nvarious text manipulation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training data in large language models is key to their success, but it\nalso presents privacy and security risks, as it may contain sensitive\ninformation. Detecting pre-training data is crucial for mitigating these\nconcerns. Existing methods typically analyze target text in isolation or solely\nwith non-member contexts, overlooking potential insights from simultaneously\nconsidering both member and non-member contexts. While previous work suggested\nthat member contexts provide little information due to the minor distributional\nshift they induce, our analysis reveals that these subtle shifts can be\neffectively leveraged when contrasted with non-member contexts. In this paper,\nwe propose Con-ReCall, a novel approach that leverages the asymmetric\ndistributional shifts induced by member and non-member contexts through\ncontrastive decoding, amplifying subtle differences to enhance membership\ninference. Extensive empirical evaluations demonstrate that Con-ReCall achieves\nstate-of-the-art performance on the WikiMIA benchmark and is robust against\nvarious text manipulation techniques."
                },
                "authors": [
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03351v2",
                "updated": "2024-09-06T07:29:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    29,
                    0,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T08:53:23Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    8,
                    53,
                    23,
                    3,
                    249,
                    0
                ],
                "title": "Digital Ecosystem for FAIR Time Series Data Management in Environmental\n  System Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Ecosystem for FAIR Time Series Data Management in Environmental\n  System Science"
                },
                "summary": "Addressing the challenges posed by climate change, biodiversity loss, and\nenvironmental pollution requires comprehensive monitoring and effective data\nmanagement strategies that are applicable across various scales in\nenvironmental system science. This paper introduces a versatile and\ntransferable digital ecosystem for managing time series data, designed to\nadhere to the FAIR principles (Findable, Accessible, Interoperable, and\nReusable). The system is highly adaptable, cloud-ready, and suitable for\ndeployment in a wide range of settings, from small-scale projects to\nlarge-scale monitoring initiatives. The ecosystem comprises three core\ncomponents: the Sensor Management System (SMS) for detailed metadata\nregistration and management; \\nolinkurl{time.IO}, a platform for efficient time\nseries data storage, transfer, and real-time visualization; and the System for\nAutomated Quality Control (SaQC), which ensures data integrity through\nreal-time analysis and quality assurance. The modular architecture, combined\nwith standardized protocols and interfaces, ensures that the ecosystem can be\neasily transferred and deployed across different environments and institutions.\nThis approach enhances data accessibility for a broad spectrum of stakeholders,\nincluding researchers, policymakers, and the public, while fostering\ncollaboration and advancing scientific research in environmental monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the challenges posed by climate change, biodiversity loss, and\nenvironmental pollution requires comprehensive monitoring and effective data\nmanagement strategies that are applicable across various scales in\nenvironmental system science. This paper introduces a versatile and\ntransferable digital ecosystem for managing time series data, designed to\nadhere to the FAIR principles (Findable, Accessible, Interoperable, and\nReusable). The system is highly adaptable, cloud-ready, and suitable for\ndeployment in a wide range of settings, from small-scale projects to\nlarge-scale monitoring initiatives. The ecosystem comprises three core\ncomponents: the Sensor Management System (SMS) for detailed metadata\nregistration and management; \\nolinkurl{time.IO}, a platform for efficient time\nseries data storage, transfer, and real-time visualization; and the System for\nAutomated Quality Control (SaQC), which ensures data integrity through\nreal-time analysis and quality assurance. The modular architecture, combined\nwith standardized protocols and interfaces, ensures that the ecosystem can be\neasily transferred and deployed across different environments and institutions.\nThis approach enhances data accessibility for a broad spectrum of stakeholders,\nincluding researchers, policymakers, and the public, while fostering\ncollaboration and advancing scientific research in environmental monitoring."
                },
                "authors": [
                    {
                        "name": "J. Bumberger"
                    },
                    {
                        "name": "M. Abbrent"
                    },
                    {
                        "name": "N. Brinckmann"
                    },
                    {
                        "name": "J. Hemmen"
                    },
                    {
                        "name": "R. Kunkel"
                    },
                    {
                        "name": "C. Lorenz"
                    },
                    {
                        "name": "P. Lnenschlo"
                    },
                    {
                        "name": "B. Palm"
                    },
                    {
                        "name": "T. Schnicke"
                    },
                    {
                        "name": "C. Schulz"
                    },
                    {
                        "name": "H. van der Schaaf"
                    },
                    {
                        "name": "D. Schfer"
                    }
                ],
                "author_detail": {
                    "name": "D. Schfer"
                },
                "author": "D. Schfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03346v1",
                "updated": "2024-09-05T08:45:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    8,
                    45,
                    44,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T08:45:44Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    8,
                    45,
                    44,
                    3,
                    249,
                    0
                ],
                "title": "Sketch: A Toolkit for Streamlining LLM Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sketch: A Toolkit for Streamlining LLM Operations"
                },
                "summary": "Large language models (LLMs) represented by GPT family have achieved\nremarkable success. The characteristics of LLMs lie in their ability to\naccommodate a wide range of tasks through a generative approach. However, the\nflexibility of their output format poses challenges in controlling and\nharnessing the model's outputs, thereby constraining the application of LLMs in\nvarious domains. In this work, we present Sketch, an innovative toolkit\ndesigned to streamline LLM operations across diverse fields. Sketch comprises\nthe following components: (1) a suite of task description schemas and prompt\ntemplates encompassing various NLP tasks; (2) a user-friendly, interactive\nprocess for building structured output LLM services tailored to various NLP\ntasks; (3) an open-source dataset for output format control, along with tools\nfor dataset construction; and (4) an open-source model based on\nLLaMA3-8B-Instruct that adeptly comprehends and adheres to output formatting\ninstructions. We anticipate this initiative to bring considerable convenience\nto LLM users, achieving the goal of ''plug-and-play'' for various applications.\nThe components of Sketch will be progressively open-sourced at\nhttps://github.com/cofe-ai/Sketch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represented by GPT family have achieved\nremarkable success. The characteristics of LLMs lie in their ability to\naccommodate a wide range of tasks through a generative approach. However, the\nflexibility of their output format poses challenges in controlling and\nharnessing the model's outputs, thereby constraining the application of LLMs in\nvarious domains. In this work, we present Sketch, an innovative toolkit\ndesigned to streamline LLM operations across diverse fields. Sketch comprises\nthe following components: (1) a suite of task description schemas and prompt\ntemplates encompassing various NLP tasks; (2) a user-friendly, interactive\nprocess for building structured output LLM services tailored to various NLP\ntasks; (3) an open-source dataset for output format control, along with tools\nfor dataset construction; and (4) an open-source model based on\nLLaMA3-8B-Instruct that adeptly comprehends and adheres to output formatting\ninstructions. We anticipate this initiative to bring considerable convenience\nto LLM users, achieving the goal of ''plug-and-play'' for various applications.\nThe components of Sketch will be progressively open-sourced at\nhttps://github.com/cofe-ai/Sketch."
                },
                "authors": [
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Wenjia Ma"
                    },
                    {
                        "name": "Xuezhi Fang"
                    },
                    {
                        "name": "Yiqun Yao"
                    },
                    {
                        "name": "Naitong Yu"
                    },
                    {
                        "name": "Xuying Meng"
                    },
                    {
                        "name": "Peng Han"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Aixin Sun"
                    },
                    {
                        "name": "Yequan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yequan Wang"
                },
                "author": "Yequan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03307v1",
                "updated": "2024-09-05T07:23:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    23,
                    30,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T07:23:30Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    23,
                    30,
                    3,
                    249,
                    0
                ],
                "title": "AI data transparency: an exploration through the lens of AI incidents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI data transparency: an exploration through the lens of AI incidents"
                },
                "summary": "Knowing more about the data used to build AI systems is critical for allowing\ndifferent stakeholders to play their part in ensuring responsible and\nappropriate deployment and use. Meanwhile, a 2023 report shows that data\ntransparency lags significantly behind other areas of AI transparency in\npopular foundation models. In this research, we sought to build on these\nfindings, exploring the status of public documentation about data practices\nwithin AI systems generating public concern.\n  Our findings demonstrate that low data transparency persists across a wide\nrange of systems, and further that issues of transparency and explainability at\nmodel- and system- level create barriers for investigating data transparency\ninformation to address public concerns about AI systems. We highlight a need to\ndevelop systematic ways of monitoring AI data transparency that account for the\ndiversity of AI system types, and for such efforts to build on further\nunderstanding of the needs of those both supplying and using data transparency\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing more about the data used to build AI systems is critical for allowing\ndifferent stakeholders to play their part in ensuring responsible and\nappropriate deployment and use. Meanwhile, a 2023 report shows that data\ntransparency lags significantly behind other areas of AI transparency in\npopular foundation models. In this research, we sought to build on these\nfindings, exploring the status of public documentation about data practices\nwithin AI systems generating public concern.\n  Our findings demonstrate that low data transparency persists across a wide\nrange of systems, and further that issues of transparency and explainability at\nmodel- and system- level create barriers for investigating data transparency\ninformation to address public concerns about AI systems. We highlight a need to\ndevelop systematic ways of monitoring AI data transparency that account for the\ndiversity of AI system types, and for such efforts to build on further\nunderstanding of the needs of those both supplying and using data transparency\ninformation."
                },
                "authors": [
                    {
                        "name": "Sophia Worth"
                    },
                    {
                        "name": "Ben Snaith"
                    },
                    {
                        "name": "Arunav Das"
                    },
                    {
                        "name": "Gefion Thuermer"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02727v2",
                "updated": "2024-09-05T07:17:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    17,
                    59,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T14:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    14,
                    1,
                    48,
                    2,
                    248,
                    0
                ],
                "title": "Pooling And Attention: What Are Effective Designs For LLM-Based\n  Embedding Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pooling And Attention: What Are Effective Designs For LLM-Based\n  Embedding Models?"
                },
                "summary": "The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models."
                },
                "authors": [
                    {
                        "name": "Yixuan Tang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "https://github.com/yixuantt/PoolingAndAttn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03295v1",
                "updated": "2024-09-05T07:03:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    3,
                    23,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T07:03:23Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    7,
                    3,
                    23,
                    3,
                    249,
                    0
                ],
                "title": "N-gram Prediction and Word Difference Representations for Language\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-gram Prediction and Word Difference Representations for Language\n  Modeling"
                },
                "summary": "Causal language modeling (CLM) serves as the foundational framework\nunderpinning remarkable successes of recent large language models (LLMs).\nDespite its success, the training approach for next word prediction poses a\npotential risk of causing the model to overly focus on local dependencies\nwithin a sentence. While prior studies have been introduced to predict future N\nwords simultaneously, they were primarily applied to tasks such as masked\nlanguage modeling (MLM) and neural machine translation (NMT). In this study, we\nintroduce a simple N-gram prediction framework for the CLM task. Moreover, we\nintroduce word difference representation (WDR) as a surrogate and\ncontextualized target representation during model training on the basis of\nN-gram prediction framework. To further enhance the quality of next word\nprediction, we propose an ensemble method that incorporates the future N words'\nprediction results. Empirical evaluations across multiple benchmark datasets\nencompassing CLM and NMT tasks demonstrate the significant advantages of our\nproposed methods over the conventional CLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal language modeling (CLM) serves as the foundational framework\nunderpinning remarkable successes of recent large language models (LLMs).\nDespite its success, the training approach for next word prediction poses a\npotential risk of causing the model to overly focus on local dependencies\nwithin a sentence. While prior studies have been introduced to predict future N\nwords simultaneously, they were primarily applied to tasks such as masked\nlanguage modeling (MLM) and neural machine translation (NMT). In this study, we\nintroduce a simple N-gram prediction framework for the CLM task. Moreover, we\nintroduce word difference representation (WDR) as a surrogate and\ncontextualized target representation during model training on the basis of\nN-gram prediction framework. To further enhance the quality of next word\nprediction, we propose an ensemble method that incorporates the future N words'\nprediction results. Empirical evaluations across multiple benchmark datasets\nencompassing CLM and NMT tasks demonstrate the significant advantages of our\nproposed methods over the conventional CLM."
                },
                "authors": [
                    {
                        "name": "DongNyeong Heo"
                    },
                    {
                        "name": "Daniela Noemi Rim"
                    },
                    {
                        "name": "Heeyoul Choi"
                    }
                ],
                "author_detail": {
                    "name": "Heeyoul Choi"
                },
                "author": "Heeyoul Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03291v1",
                "updated": "2024-09-05T06:55:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    55,
                    13,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:55:13Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    55,
                    13,
                    3,
                    249,
                    0
                ],
                "title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated\n  Short News-Like Posts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated\n  Short News-Like Posts"
                },
                "summary": "With the emergence of widely available powerful LLMs, disinformation\ngenerated by large Language Models (LLMs) has become a major concern.\nHistorically, LLM detectors have been touted as a solution, but their\neffectiveness in the real world is still to be proven. In this paper, we focus\non an important setting in information operations -- short news-like posts\ngenerated by moderately sophisticated attackers.\n  We demonstrate that existing LLM detectors, whether zero-shot or\npurpose-trained, are not ready for real-world use in that setting. All tested\nzero-shot detectors perform inconsistently with prior benchmarks and are highly\nvulnerable to sampling temperature increase, a trivial attack absent from\nrecent benchmarks. A purpose-trained detector generalizing across LLMs and\nunseen attacks can be developed, but it fails to generalize to new\nhuman-written texts.\n  We argue that the former indicates domain-specific benchmarking is needed,\nwhile the latter suggests a trade-off between the adversarial evasion\nresilience and overfitting to the reference human text, with both needing\nevaluation in benchmarks and currently absent. We believe this suggests a\nre-consideration of current LLM detector benchmarking approaches and provides a\ndynamically extensible benchmark to allow it\n(https://github.com/Reliable-Information-Lab-HEVS/dynamic_llm_detector_benchmark).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of widely available powerful LLMs, disinformation\ngenerated by large Language Models (LLMs) has become a major concern.\nHistorically, LLM detectors have been touted as a solution, but their\neffectiveness in the real world is still to be proven. In this paper, we focus\non an important setting in information operations -- short news-like posts\ngenerated by moderately sophisticated attackers.\n  We demonstrate that existing LLM detectors, whether zero-shot or\npurpose-trained, are not ready for real-world use in that setting. All tested\nzero-shot detectors perform inconsistently with prior benchmarks and are highly\nvulnerable to sampling temperature increase, a trivial attack absent from\nrecent benchmarks. A purpose-trained detector generalizing across LLMs and\nunseen attacks can be developed, but it fails to generalize to new\nhuman-written texts.\n  We argue that the former indicates domain-specific benchmarking is needed,\nwhile the latter suggests a trade-off between the adversarial evasion\nresilience and overfitting to the reference human text, with both needing\nevaluation in benchmarks and currently absent. We believe this suggests a\nre-consideration of current LLM detector benchmarking approaches and provides a\ndynamically extensible benchmark to allow it\n(https://github.com/Reliable-Information-Lab-HEVS/dynamic_llm_detector_benchmark)."
                },
                "authors": [
                    {
                        "name": "Henrique Da Silva Gameiro"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    },
                    {
                        "name": "Ljiljana Dolamic"
                    }
                ],
                "author_detail": {
                    "name": "Ljiljana Dolamic"
                },
                "author": "Ljiljana Dolamic",
                "arxiv_comment": "20 pages, 7 tables, 13 figures, under consideration for EMNLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03277v1",
                "updated": "2024-09-05T06:41:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    41,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:41:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    41,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding"
                },
                "summary": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, the application of alignment training\nwithin the chart domain is still underexplored. To address this, we propose\nChartMoE, which employs the mixture of expert (MoE) architecture to replace the\ntraditional linear projector to bridge the modality gap. Specifically, we train\nmultiple linear connectors through distinct alignment tasks, which are utilized\nas the foundational initialization parameters for different experts.\nAdditionally, we introduce ChartMoE-Align, a dataset with over 900K\nchart-table-JSON-code quadruples to conduct three alignment tasks\n(chart-table/JSON/code). Combined with the vanilla connector, we initialize\ndifferent experts in four distinct ways and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, the application of alignment training\nwithin the chart domain is still underexplored. To address this, we propose\nChartMoE, which employs the mixture of expert (MoE) architecture to replace the\ntraditional linear projector to bridge the modality gap. Specifically, we train\nmultiple linear connectors through distinct alignment tasks, which are utilized\nas the foundational initialization parameters for different experts.\nAdditionally, we introduce ChartMoE-Align, a dataset with over 900K\nchart-table-JSON-code quadruples to conduct three alignment tasks\n(chart-table/JSON/code). Combined with the vanilla connector, we initialize\ndifferent experts in four distinct ways and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48% to 84.64% on the ChartQA benchmark."
                },
                "authors": [
                    {
                        "name": "Zhengzhuo Xu"
                    },
                    {
                        "name": "Bowen Qu"
                    },
                    {
                        "name": "Yiyan Qi"
                    },
                    {
                        "name": "Sinan Du"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16567v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16567v3",
                "updated": "2024-09-05T06:34:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    34,
                    11,
                    3,
                    249,
                    0
                ],
                "published": "2024-02-26T13:46:51Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    13,
                    46,
                    51,
                    0,
                    57,
                    0
                ],
                "title": "Aligning Large Language Models to a Domain-specific Graph Database for\n  NL2GQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models to a Domain-specific Graph Database for\n  NL2GQL"
                },
                "summary": "Graph Databases (Graph DB) find extensive application across diverse domains\nsuch as finance, social networks, and medicine. Yet, the translation of Natural\nLanguage (NL) into the Graph Query Language (GQL), referred to as NL2GQL, poses\nsignificant challenges owing to its intricate and specialized nature. Some\napproaches have sought to utilize Large Language Models (LLMs) to address\nanalogous tasks like text2SQL. Nonetheless, in the realm of NL2GQL tasks\ntailored to a particular domain, the absence of domain-specific NL-GQL data\npairs adds complexity to aligning LLMs with the graph DB. To tackle this\nchallenge, we present a well-defined pipeline. Initially, we utilize ChatGPT to\ngenerate NL-GQL data pairs, leveraging the provided graph DB with\nself-instruction. Subsequently, we employ the generated data to fine-tune LLMs,\nensuring alignment between LLMs and the graph DB. Moreover, we find the\nimportance of relevant schema in efficiently generating accurate GQLs. Thus, we\nintroduce a method to extract relevant schema as the input context. We evaluate\nour method using two carefully constructed datasets derived from graph DBs in\nthe finance and medicine domains, named FinGQL and MediGQL. Experimental\nresults reveal that our approach significantly outperforms a set of baseline\nmethods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and\n7.09 absolute points on EX for FinGQL and MediGQL, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Databases (Graph DB) find extensive application across diverse domains\nsuch as finance, social networks, and medicine. Yet, the translation of Natural\nLanguage (NL) into the Graph Query Language (GQL), referred to as NL2GQL, poses\nsignificant challenges owing to its intricate and specialized nature. Some\napproaches have sought to utilize Large Language Models (LLMs) to address\nanalogous tasks like text2SQL. Nonetheless, in the realm of NL2GQL tasks\ntailored to a particular domain, the absence of domain-specific NL-GQL data\npairs adds complexity to aligning LLMs with the graph DB. To tackle this\nchallenge, we present a well-defined pipeline. Initially, we utilize ChatGPT to\ngenerate NL-GQL data pairs, leveraging the provided graph DB with\nself-instruction. Subsequently, we employ the generated data to fine-tune LLMs,\nensuring alignment between LLMs and the graph DB. Moreover, we find the\nimportance of relevant schema in efficiently generating accurate GQLs. Thus, we\nintroduce a method to extract relevant schema as the input context. We evaluate\nour method using two carefully constructed datasets derived from graph DBs in\nthe finance and medicine domains, named FinGQL and MediGQL. Experimental\nresults reveal that our approach significantly outperforms a set of baseline\nmethods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and\n7.09 absolute points on EX for FinGQL and MediGQL, respectively."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Liang"
                    },
                    {
                        "name": "Keren Tan"
                    },
                    {
                        "name": "Tingyu Xie"
                    },
                    {
                        "name": "Wenbiao Tao"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Yunshi Lan"
                    },
                    {
                        "name": "Weining Qian"
                    }
                ],
                "author_detail": {
                    "name": "Weining Qian"
                },
                "author": "Weining Qian",
                "arxiv_comment": "13 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16567v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16567v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15966v2",
                "updated": "2024-09-05T06:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    33,
                    31,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-28T17:38:44Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    44,
                    2,
                    241,
                    0
                ],
                "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding"
                },
                "summary": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM."
                },
                "authors": [
                    {
                        "name": "Yuan Tang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Qiao Yu"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Yixue Hao"
                    },
                    {
                        "name": "Long Hu"
                    },
                    {
                        "name": "Min Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min Chen"
                },
                "author": "Min Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03274v2",
                "updated": "2024-09-06T10:31:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    10,
                    31,
                    7,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-05T06:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    31,
                    37,
                    3,
                    249,
                    0
                ],
                "title": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Attack and Defense Approaches of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence and\nmachine learning through their advanced text processing and generating\ncapabilities. However, their widespread deployment has raised significant\nsafety and reliability concerns. Established vulnerabilities in deep neural\nnetworks, coupled with emerging threat models, may compromise security\nevaluations and create a false sense of security. Given the extensive research\nin the field of LLM security, we believe that summarizing the current state of\naffairs will help the research community better understand the present\nlandscape and inform future developments. This paper reviews current research\non LLM vulnerabilities and threats, and evaluates the effectiveness of\ncontemporary defense mechanisms. We analyze recent studies on attack vectors\nand model weaknesses, providing insights into attack mechanisms and the\nevolving threat landscape. We also examine current defense strategies,\nhighlighting their strengths and limitations. By contrasting advancements in\nattack and defense methodologies, we identify research gaps and propose future\ndirections to enhance LLM security. Our goal is to advance the understanding of\nLLM safety challenges and guide the development of more robust security\nmeasures."
                },
                "authors": [
                    {
                        "name": "Jing Cui"
                    },
                    {
                        "name": "Yishi Xu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Shuchang Zhou"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03272v1",
                "updated": "2024-09-05T06:30:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    30,
                    1,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:30:01Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    30,
                    1,
                    3,
                    249,
                    0
                ],
                "title": "OccLLaMA: An Occupancy-Language-Action Generative World Model for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OccLLaMA: An Occupancy-Language-Action Generative World Model for\n  Autonomous Driving"
                },
                "summary": "The rise of multi-modal large language models(MLLMs) has spurred their\napplications in autonomous driving. Recent MLLM-based methods perform action by\nlearning a direct mapping from perception to action, neglecting the dynamics of\nthe world and the relations between action and world dynamics. In contrast,\nhuman beings possess world model that enables them to simulate the future\nstates based on 3D internal visual representation and plan actions accordingly.\nTo this end, we propose OccLLaMA, an occupancy-language-action generative world\nmodel, which uses semantic occupancy as a general visual representation and\nunifies vision-language-action(VLA) modalities through an autoregressive model.\nSpecifically, we introduce a novel VQVAE-like scene tokenizer to efficiently\ndiscretize and reconstruct semantic occupancy scenes, considering its sparsity\nand classes imbalance. Then, we build a unified multi-modal vocabulary for\nvision, language and action. Furthermore, we enhance LLM, specifically LLaMA,\nto perform the next token/scene prediction on the unified vocabulary to\ncomplete multiple tasks in autonomous driving. Extensive experiments\ndemonstrate that OccLLaMA achieves competitive performance across multiple\ntasks, including 4D occupancy forecasting, motion planning, and visual question\nanswering, showcasing its potential as a foundation model in autonomous\ndriving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of multi-modal large language models(MLLMs) has spurred their\napplications in autonomous driving. Recent MLLM-based methods perform action by\nlearning a direct mapping from perception to action, neglecting the dynamics of\nthe world and the relations between action and world dynamics. In contrast,\nhuman beings possess world model that enables them to simulate the future\nstates based on 3D internal visual representation and plan actions accordingly.\nTo this end, we propose OccLLaMA, an occupancy-language-action generative world\nmodel, which uses semantic occupancy as a general visual representation and\nunifies vision-language-action(VLA) modalities through an autoregressive model.\nSpecifically, we introduce a novel VQVAE-like scene tokenizer to efficiently\ndiscretize and reconstruct semantic occupancy scenes, considering its sparsity\nand classes imbalance. Then, we build a unified multi-modal vocabulary for\nvision, language and action. Furthermore, we enhance LLM, specifically LLaMA,\nto perform the next token/scene prediction on the unified vocabulary to\ncomplete multiple tasks in autonomous driving. Extensive experiments\ndemonstrate that OccLLaMA achieves competitive performance across multiple\ntasks, including 4D occupancy forecasting, motion planning, and visual question\nanswering, showcasing its potential as a foundation model in autonomous\ndriving."
                },
                "authors": [
                    {
                        "name": "Julong Wei"
                    },
                    {
                        "name": "Shanshuai Yuan"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Qingda Hu"
                    },
                    {
                        "name": "Zhongxue Gan"
                    },
                    {
                        "name": "Wenchao Ding"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Ding"
                },
                "author": "Wenchao Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03271v1",
                "updated": "2024-09-05T06:28:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    28,
                    5,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:28:05Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    28,
                    5,
                    3,
                    249,
                    0
                ],
                "title": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through\n  Strategy Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through\n  Strategy Elicitation"
                },
                "summary": "The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for\nenhancing the reasoning capabilities of large language models (LLMs). However,\ndespite their widespread adoption and success, CoT methods often exhibit\ninstability due to their inability to consistently ensure the quality of\ngenerated reasoning paths, leading to sub-optimal reasoning performance. To\naddress this challenge, we propose the \\textbf{Strategic Chain-of-Thought}\n(SCoT), a novel methodology designed to refine LLM performance by integrating\nstrategic knowledge prior to generating intermediate reasoning steps. SCoT\nemploys a two-stage approach within a single prompt: first eliciting an\neffective problem-solving strategy, which is then used to guide the generation\nof high-quality CoT paths and final answers. Our experiments across eight\nchallenging reasoning datasets demonstrate significant improvements, including\na 21.05\\% increase on the GSM8K dataset and 24.13\\% on the Tracking\\_Objects\ndataset, respectively, using the Llama3-8b model. Additionally, we extend the\nSCoT framework to develop a few-shot method with automatically matched\ndemonstrations, yielding even stronger results. These findings underscore the\nefficacy of SCoT, highlighting its potential to substantially enhance LLM\nperformance in complex reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for\nenhancing the reasoning capabilities of large language models (LLMs). However,\ndespite their widespread adoption and success, CoT methods often exhibit\ninstability due to their inability to consistently ensure the quality of\ngenerated reasoning paths, leading to sub-optimal reasoning performance. To\naddress this challenge, we propose the \\textbf{Strategic Chain-of-Thought}\n(SCoT), a novel methodology designed to refine LLM performance by integrating\nstrategic knowledge prior to generating intermediate reasoning steps. SCoT\nemploys a two-stage approach within a single prompt: first eliciting an\neffective problem-solving strategy, which is then used to guide the generation\nof high-quality CoT paths and final answers. Our experiments across eight\nchallenging reasoning datasets demonstrate significant improvements, including\na 21.05\\% increase on the GSM8K dataset and 24.13\\% on the Tracking\\_Objects\ndataset, respectively, using the Llama3-8b model. Additionally, we extend the\nSCoT framework to develop a few-shot method with automatically matched\ndemonstrations, yielding even stronger results. These findings underscore the\nefficacy of SCoT, highlighting its potential to substantially enhance LLM\nperformance in complex reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Zhihu Wang"
                    },
                    {
                        "name": "Heyuan Huang"
                    },
                    {
                        "name": "Ming Fan"
                    },
                    {
                        "name": "Yubo Zhang"
                    },
                    {
                        "name": "Zhixing Wang"
                    },
                    {
                        "name": "Haijun Wang"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03267v1",
                "updated": "2024-09-05T06:24:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    24,
                    29,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T06:24:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    6,
                    24,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "No Man is an Island: Towards Fully Automatic Programming by Code Search,\n  Code Generation and Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Man is an Island: Towards Fully Automatic Programming by Code Search,\n  Code Generation and Program Repair"
                },
                "summary": "Automatic programming attempts to minimize human intervention in the\ngeneration of executable code, and has been a long-standing challenge in the\nsoftware engineering community. To advance automatic programming, researchers\nare focusing on three primary directions: (1) code search that reuses existing\ncode snippets from external databases; (2) code generation that produces new\ncode snippets from natural language; and (3) program repair that refines\nexisting code snippets by fixing detected bugs. Despite significant\nadvancements, the effectiveness of state-of-the-art techniques is still\nlimited, such as the usability of searched code and the correctness of\ngenerated code.\n  Motivated by the real-world programming process, where developers usually use\nvarious external tools to aid their coding processes, such as code search\nengines and code testing tools, in this work, we propose \\toolname{}, an\nautomatic programming framework that leverages recent large language models\n(LLMs) to integrate the three research areas to address their inherent\nlimitations. In particular, our framework first leverages different code search\nstrategies to retrieve similar code snippets, which are then used to further\nguide the code generation process of LLMs. Our framework further validates the\nquality of generated code by compilers and test cases, and constructs repair\nprompts to query LLMs for generating correct patches. We conduct preliminary\nexperiments to demonstrate the potential of our framework, \\eg helping\nCodeLlama solve 267 programming problems with an improvement of 62.53\\%. As a\ngeneric framework, \\toolname{} can integrate various code search, generation,\nand repair tools, combining these three research areas together for the first\ntime. More importantly, it demonstrates the potential of using traditional SE\ntools to enhance the usability of LLMs in automatic programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic programming attempts to minimize human intervention in the\ngeneration of executable code, and has been a long-standing challenge in the\nsoftware engineering community. To advance automatic programming, researchers\nare focusing on three primary directions: (1) code search that reuses existing\ncode snippets from external databases; (2) code generation that produces new\ncode snippets from natural language; and (3) program repair that refines\nexisting code snippets by fixing detected bugs. Despite significant\nadvancements, the effectiveness of state-of-the-art techniques is still\nlimited, such as the usability of searched code and the correctness of\ngenerated code.\n  Motivated by the real-world programming process, where developers usually use\nvarious external tools to aid their coding processes, such as code search\nengines and code testing tools, in this work, we propose \\toolname{}, an\nautomatic programming framework that leverages recent large language models\n(LLMs) to integrate the three research areas to address their inherent\nlimitations. In particular, our framework first leverages different code search\nstrategies to retrieve similar code snippets, which are then used to further\nguide the code generation process of LLMs. Our framework further validates the\nquality of generated code by compilers and test cases, and constructs repair\nprompts to query LLMs for generating correct patches. We conduct preliminary\nexperiments to demonstrate the potential of our framework, \\eg helping\nCodeLlama solve 267 programming problems with an improvement of 62.53\\%. As a\ngeneric framework, \\toolname{} can integrate various code search, generation,\nand repair tools, combining these three research areas together for the first\ntime. More importantly, it demonstrates the potential of using traditional SE\ntools to enhance the usability of LLMs in automatic programming."
                },
                "authors": [
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Ye Shang"
                    },
                    {
                        "name": "Tongke Zhang"
                    },
                    {
                        "name": "Shengcheng Yu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02387v2",
                "updated": "2024-09-05T05:36:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    36,
                    10,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T02:30:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    30,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges"
                },
                "summary": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Keyu Chen"
                },
                "author": "Keyu Chen",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03258v1",
                "updated": "2024-09-05T05:34:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    34,
                    16,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T05:34:16Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    34,
                    16,
                    3,
                    249,
                    0
                ],
                "title": "GraphInsight: Unlocking Insights in Large Language Models for Graph\n  Structure Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphInsight: Unlocking Insights in Large Language Models for Graph\n  Structure Understanding"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated potential in\nprocessing graphs, they struggle with comprehending graphical structure\ninformation through prompts of graph description sequences, especially as the\ngraph size increases. We attribute this challenge to the uneven memory\nperformance of LLMs across different positions in graph description sequences,\nknown as ''positional biases''. To address this, we propose GraphInsight, a\nnovel framework aimed at improving LLMs' comprehension of both macro- and\nmicro-level graphical information. GraphInsight is grounded in two key\nstrategies: 1) placing critical graphical information in positions where LLMs\nexhibit stronger memory performance, and 2) investigating a lightweight\nexternal knowledge base for regions with weaker memory performance, inspired by\nretrieval-augmented generation (RAG). Moreover, GraphInsight explores\nintegrating these two strategies into LLM agent processes for composite graph\ntasks that require multi-step reasoning. Extensive empirical studies on\nbenchmarks with a wide range of evaluation tasks show that GraphInsight\nsignificantly outperforms all other graph description methods (e.g., prompting\ntechniques and reordering strategies) in understanding graph structures of\nvarying sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated potential in\nprocessing graphs, they struggle with comprehending graphical structure\ninformation through prompts of graph description sequences, especially as the\ngraph size increases. We attribute this challenge to the uneven memory\nperformance of LLMs across different positions in graph description sequences,\nknown as ''positional biases''. To address this, we propose GraphInsight, a\nnovel framework aimed at improving LLMs' comprehension of both macro- and\nmicro-level graphical information. GraphInsight is grounded in two key\nstrategies: 1) placing critical graphical information in positions where LLMs\nexhibit stronger memory performance, and 2) investigating a lightweight\nexternal knowledge base for regions with weaker memory performance, inspired by\nretrieval-augmented generation (RAG). Moreover, GraphInsight explores\nintegrating these two strategies into LLM agent processes for composite graph\ntasks that require multi-step reasoning. Extensive empirical studies on\nbenchmarks with a wide range of evaluation tasks show that GraphInsight\nsignificantly outperforms all other graph description methods (e.g., prompting\ntechniques and reordering strategies) in understanding graph structures of\nvarying sizes."
                },
                "authors": [
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Shuo Han"
                    },
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Zezhong Ding"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03257v1",
                "updated": "2024-09-05T05:31:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    31,
                    29,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T05:31:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    5,
                    31,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "Understanding LLM Development Through Longitudinal Study: Insights from\n  the Open Ko-LLM Leaderboard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Development Through Longitudinal Study: Insights from\n  the Open Ko-LLM Leaderboard"
                },
                "summary": "This paper conducts a longitudinal study over eleven months to address the\nlimitations of prior research on the Open Ko-LLM Leaderboard, which have relied\non empirical studies with restricted observation periods of only five months.\nBy extending the analysis duration, we aim to provide a more comprehensive\nunderstanding of the progression in developing Korean large language models\n(LLMs). Our study is guided by three primary research questions: (1) What are\nthe specific challenges in improving LLM performance across diverse tasks on\nthe Open Ko-LLM Leaderboard over time? (2) How does model size impact task\nperformance correlations across various benchmarks? (3) How have the patterns\nin leaderboard rankings shifted over time on the Open Ko-LLM Leaderboard?. By\nanalyzing 1,769 models over this period, our research offers a comprehensive\nexamination of the ongoing advancements in LLMs and the evolving nature of\nevaluation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper conducts a longitudinal study over eleven months to address the\nlimitations of prior research on the Open Ko-LLM Leaderboard, which have relied\non empirical studies with restricted observation periods of only five months.\nBy extending the analysis duration, we aim to provide a more comprehensive\nunderstanding of the progression in developing Korean large language models\n(LLMs). Our study is guided by three primary research questions: (1) What are\nthe specific challenges in improving LLM performance across diverse tasks on\nthe Open Ko-LLM Leaderboard over time? (2) How does model size impact task\nperformance correlations across various benchmarks? (3) How have the patterns\nin leaderboard rankings shifted over time on the Open Ko-LLM Leaderboard?. By\nanalyzing 1,769 models over this period, our research offers a comprehensive\nexamination of the ongoing advancements in LLMs and the evolving nature of\nevaluation frameworks."
                },
                "authors": [
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Hyeonwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeonwoo Kim"
                },
                "author": "Hyeonwoo Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03247v1",
                "updated": "2024-09-05T04:51:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    4,
                    51,
                    18,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T04:51:18Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    4,
                    51,
                    18,
                    3,
                    249,
                    0
                ],
                "title": "End User Authoring of Personalized Content Classifiers: Comparing\n  Example Labeling, Rule Writing, and LLM Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End User Authoring of Personalized Content Classifiers: Comparing\n  Example Labeling, Rule Writing, and LLM Prompting"
                },
                "summary": "Existing tools for laypeople to create personal classifiers often assume a\nmotivated user working uninterrupted in a single, lengthy session. However,\nusers tend to engage with social media casually, with many short sessions on an\nongoing, daily basis. To make creating personal classifiers for content\ncuration easier for such users, tools should support rapid initialization and\niterative refinement. In this work, we compare three strategies -- (1) example\nlabeling, (2) rule writing, and (3) large language model (LLM) prompting -- for\nend users to build personal content classifiers. From an experiment with 37\nnon-programmers tasked with creating personalized comment moderation filters,\nwe found that with LLM prompting, participants reached 95\\% of peak performance\nin 5 minutes, beating other strategies due to higher recall, but all strategies\nstruggled with iterative refinement. Despite LLM prompting's better\nperformance, participants preferred different strategies in different contexts\nand, even when prompting, provided examples or wrote rule-like prompts,\nsuggesting hybrid approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing tools for laypeople to create personal classifiers often assume a\nmotivated user working uninterrupted in a single, lengthy session. However,\nusers tend to engage with social media casually, with many short sessions on an\nongoing, daily basis. To make creating personal classifiers for content\ncuration easier for such users, tools should support rapid initialization and\niterative refinement. In this work, we compare three strategies -- (1) example\nlabeling, (2) rule writing, and (3) large language model (LLM) prompting -- for\nend users to build personal content classifiers. From an experiment with 37\nnon-programmers tasked with creating personalized comment moderation filters,\nwe found that with LLM prompting, participants reached 95\\% of peak performance\nin 5 minutes, beating other strategies due to higher recall, but all strategies\nstruggled with iterative refinement. Despite LLM prompting's better\nperformance, participants preferred different strategies in different contexts\nand, even when prompting, provided examples or wrote rule-like prompts,\nsuggesting hybrid approaches."
                },
                "authors": [
                    {
                        "name": "Leijie Wang"
                    },
                    {
                        "name": "Kathryn Yurechko"
                    },
                    {
                        "name": "Pranati Dani"
                    },
                    {
                        "name": "Quan Ze Chen"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02897v2",
                "updated": "2024-09-05T03:53:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    53,
                    13,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-04T17:41:19Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    41,
                    19,
                    2,
                    248,
                    0
                ],
                "title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA"
                },
                "summary": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o."
                },
                "authors": [
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Wanjun Gu"
                    },
                    {
                        "name": "Danqing Liu"
                    },
                    {
                        "name": "Minhao Zou"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Ling Feng"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03225v1",
                "updated": "2024-09-05T03:45:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    45,
                    35,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T03:45:35Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    45,
                    35,
                    3,
                    249,
                    0
                ],
                "title": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration"
                },
                "summary": "Black-box large language models (LLMs) are increasingly deployed in various\nenvironments, making it essential for these models to effectively convey their\nconfidence and uncertainty, especially in high-stakes settings. However, these\nmodels often exhibit overconfidence, leading to potential risks and\nmisjudgments. Existing techniques for eliciting and calibrating LLM confidence\nhave primarily focused on general reasoning datasets, yielding only modest\nimprovements. Accurate calibration is crucial for informed decision-making and\npreventing adverse outcomes but remains challenging due to the complexity and\nvariability of tasks these models perform. In this work, we investigate the\nmiscalibration behavior of black-box LLMs within the healthcare setting. We\npropose a novel method, \\textit{Atypical Presentations Recalibration}, which\nleverages atypical presentations to adjust the model's confidence estimates.\nOur approach significantly improves calibration, reducing calibration errors by\napproximately 60\\% on three medical question answering datasets and\noutperforming existing methods such as vanilla verbalized confidence, CoT\nverbalized confidence and others. Additionally, we provide an in-depth analysis\nof the role of atypicality within the recalibration framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box large language models (LLMs) are increasingly deployed in various\nenvironments, making it essential for these models to effectively convey their\nconfidence and uncertainty, especially in high-stakes settings. However, these\nmodels often exhibit overconfidence, leading to potential risks and\nmisjudgments. Existing techniques for eliciting and calibrating LLM confidence\nhave primarily focused on general reasoning datasets, yielding only modest\nimprovements. Accurate calibration is crucial for informed decision-making and\npreventing adverse outcomes but remains challenging due to the complexity and\nvariability of tasks these models perform. In this work, we investigate the\nmiscalibration behavior of black-box LLMs within the healthcare setting. We\npropose a novel method, \\textit{Atypical Presentations Recalibration}, which\nleverages atypical presentations to adjust the model's confidence estimates.\nOur approach significantly improves calibration, reducing calibration errors by\napproximately 60\\% on three medical question answering datasets and\noutperforming existing methods such as vanilla verbalized confidence, CoT\nverbalized confidence and others. Additionally, we provide an in-depth analysis\nof the role of atypicality within the recalibration framework."
                },
                "authors": [
                    {
                        "name": "Jeremy Qin"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Quoc Dinh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Quoc Dinh Nguyen"
                },
                "author": "Quoc Dinh Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03219v1",
                "updated": "2024-09-05T03:33:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    33,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T03:33:54Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    33,
                    54,
                    3,
                    249,
                    0
                ],
                "title": "Content Moderation by LLM: From Accuracy to Legitimacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Moderation by LLM: From Accuracy to Legitimacy"
                },
                "summary": "One trending application of LLM (large language model) is to use it for\ncontent moderation in online platforms. Most current studies on this\napplication have focused on the metric of accuracy - the extent to which LLM\nmakes correct decisions about content. This article argues that accuracy is\ninsufficient and misleading, because it fails to grasp the distinction between\neasy cases and hard cases as well as the inevitable trade-offs in achieving\nhigher accuracy. Closer examination reveals that content moderation is a\nconstitutive part of platform governance, the key of which is to gain and\nenhance legitimacy. Instead of making moderation decisions correct, the chief\ngoal of LLM is to make them legitimate. In this regard, this article proposes a\nparadigm shift from the single benchmark of accuracy towards a legitimacy-based\nframework of evaluating the performance of LLM moderators. The framework\nsuggests that for easy cases, the key is to ensure accuracy, speed and\ntransparency, while for hard cases, what matters is reasoned justification and\nuser participation. Examined under this framework, LLM's real potential in\nmoderation is not accuracy improvement. Rather, LLM can better contribute in\nfour other aspects: to conduct screening of hard cases from easy cases, to\nprovide quality explanations for moderation decisions, to assist human\nreviewers in getting more contextual information, and to facilitate user\nparticipation in a more interactive way. Using normative theories from law and\nsocial sciences to critically assess the new technological application, this\narticle seeks to redefine LLM's role in content moderation and redirect\nrelevant research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One trending application of LLM (large language model) is to use it for\ncontent moderation in online platforms. Most current studies on this\napplication have focused on the metric of accuracy - the extent to which LLM\nmakes correct decisions about content. This article argues that accuracy is\ninsufficient and misleading, because it fails to grasp the distinction between\neasy cases and hard cases as well as the inevitable trade-offs in achieving\nhigher accuracy. Closer examination reveals that content moderation is a\nconstitutive part of platform governance, the key of which is to gain and\nenhance legitimacy. Instead of making moderation decisions correct, the chief\ngoal of LLM is to make them legitimate. In this regard, this article proposes a\nparadigm shift from the single benchmark of accuracy towards a legitimacy-based\nframework of evaluating the performance of LLM moderators. The framework\nsuggests that for easy cases, the key is to ensure accuracy, speed and\ntransparency, while for hard cases, what matters is reasoned justification and\nuser participation. Examined under this framework, LLM's real potential in\nmoderation is not accuracy improvement. Rather, LLM can better contribute in\nfour other aspects: to conduct screening of hard cases from easy cases, to\nprovide quality explanations for moderation decisions, to assist human\nreviewers in getting more contextual information, and to facilitate user\nparticipation in a more interactive way. Using normative theories from law and\nsocial sciences to critically assess the new technological application, this\narticle seeks to redefine LLM's role in content moderation and redirect\nrelevant research in this field."
                },
                "authors": [
                    {
                        "name": "Tao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Huang"
                },
                "author": "Tao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03215v1",
                "updated": "2024-09-05T03:22:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    22,
                    22,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T03:22:22Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    3,
                    22,
                    22,
                    3,
                    249,
                    0
                ],
                "title": "xLAM: A Family of Large Action Models to Empower AI Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLAM: A Family of Large Action Models to Empower AI Agent Systems"
                },
                "summary": "Autonomous agents powered by large language models (LLMs) have attracted\nsignificant research interest. However, the open-source community faces many\nchallenges in developing specialized models for agent tasks, driven by the\nscarcity of high-quality agent datasets and the absence of standard protocols\nin this area. We introduce and publicly release xLAM, a series of large action\nmodels designed for AI agent tasks. The xLAM series includes five models with\nboth dense and mixture-of-expert architectures, ranging from 1B to 8x22B\nparameters, trained using a scalable, flexible pipeline that unifies, augments,\nand synthesizes diverse datasets to enhance AI agents' generalizability and\nperformance across varied environments. Our experimental results demonstrate\nthat xLAM consistently delivers exceptional performance across multiple agent\nability benchmarks, notably securing the 1st position on the Berkeley\nFunction-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other\nmodels in terms of tool use. By releasing the xLAM series, we aim to advance\nthe performance of open-source LLMs for autonomous AI agents, potentially\naccelerating progress and democratizing access to high-performance models for\nagent tasks. Models are available at\nhttps://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by large language models (LLMs) have attracted\nsignificant research interest. However, the open-source community faces many\nchallenges in developing specialized models for agent tasks, driven by the\nscarcity of high-quality agent datasets and the absence of standard protocols\nin this area. We introduce and publicly release xLAM, a series of large action\nmodels designed for AI agent tasks. The xLAM series includes five models with\nboth dense and mixture-of-expert architectures, ranging from 1B to 8x22B\nparameters, trained using a scalable, flexible pipeline that unifies, augments,\nand synthesizes diverse datasets to enhance AI agents' generalizability and\nperformance across varied environments. Our experimental results demonstrate\nthat xLAM consistently delivers exceptional performance across multiple agent\nability benchmarks, notably securing the 1st position on the Berkeley\nFunction-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other\nmodels in terms of tool use. By releasing the xLAM series, we aim to advance\nthe performance of open-source LLMs for autonomous AI agents, potentially\naccelerating progress and democratizing access to high-performance models for\nagent tasks. Models are available at\nhttps://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4"
                },
                "authors": [
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Shirley Kokane"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Tulika Awalgaonkar"
                    },
                    {
                        "name": "Rithesh Murthy"
                    },
                    {
                        "name": "Eric Hu"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "Technical report for the Salesforce xLAM model series",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03206v1",
                "updated": "2024-09-05T02:54:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    54,
                    17,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T02:54:17Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    54,
                    17,
                    3,
                    249,
                    0
                ],
                "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with\n  Temporal Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with\n  Temporal Considerations"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have significantly improved\nperformance across various image-language applications. Recently, there has\nbeen a growing interest in adapting image pre-trained MLLMs for video-related\ntasks. However, most efforts concentrate on enhancing the vision encoder and\nprojector components, while the core part, Large Language Models (LLMs),\nremains comparatively under-explored. In this paper, we propose two strategies\nto enhance the model's capability in video understanding tasks by improving\ninter-layer attention computation in LLMs. Specifically, the first approach\nfocuses on the enhancement of Rotary Position Embedding (RoPE) with\nTemporal-Aware Dual RoPE, which introduces temporal position information to\nstrengthen the MLLM's temporal modeling capabilities while preserving the\nrelative position relationships of both visual and text tokens. The second\napproach involves enhancing the Attention Mask with the Frame-wise Block Causal\nAttention Mask, a simple yet effective method that broadens visual token\ninteractions within and across video frames while maintaining the causal\ninference mechanism. Based on these proposed methods, we adapt LLaVA for video\nunderstanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our\nTC-LLaVA achieves new state-of-the-art performance across various video\nunderstanding benchmarks with only supervised fine-tuning (SFT) on\nvideo-related datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have significantly improved\nperformance across various image-language applications. Recently, there has\nbeen a growing interest in adapting image pre-trained MLLMs for video-related\ntasks. However, most efforts concentrate on enhancing the vision encoder and\nprojector components, while the core part, Large Language Models (LLMs),\nremains comparatively under-explored. In this paper, we propose two strategies\nto enhance the model's capability in video understanding tasks by improving\ninter-layer attention computation in LLMs. Specifically, the first approach\nfocuses on the enhancement of Rotary Position Embedding (RoPE) with\nTemporal-Aware Dual RoPE, which introduces temporal position information to\nstrengthen the MLLM's temporal modeling capabilities while preserving the\nrelative position relationships of both visual and text tokens. The second\napproach involves enhancing the Attention Mask with the Frame-wise Block Causal\nAttention Mask, a simple yet effective method that broadens visual token\ninteractions within and across video frames while maintaining the causal\ninference mechanism. Based on these proposed methods, we adapt LLaVA for video\nunderstanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our\nTC-LLaVA achieves new state-of-the-art performance across various video\nunderstanding benchmarks with only supervised fine-tuning (SFT) on\nvideo-related datasets."
                },
                "authors": [
                    {
                        "name": "Mingze Gao"
                    },
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Mingda Li"
                    },
                    {
                        "name": "Jiangtao Xie"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03203v1",
                "updated": "2024-09-05T02:51:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    51,
                    28,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T02:51:28Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    51,
                    28,
                    3,
                    249,
                    0
                ],
                "title": "An Effective Deployment of Diffusion LM for Data Augmentation in\n  Low-Resource Sentiment Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Effective Deployment of Diffusion LM for Data Augmentation in\n  Low-Resource Sentiment Classification"
                },
                "summary": "Sentiment classification (SC) often suffers from low-resource challenges such\nas domain-specific contexts, imbalanced label distributions, and few-shot\nscenarios. The potential of the diffusion language model (LM) for textual data\naugmentation (DA) remains unexplored, moreover, textual DA methods struggle to\nbalance the diversity and consistency of new samples. Most DA methods either\nperform logical modifications or rephrase less important tokens in the original\nsequence with the language model. In the context of SC, strong emotional tokens\ncould act critically on the sentiment of the whole sequence. Therefore,\ncontrary to rephrasing less important context, we propose DiffusionCLS to\nleverage a diffusion LM to capture in-domain knowledge and generate pseudo\nsamples by reconstructing strong label-related tokens. This approach ensures a\nbalance between consistency and diversity, avoiding the introduction of noise\nand augmenting crucial features of datasets. DiffusionCLS also comprises a\nNoise-Resistant Training objective to help the model generalize. Experiments\ndemonstrate the effectiveness of our method in various low-resource scenarios\nincluding domain-specific and domain-general problems. Ablation studies confirm\nthe effectiveness of our framework's modules, and visualization studies\nhighlight optimal deployment conditions, reinforcing our conclusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment classification (SC) often suffers from low-resource challenges such\nas domain-specific contexts, imbalanced label distributions, and few-shot\nscenarios. The potential of the diffusion language model (LM) for textual data\naugmentation (DA) remains unexplored, moreover, textual DA methods struggle to\nbalance the diversity and consistency of new samples. Most DA methods either\nperform logical modifications or rephrase less important tokens in the original\nsequence with the language model. In the context of SC, strong emotional tokens\ncould act critically on the sentiment of the whole sequence. Therefore,\ncontrary to rephrasing less important context, we propose DiffusionCLS to\nleverage a diffusion LM to capture in-domain knowledge and generate pseudo\nsamples by reconstructing strong label-related tokens. This approach ensures a\nbalance between consistency and diversity, avoiding the introduction of noise\nand augmenting crucial features of datasets. DiffusionCLS also comprises a\nNoise-Resistant Training objective to help the model generalize. Experiments\ndemonstrate the effectiveness of our method in various low-resource scenarios\nincluding domain-specific and domain-general problems. Ablation studies confirm\nthe effectiveness of our framework's modules, and visualization studies\nhighlight optimal deployment conditions, reinforcing our conclusions."
                },
                "authors": [
                    {
                        "name": "Zhuowei Chen"
                    },
                    {
                        "name": "Lianxi Wang"
                    },
                    {
                        "name": "Yuben Wu"
                    },
                    {
                        "name": "Xinfeng Liao"
                    },
                    {
                        "name": "Yujia Tian"
                    },
                    {
                        "name": "Junyang Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Zhong"
                },
                "author": "Junyang Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00935v2",
                "updated": "2024-09-05T02:39:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    39,
                    23,
                    3,
                    249,
                    0
                ],
                "published": "2023-10-02T06:57:45Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    6,
                    57,
                    45,
                    0,
                    275,
                    0
                ],
                "title": "Resolving Knowledge Conflicts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving Knowledge Conflicts in Large Language Models"
                },
                "summary": "Large language models (LLMs) often encounter knowledge conflicts, scenarios\nwhere discrepancy arises between the internal parametric knowledge of LLMs and\nnon-parametric information provided in the prompt context. In this work we ask\nwhat are the desiderata for LLMs when a knowledge conflict arises and whether\nexisting LLMs fulfill them. We posit that LLMs should 1) identify knowledge\nconflicts, 2) pinpoint conflicting information segments, and 3) provide\ndistinct answers or viewpoints in conflicting scenarios. To this end, we\nintroduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual\nknowledge conflicts and quantitatively evaluating to what extent LLMs achieve\nthese goals. KNOWLEDGE CONFLICT includes diverse and complex situations of\nknowledge conflict, knowledge from diverse entities and domains, two synthetic\nconflict creation methods, and settings with progressively increasing\ndifficulty to reflect realistic knowledge conflicts. Extensive experiments with\nthe KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in\nidentifying the existence of knowledge conflicts, they struggle to determine\nthe specific conflicting knowledge and produce a response with distinct answers\namidst conflicting information. To address these challenges, we propose new\ninstruction-based approaches that augment LLMs to better achieve the three\ngoals. Further analysis shows that abilities to tackle knowledge conflicts are\ngreatly impacted by factors such as knowledge domain and prompt text, while\ngenerating robust responses to knowledge conflict scenarios remains an open\nresearch question.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often encounter knowledge conflicts, scenarios\nwhere discrepancy arises between the internal parametric knowledge of LLMs and\nnon-parametric information provided in the prompt context. In this work we ask\nwhat are the desiderata for LLMs when a knowledge conflict arises and whether\nexisting LLMs fulfill them. We posit that LLMs should 1) identify knowledge\nconflicts, 2) pinpoint conflicting information segments, and 3) provide\ndistinct answers or viewpoints in conflicting scenarios. To this end, we\nintroduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual\nknowledge conflicts and quantitatively evaluating to what extent LLMs achieve\nthese goals. KNOWLEDGE CONFLICT includes diverse and complex situations of\nknowledge conflict, knowledge from diverse entities and domains, two synthetic\nconflict creation methods, and settings with progressively increasing\ndifficulty to reflect realistic knowledge conflicts. Extensive experiments with\nthe KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in\nidentifying the existence of knowledge conflicts, they struggle to determine\nthe specific conflicting knowledge and produce a response with distinct answers\namidst conflicting information. To address these challenges, we propose new\ninstruction-based approaches that augment LLMs to better achieve the three\ngoals. Further analysis shows that abilities to tackle knowledge conflicts are\ngreatly impacted by factors such as knowledge domain and prompt text, while\ngenerating robust responses to knowledge conflict scenarios remains an open\nresearch question."
                },
                "authors": [
                    {
                        "name": "Yike Wang"
                    },
                    {
                        "name": "Shangbin Feng"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Vidhisha Balachandran"
                    },
                    {
                        "name": "Tianxing He"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    }
                ],
                "author_detail": {
                    "name": "Yulia Tsvetkov"
                },
                "author": "Yulia Tsvetkov",
                "arxiv_comment": "Published at COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13993v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13993v4",
                "updated": "2024-09-05T02:21:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    2,
                    21,
                    42,
                    3,
                    249,
                    0
                ],
                "published": "2024-04-22T08:59:35Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    8,
                    59,
                    35,
                    0,
                    113,
                    0
                ],
                "title": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion"
                },
                "summary": "Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series."
                },
                "authors": [
                    {
                        "name": "Yingxuan Li"
                    },
                    {
                        "name": "Ryota Hinami"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    },
                    {
                        "name": "Yusuke Matsui"
                    }
                ],
                "author_detail": {
                    "name": "Yusuke Matsui"
                },
                "author": "Yusuke Matsui",
                "arxiv_comment": "Accepted to ACM Multimedia 2024. Project page:\n  https://liyingxuan1012.github.io/zeroshot-speaker-prediction ; Github repo:\n  https://github.com/liyingxuan1012/zeroshot-speaker-prediction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13993v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13993v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03171v1",
                "updated": "2024-09-05T01:58:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    58,
                    29,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T01:58:29Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    58,
                    29,
                    3,
                    249,
                    0
                ],
                "title": "MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented\n  Generation Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented\n  Generation Question Answering"
                },
                "summary": "In this paper we present a multi-adapter retrieval augmented generation\nsystem (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP\n2024. CRAG is a question answering dataset contains 3 different subtasks aimed\nat realistic question and answering RAG related tasks, with a diverse set of\nquestion topics, question types, time dynamic answers, and questions featuring\nentities of varying popularity.\n  Our system follows a standard setup for web based RAG, which uses processed\nweb pages to provide context for an LLM to produce generations, while also\nquerying API endpoints for additional information. MARAGS also utilizes\nmultiple different adapters to solve the various requirements for these tasks\nwith a standard cross-encoder model for ranking candidate passages relevant for\nanswering the question. Our system achieved 2nd place for Task 1 as well as 3rd\nplace on Task 2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present a multi-adapter retrieval augmented generation\nsystem (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP\n2024. CRAG is a question answering dataset contains 3 different subtasks aimed\nat realistic question and answering RAG related tasks, with a diverse set of\nquestion topics, question types, time dynamic answers, and questions featuring\nentities of varying popularity.\n  Our system follows a standard setup for web based RAG, which uses processed\nweb pages to provide context for an LLM to produce generations, while also\nquerying API endpoints for additional information. MARAGS also utilizes\nmultiple different adapters to solve the various requirements for these tasks\nwith a standard cross-encoder model for ranking candidate passages relevant for\nanswering the question. Our system achieved 2nd place for Task 1 as well as 3rd\nplace on Task 2."
                },
                "authors": [
                    {
                        "name": "Mitchell DeHaven"
                    }
                ],
                "author_detail": {
                    "name": "Mitchell DeHaven"
                },
                "author": "Mitchell DeHaven",
                "arxiv_comment": "Accepted to CRAG KDD Cup 24 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03166v1",
                "updated": "2024-09-05T01:51:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    51,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T01:51:54Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    51,
                    54,
                    3,
                    249,
                    0
                ],
                "title": "Continual Skill and Task Learning via Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Skill and Task Learning via Dialogue"
                },
                "summary": "Continual and interactive robot learning is a challenging problem as the\nrobot is present with human users who expect the robot to learn novel skills to\nsolve novel tasks perpetually with sample efficiency. In this work we present a\nframework for robots to query and learn visuo-motor robot skills and task\nrelevant information via natural language dialog interactions with human users.\nPrevious approaches either focus on improving the performance of instruction\nfollowing agents, or passively learn novel skills or concepts. Instead, we used\ndialog combined with a language-skill grounding embedding to query or confirm\nskills and/or tasks requested by a user. To achieve this goal, we developed and\nintegrated three different components for our agent. Firstly, we propose a\nnovel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA),\nwhich enables the existing SoTA ACT model to perform few-shot continual\nlearning. Secondly, we develop an alignment model that projects demonstrations\nacross skill embodiments into a shared embedding allowing us to know when to\nask questions and/or demonstrations from users. Finally, we integrated an\nexisting LLM to interact with a human user to perform grounded interactive\ncontinual skill learning to solve a task. Our ACT-LoRA model learns novel\nfine-tuned skills with a 100% accuracy when trained with only five\ndemonstrations for a novel skill while still maintaining a 74.75% accuracy on\npre-trained skills in the RLBench dataset where other models fall significantly\nshort. We also performed a human-subjects study with 8 subjects to demonstrate\nthe continual learning capabilities of our combined framework. We achieve a\nsuccess rate of 75% in the task of sandwich making with the real robot learning\nfrom participant data demonstrating that robots can learn novel skills or task\nknowledge from dialogue with non-expert users using our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual and interactive robot learning is a challenging problem as the\nrobot is present with human users who expect the robot to learn novel skills to\nsolve novel tasks perpetually with sample efficiency. In this work we present a\nframework for robots to query and learn visuo-motor robot skills and task\nrelevant information via natural language dialog interactions with human users.\nPrevious approaches either focus on improving the performance of instruction\nfollowing agents, or passively learn novel skills or concepts. Instead, we used\ndialog combined with a language-skill grounding embedding to query or confirm\nskills and/or tasks requested by a user. To achieve this goal, we developed and\nintegrated three different components for our agent. Firstly, we propose a\nnovel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA),\nwhich enables the existing SoTA ACT model to perform few-shot continual\nlearning. Secondly, we develop an alignment model that projects demonstrations\nacross skill embodiments into a shared embedding allowing us to know when to\nask questions and/or demonstrations from users. Finally, we integrated an\nexisting LLM to interact with a human user to perform grounded interactive\ncontinual skill learning to solve a task. Our ACT-LoRA model learns novel\nfine-tuned skills with a 100% accuracy when trained with only five\ndemonstrations for a novel skill while still maintaining a 74.75% accuracy on\npre-trained skills in the RLBench dataset where other models fall significantly\nshort. We also performed a human-subjects study with 8 subjects to demonstrate\nthe continual learning capabilities of our combined framework. We achieve a\nsuccess rate of 75% in the task of sandwich making with the real robot learning\nfrom participant data demonstrating that robots can learn novel skills or task\nknowledge from dialogue with non-expert users using our approach."
                },
                "authors": [
                    {
                        "name": "Weiwei Gu"
                    },
                    {
                        "name": "Suresh Kondepudi"
                    },
                    {
                        "name": "Lixiao Huang"
                    },
                    {
                        "name": "Nakul Gopalan"
                    }
                ],
                "author_detail": {
                    "name": "Nakul Gopalan"
                },
                "author": "Nakul Gopalan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03163v1",
                "updated": "2024-09-05T01:47:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    47,
                    43,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T01:47:43Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    47,
                    43,
                    3,
                    249,
                    0
                ],
                "title": "CyberDep: Towards the Analysis of Cyber-Physical Power System\n  Interdependencies Using Bayesian Networks and Temporal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberDep: Towards the Analysis of Cyber-Physical Power System\n  Interdependencies Using Bayesian Networks and Temporal Data"
                },
                "summary": "Modern-day power systems have become increasingly cyber-physical due to the\nongoing developments to the grid that include the rise of distributed energy\ngeneration and the increase of the deployment of many cyber devices for\nmonitoring and control, such as the Supervisory Control and Data Acquisition\n(SCADA) system. Such capabilities have made the power system more vulnerable to\ncyber-attacks that can harm the physical components of the system. As such, it\nis of utmost importance to study both the physical and cyber components\ntogether, focusing on characterizing and quantifying the interdependency\nbetween these components. This paper focuses on developing an algorithm, named\nCyberDep, for Bayesian network generation through conditional probability\ncalculations of cyber traffic flows between system nodes. Additionally,\nCyberDep is implemented on the temporal data of the cyber-physical emulation of\nthe WSCC 9-bus power system. The results of this work provide a visual\nrepresentation of the probabilistic relationships within the cyber and physical\ncomponents of the system, aiding in cyber-physical interdependency\nquantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern-day power systems have become increasingly cyber-physical due to the\nongoing developments to the grid that include the rise of distributed energy\ngeneration and the increase of the deployment of many cyber devices for\nmonitoring and control, such as the Supervisory Control and Data Acquisition\n(SCADA) system. Such capabilities have made the power system more vulnerable to\ncyber-attacks that can harm the physical components of the system. As such, it\nis of utmost importance to study both the physical and cyber components\ntogether, focusing on characterizing and quantifying the interdependency\nbetween these components. This paper focuses on developing an algorithm, named\nCyberDep, for Bayesian network generation through conditional probability\ncalculations of cyber traffic flows between system nodes. Additionally,\nCyberDep is implemented on the temporal data of the cyber-physical emulation of\nthe WSCC 9-bus power system. The results of this work provide a visual\nrepresentation of the probabilistic relationships within the cyber and physical\ncomponents of the system, aiding in cyber-physical interdependency\nquantification."
                },
                "authors": [
                    {
                        "name": "Leen Al Homoud"
                    },
                    {
                        "name": "Katherine Davis"
                    },
                    {
                        "name": "Shamina Hossain-McKenzie"
                    },
                    {
                        "name": "Nicholas Jacobs"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Jacobs"
                },
                "author": "Nicholas Jacobs",
                "arxiv_comment": "Accepted and Presented at the 2024 Kansas Power and Energy Conference\n  (KPEC 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03161v1",
                "updated": "2024-09-05T01:36:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    36,
                    0,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T01:36:00Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    36,
                    0,
                    3,
                    249,
                    0
                ],
                "title": "MaterialBENCH: Evaluating College-Level Materials Science\n  Problem-Solving Abilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaterialBENCH: Evaluating College-Level Materials Science\n  Problem-Solving Abilities of Large Language Models"
                },
                "summary": "A college-level benchmark dataset for large language models (LLMs) in the\nmaterials science field, MaterialBENCH, is constructed. This dataset consists\nof problem-answer pairs, based on university textbooks. There are two types of\nproblems: one is the free-response answer type, and the other is the\nmultiple-choice type. Multiple-choice problems are constructed by adding three\nincorrect answers as choices to a correct answer, so that LLMs can choose one\nof the four as a response. Most of the problems for free-response answer and\nmultiple-choice types overlap except for the format of the answers. We also\nconduct experiments using the MaterialBENCH on LLMs, including ChatGPT-3.5,\nChatGPT-4, Bard (at the time of the experiments), and GPT-3.5 and GPT-4 with\nthe OpenAI API. The differences and similarities in the performance of LLMs\nmeasured by the MaterialBENCH are analyzed and discussed. Performance\ndifferences between the free-response type and multiple-choice type in the same\nmodels and the influence of using system massages on multiple-choice problems\nare also studied. We anticipate that MaterialBENCH will encourage further\ndevelopments of LLMs in reasoning abilities to solve more complicated problems\nand eventually contribute to materials research and discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A college-level benchmark dataset for large language models (LLMs) in the\nmaterials science field, MaterialBENCH, is constructed. This dataset consists\nof problem-answer pairs, based on university textbooks. There are two types of\nproblems: one is the free-response answer type, and the other is the\nmultiple-choice type. Multiple-choice problems are constructed by adding three\nincorrect answers as choices to a correct answer, so that LLMs can choose one\nof the four as a response. Most of the problems for free-response answer and\nmultiple-choice types overlap except for the format of the answers. We also\nconduct experiments using the MaterialBENCH on LLMs, including ChatGPT-3.5,\nChatGPT-4, Bard (at the time of the experiments), and GPT-3.5 and GPT-4 with\nthe OpenAI API. The differences and similarities in the performance of LLMs\nmeasured by the MaterialBENCH are analyzed and discussed. Performance\ndifferences between the free-response type and multiple-choice type in the same\nmodels and the influence of using system massages on multiple-choice problems\nare also studied. We anticipate that MaterialBENCH will encourage further\ndevelopments of LLMs in reasoning abilities to solve more complicated problems\nand eventually contribute to materials research and discovery."
                },
                "authors": [
                    {
                        "name": "Michiko Yoshitake"
                    },
                    {
                        "name": "Yuta Suzuki"
                    },
                    {
                        "name": "Ryo Igarashi"
                    },
                    {
                        "name": "Yoshitaka Ushiku"
                    },
                    {
                        "name": "Keisuke Nagato"
                    }
                ],
                "author_detail": {
                    "name": "Keisuke Nagato"
                },
                "arxiv_affiliation": "Univ. Tokyo",
                "author": "Keisuke Nagato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03155v1",
                "updated": "2024-09-05T01:11:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    11,
                    58,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T01:11:58Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    11,
                    58,
                    3,
                    249,
                    0
                ],
                "title": "Debate on Graph: a Flexible and Reliable Reasoning Framework for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debate on Graph: a Flexible and Reliable Reasoning Framework for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) may suffer from hallucinations in real-world\napplications due to the lack of relevant knowledge. In contrast, knowledge\ngraphs encompass extensive, multi-relational structures that store a vast array\nof symbolic facts. Consequently, integrating LLMs with knowledge graphs has\nbeen extensively explored, with Knowledge Graph Question Answering (KGQA)\nserving as a critical touchstone for the integration. This task requires LLMs\nto answer natural language questions by retrieving relevant triples from\nknowledge graphs. However, existing methods face two significant challenges:\n\\textit{excessively long reasoning paths distracting from the answer\ngeneration}, and \\textit{false-positive relations hindering the path\nrefinement}. In this paper, we propose an iterative interactive KGQA framework\nthat leverages the interactive learning capabilities of LLMs to perform\nreasoning and Debating over Graphs (DoG). Specifically, DoG employs a\nsubgraph-focusing mechanism, allowing LLMs to perform answer trying after each\nreasoning step, thereby mitigating the impact of lengthy reasoning paths. On\nthe other hand, DoG utilizes a multi-role debate team to gradually simplify\ncomplex questions, reducing the influence of false-positive relations. This\ndebate mechanism ensures the reliability of the reasoning process. Experimental\nresults on five public datasets demonstrate the effectiveness and superiority\nof our architecture. Notably, DoG outperforms the state-of-the-art method ToG\nby 23.7\\% and 9.1\\% in accuracy on WebQuestions and GrailQA, respectively.\nFurthermore, the integration experiments with various LLMs on the mentioned\ndatasets highlight the flexibility of DoG. Code is available at\n\\url{https://github.com/reml-group/DoG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) may suffer from hallucinations in real-world\napplications due to the lack of relevant knowledge. In contrast, knowledge\ngraphs encompass extensive, multi-relational structures that store a vast array\nof symbolic facts. Consequently, integrating LLMs with knowledge graphs has\nbeen extensively explored, with Knowledge Graph Question Answering (KGQA)\nserving as a critical touchstone for the integration. This task requires LLMs\nto answer natural language questions by retrieving relevant triples from\nknowledge graphs. However, existing methods face two significant challenges:\n\\textit{excessively long reasoning paths distracting from the answer\ngeneration}, and \\textit{false-positive relations hindering the path\nrefinement}. In this paper, we propose an iterative interactive KGQA framework\nthat leverages the interactive learning capabilities of LLMs to perform\nreasoning and Debating over Graphs (DoG). Specifically, DoG employs a\nsubgraph-focusing mechanism, allowing LLMs to perform answer trying after each\nreasoning step, thereby mitigating the impact of lengthy reasoning paths. On\nthe other hand, DoG utilizes a multi-role debate team to gradually simplify\ncomplex questions, reducing the influence of false-positive relations. This\ndebate mechanism ensures the reliability of the reasoning process. Experimental\nresults on five public datasets demonstrate the effectiveness and superiority\nof our architecture. Notably, DoG outperforms the state-of-the-art method ToG\nby 23.7\\% and 9.1\\% in accuracy on WebQuestions and GrailQA, respectively.\nFurthermore, the integration experiments with various LLMs on the mentioned\ndatasets highlight the flexibility of DoG. Code is available at\n\\url{https://github.com/reml-group/DoG}."
                },
                "authors": [
                    {
                        "name": "Jie Ma"
                    },
                    {
                        "name": "Zhitao Gao"
                    },
                    {
                        "name": "Qi Chai"
                    },
                    {
                        "name": "Wangchun Sun"
                    },
                    {
                        "name": "Pinghui Wang"
                    },
                    {
                        "name": "Hongbin Pei"
                    },
                    {
                        "name": "Jing Tao"
                    },
                    {
                        "name": "Lingyun Song"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Lizhen Cui"
                    }
                ],
                "author_detail": {
                    "name": "Lizhen Cui"
                },
                "author": "Lizhen Cui",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03147v1",
                "updated": "2024-09-05T00:52:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    0,
                    52,
                    59,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T00:52:59Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    0,
                    52,
                    59,
                    3,
                    249,
                    0
                ],
                "title": "Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced\n  Diagnostic Models through Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the Gaps in Early Dementia Detection: A Path Towards Enhanced\n  Diagnostic Models through Machine Learning"
                },
                "summary": "The rapid global aging trend has led to an increase in dementia cases,\nincluding Alzheimer's disease, underscoring the urgent need for early and\naccurate diagnostic methods. Traditional diagnostic techniques, such as\ncognitive tests, neuroimaging, and biomarker analysis, face significant\nlimitations in sensitivity, accessibility, and cost, particularly in the early\nstages. This study explores the potential of machine learning (ML) as a\ntransformative approach to enhance early dementia detection by leveraging ML\nmodels to analyze and integrate complex multimodal datasets, including\ncognitive assessments, neuroimaging, and genetic information. A comprehensive\nreview of existing literature was conducted to evaluate various ML models,\nincluding supervised learning, deep learning, and advanced techniques such as\nensemble learning and transformer models, assessing their accuracy,\ninterpretability, and potential for clinical integration. The findings indicate\nthat while ML models show significant promise in improving diagnostic precision\nand enabling earlier interventions, challenges remain in their\ngeneralizability, interpretability, and ethical deployment. This research\nconcludes by outlining future directions aimed at enhancing the clinical\nutility of ML models in dementia detection, emphasizing interdisciplinary\ncollaboration and ethically sound frameworks to improve early detection and\nintervention strategies for Alzheimer's disease and other forms of dementia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid global aging trend has led to an increase in dementia cases,\nincluding Alzheimer's disease, underscoring the urgent need for early and\naccurate diagnostic methods. Traditional diagnostic techniques, such as\ncognitive tests, neuroimaging, and biomarker analysis, face significant\nlimitations in sensitivity, accessibility, and cost, particularly in the early\nstages. This study explores the potential of machine learning (ML) as a\ntransformative approach to enhance early dementia detection by leveraging ML\nmodels to analyze and integrate complex multimodal datasets, including\ncognitive assessments, neuroimaging, and genetic information. A comprehensive\nreview of existing literature was conducted to evaluate various ML models,\nincluding supervised learning, deep learning, and advanced techniques such as\nensemble learning and transformer models, assessing their accuracy,\ninterpretability, and potential for clinical integration. The findings indicate\nthat while ML models show significant promise in improving diagnostic precision\nand enabling earlier interventions, challenges remain in their\ngeneralizability, interpretability, and ethical deployment. This research\nconcludes by outlining future directions aimed at enhancing the clinical\nutility of ML models in dementia detection, emphasizing interdisciplinary\ncollaboration and ethically sound frameworks to improve early detection and\nintervention strategies for Alzheimer's disease and other forms of dementia."
                },
                "authors": [
                    {
                        "name": "Juan A. Berrios Moya"
                    }
                ],
                "author_detail": {
                    "name": "Juan A. Berrios Moya"
                },
                "author": "Juan A. Berrios Moya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03137v1",
                "updated": "2024-09-05T00:13:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    0,
                    13,
                    16,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T00:13:16Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    0,
                    13,
                    16,
                    3,
                    249,
                    0
                ],
                "title": "The AdEMAMix Optimizer: Better, Faster, Older",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AdEMAMix Optimizer: Better, Faster, Older"
                },
                "summary": "Momentum based optimizers are central to a wide range of machine learning\napplications. These typically rely on an Exponential Moving Average (EMA) of\ngradients, which decays exponentially the present contribution of older\ngradients. This accounts for gradients being local linear approximations which\nlose their relevance as the iterate moves along the loss landscape. This work\nquestions the use of a single EMA to accumulate past gradients and empirically\ndemonstrates how this choice can be sub-optimal: a single EMA cannot\nsimultaneously give a high weight to the immediate past, and a non-negligible\nweight to older gradients. Building on this observation, we propose AdEMAMix, a\nsimple modification of the Adam optimizer with a mixture of two EMAs to better\ntake advantage of past gradients. Our experiments on language modeling and\nimage classification show -- quite surprisingly -- that gradients can stay\nrelevant for tens of thousands of steps. They help to converge faster, and\noften to lower minima: e.g., a $1.3$B parameter AdEMAMix LLM trained on $101$B\ntokens performs comparably to an AdamW model trained on $197$B tokens\n($+95\\%$). Moreover, our method significantly slows-down model forgetting\nduring training. Our work motivates further exploration of different types of\nfunctions to leverage past gradients, beyond EMAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Momentum based optimizers are central to a wide range of machine learning\napplications. These typically rely on an Exponential Moving Average (EMA) of\ngradients, which decays exponentially the present contribution of older\ngradients. This accounts for gradients being local linear approximations which\nlose their relevance as the iterate moves along the loss landscape. This work\nquestions the use of a single EMA to accumulate past gradients and empirically\ndemonstrates how this choice can be sub-optimal: a single EMA cannot\nsimultaneously give a high weight to the immediate past, and a non-negligible\nweight to older gradients. Building on this observation, we propose AdEMAMix, a\nsimple modification of the Adam optimizer with a mixture of two EMAs to better\ntake advantage of past gradients. Our experiments on language modeling and\nimage classification show -- quite surprisingly -- that gradients can stay\nrelevant for tens of thousands of steps. They help to converge faster, and\noften to lower minima: e.g., a $1.3$B parameter AdEMAMix LLM trained on $101$B\ntokens performs comparably to an AdamW model trained on $197$B tokens\n($+95\\%$). Moreover, our method significantly slows-down model forgetting\nduring training. Our work motivates further exploration of different types of\nfunctions to leverage past gradients, beyond EMAs."
                },
                "authors": [
                    {
                        "name": "Matteo Pagliardini"
                    },
                    {
                        "name": "Pierre Ablin"
                    },
                    {
                        "name": "David Grangier"
                    }
                ],
                "author_detail": {
                    "name": "David Grangier"
                },
                "author": "David Grangier",
                "arxiv_comment": "38 pages, 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05074v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05074v3",
                "updated": "2024-09-04T23:47:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    23,
                    47,
                    8,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-09T14:02:24Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    2,
                    24,
                    4,
                    222,
                    0
                ],
                "title": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Surv: Improving Mortality Prediction After Radiotherapy with Large\n  Language Model Structuring of Large-Scale Unstructured Electronic Health\n  Records"
                },
                "summary": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate patient selection is critical in radiotherapy (RT) to prevent\nineffective treatments. Traditional survival prediction models, relying on\nstructured data, often lack precision. This study explores the potential of\nlarge language models (LLMs) to structure unstructured electronic health record\n(EHR) data, thereby improving survival prediction accuracy through\ncomprehensive clinical information integration. Data from 34,276 patients\ntreated with RT at Yonsei Cancer Center between 2013 and 2023 were analyzed,\nencompassing both structured and unstructured data. An open-source LLM was used\nto structure the unstructured EHR data via single-shot learning, with its\nperformance compared against a domain-specific medical LLM and a smaller\nvariant. Survival prediction models were developed using statistical, machine\nlearning, and deep learning approaches, incorporating both structured and\nLLM-structured data. Clinical experts evaluated the accuracy of the\nLLM-structured data. The open-source LLM achieved 87.5% accuracy in structuring\nunstructured EHR data without additional training, significantly outperforming\nthe domain-specific medical LLM, which reached only 35.8% accuracy. Larger LLMs\nwere more effective, particularly in extracting clinically relevant features\nlike general condition and disease extent, which closely correlated with\npatient survival. Incorporating LLM-structured clinical features into survival\nprediction models significantly improved accuracy, with the C-index of deep\nlearning models increasing from 0.737 to 0.820. These models also became more\ninterpretable by emphasizing clinically significant factors. This study shows\nthat general-domain LLMs, even without specific medical training, can\neffectively structure large-scale unstructured EHR data, substantially\nenhancing the accuracy and interpretability of clinical predictive models."
                },
                "authors": [
                    {
                        "name": "Sangjoon Park"
                    },
                    {
                        "name": "Chan Woo Wee"
                    },
                    {
                        "name": "Seo Hee Choi"
                    },
                    {
                        "name": "Kyung Hwan Kim"
                    },
                    {
                        "name": "Jee Suk Chang"
                    },
                    {
                        "name": "Hong In Yoon"
                    },
                    {
                        "name": "Ik Jae Lee"
                    },
                    {
                        "name": "Yong Bae Kim"
                    },
                    {
                        "name": "Jaeho Cho"
                    },
                    {
                        "name": "Ki Chang Keum"
                    },
                    {
                        "name": "Chang Geol Lee"
                    },
                    {
                        "name": "Hwa Kyung Byun"
                    },
                    {
                        "name": "Woong Sub Koom"
                    }
                ],
                "author_detail": {
                    "name": "Woong Sub Koom"
                },
                "author": "Woong Sub Koom",
                "arxiv_comment": "23 pages, 2 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05074v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05074v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03131v1",
                "updated": "2024-09-04T23:45:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    23,
                    45,
                    10,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T23:45:10Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    23,
                    45,
                    10,
                    2,
                    248,
                    0
                ],
                "title": "Well, that escalated quickly: The Single-Turn Crescendo Attack (STCA)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Well, that escalated quickly: The Single-Turn Crescendo Attack (STCA)"
                },
                "summary": "This paper explores a novel approach to adversarial attacks on large language\nmodels (LLM): the Single-Turn Crescendo Attack (STCA). The STCA builds upon the\nmulti-turn crescendo attack established by Mark Russinovich, Ahmed Salem, Ronen\nEldan. Traditional multi-turn adversarial strategies gradually escalate the\ncontext to elicit harmful or controversial responses from LLMs. However, this\npaper introduces a more efficient method where the escalation is condensed into\na single interaction. By carefully crafting the prompt to simulate an extended\ndialogue, the attack bypasses typical content moderation systems, leading to\nthe generation of responses that would normally be filtered out. I demonstrate\nthis technique through a few case studies. The results highlight\nvulnerabilities in current LLMs and underscore the need for more robust\nsafeguards. This work contributes to the broader discourse on responsible AI\n(RAI) safety and adversarial testing, providing insights and practical examples\nfor researchers and developers. This method is unexplored in the literature,\nmaking it a novel contribution to the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores a novel approach to adversarial attacks on large language\nmodels (LLM): the Single-Turn Crescendo Attack (STCA). The STCA builds upon the\nmulti-turn crescendo attack established by Mark Russinovich, Ahmed Salem, Ronen\nEldan. Traditional multi-turn adversarial strategies gradually escalate the\ncontext to elicit harmful or controversial responses from LLMs. However, this\npaper introduces a more efficient method where the escalation is condensed into\na single interaction. By carefully crafting the prompt to simulate an extended\ndialogue, the attack bypasses typical content moderation systems, leading to\nthe generation of responses that would normally be filtered out. I demonstrate\nthis technique through a few case studies. The results highlight\nvulnerabilities in current LLMs and underscore the need for more robust\nsafeguards. This work contributes to the broader discourse on responsible AI\n(RAI) safety and adversarial testing, providing insights and practical examples\nfor researchers and developers. This method is unexplored in the literature,\nmaking it a novel contribution to the field."
                },
                "authors": [
                    {
                        "name": "Alan Aqrawi"
                    }
                ],
                "author_detail": {
                    "name": "Alan Aqrawi"
                },
                "author": "Alan Aqrawi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.00889v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.00889v3",
                "updated": "2024-09-04T23:37:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    23,
                    37,
                    11,
                    2,
                    248,
                    0
                ],
                "published": "2023-11-01T22:46:31Z",
                "published_parsed": [
                    2023,
                    11,
                    1,
                    22,
                    46,
                    31,
                    2,
                    305,
                    0
                ],
                "title": "SALLM: Security Assessment of Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALLM: Security Assessment of Generated Code"
                },
                "summary": "With the growing popularity of Large Language Models (LLMs) in software\nengineers' daily practices, it is important to ensure that the code generated\nby these tools is not only functionally correct but also free of\nvulnerabilities. Although LLMs can help developers to be more productive, prior\nempirical studies have shown that LLMs can generate insecure code. There are\ntwo contributing factors to the insecure code generation. First, existing\ndatasets used to evaluate LLMs do not adequately represent genuine software\nengineering tasks sensitive to security. Instead, they are often based on\ncompetitive programming challenges or classroom-type coding tasks. In\nreal-world applications, the code produced is integrated into larger codebases,\nintroducing potential security risks. Second, existing evaluation metrics\nprimarily focus on the functional correctness of the generated code while\nignoring security considerations. Therefore, in this paper, we described SALLM,\na framework to benchmark LLMs' abilities to generate secure code\nsystematically. This framework has three major components: a novel dataset of\nsecurity-centric Python prompts, configurable assessment techniques to evaluate\nthe generated code, and novel metrics to evaluate the models' performance from\nthe perspective of secure code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing popularity of Large Language Models (LLMs) in software\nengineers' daily practices, it is important to ensure that the code generated\nby these tools is not only functionally correct but also free of\nvulnerabilities. Although LLMs can help developers to be more productive, prior\nempirical studies have shown that LLMs can generate insecure code. There are\ntwo contributing factors to the insecure code generation. First, existing\ndatasets used to evaluate LLMs do not adequately represent genuine software\nengineering tasks sensitive to security. Instead, they are often based on\ncompetitive programming challenges or classroom-type coding tasks. In\nreal-world applications, the code produced is integrated into larger codebases,\nintroducing potential security risks. Second, existing evaluation metrics\nprimarily focus on the functional correctness of the generated code while\nignoring security considerations. Therefore, in this paper, we described SALLM,\na framework to benchmark LLMs' abilities to generate secure code\nsystematically. This framework has three major components: a novel dataset of\nsecurity-centric Python prompts, configurable assessment techniques to evaluate\nthe generated code, and novel metrics to evaluate the models' performance from\nthe perspective of secure code generation."
                },
                "authors": [
                    {
                        "name": "Mohammed Latif Siddiq"
                    },
                    {
                        "name": "Joanna C. S. Santos"
                    },
                    {
                        "name": "Sajith Devareddy"
                    },
                    {
                        "name": "Anna Muller"
                    }
                ],
                "author_detail": {
                    "name": "Anna Muller"
                },
                "author": "Anna Muller",
                "arxiv_doi": "10.1145/3691621.3694934",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3691621.3694934",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.00889v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.00889v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 6th International Workshop on Automated and\n  verifiable Software sYstem DEvelopment (ASYDE) with ASE Conference 2024",
                "arxiv_journal_ref": "39th IEEE/ACM International Conference on Automated Software\n  Engineering Workshops (ASEW '24), October 27-November 1, 2024, Sacramento,\n  CA, USA, ACM, New York, NY, USA, 12 pages",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03121v1",
                "updated": "2024-09-04T23:11:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    23,
                    11,
                    25,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T23:11:25Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    23,
                    11,
                    25,
                    2,
                    248,
                    0
                ],
                "title": "QHDOPT: A Software for Nonlinear Optimization with Quantum Hamiltonian\n  Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QHDOPT: A Software for Nonlinear Optimization with Quantum Hamiltonian\n  Descent"
                },
                "summary": "We develop an open-source, end-to-end software (named QHDOPT), which can\nsolve nonlinear optimization problems using the quantum Hamiltonian descent\n(QHD) algorithm. QHDOPT offers an accessible interface and automatically maps\ntasks to various supported quantum backends (i.e., quantum hardware machines).\nThese features enable users, even those without prior knowledge or experience\nin quantum computing, to utilize the power of existing quantum devices for\nnonlinear and nonconvex optimization tasks. In its intermediate compilation\nlayer, QHDOPT employs SimuQ, an efficient interface for Hamiltonian-oriented\nprogramming, to facilitate multiple algorithmic specifications and ensure\ncompatible cross-hardware deployment. The detailed documentation of QHDOPT is\navailable at https://github.com/jiaqileng/QHDOPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop an open-source, end-to-end software (named QHDOPT), which can\nsolve nonlinear optimization problems using the quantum Hamiltonian descent\n(QHD) algorithm. QHDOPT offers an accessible interface and automatically maps\ntasks to various supported quantum backends (i.e., quantum hardware machines).\nThese features enable users, even those without prior knowledge or experience\nin quantum computing, to utilize the power of existing quantum devices for\nnonlinear and nonconvex optimization tasks. In its intermediate compilation\nlayer, QHDOPT employs SimuQ, an efficient interface for Hamiltonian-oriented\nprogramming, to facilitate multiple algorithmic specifications and ensure\ncompatible cross-hardware deployment. The detailed documentation of QHDOPT is\navailable at https://github.com/jiaqileng/QHDOPT."
                },
                "authors": [
                    {
                        "name": "Samuel Kushnir"
                    },
                    {
                        "name": "Jiaqi Leng"
                    },
                    {
                        "name": "Yuxiang Peng"
                    },
                    {
                        "name": "Lei Fan"
                    },
                    {
                        "name": "Xiaodi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodi Wu"
                },
                "author": "Xiaodi Wu",
                "arxiv_comment": "23 pages, 7 figures. The full repository is available at\n  https://github.com/jiaqileng/QHDOPT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03103v1",
                "updated": "2024-09-04T22:03:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    22,
                    3,
                    7,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T22:03:07Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    22,
                    3,
                    7,
                    2,
                    248,
                    0
                ],
                "title": "Leveraging Interpretability in the Transformer to Automate the Proactive\n  Scaling of Cloud Resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Interpretability in the Transformer to Automate the Proactive\n  Scaling of Cloud Resources"
                },
                "summary": "Modern web services adopt cloud-native principles to leverage the advantages\nof microservices. To consistently guarantee high Quality of Service (QoS)\naccording to Service Level Agreements (SLAs), ensure satisfactory user\nexperiences, and minimize operational costs, each microservice must be\nprovisioned with the right amount of resources. However, accurately\nprovisioning microservices with adequate resources is complex and depends on\nmany factors, including workload intensity and the complex interconnections\nbetween microservices. To address this challenge, we develop a model that\ncaptures the relationship between an end-to-end latency, requests at the\nfront-end level, and resource utilization. We then use the developed model to\npredict the end-to-end latency. Our solution leverages the Temporal Fusion\nTransformer (TFT), an attention-based architecture equipped with\ninterpretability features. When the prediction results indicate SLA\nnon-compliance, we use the feature importance provided by the TFT as covariates\nin Kernel Ridge Regression (KRR), with the response variable being the desired\nlatency, to learn the parameters associated with the feature importance. These\nlearned parameters reflect the adjustments required to the features to ensure\nSLA compliance. We demonstrate the merit of our approach with a\nmicroservice-based application and provide a roadmap to deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern web services adopt cloud-native principles to leverage the advantages\nof microservices. To consistently guarantee high Quality of Service (QoS)\naccording to Service Level Agreements (SLAs), ensure satisfactory user\nexperiences, and minimize operational costs, each microservice must be\nprovisioned with the right amount of resources. However, accurately\nprovisioning microservices with adequate resources is complex and depends on\nmany factors, including workload intensity and the complex interconnections\nbetween microservices. To address this challenge, we develop a model that\ncaptures the relationship between an end-to-end latency, requests at the\nfront-end level, and resource utilization. We then use the developed model to\npredict the end-to-end latency. Our solution leverages the Temporal Fusion\nTransformer (TFT), an attention-based architecture equipped with\ninterpretability features. When the prediction results indicate SLA\nnon-compliance, we use the feature importance provided by the TFT as covariates\nin Kernel Ridge Regression (KRR), with the response variable being the desired\nlatency, to learn the parameters associated with the feature importance. These\nlearned parameters reflect the adjustments required to the features to ensure\nSLA compliance. We demonstrate the merit of our approach with a\nmicroservice-based application and provide a roadmap to deployment."
                },
                "authors": [
                    {
                        "name": "Amadou Ba"
                    },
                    {
                        "name": "Pavithra Harsha"
                    },
                    {
                        "name": "Chitra Subramanian"
                    }
                ],
                "author_detail": {
                    "name": "Chitra Subramanian"
                },
                "author": "Chitra Subramanian",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03093v1",
                "updated": "2024-09-04T21:46:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    21,
                    46,
                    18,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T21:46:18Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    21,
                    46,
                    18,
                    2,
                    248,
                    0
                ],
                "title": "Multi-language Unit Test Generation using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-language Unit Test Generation using LLMs"
                },
                "summary": "Implementing automated unit tests is an important but time consuming activity\nin software development. Developers dedicate substantial time to writing tests\nfor validating an application and preventing regressions. To support developers\nin this task, software engineering research over the past few decades has\ndeveloped many techniques for automating unit test generation. However, despite\nthis effort, usable tools exist for very few programming languages -- mainly\nJava, C, and C# and, more recently, for Python. Moreover, studies have found\nthat automatically generated tests suffer poor readability and often do not\nresemble developer-written tests. In this work, we present a rigorous\ninvestigation of how large language models (LLMs) can help bridge the gap. We\ndescribe a generic pipeline that incorporates static analysis to guide LLMs in\ngenerating compilable and high-coverage test cases. We illustrate how the\npipeline can be applied to different programming languages, specifically Java\nand Python, and to complex software requiring environment mocking. We conducted\na through empirical study to assess the quality of the generated tests in terms\nof coverage, mutation score, and test naturalness -- evaluating them on\nstandard as well as enterprise Java applications and a large Python benchmark.\nOur results demonstrate that LLM-based test generation, when guided by static\nanalysis, can be competitive with, and even outperform, state-of-the-art\ntest-generation techniques in coverage achieved while also producing\nconsiderably more natural test cases that developers find easy to read and\nunderstand. We also present the results of a user study, conducted with 161\nprofessional developers, that highlights the naturalness characteristics of the\ntests generated by our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing automated unit tests is an important but time consuming activity\nin software development. Developers dedicate substantial time to writing tests\nfor validating an application and preventing regressions. To support developers\nin this task, software engineering research over the past few decades has\ndeveloped many techniques for automating unit test generation. However, despite\nthis effort, usable tools exist for very few programming languages -- mainly\nJava, C, and C# and, more recently, for Python. Moreover, studies have found\nthat automatically generated tests suffer poor readability and often do not\nresemble developer-written tests. In this work, we present a rigorous\ninvestigation of how large language models (LLMs) can help bridge the gap. We\ndescribe a generic pipeline that incorporates static analysis to guide LLMs in\ngenerating compilable and high-coverage test cases. We illustrate how the\npipeline can be applied to different programming languages, specifically Java\nand Python, and to complex software requiring environment mocking. We conducted\na through empirical study to assess the quality of the generated tests in terms\nof coverage, mutation score, and test naturalness -- evaluating them on\nstandard as well as enterprise Java applications and a large Python benchmark.\nOur results demonstrate that LLM-based test generation, when guided by static\nanalysis, can be competitive with, and even outperform, state-of-the-art\ntest-generation techniques in coverage achieved while also producing\nconsiderably more natural test cases that developers find easy to read and\nunderstand. We also present the results of a user study, conducted with 161\nprofessional developers, that highlights the naturalness characteristics of the\ntests generated by our approach."
                },
                "authors": [
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Rahul Krishna"
                    },
                    {
                        "name": "Raju Pavuluri"
                    },
                    {
                        "name": "Saurabh Sinha"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Sinha"
                },
                "author": "Saurabh Sinha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03080v1",
                "updated": "2024-09-04T21:08:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    21,
                    8,
                    55,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T21:08:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    21,
                    8,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Explainable AI for computational pathology identifies model limitations\n  and tissue biomarkers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI for computational pathology identifies model limitations\n  and tissue biomarkers"
                },
                "summary": "Deep learning models have shown promise in histopathology image analysis, but\ntheir opaque decision-making process poses challenges in high-risk medical\nscenarios. Here we introduce HIPPO, an explainable AI method that interrogates\nattention-based multiple instance learning (ABMIL) models in computational\npathology by generating counterfactual examples through tissue patch\nmodifications in whole slide images. Applying HIPPO to ABMIL models trained to\ndetect breast cancer metastasis reveals that they may overlook small tumors and\ncan be misled by non-tumor tissue, while attention maps$\\unicode{x2014}$widely\nused for interpretation$\\unicode{x2014}$often highlight regions that do not\ndirectly influence predictions. By interpreting ABMIL models trained on a\nprognostic prediction task, HIPPO identified tissue areas with stronger\nprognostic effects than high-attention regions, which sometimes showed\ncounterintuitive influences on risk scores. These findings demonstrate HIPPO's\ncapacity for comprehensive model evaluation, bias detection, and quantitative\nhypothesis testing. HIPPO greatly expands the capabilities of explainable AI\ntools to assess the trustworthy and reliable development, deployment, and\nregulation of weakly-supervised models in computational pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have shown promise in histopathology image analysis, but\ntheir opaque decision-making process poses challenges in high-risk medical\nscenarios. Here we introduce HIPPO, an explainable AI method that interrogates\nattention-based multiple instance learning (ABMIL) models in computational\npathology by generating counterfactual examples through tissue patch\nmodifications in whole slide images. Applying HIPPO to ABMIL models trained to\ndetect breast cancer metastasis reveals that they may overlook small tumors and\ncan be misled by non-tumor tissue, while attention maps$\\unicode{x2014}$widely\nused for interpretation$\\unicode{x2014}$often highlight regions that do not\ndirectly influence predictions. By interpreting ABMIL models trained on a\nprognostic prediction task, HIPPO identified tissue areas with stronger\nprognostic effects than high-attention regions, which sometimes showed\ncounterintuitive influences on risk scores. These findings demonstrate HIPPO's\ncapacity for comprehensive model evaluation, bias detection, and quantitative\nhypothesis testing. HIPPO greatly expands the capabilities of explainable AI\ntools to assess the trustworthy and reliable development, deployment, and\nregulation of weakly-supervised models in computational pathology."
                },
                "authors": [
                    {
                        "name": "Jakub R. Kaczmarzyk"
                    },
                    {
                        "name": "Joel H. Saltz"
                    },
                    {
                        "name": "Peter K. Koo"
                    }
                ],
                "author_detail": {
                    "name": "Peter K. Koo"
                },
                "author": "Peter K. Koo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.TO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.TO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03067v1",
                "updated": "2024-09-04T20:38:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    20,
                    38,
                    14,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T20:38:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    20,
                    38,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "A Comparative Study of Offline Models and Online LLMs in Fake News\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of Offline Models and Online LLMs in Fake News\n  Detection"
                },
                "summary": "Fake news detection remains a critical challenge in today's rapidly evolving\ndigital landscape, where misinformation can spread faster than ever before.\nTraditional fake news detection models often rely on static datasets and\nauxiliary information, such as metadata or social media interactions, which\nlimits their adaptability to real-time scenarios. Recent advancements in Large\nLanguage Models (LLMs) have demonstrated significant potential in addressing\nthese challenges due to their extensive pre-trained knowledge and ability to\nanalyze textual content without relying on auxiliary data. However, many of\nthese LLM-based approaches are still rooted in static datasets, with limited\nexploration into their real-time processing capabilities. This paper presents a\nsystematic evaluation of both traditional offline models and state-of-the-art\nLLMs for real-time fake news detection. We demonstrate the limitations of\nexisting offline models, including their inability to adapt to dynamic\nmisinformation patterns. Furthermore, we show that newer LLM models with online\ncapabilities, such as GPT-4, Claude, and Gemini, are better suited for\ndetecting emerging fake news in real-time contexts. Our findings emphasize the\nimportance of transitioning from offline to online LLM models for real-time\nfake news detection. Additionally, the public accessibility of LLMs enhances\ntheir scalability and democratizes the tools needed to combat misinformation.\nBy leveraging real-time data, our work marks a significant step toward more\nadaptive, effective, and scalable fake news detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake news detection remains a critical challenge in today's rapidly evolving\ndigital landscape, where misinformation can spread faster than ever before.\nTraditional fake news detection models often rely on static datasets and\nauxiliary information, such as metadata or social media interactions, which\nlimits their adaptability to real-time scenarios. Recent advancements in Large\nLanguage Models (LLMs) have demonstrated significant potential in addressing\nthese challenges due to their extensive pre-trained knowledge and ability to\nanalyze textual content without relying on auxiliary data. However, many of\nthese LLM-based approaches are still rooted in static datasets, with limited\nexploration into their real-time processing capabilities. This paper presents a\nsystematic evaluation of both traditional offline models and state-of-the-art\nLLMs for real-time fake news detection. We demonstrate the limitations of\nexisting offline models, including their inability to adapt to dynamic\nmisinformation patterns. Furthermore, we show that newer LLM models with online\ncapabilities, such as GPT-4, Claude, and Gemini, are better suited for\ndetecting emerging fake news in real-time contexts. Our findings emphasize the\nimportance of transitioning from offline to online LLM models for real-time\nfake news detection. Additionally, the public accessibility of LLMs enhances\ntheir scalability and democratizes the tools needed to combat misinformation.\nBy leveraging real-time data, our work marks a significant step toward more\nadaptive, effective, and scalable fake news detection systems."
                },
                "authors": [
                    {
                        "name": "Ruoyu Xu"
                    },
                    {
                        "name": "Gaoxiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Gaoxiang Li"
                },
                "author": "Gaoxiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00557v2",
                "updated": "2024-09-04T20:34:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    20,
                    34,
                    27,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-31T23:06:12Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    23,
                    6,
                    12,
                    5,
                    244,
                    0
                ],
                "title": "Learning to Ask: When LLMs Meet Unclear Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Ask: When LLMs Meet Unclear Instruction"
                },
                "summary": "Equipped with the capability to call functions, modern large language models\n(LLMs) can leverage external tools for addressing a range of tasks unattainable\nthrough language skills alone. However, the effective execution of these tools\nrelies heavily not just on the advanced capabilities of LLMs but also on\nprecise user instructions, which often cannot be ensured in the real world. To\nevaluate the performance of LLMs tool-use under imperfect instructions, we\nmeticulously examine the real-world instructions queried from users, analyze\nthe error patterns, and build a challenging tool-use benchmark called Noisy\nToolBench (NoisyToolBench). We find that due to the next-token prediction\ntraining objective, LLMs tend to arbitrarily generate the missed argument,\nwhich may lead to hallucinations and risks. To address this issue, we propose a\nnovel framework, Ask-when-Needed (AwN), which prompts LLMs to ask questions to\nusers whenever they encounter obstacles due to unclear instructions. Moreover,\nto reduce the manual labor involved in user-LLM interaction and assess LLMs\nperformance in tool utilization from both accuracy and efficiency perspectives,\nwe design an automated evaluation tool named ToolEvaluator. Our experiments\ndemonstrate that the AwN significantly outperforms existing frameworks for tool\nlearning in the NoisyToolBench. We will release all related code and datasets\nto support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equipped with the capability to call functions, modern large language models\n(LLMs) can leverage external tools for addressing a range of tasks unattainable\nthrough language skills alone. However, the effective execution of these tools\nrelies heavily not just on the advanced capabilities of LLMs but also on\nprecise user instructions, which often cannot be ensured in the real world. To\nevaluate the performance of LLMs tool-use under imperfect instructions, we\nmeticulously examine the real-world instructions queried from users, analyze\nthe error patterns, and build a challenging tool-use benchmark called Noisy\nToolBench (NoisyToolBench). We find that due to the next-token prediction\ntraining objective, LLMs tend to arbitrarily generate the missed argument,\nwhich may lead to hallucinations and risks. To address this issue, we propose a\nnovel framework, Ask-when-Needed (AwN), which prompts LLMs to ask questions to\nusers whenever they encounter obstacles due to unclear instructions. Moreover,\nto reduce the manual labor involved in user-LLM interaction and assess LLMs\nperformance in tool utilization from both accuracy and efficiency perspectives,\nwe design an automated evaluation tool named ToolEvaluator. Our experiments\ndemonstrate that the AwN significantly outperforms existing frameworks for tool\nlearning in the NoisyToolBench. We will release all related code and datasets\nto support future research."
                },
                "authors": [
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Juluan Shi"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Cheryl Lee"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13890v2",
                "updated": "2024-09-04T20:23:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    20,
                    23,
                    59,
                    2,
                    248,
                    0
                ],
                "published": "2024-05-22T18:01:24Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    18,
                    1,
                    24,
                    2,
                    143,
                    0
                ],
                "title": "An empirical study to understand how students use ChatGPT for writing\n  essays and how it affects their ownership",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An empirical study to understand how students use ChatGPT for writing\n  essays and how it affects their ownership"
                },
                "summary": "As large language models (LLMs) become more powerful and ubiquitous, systems\nlike ChatGPT are increasingly used by students to help them with writing tasks.\nTo better understand how these tools are used, we investigate how students\nmight use an LLM for essay writing, for example, to study the queries asked to\nChatGPT and the responses that ChatGPT gives. To that end, we plan to conduct a\nuser study that will record the user writing process and present them with the\nopportunity to use ChatGPT as an AI assistant. This study's findings will help\nus understand how these tools are used and how practitioners -- such as\neducators and essay readers -- should consider writing education and evaluation\nbased on essay writing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more powerful and ubiquitous, systems\nlike ChatGPT are increasingly used by students to help them with writing tasks.\nTo better understand how these tools are used, we investigate how students\nmight use an LLM for essay writing, for example, to study the queries asked to\nChatGPT and the responses that ChatGPT gives. To that end, we plan to conduct a\nuser study that will record the user writing process and present them with the\nopportunity to use ChatGPT as an AI assistant. This study's findings will help\nus understand how these tools are used and how practitioners -- such as\neducators and essay readers -- should consider writing education and evaluation\nbased on essay writing."
                },
                "authors": [
                    {
                        "name": "Andrew Jelson"
                    },
                    {
                        "name": "Sang Won Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sang Won Lee"
                },
                "author": "Sang Won Lee",
                "arxiv_comment": "5 pages, 2 figures, submitted and accepted to ACM CHI Workshop\n  In2Writing in 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03031v1",
                "updated": "2024-09-04T18:50:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    18,
                    50,
                    34,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T18:50:34Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    18,
                    50,
                    34,
                    2,
                    248,
                    0
                ],
                "title": "Debugging with Open-Source Large Language Models: An Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging with Open-Source Large Language Models: An Evaluation"
                },
                "summary": "Large language models have shown good potential in supporting software\ndevelopment tasks. This is why more and more developers turn to LLMs (e.g.\nChatGPT) to support them in fixing their buggy code. While this can save time\nand effort, many companies prohibit it due to strict code sharing policies. To\naddress this, companies can run open-source LLMs locally. But until now there\nis not much research evaluating the performance of open-source large language\nmodels in debugging. This work is a preliminary evaluation of the capabilities\nof open-source LLMs in fixing buggy code. The evaluation covers five\nopen-source large language models and uses the benchmark DebugBench which\nincludes more than 4000 buggy code instances written in Python, Java and C++.\nOpen-source LLMs achieved scores ranging from 43.9% to 66.6% with\nDeepSeek-Coder achieving the best score for all three programming languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown good potential in supporting software\ndevelopment tasks. This is why more and more developers turn to LLMs (e.g.\nChatGPT) to support them in fixing their buggy code. While this can save time\nand effort, many companies prohibit it due to strict code sharing policies. To\naddress this, companies can run open-source LLMs locally. But until now there\nis not much research evaluating the performance of open-source large language\nmodels in debugging. This work is a preliminary evaluation of the capabilities\nof open-source LLMs in fixing buggy code. The evaluation covers five\nopen-source large language models and uses the benchmark DebugBench which\nincludes more than 4000 buggy code instances written in Python, Java and C++.\nOpen-source LLMs achieved scores ranging from 43.9% to 66.6% with\nDeepSeek-Coder achieving the best score for all three programming languages."
                },
                "authors": [
                    {
                        "name": "Yacine Majdoub"
                    },
                    {
                        "name": "Eya Ben Charrada"
                    }
                ],
                "author_detail": {
                    "name": "Eya Ben Charrada"
                },
                "author": "Eya Ben Charrada",
                "arxiv_doi": "10.1145/3674805.3690758",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3674805.3690758",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 2 figure, ESEM2024",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03021v1",
                "updated": "2024-09-04T18:27:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    18,
                    27,
                    12,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T18:27:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    18,
                    27,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "CLUE: Concept-Level Uncertainty Estimation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLUE: Concept-Level Uncertainty Estimation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nvarious natural language generation (NLG) tasks. Previous studies suggest that\nLLMs' generation process involves uncertainty. However, existing approaches to\nuncertainty estimation mainly focus on sequence-level uncertainty, overlooking\nindividual pieces of information within sequences. These methods fall short in\nseparately assessing the uncertainty of each component in a sequence. In\nresponse, we propose a novel framework for Concept-Level Uncertainty Estimation\n(CLUE) for LLMs. We leverage LLMs to convert output sequences into\nconcept-level representations, breaking down sequences into individual concepts\nand measuring the uncertainty of each concept separately. We conduct\nexperiments to demonstrate that CLUE can provide more interpretable uncertainty\nestimation results compared with sentence-level uncertainty, and could be a\nuseful tool for various tasks such as hallucination detection and story\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nvarious natural language generation (NLG) tasks. Previous studies suggest that\nLLMs' generation process involves uncertainty. However, existing approaches to\nuncertainty estimation mainly focus on sequence-level uncertainty, overlooking\nindividual pieces of information within sequences. These methods fall short in\nseparately assessing the uncertainty of each component in a sequence. In\nresponse, we propose a novel framework for Concept-Level Uncertainty Estimation\n(CLUE) for LLMs. We leverage LLMs to convert output sequences into\nconcept-level representations, breaking down sequences into individual concepts\nand measuring the uncertainty of each concept separately. We conduct\nexperiments to demonstrate that CLUE can provide more interpretable uncertainty\nestimation results compared with sentence-level uncertainty, and could be a\nuseful tool for various tasks such as hallucination detection and story\ngeneration."
                },
                "authors": [
                    {
                        "name": "Yu-Hsiang Wang"
                    },
                    {
                        "name": "Andrew Bai"
                    },
                    {
                        "name": "Che-Ping Tsai"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Cho-Jui Hsieh"
                },
                "author": "Cho-Jui Hsieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.15991v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.15991v3",
                "updated": "2024-09-04T18:25:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    18,
                    25,
                    9,
                    2,
                    248,
                    0
                ],
                "published": "2023-10-24T16:39:06Z",
                "published_parsed": [
                    2023,
                    10,
                    24,
                    16,
                    39,
                    6,
                    1,
                    297,
                    0
                ],
                "title": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WhiteFox: White-Box Compiler Fuzzing Empowered by Large Language Models"
                },
                "summary": "Compiler correctness is crucial, as miscompilation can falsify program\nbehaviors, leading to serious consequences. Fuzzing has been studied to uncover\ncompiler defects. However, compiler fuzzing remains challenging: Existing arts\nfocus on black- and grey-box fuzzing, which generates tests without sufficient\nunderstanding of internal compiler behaviors. Meanwhile, traditional white-box\ntechniques, like symbolic execution, are computationally inapplicable to the\ngiant codebase of compilers. Recent advances demonstrate that Large Language\nModels (LLMs) excel in code generation/understanding tasks. Nonetheless,\nguiding LLMs with compiler source-code information remains a missing piece of\nresearch in compiler testing.\n  To this end, we propose WhiteFox, the first white-box compiler fuzzer using\nLLMs with source-code information to test compiler optimization, with a\nspotlight on detecting deep logic bugs in the deep learning (DL) compilers.\nWhiteFox adopts a multi-agent framework: an LLM-based analysis agent examines\nthe low-level optimization source code and produces requirements on the\nhigh-level test programs that can trigger the optimization; an LLM-based\ngeneration agent produces test programs based on the summarized requirements.\nAdditionally, optimization-triggering tests are used as feedback to enhance the\ngeneration on the fly. Our evaluation on the three most popular DL compilers\n(i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows WhiteFox\ncan generate high-quality test programs to exercise deep optimizations,\npracticing up to 8X more than state-of-the-art fuzzers. WhiteFox has found 101\nbugs for the DL compilers, with 92 confirmed as previously unknown and 70\nfixed. WhiteFox has been acknowledged by the PyTorch team and is being\nincorporated into its development workflow. Beyond DL compilers, WhiteFox can\nalso be adapted for compilers in different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compiler correctness is crucial, as miscompilation can falsify program\nbehaviors, leading to serious consequences. Fuzzing has been studied to uncover\ncompiler defects. However, compiler fuzzing remains challenging: Existing arts\nfocus on black- and grey-box fuzzing, which generates tests without sufficient\nunderstanding of internal compiler behaviors. Meanwhile, traditional white-box\ntechniques, like symbolic execution, are computationally inapplicable to the\ngiant codebase of compilers. Recent advances demonstrate that Large Language\nModels (LLMs) excel in code generation/understanding tasks. Nonetheless,\nguiding LLMs with compiler source-code information remains a missing piece of\nresearch in compiler testing.\n  To this end, we propose WhiteFox, the first white-box compiler fuzzer using\nLLMs with source-code information to test compiler optimization, with a\nspotlight on detecting deep logic bugs in the deep learning (DL) compilers.\nWhiteFox adopts a multi-agent framework: an LLM-based analysis agent examines\nthe low-level optimization source code and produces requirements on the\nhigh-level test programs that can trigger the optimization; an LLM-based\ngeneration agent produces test programs based on the summarized requirements.\nAdditionally, optimization-triggering tests are used as feedback to enhance the\ngeneration on the fly. Our evaluation on the three most popular DL compilers\n(i.e., PyTorch Inductor, TensorFlow-XLA, and TensorFlow Lite) shows WhiteFox\ncan generate high-quality test programs to exercise deep optimizations,\npracticing up to 8X more than state-of-the-art fuzzers. WhiteFox has found 101\nbugs for the DL compilers, with 92 confirmed as previously unknown and 70\nfixed. WhiteFox has been acknowledged by the PyTorch team and is being\nincorporated into its development workflow. Beyond DL compilers, WhiteFox can\nalso be adapted for compilers in different domains."
                },
                "authors": [
                    {
                        "name": "Chenyuan Yang"
                    },
                    {
                        "name": "Yinlin Deng"
                    },
                    {
                        "name": "Runyu Lu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "arxiv_doi": "10.1145/3689736",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689736",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.15991v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.15991v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in OOPSLA 2024",
                "arxiv_journal_ref": "Proc. ACM Program. Lang., Vol. 8, No. OOPSLA2, Article 296.\n  Publication date: October 2024",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00494v2",
                "updated": "2024-09-04T18:00:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    18,
                    0,
                    53,
                    2,
                    248,
                    0
                ],
                "published": "2024-08-31T16:14:42Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    16,
                    14,
                    42,
                    5,
                    244,
                    0
                ],
                "title": "GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility:\n  Opportunities and Challenges for Integrating Large Language Models (LLMs) and\n  Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility:\n  Opportunities and Challenges for Integrating Large Language Models (LLMs) and\n  Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems"
                },
                "summary": "Leveraging recent advances in generative AI, multi-agent systems are\nincreasingly being developed to enhance the functionality and efficiency of\nsmart city applications. This paper explores the transformative potential of\nlarge language models (LLMs) and emerging Retrieval-Augmented Generation (RAG)\ntechnologies in Intelligent Transportation Systems (ITS), paving the way for\ninnovative solutions to address critical challenges in urban mobility. We begin\nby providing a comprehensive overview of the current state-of-the-art in\nmobility data, ITS, and Connected Vehicles (CV) applications. Building on this\nreview, we discuss the rationale behind RAG and examine the opportunities for\nintegrating these Generative AI (GenAI) technologies into the smart mobility\nsector. We propose a conceptual framework aimed at developing multi-agent\nsystems capable of intelligently and conversationally delivering smart mobility\nservices to urban commuters, transportation operators, and decision-makers. Our\napproach seeks to foster an autonomous and intelligent approach that (a)\npromotes science-based advisory to reduce traffic congestion, accidents, and\ncarbon emissions at multiple scales, (b) facilitates public education and\nengagement in participatory mobility management, and (c) automates specialized\ntransportation management tasks and the development of critical ITS platforms,\nsuch as data analytics and interpretation, knowledge representation, and\ntraffic simulations. By integrating LLM and RAG, our approach seeks to overcome\nthe limitations of traditional rule-based multi-agent systems, which rely on\nfixed knowledge bases and limited reasoning capabilities. This integration\npaves the way for a more scalable, intuitive, and automated multi-agent\nparadigm, driving advancements in ITS and urban mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging recent advances in generative AI, multi-agent systems are\nincreasingly being developed to enhance the functionality and efficiency of\nsmart city applications. This paper explores the transformative potential of\nlarge language models (LLMs) and emerging Retrieval-Augmented Generation (RAG)\ntechnologies in Intelligent Transportation Systems (ITS), paving the way for\ninnovative solutions to address critical challenges in urban mobility. We begin\nby providing a comprehensive overview of the current state-of-the-art in\nmobility data, ITS, and Connected Vehicles (CV) applications. Building on this\nreview, we discuss the rationale behind RAG and examine the opportunities for\nintegrating these Generative AI (GenAI) technologies into the smart mobility\nsector. We propose a conceptual framework aimed at developing multi-agent\nsystems capable of intelligently and conversationally delivering smart mobility\nservices to urban commuters, transportation operators, and decision-makers. Our\napproach seeks to foster an autonomous and intelligent approach that (a)\npromotes science-based advisory to reduce traffic congestion, accidents, and\ncarbon emissions at multiple scales, (b) facilitates public education and\nengagement in participatory mobility management, and (c) automates specialized\ntransportation management tasks and the development of critical ITS platforms,\nsuch as data analytics and interpretation, knowledge representation, and\ntraffic simulations. By integrating LLM and RAG, our approach seeks to overcome\nthe limitations of traditional rule-based multi-agent systems, which rely on\nfixed knowledge bases and limited reasoning capabilities. This integration\npaves the way for a more scalable, intuitive, and automated multi-agent\nparadigm, driving advancements in ITS and urban mobility."
                },
                "authors": [
                    {
                        "name": "Haowen Xu"
                    },
                    {
                        "name": "Jinghui Yuan"
                    },
                    {
                        "name": "Anye Zhou"
                    },
                    {
                        "name": "Guanhao Xu"
                    },
                    {
                        "name": "Wan Li"
                    },
                    {
                        "name": "Xuegang Ban"
                    },
                    {
                        "name": "Xinyue Ye"
                    }
                ],
                "author_detail": {
                    "name": "Xinyue Ye"
                },
                "author": "Xinyue Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13989v3",
                "updated": "2024-09-04T17:52:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    52,
                    37,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-19T02:34:10Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    2,
                    34,
                    10,
                    4,
                    201,
                    0
                ],
                "title": "Enhancing Graph Neural Networks with Limited Labeled Data by Actively\n  Distilling Knowledge from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Graph Neural Networks with Limited Labeled Data by Actively\n  Distilling Knowledge from Large Language Models"
                },
                "summary": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are pervasive in the real-world, such as social network analysis,\nbioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great\nability in node classification, a fundamental task on graphs. Unfortunately,\nconventional GNNs still face challenges in scenarios with few labeled nodes,\ndespite the prevalence of few-shot node classification tasks in real-world\napplications. To address this challenge, various approaches have been proposed,\nincluding graph meta-learning, transfer learning, and methods based on Large\nLanguage Models (LLMs). However, traditional meta-learning and transfer\nlearning methods often require prior knowledge from base classes or fail to\nexploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based\nmethods may overlook the zero-shot capabilities of LLMs and rely heavily on the\nquality of generated contexts. In this paper, we propose a novel approach that\nintegrates LLMs and GNNs, leveraging the zero-shot inference and reasoning\ncapabilities of LLMs and employing a Graph-LLM-based active learning paradigm\nto enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins."
                },
                "authors": [
                    {
                        "name": "Quan Li"
                    },
                    {
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "name": "Lingwei Chen"
                    },
                    {
                        "name": "Junjie Xu"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "10 pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02912v1",
                "updated": "2024-09-04T17:51:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    51,
                    18,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T17:51:18Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    51,
                    18,
                    2,
                    248,
                    0
                ],
                "title": "Design of a Standard-Compliant Real-Time Neural Receiver for 5G NR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design of a Standard-Compliant Real-Time Neural Receiver for 5G NR"
                },
                "summary": "We detail the steps required to deploy a multi-user multiple-input\nmultiple-output (MU-MIMO) neural receiver (NRX) in an actual cellular\ncommunication system. This raises several exciting research challenges,\nincluding the need for real-time inference and compatibility with the 5G NR\nstandard. As the network configuration in a practical setup can change\ndynamically within milliseconds, we propose an adaptive NRX architecture\ncapable of supporting dynamic modulation and coding scheme (MCS) configurations\nwithout the need for any re-training and without additional inference cost. We\noptimize the latency of the neural network (NN) architecture to achieve\ninference times of less than 1ms on an NVIDIA A100 GPU using the TensorRT\ninference library. These latency constraints effectively limit the size of the\nNN and we quantify the resulting signal-to-noise ratio (SNR) degradation as\nless than 0.7 dB when compared to a preliminary non-real-time NRX architecture.\nFinally, we explore the potential for site-specific adaptation of the receiver\nby investigating the required size of the training dataset and the number of\nfine-tuning iterations to optimize the NRX for specific radio environments\nusing a ray tracing-based channel model. The resulting NRX is ready for\ndeployment in a real-time 5G NR system and the source code including the\nTensorRT experiments is available online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail the steps required to deploy a multi-user multiple-input\nmultiple-output (MU-MIMO) neural receiver (NRX) in an actual cellular\ncommunication system. This raises several exciting research challenges,\nincluding the need for real-time inference and compatibility with the 5G NR\nstandard. As the network configuration in a practical setup can change\ndynamically within milliseconds, we propose an adaptive NRX architecture\ncapable of supporting dynamic modulation and coding scheme (MCS) configurations\nwithout the need for any re-training and without additional inference cost. We\noptimize the latency of the neural network (NN) architecture to achieve\ninference times of less than 1ms on an NVIDIA A100 GPU using the TensorRT\ninference library. These latency constraints effectively limit the size of the\nNN and we quantify the resulting signal-to-noise ratio (SNR) degradation as\nless than 0.7 dB when compared to a preliminary non-real-time NRX architecture.\nFinally, we explore the potential for site-specific adaptation of the receiver\nby investigating the required size of the training dataset and the number of\nfine-tuning iterations to optimize the NRX for specific radio environments\nusing a ray tracing-based channel model. The resulting NRX is ready for\ndeployment in a real-time 5G NR system and the source code including the\nTensorRT experiments is available online."
                },
                "authors": [
                    {
                        "name": "Reinhard Wiesmayr"
                    },
                    {
                        "name": "Sebastian Cammerer"
                    },
                    {
                        "name": "Fayal At Aoudia"
                    },
                    {
                        "name": "Jakob Hoydis"
                    },
                    {
                        "name": "Jakub Zakrzewski"
                    },
                    {
                        "name": "Alexander Keller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Keller"
                },
                "author": "Alexander Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07832v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07832v3",
                "updated": "2024-09-04T17:31:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    31,
                    0,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-31T14:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    49,
                    35,
                    2,
                    213,
                    0
                ],
                "title": "LADDER: Language Driven Slice Discovery and Error Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Language Driven Slice Discovery and Error Rectification"
                },
                "summary": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets. The code is\navailable (https://github.com/batmanlab/Ladder)."
                },
                "authors": [
                    {
                        "name": "Shantanu Ghosh"
                    },
                    {
                        "name": "Rayan Syed"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Clare B. Poynton"
                    },
                    {
                        "name": "Kayhan Batmanghelich"
                    }
                ],
                "author_detail": {
                    "name": "Kayhan Batmanghelich"
                },
                "author": "Kayhan Batmanghelich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07832v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07832v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02889v1",
                "updated": "2024-09-04T17:25:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    25,
                    21,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T17:25:21Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    25,
                    21,
                    2,
                    248,
                    0
                ],
                "title": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via\n  Hybrid Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via\n  Hybrid Architecture"
                },
                "summary": "Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "19 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18322v2",
                "updated": "2024-09-04T17:16:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    16,
                    5,
                    2,
                    248,
                    0
                ],
                "published": "2024-07-01T19:52:41Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    19,
                    52,
                    41,
                    0,
                    183,
                    0
                ],
                "title": "The Need for Guardrails with Large Language Models in Medical\n  Safety-Critical Settings: An Artificial Intelligence Application in the\n  Pharmacovigilance Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Need for Guardrails with Large Language Models in Medical\n  Safety-Critical Settings: An Artificial Intelligence Application in the\n  Pharmacovigilance Ecosystem"
                },
                "summary": "Large language models (LLMs) are useful tools with the capacity for\nperforming specific types of knowledge work at an effective scale. However, LLM\ndeployments in high-risk and safety-critical domains pose unique challenges,\nnotably the issue of ``hallucination,'' where LLMs can generate fabricated\ninformation. This is particularly concerning in settings such as drug safety,\nwhere inaccuracies could lead to patient harm. To mitigate these risks, we have\ndeveloped and demonstrated a proof of concept suite of guardrails specifically\ndesigned to mitigate certain types of hallucinations and errors for drug\nsafety, and potentially applicable to other medical safety-critical contexts.\nThese guardrails include mechanisms to detect anomalous documents to prevent\nthe ingestion of inappropriate data, identify incorrect drug names or adverse\nevent terms, and convey uncertainty in generated content. We integrated these\nguardrails with an LLM fine-tuned for a text-to-text task, which involves\nconverting both structured and unstructured data within adverse event reports\ninto natural language. This method was applied to translate individual case\nsafety reports, demonstrating effective application in a pharmacovigilance\nprocessing task. Our guardrail framework offers a set of tools with broad\napplicability across various domains, ensuring LLMs can be safely used in\nhigh-risk situations by eliminating the occurrence of key errors, including the\ngeneration of incorrect pharmacovigilance-related terms, thus adhering to\nstringent regulatory and quality standards in medical safety-critical\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are useful tools with the capacity for\nperforming specific types of knowledge work at an effective scale. However, LLM\ndeployments in high-risk and safety-critical domains pose unique challenges,\nnotably the issue of ``hallucination,'' where LLMs can generate fabricated\ninformation. This is particularly concerning in settings such as drug safety,\nwhere inaccuracies could lead to patient harm. To mitigate these risks, we have\ndeveloped and demonstrated a proof of concept suite of guardrails specifically\ndesigned to mitigate certain types of hallucinations and errors for drug\nsafety, and potentially applicable to other medical safety-critical contexts.\nThese guardrails include mechanisms to detect anomalous documents to prevent\nthe ingestion of inappropriate data, identify incorrect drug names or adverse\nevent terms, and convey uncertainty in generated content. We integrated these\nguardrails with an LLM fine-tuned for a text-to-text task, which involves\nconverting both structured and unstructured data within adverse event reports\ninto natural language. This method was applied to translate individual case\nsafety reports, demonstrating effective application in a pharmacovigilance\nprocessing task. Our guardrail framework offers a set of tools with broad\napplicability across various domains, ensuring LLMs can be safely used in\nhigh-risk situations by eliminating the occurrence of key errors, including the\ngeneration of incorrect pharmacovigilance-related terms, thus adhering to\nstringent regulatory and quality standards in medical safety-critical\nenvironments."
                },
                "authors": [
                    {
                        "name": "Joe B Hakim"
                    },
                    {
                        "name": "Jeffery L Painter"
                    },
                    {
                        "name": "Darmendra Ramcharran"
                    },
                    {
                        "name": "Vijay Kara"
                    },
                    {
                        "name": "Greg Powell"
                    },
                    {
                        "name": "Paulina Sobczak"
                    },
                    {
                        "name": "Chiho Sato"
                    },
                    {
                        "name": "Andrew Bate"
                    },
                    {
                        "name": "Andrew Beam"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Beam"
                },
                "author": "Andrew Beam",
                "arxiv_comment": "27 pages, 6 figures, 4 tables and supplementary material provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7; I.7.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02877v1",
                "updated": "2024-09-04T17:01:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    1,
                    2,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T17:01:02Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    17,
                    1,
                    2,
                    2,
                    248,
                    0
                ],
                "title": "Configurable Foundation Models: Building LLMs from a Modular Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configurable Foundation Models: Building LLMs from a Modular Perspective"
                },
                "summary": "Advancements in LLMs have recently unveiled challenges tied to computational\nefficiency and continual scalability due to their requirements of huge\nparameters, making the applications and evolution of these models on devices\nwith limited computation resources and scenarios requiring various abilities\nincreasingly cumbersome. Inspired by modularity within the human brain, there\nis a growing tendency to decompose LLMs into numerous functional modules,\nallowing for inference with part of modules and dynamic assembly of modules to\ntackle complex tasks, such as mixture-of-experts. To highlight the inherent\nefficiency and composability of the modular approach, we coin the term brick to\nrepresent each functional module, designating the modularized structure as\nconfigurable foundation models. In this paper, we offer a comprehensive\noverview and investigation of the construction, utilization, and limitation of\nconfigurable foundation models. We first formalize modules into emergent bricks\n- functional neuron partitions that emerge during the pre-training phase, and\ncustomized bricks - bricks constructed via additional post-training to improve\nthe capabilities and knowledge of LLMs. Based on diverse functional bricks, we\nfurther present four brick-oriented operations: retrieval and routing, merging,\nupdating, and growing. These operations allow for dynamic configuration of LLMs\nbased on instructions to handle complex tasks. To verify our perspective, we\nconduct an empirical analysis on widely-used LLMs. We find that the FFN layers\nfollow modular patterns with functional specialization of neurons and\nfunctional neuron partitions. Finally, we highlight several open issues and\ndirections for future research. Overall, this paper aims to offer a fresh\nmodular perspective on existing LLM research and inspire the future creation of\nmore efficient and scalable foundational models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in LLMs have recently unveiled challenges tied to computational\nefficiency and continual scalability due to their requirements of huge\nparameters, making the applications and evolution of these models on devices\nwith limited computation resources and scenarios requiring various abilities\nincreasingly cumbersome. Inspired by modularity within the human brain, there\nis a growing tendency to decompose LLMs into numerous functional modules,\nallowing for inference with part of modules and dynamic assembly of modules to\ntackle complex tasks, such as mixture-of-experts. To highlight the inherent\nefficiency and composability of the modular approach, we coin the term brick to\nrepresent each functional module, designating the modularized structure as\nconfigurable foundation models. In this paper, we offer a comprehensive\noverview and investigation of the construction, utilization, and limitation of\nconfigurable foundation models. We first formalize modules into emergent bricks\n- functional neuron partitions that emerge during the pre-training phase, and\ncustomized bricks - bricks constructed via additional post-training to improve\nthe capabilities and knowledge of LLMs. Based on diverse functional bricks, we\nfurther present four brick-oriented operations: retrieval and routing, merging,\nupdating, and growing. These operations allow for dynamic configuration of LLMs\nbased on instructions to handle complex tasks. To verify our perspective, we\nconduct an empirical analysis on widely-used LLMs. We find that the FFN layers\nfollow modular patterns with functional specialization of neurons and\nfunctional neuron partitions. Finally, we highlight several open issues and\ndirections for future research. Overall, this paper aims to offer a fresh\nmodular perspective on existing LLM research and inspire the future creation of\nmore efficient and scalable foundational models."
                },
                "authors": [
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Dazhi Jiang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Guanyu Lin"
                    },
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Yuge Tu"
                    },
                    {
                        "name": "Zexuan Zhong"
                    },
                    {
                        "name": "Ao Zhang"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Khai Hao Moo"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Huimin Chen"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10690v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10690v3",
                "updated": "2024-09-04T16:58:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    58,
                    25,
                    2,
                    248,
                    0
                ],
                "published": "2024-06-15T17:07:31Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    17,
                    7,
                    31,
                    5,
                    167,
                    0
                ],
                "title": "Automating Pharmacovigilance Evidence Generation: Using Large Language\n  Models to Produce Context-Aware SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Pharmacovigilance Evidence Generation: Using Large Language\n  Models to Produce Context-Aware SQL"
                },
                "summary": "Objective: To enhance the efficiency and accuracy of information retrieval\nfrom pharmacovigilance (PV) databases by employing Large Language Models (LLMs)\nto convert natural language queries (NLQs) into Structured Query Language (SQL)\nqueries, leveraging a business context document.\n  Materials and Methods: We utilized OpenAI's GPT-4 model within a\nretrieval-augmented generation (RAG) framework, enriched with a business\ncontext document, to transform NLQs into syntactically precise SQL queries.\nEach NLQ was presented to the LLM randomly and independently to prevent\nmemorization. The study was conducted in three phases, varying query\ncomplexity, and assessing the LLM's performance both with and without the\nbusiness context document.\n  Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing\nfrom 8.3\\% with the database schema alone to 78.3\\% with the business context\ndocument. This enhancement was consistent across low, medium, and high\ncomplexity queries, indicating the critical role of contextual knowledge in\nquery generation.\n  Discussion: The integration of a business context document markedly improved\nthe LLM's ability to generate accurate and contextually relevant SQL queries.\nPerformance achieved a maximum of 85\\% when high complexity queries are\nexcluded, suggesting promise for routine deployment.\n  Conclusion: This study presents a novel approach to employing LLMs for safety\ndata retrieval and analysis, demonstrating significant advancements in query\ngeneration accuracy. The methodology offers a framework applicable to various\ndata-intensive domains, enhancing the accessibility and efficiency of\ninformation retrieval for non-technical users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: To enhance the efficiency and accuracy of information retrieval\nfrom pharmacovigilance (PV) databases by employing Large Language Models (LLMs)\nto convert natural language queries (NLQs) into Structured Query Language (SQL)\nqueries, leveraging a business context document.\n  Materials and Methods: We utilized OpenAI's GPT-4 model within a\nretrieval-augmented generation (RAG) framework, enriched with a business\ncontext document, to transform NLQs into syntactically precise SQL queries.\nEach NLQ was presented to the LLM randomly and independently to prevent\nmemorization. The study was conducted in three phases, varying query\ncomplexity, and assessing the LLM's performance both with and without the\nbusiness context document.\n  Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing\nfrom 8.3\\% with the database schema alone to 78.3\\% with the business context\ndocument. This enhancement was consistent across low, medium, and high\ncomplexity queries, indicating the critical role of contextual knowledge in\nquery generation.\n  Discussion: The integration of a business context document markedly improved\nthe LLM's ability to generate accurate and contextually relevant SQL queries.\nPerformance achieved a maximum of 85\\% when high complexity queries are\nexcluded, suggesting promise for routine deployment.\n  Conclusion: This study presents a novel approach to employing LLMs for safety\ndata retrieval and analysis, demonstrating significant advancements in query\ngeneration accuracy. The methodology offers a framework applicable to various\ndata-intensive domains, enhancing the accessibility and efficiency of\ninformation retrieval for non-technical users."
                },
                "authors": [
                    {
                        "name": "Jeffery L. Painter"
                    },
                    {
                        "name": "Venkateswara Rao Chalamalasetti"
                    },
                    {
                        "name": "Raymond Kassekert"
                    },
                    {
                        "name": "Andrew Bate"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Bate"
                },
                "author": "Andrew Bate",
                "arxiv_comment": "15 pages, 3 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10690v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10690v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00847v2",
                "updated": "2024-09-04T16:39:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    39,
                    22,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-01T21:30:14Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    21,
                    30,
                    14,
                    6,
                    245,
                    0
                ],
                "title": "The Design of an LLM-powered Unstructured Analytics System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Design of an LLM-powered Unstructured Analytics System"
                },
                "summary": "LLMs demonstrate an uncanny ability to process unstructured data, and as\nsuch, have the potential to go beyond search and run complex, semantic analyses\nat scale. We describe the design of an unstructured analytics system, Aryn, and\nthe tenets and use cases that motivate its design. With Aryn, users can specify\nqueries in natural language and the system automatically determines a semantic\nplan and executes it to compute an answer from a large collection of\nunstructured documents using LLMs. At the core of Aryn is Sycamore, a\ndeclarative document processing engine, built using Ray, that provides a\nreliable distributed abstraction called DocSets. Sycamore allows users to\nanalyze, enrich, and transform complex documents at scale. Aryn also comprises\nLuna, a query planner that translates natural language queries to Sycamore\nscripts, and the Aryn Partitioner, which takes raw PDFs and document images,\nand converts them to DocSets for downstream processing. Using Aryn, we\ndemonstrate a real world use case for analyzing accident reports from the\nNational Transportation Safety Board (NTSB), and discuss some of the major\nchallenges we encountered in deploying Aryn in the wild.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs demonstrate an uncanny ability to process unstructured data, and as\nsuch, have the potential to go beyond search and run complex, semantic analyses\nat scale. We describe the design of an unstructured analytics system, Aryn, and\nthe tenets and use cases that motivate its design. With Aryn, users can specify\nqueries in natural language and the system automatically determines a semantic\nplan and executes it to compute an answer from a large collection of\nunstructured documents using LLMs. At the core of Aryn is Sycamore, a\ndeclarative document processing engine, built using Ray, that provides a\nreliable distributed abstraction called DocSets. Sycamore allows users to\nanalyze, enrich, and transform complex documents at scale. Aryn also comprises\nLuna, a query planner that translates natural language queries to Sycamore\nscripts, and the Aryn Partitioner, which takes raw PDFs and document images,\nand converts them to DocSets for downstream processing. Using Aryn, we\ndemonstrate a real world use case for analyzing accident reports from the\nNational Transportation Safety Board (NTSB), and discuss some of the major\nchallenges we encountered in deploying Aryn in the wild."
                },
                "authors": [
                    {
                        "name": "Eric Anderson"
                    },
                    {
                        "name": "Jonathan Fritz"
                    },
                    {
                        "name": "Austin Lee"
                    },
                    {
                        "name": "Bohou Li"
                    },
                    {
                        "name": "Mark Lindblad"
                    },
                    {
                        "name": "Henry Lindeman"
                    },
                    {
                        "name": "Alex Meyer"
                    },
                    {
                        "name": "Parth Parmar"
                    },
                    {
                        "name": "Tanvi Ranade"
                    },
                    {
                        "name": "Mehul A. Shah"
                    },
                    {
                        "name": "Benjamin Sowell"
                    },
                    {
                        "name": "Dan Tecuci"
                    },
                    {
                        "name": "Vinayak Thapliyal"
                    },
                    {
                        "name": "Matt Welsh"
                    }
                ],
                "author_detail": {
                    "name": "Matt Welsh"
                },
                "author": "Matt Welsh",
                "arxiv_comment": "6 pages, 3 figures, fixed typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02849v1",
                "updated": "2024-09-04T16:19:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    19,
                    55,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T16:19:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    19,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Anomaly Detection in Offshore Open Radio Access Network Using Long\n  Short-Term Memory Models on a Novel Artificial Intelligence-Driven\n  Cloud-Native Data Platform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly Detection in Offshore Open Radio Access Network Using Long\n  Short-Term Memory Models on a Novel Artificial Intelligence-Driven\n  Cloud-Native Data Platform"
                },
                "summary": "The radio access network (RAN) is a critical component of modern telecom\ninfrastructure, currently undergoing significant transformation towards\ndisaggregated and open architectures. These advancements are pivotal for\nintegrating intelligent, data-driven applications aimed at enhancing network\nreliability and operational autonomy through the introduction of cognition\ncapabilities, exemplified by the set of enhancements proposed by the emerging\nOpen radio access network (O-RAN) standards. Despite its potential, the nascent\nnature of O-RAN technology presents challenges, primarily due to the absence of\nmature operational standards. This complicates the management of data and\napplications, particularly in integrating with traditional network management\nand operational support systems. Divergent vendor-specific design approaches\nfurther hinder migration and limit solution reusability. Addressing the skills\ngap in telecom business-oriented engineering is crucial for the effective\ndeployment of O-RAN and the development of robust data-driven applications. To\naddress these challenges, Boldyn Networks, a global Neutral Host provider, has\nimplemented a novel cloud-native data analytics platform. This platform\nunderwent rigorous testing in real-world scenarios of using advanced artificial\nintelligence (AI) techniques, significantly improving operational efficiency,\nand enhancing customer experience. Implementation involved adopting development\noperations (DevOps) practices, leveraging data lakehouse architectures tailored\nfor AI applications, and employing sophisticated data engineering strategies.\nThe platform successfully addresses connectivity challenges inherent in\noffshore windfarm deployments using long short-term memory (LSTM) Models for\nanomaly detection of the connectivity, providing detailed insights into its\nspecialized architecture developed for this purpose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The radio access network (RAN) is a critical component of modern telecom\ninfrastructure, currently undergoing significant transformation towards\ndisaggregated and open architectures. These advancements are pivotal for\nintegrating intelligent, data-driven applications aimed at enhancing network\nreliability and operational autonomy through the introduction of cognition\ncapabilities, exemplified by the set of enhancements proposed by the emerging\nOpen radio access network (O-RAN) standards. Despite its potential, the nascent\nnature of O-RAN technology presents challenges, primarily due to the absence of\nmature operational standards. This complicates the management of data and\napplications, particularly in integrating with traditional network management\nand operational support systems. Divergent vendor-specific design approaches\nfurther hinder migration and limit solution reusability. Addressing the skills\ngap in telecom business-oriented engineering is crucial for the effective\ndeployment of O-RAN and the development of robust data-driven applications. To\naddress these challenges, Boldyn Networks, a global Neutral Host provider, has\nimplemented a novel cloud-native data analytics platform. This platform\nunderwent rigorous testing in real-world scenarios of using advanced artificial\nintelligence (AI) techniques, significantly improving operational efficiency,\nand enhancing customer experience. Implementation involved adopting development\noperations (DevOps) practices, leveraging data lakehouse architectures tailored\nfor AI applications, and employing sophisticated data engineering strategies.\nThe platform successfully addresses connectivity challenges inherent in\noffshore windfarm deployments using long short-term memory (LSTM) Models for\nanomaly detection of the connectivity, providing detailed insights into its\nspecialized architecture developed for this purpose."
                },
                "authors": [
                    {
                        "name": "Abdelrahim Ahmad"
                    },
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Robert Piechocki"
                    },
                    {
                        "name": "Rui Inacio"
                    }
                ],
                "author_detail": {
                    "name": "Rui Inacio"
                },
                "author": "Rui Inacio",
                "arxiv_comment": "16 pages, 12 figures. This manuscript has been submitted to Elsevier\n  for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02842v1",
                "updated": "2024-09-04T16:14:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    14,
                    14,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T16:14:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    14,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "SNNAX -- Spiking Neural Networks in JAX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SNNAX -- Spiking Neural Networks in JAX"
                },
                "summary": "Spiking Neural Networks (SNNs) simulators are essential tools to prototype\nbiologically inspired models and neuromorphic hardware architectures and\npredict their performance. For such a tool, ease of use and flexibility are\ncritical, but so is simulation speed especially given the complexity inherent\nto simulating SNN. Here, we present SNNAX, a JAX-based framework for simulating\nand training such models with PyTorch-like intuitiveness and JAX-like execution\nspeed. SNNAX models are easily extended and customized to fit the desired model\nspecifications and target neuromorphic hardware. Additionally, SNNAX offers key\nfeatures for optimizing the training and deployment of SNNs such as flexible\nautomatic differentiation and just-in-time compilation. We evaluate and compare\nSNNAX to other commonly used machine learning (ML) frameworks used for\nprogramming SNNs. We provide key performance metrics, best practices,\ndocumented examples for simulating SNNs in SNNAX, and implement several\nbenchmarks used in the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) simulators are essential tools to prototype\nbiologically inspired models and neuromorphic hardware architectures and\npredict their performance. For such a tool, ease of use and flexibility are\ncritical, but so is simulation speed especially given the complexity inherent\nto simulating SNN. Here, we present SNNAX, a JAX-based framework for simulating\nand training such models with PyTorch-like intuitiveness and JAX-like execution\nspeed. SNNAX models are easily extended and customized to fit the desired model\nspecifications and target neuromorphic hardware. Additionally, SNNAX offers key\nfeatures for optimizing the training and deployment of SNNs such as flexible\nautomatic differentiation and just-in-time compilation. We evaluate and compare\nSNNAX to other commonly used machine learning (ML) frameworks used for\nprogramming SNNs. We provide key performance metrics, best practices,\ndocumented examples for simulating SNNs in SNNAX, and implement several\nbenchmarks used in the literature."
                },
                "authors": [
                    {
                        "name": "Jamie Lohoff"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08763v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08763v4",
                "updated": "2024-09-04T16:13:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    13,
                    18,
                    2,
                    248,
                    0
                ],
                "published": "2024-03-13T17:58:57Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    58,
                    57,
                    2,
                    73,
                    0
                ],
                "title": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget."
                },
                "authors": [
                    {
                        "name": "Adam Ibrahim"
                    },
                    {
                        "name": "Benjamin Thrien"
                    },
                    {
                        "name": "Kshitij Gupta"
                    },
                    {
                        "name": "Mats L. Richter"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Timothe Lesort"
                    },
                    {
                        "name": "Eugene Belilovsky"
                    },
                    {
                        "name": "Irina Rish"
                    }
                ],
                "author_detail": {
                    "name": "Irina Rish"
                },
                "author": "Irina Rish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08763v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08763v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02839v1",
                "updated": "2024-09-04T16:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    9,
                    28,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T16:09:28Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    9,
                    28,
                    2,
                    248,
                    0
                ],
                "title": "Jger: Automated Telephone Call Traceback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jger: Automated Telephone Call Traceback"
                },
                "summary": "Unsolicited telephone calls that facilitate fraud or unlawful telemarketing\ncontinue to overwhelm network users and the regulators who prosecute them. The\nfirst step in prosecuting phone abuse is traceback -- identifying the call\noriginator. This fundamental investigative task currently requires hours of\nmanual effort per call. In this paper, we introduce J\\\"ager, a distributed\nsecure call traceback system. J\\\"ager can trace a call in a few seconds, even\nwith partial deployment, while cryptographically preserving the privacy of call\nparties, carrier trade secrets like peers and call volume, and limiting the\nthreat of bulk analysis. We establish definitions and requirements of secure\ntraceback, then develop a suite of protocols that meet these requirements using\nwitness encryption, oblivious pseudorandom functions, and group signatures. We\nprove these protocols secure in the universal composibility framework. We then\ndemonstrate that J\\\"ager has low compute and bandwidth costs per call, and\nthese costs scale linearly with call volume. J\\\"ager provides an efficient,\nsecure, privacy-preserving system to revolutionize telephone abuse\ninvestigation with minimal costs to operators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsolicited telephone calls that facilitate fraud or unlawful telemarketing\ncontinue to overwhelm network users and the regulators who prosecute them. The\nfirst step in prosecuting phone abuse is traceback -- identifying the call\noriginator. This fundamental investigative task currently requires hours of\nmanual effort per call. In this paper, we introduce J\\\"ager, a distributed\nsecure call traceback system. J\\\"ager can trace a call in a few seconds, even\nwith partial deployment, while cryptographically preserving the privacy of call\nparties, carrier trade secrets like peers and call volume, and limiting the\nthreat of bulk analysis. We establish definitions and requirements of secure\ntraceback, then develop a suite of protocols that meet these requirements using\nwitness encryption, oblivious pseudorandom functions, and group signatures. We\nprove these protocols secure in the universal composibility framework. We then\ndemonstrate that J\\\"ager has low compute and bandwidth costs per call, and\nthese costs scale linearly with call volume. J\\\"ager provides an efficient,\nsecure, privacy-preserving system to revolutionize telephone abuse\ninvestigation with minimal costs to operators."
                },
                "authors": [
                    {
                        "name": "David Adei"
                    },
                    {
                        "name": "Varun Madathil"
                    },
                    {
                        "name": "Sathvik Prasad"
                    },
                    {
                        "name": "Bradley Reaves"
                    },
                    {
                        "name": "Alessandra Scafuro"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Scafuro"
                },
                "author": "Alessandra Scafuro",
                "arxiv_doi": "10.1145/3658644.3690290",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690290",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 2024 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS '24), October 14---18, 2024, Salt Lake City, UT,\n  USA. ACM, New York, NY, USA, 24 pages",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]