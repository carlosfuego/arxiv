[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.06095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06095v3",
                "updated": "2025-05-09T07:26:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    26,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2024-06-10T08:26:27Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    8,
                    26,
                    27,
                    0,
                    162,
                    0
                ],
                "title": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development"
                },
                "summary": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity."
                },
                "authors": [
                    {
                        "name": "Pawel K. Radtke"
                    },
                    {
                        "name": "Cristian G. Barrera-Hinojosa"
                    },
                    {
                        "name": "Mladen Ivkovic"
                    },
                    {
                        "name": "Tobias Weinzierl"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Weinzierl"
                },
                "author": "Tobias Weinzierl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05829v1",
                "updated": "2025-05-09T06:56:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T06:56:17Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition"
                },
                "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Keyi Li"
                    },
                    {
                        "name": "Yifan Jia"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Yufei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ma"
                },
                "author": "Yufei Ma",
                "arxiv_comment": "accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05772v1",
                "updated": "2025-05-09T04:17:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:17:05Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM"
                },
                "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."
                },
                "authors": [
                    {
                        "name": "Zehao Fan"
                    },
                    {
                        "name": "Garrett Gagnon"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Liu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liu Liu"
                },
                "author": "Liu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v3",
                "updated": "2025-05-09T00:31:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    31,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05251v1",
                "updated": "2025-05-08T13:56:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:56:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity"
                },
                "summary": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity."
                },
                "authors": [
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "arxiv_comment": "13 pages, 8 figures, submitted to IEEE journals for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v1",
                "updated": "2025-05-08T11:07:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03762v2",
                "updated": "2025-05-08T09:05:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    5,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-20T17:48:54Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    17,
                    48,
                    54,
                    6,
                    110,
                    0
                ],
                "title": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture"
                },
                "summary": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Côme Allart"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Zexin Fu"
                    },
                    {
                        "name": "Filippo Grillotti"
                    },
                    {
                        "name": "Fabio De Ambroggi"
                    },
                    {
                        "name": "Elio Guidetti"
                    },
                    {
                        "name": "Jean-Baptiste Rigaud"
                    },
                    {
                        "name": "Olivier Potin"
                    },
                    {
                        "name": "Jean Roch Coulon"
                    },
                    {
                        "name": "César Fuguet"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12110v2",
                "updated": "2025-05-08T07:55:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    55,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2024-06-17T21:43:39Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    43,
                    39,
                    0,
                    169,
                    0
                ],
                "title": "CacheSquash: Making caches speculation-aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheSquash: Making caches speculation-aware"
                },
                "summary": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads."
                },
                "authors": [
                    {
                        "name": "Hossam ElAtali"
                    },
                    {
                        "name": "N. Asokan"
                    }
                ],
                "author_detail": {
                    "name": "N. Asokan"
                },
                "author": "N. Asokan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v2",
                "updated": "2025-05-08T07:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    8,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04896v1",
                "updated": "2025-05-08T02:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T02:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory"
                },
                "summary": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks."
                },
                "authors": [
                    {
                        "name": "MD Mahady Hassan"
                    },
                    {
                        "name": "Shanto Roy"
                    },
                    {
                        "name": "Reza Rahaeimehr"
                    }
                ],
                "author_detail": {
                    "name": "Reza Rahaeimehr"
                },
                "author": "Reza Rahaeimehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04556v1",
                "updated": "2025-05-07T16:44:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:44:21Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "title": "Comparing CPU and GPU compute of PERMANOVA on MI300A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing CPU and GPU compute of PERMANOVA on MI300A"
                },
                "summary": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise."
                },
                "authors": [
                    {
                        "name": "Igor Sfiligoi"
                    }
                ],
                "author_detail": {
                    "name": "Igor Sfiligoi"
                },
                "author": "Igor Sfiligoi",
                "arxiv_comment": "7 pages, 1 figure, Accepted at PEARC25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04466v1",
                "updated": "2025-05-07T14:37:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:37:13Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption"
                },
                "summary": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
                },
                "authors": [
                    {
                        "name": "Mohammad Waquas Usmani"
                    },
                    {
                        "name": "Susmit Shannigrahi"
                    },
                    {
                        "name": "Michael Zink"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zink"
                },
                "author": "Michael Zink",
                "arxiv_comment": "8 pages plus references, 10 figures, some with subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v1",
                "updated": "2025-05-07T13:54:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v2",
                "updated": "2025-05-07T13:07:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    7,
                    25,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.05890",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04326v1",
                "updated": "2025-05-07T11:21:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T11:21:12Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "title": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins"
                },
                "summary": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zihan Jia"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Lin Cui"
                    },
                    {
                        "name": "Fung Po Tso"
                    }
                ],
                "author_detail": {
                    "name": "Fung Po Tso"
                },
                "author": "Fung Po Tso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04216v1",
                "updated": "2025-05-07T08:10:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "title": "Computational Model for Photoionization in Pure SF6 Streamer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer"
                },
                "summary": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Feng"
                },
                "author": "Zihao Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v2",
                "updated": "2025-05-07T07:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    57,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04129v1",
                "updated": "2025-05-07T05:00:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:00:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator"
                },
                "summary": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%."
                },
                "authors": [
                    {
                        "name": "Turan Vural"
                    },
                    {
                        "name": "Yuki Yuminaga"
                    },
                    {
                        "name": "Alex Petrosyan"
                    },
                    {
                        "name": "Ben Livshits"
                    }
                ],
                "author_detail": {
                    "name": "Ben Livshits"
                },
                "author": "Ben Livshits",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v6",
                "updated": "2025-05-07T01:29:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    1,
                    29,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v6",
                "updated": "2025-05-06T19:28:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    19,
                    28,
                    56,
                    1,
                    126,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v3",
                "updated": "2025-05-06T15:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    23,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v1",
                "updated": "2025-05-05T18:01:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02346v1",
                "updated": "2025-05-05T04:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T04:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code"
                },
                "summary": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility."
                },
                "authors": [
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Andrei Calin Dragomir"
                    },
                    {
                        "name": "Patricia Lago"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Lago"
                },
                "author": "Patricia Lago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10375v2",
                "updated": "2025-05-04T09:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    9,
                    49,
                    42,
                    6,
                    124,
                    0
                ],
                "published": "2024-12-16T07:59:21Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    21,
                    0,
                    351,
                    0
                ],
                "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "7 pages, 10 figures, Accepted by DATE Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02027v1",
                "updated": "2025-05-04T08:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T08:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning"
                },
                "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter."
                },
                "authors": [
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zaixi Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiaxia Yan"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhou Yao"
                },
                "author": "Fangzhou Yao",
                "arxiv_comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v3",
                "updated": "2025-05-03T04:07:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    4,
                    7,
                    7,
                    5,
                    123,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v4",
                "updated": "2025-05-02T11:29:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    29,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v1",
                "updated": "2025-05-01T18:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "19 pages, 9 figures. Supplement 29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v1",
                "updated": "2025-05-01T14:53:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00315v1",
                "updated": "2025-05-01T05:22:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing"
                },
                "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."
                },
                "authors": [
                    {
                        "name": "Piotr Piękos"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04532v3",
                "updated": "2025-05-01T02:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    2,
                    14,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2024-05-07T17:59:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    59,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving"
                },
                "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Chuang Gan"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v2",
                "updated": "2025-04-30T19:48:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    19,
                    48,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00074v1",
                "updated": "2025-04-30T18:00:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O"
                },
                "summary": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research."
                },
                "authors": [
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Ziyin Song"
                    },
                    {
                        "name": "Juntao Song"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    },
                    {
                        "name": "Quansheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Wu"
                },
                "author": "Quansheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00745v1",
                "updated": "2025-04-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations"
                },
                "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively."
                },
                "authors": [
                    {
                        "name": "Maozhe Zhao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Sensys 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03763v1",
                "updated": "2025-04-21T00:21:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    21,
                    8,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:21:08Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    21,
                    8,
                    0,
                    111,
                    0
                ],
                "title": "Splitwiser: Efficient LM inference with constrained resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Splitwiser: Efficient LM inference with constrained resources"
                },
                "summary": "Efficient inference of LLMs remains a crucial challenge, with two main\nphases: a compute-intensive prompt computation and a memory-intensive token\ngeneration. Despite existing batching and scheduling techniques, token\ngeneration phases fail to fully utilize compute resources, especially when\ncompared to prompt computation phases. To address these challenges, we propose\nSplitwiser, a methodology that splits the two phases of an LLM inference\nrequest onto the same GPU, thereby reducing overhead and improving memory\naccess and cache utilization. By eliminating the need to transfer data across\ndevices, Splitwiser aims to minimize network-related overheads. In this report,\nwe describe the basic structure of our proposed pipeline while sharing\npreliminary results and analysis. We implement our proposed multiprocessing\ndesign on two widely-used and independent LLM architectures: Huggingface and\nvLLM. We open-source our code for the respective implementations: 1)\nHuggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM\n(https://github.com/adney11/vllm-sysml).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of LLMs remains a crucial challenge, with two main\nphases: a compute-intensive prompt computation and a memory-intensive token\ngeneration. Despite existing batching and scheduling techniques, token\ngeneration phases fail to fully utilize compute resources, especially when\ncompared to prompt computation phases. To address these challenges, we propose\nSplitwiser, a methodology that splits the two phases of an LLM inference\nrequest onto the same GPU, thereby reducing overhead and improving memory\naccess and cache utilization. By eliminating the need to transfer data across\ndevices, Splitwiser aims to minimize network-related overheads. In this report,\nwe describe the basic structure of our proposed pipeline while sharing\npreliminary results and analysis. We implement our proposed multiprocessing\ndesign on two widely-used and independent LLM architectures: Huggingface and\nvLLM. We open-source our code for the respective implementations: 1)\nHuggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM\n(https://github.com/adney11/vllm-sysml)."
                },
                "authors": [
                    {
                        "name": "Asad Aali"
                    },
                    {
                        "name": "Adney Cardoza"
                    },
                    {
                        "name": "Melissa Capo"
                    }
                ],
                "author_detail": {
                    "name": "Melissa Capo"
                },
                "author": "Melissa Capo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.06188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06188v2",
                "updated": "2025-05-09T17:25:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    17,
                    25,
                    34,
                    4,
                    129,
                    0
                ],
                "published": "2024-07-08T17:59:36Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    17,
                    59,
                    36,
                    0,
                    190,
                    0
                ],
                "title": "CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation"
                },
                "summary": "While recent advances in text-to-motion generation have shown promising\nresults, they typically assume all individuals are grouped as a single unit.\nScaling these methods to handle larger crowds and ensuring that individuals\nrespond appropriately to specific events remains a significant challenge. This\nis primarily due to the complexities of scene planning, which involves\norganizing groups, planning their activities, and coordinating interactions,\nand controllable motion generation. In this paper, we present CrowdMoGen, the\nfirst zero-shot framework for collective motion generation, which effectively\ngroups individuals and generates event-aligned motion sequences from text\nprompts. 1) Being limited by the available datasets for training an effective\nscene planning module in a supervised manner, we instead propose a crowd scene\nplanner that leverages pre-trained large language models (LLMs) to organize\nindividuals into distinct groups. While LLMs offer high-level guidance for\ngroup divisions, they lack the low-level understanding of human motion. To\naddress this, we further propose integrating an SMPL-based joint prior to\ngenerate context-appropriate activities, which consists of both joint\ntrajectories and textual descriptions. 2) Secondly, to incorporate the assigned\nactivities into the generative network, we introduce a collective motion\ngenerator that integrates the activities into a transformer-based network in a\njoint-wise manner, maintaining the spatial constraints during the multi-step\ndenoising process. Extensive experiments demonstrate that CrowdMoGen\nsignificantly outperforms previous approaches, delivering realistic,\nevent-driven motion sequences that are spatially coherent. As the first\nframework of collective motion generation, CrowdMoGen has the potential to\nadvance applications in urban simulation, crowd planning, and other large-scale\ninteractive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent advances in text-to-motion generation have shown promising\nresults, they typically assume all individuals are grouped as a single unit.\nScaling these methods to handle larger crowds and ensuring that individuals\nrespond appropriately to specific events remains a significant challenge. This\nis primarily due to the complexities of scene planning, which involves\norganizing groups, planning their activities, and coordinating interactions,\nand controllable motion generation. In this paper, we present CrowdMoGen, the\nfirst zero-shot framework for collective motion generation, which effectively\ngroups individuals and generates event-aligned motion sequences from text\nprompts. 1) Being limited by the available datasets for training an effective\nscene planning module in a supervised manner, we instead propose a crowd scene\nplanner that leverages pre-trained large language models (LLMs) to organize\nindividuals into distinct groups. While LLMs offer high-level guidance for\ngroup divisions, they lack the low-level understanding of human motion. To\naddress this, we further propose integrating an SMPL-based joint prior to\ngenerate context-appropriate activities, which consists of both joint\ntrajectories and textual descriptions. 2) Secondly, to incorporate the assigned\nactivities into the generative network, we introduce a collective motion\ngenerator that integrates the activities into a transformer-based network in a\njoint-wise manner, maintaining the spatial constraints during the multi-step\ndenoising process. Extensive experiments demonstrate that CrowdMoGen\nsignificantly outperforms previous approaches, delivering realistic,\nevent-driven motion sequences that are spatially coherent. As the first\nframework of collective motion generation, CrowdMoGen has the potential to\nadvance applications in urban simulation, crowd planning, and other large-scale\ninteractive environments."
                },
                "authors": [
                    {
                        "name": "Yukang Cao"
                    },
                    {
                        "name": "Xinying Guo"
                    },
                    {
                        "name": "Mingyuan Zhang"
                    },
                    {
                        "name": "Haozhe Xie"
                    },
                    {
                        "name": "Chenyang Gu"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Project page: https://yukangcao.github.io/CrowdMoGen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18773v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18773v3",
                "updated": "2025-05-09T17:12:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    17,
                    12,
                    12,
                    4,
                    129,
                    0
                ],
                "published": "2024-11-27T21:53:15Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    21,
                    53,
                    15,
                    2,
                    332,
                    0
                ],
                "title": "Inference on Dynamic Spatial Autoregressive Models with Change Point\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on Dynamic Spatial Autoregressive Models with Change Point\n  Detection"
                },
                "summary": "We analyze a varying-coefficient dynamic spatial autoregressive model with\nspatial fixed effects. One salient feature of the model is the incorporation of\nmultiple spatial weight matrices through their linear combinations with varying\ncoefficients, which help solve the problem of choosing the most ``correct'' one\nfor applied econometricians who often face the availability of multiple expert\nspatial weight matrices. We estimate and make inferences on the model\ncoefficients and coefficients in basis expansions of the varying coefficients\nthrough penalized estimations, establishing the oracle properties of the\nestimators and the consistency of the overall estimated spatial weight matrix,\nwhich can be time-dependent. We further consider two applications of our model\nin change point detections in dynamic spatial autoregressive models, providing\ntheoretical justifications in consistent change point locations estimation and\npractical implementations. Simulation experiments demonstrate the performance\nof our proposed methodology, and real data analyses are also carried out.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze a varying-coefficient dynamic spatial autoregressive model with\nspatial fixed effects. One salient feature of the model is the incorporation of\nmultiple spatial weight matrices through their linear combinations with varying\ncoefficients, which help solve the problem of choosing the most ``correct'' one\nfor applied econometricians who often face the availability of multiple expert\nspatial weight matrices. We estimate and make inferences on the model\ncoefficients and coefficients in basis expansions of the varying coefficients\nthrough penalized estimations, establishing the oracle properties of the\nestimators and the consistency of the overall estimated spatial weight matrix,\nwhich can be time-dependent. We further consider two applications of our model\nin change point detections in dynamic spatial autoregressive models, providing\ntheoretical justifications in consistent change point locations estimation and\npractical implementations. Simulation experiments demonstrate the performance\nof our proposed methodology, and real data analyses are also carried out."
                },
                "authors": [
                    {
                        "name": "Zetai Cen"
                    },
                    {
                        "name": "Yudong Chen"
                    },
                    {
                        "name": "Clifford Lam"
                    }
                ],
                "author_detail": {
                    "name": "Clifford Lam"
                },
                "author": "Clifford Lam",
                "arxiv_comment": "58 pages, 7 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18773v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05423v2",
                "updated": "2025-05-09T17:02:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    17,
                    2,
                    1,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-08T17:12:56Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    12,
                    56,
                    3,
                    128,
                    0
                ],
                "title": "LiTransProQA: an LLM-based Literary Translation evaluation metric with\n  Professional Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiTransProQA: an LLM-based Literary Translation evaluation metric with\n  Professional Question Answering"
                },
                "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce LiTransProQA, a novel, reference-free,\nLLM-based question-answering framework designed specifically for literary\ntranslation evaluation. LiTransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, LiTransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art metrics by over 15\npoints in adequacy assessments. Incorporating professional translator insights\nas weights further improves performance, highlighting the value of translator\ninputs. Notably, LiTransProQA approaches human-level evaluation performance\ncomparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce LiTransProQA, a novel, reference-free,\nLLM-based question-answering framework designed specifically for literary\ntranslation evaluation. LiTransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, LiTransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art metrics by over 15\npoints in adequacy assessments. Incorporating professional translator insights\nas weights further improves performance, highlighting the value of translator\ninputs. Notably, LiTransProQA approaches human-level evaluation performance\ncomparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Lieve Macken"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "Update WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06190v1",
                "updated": "2025-05-09T17:00:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    17,
                    0,
                    28,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T17:00:28Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    17,
                    0,
                    28,
                    4,
                    129,
                    0
                ],
                "title": "Beyond the Mean: Limit Theory and Tests for Infinite-Mean Autoregressive\n  Conditional Durations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Mean: Limit Theory and Tests for Infinite-Mean Autoregressive\n  Conditional Durations"
                },
                "summary": "Integrated autoregressive conditional duration (ACD) models serve as natural\ncounterparts to the well-known integrated GARCH models used for financial\nreturns. However, despite their resemblance, asymptotic theory for ACD is\nchallenging and also not complete, in particular for integrated ACD. Central\nchallenges arise from the facts that (i) integrated ACD processes imply\ndurations with infinite expectation, and (ii) even in the non-integrated case,\nconventional asymptotic approaches break down due to the randomness in the\nnumber of durations within a fixed observation period. Addressing these\nchallenges, we provide here unified asymptotic theory for the (quasi-) maximum\nlikelihood estimator for ACD models; a unified theory which includes integrated\nACD models. Based on the new results, we also provide a novel framework for\nhypothesis testing in duration models, enabling inference on a key empirical\nquestion: whether durations possess a finite or infinite expectation. We apply\nour results to high-frequency cryptocurrency ETF trading data. Motivated by\nparameter estimates near the integrated ACD boundary, we assess whether\ndurations between trades in these markets have finite expectation, an\nassumption often made implicitly in the literature on point process models. Our\nempirical findings indicate infinite-mean durations for all the five\ncryptocurrencies examined, with the integrated ACD hypothesis rejected --\nagainst alternatives with tail index less than one -- for four out of the five\ncryptocurrencies considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated autoregressive conditional duration (ACD) models serve as natural\ncounterparts to the well-known integrated GARCH models used for financial\nreturns. However, despite their resemblance, asymptotic theory for ACD is\nchallenging and also not complete, in particular for integrated ACD. Central\nchallenges arise from the facts that (i) integrated ACD processes imply\ndurations with infinite expectation, and (ii) even in the non-integrated case,\nconventional asymptotic approaches break down due to the randomness in the\nnumber of durations within a fixed observation period. Addressing these\nchallenges, we provide here unified asymptotic theory for the (quasi-) maximum\nlikelihood estimator for ACD models; a unified theory which includes integrated\nACD models. Based on the new results, we also provide a novel framework for\nhypothesis testing in duration models, enabling inference on a key empirical\nquestion: whether durations possess a finite or infinite expectation. We apply\nour results to high-frequency cryptocurrency ETF trading data. Motivated by\nparameter estimates near the integrated ACD boundary, we assess whether\ndurations between trades in these markets have finite expectation, an\nassumption often made implicitly in the literature on point process models. Our\nempirical findings indicate infinite-mean durations for all the five\ncryptocurrencies examined, with the integrated ACD hypothesis rejected --\nagainst alternatives with tail index less than one -- for four out of the five\ncryptocurrencies considered."
                },
                "authors": [
                    {
                        "name": "Giuseppe Cavaliere"
                    },
                    {
                        "name": "Thomas Mikosch"
                    },
                    {
                        "name": "Anders Rahbek"
                    },
                    {
                        "name": "Frederik Vilandt"
                    }
                ],
                "author_detail": {
                    "name": "Frederik Vilandt"
                },
                "author": "Frederik Vilandt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04134v2",
                "updated": "2025-05-09T16:58:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    58,
                    59,
                    4,
                    129,
                    0
                ],
                "published": "2025-02-06T15:14:02Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    14,
                    2,
                    3,
                    37,
                    0
                ],
                "title": "The Order Effect: Investigating Prompt Sensitivity to Input Order in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Order Effect: Investigating Prompt Sensitivity to Input Order in\n  LLMs"
                },
                "summary": "As large language models (LLMs) become integral to diverse applications,\nensuring their reliability under varying input conditions is crucial. One key\nissue affecting this reliability is order sensitivity, wherein slight\nvariations in the input arrangement can lead to inconsistent or biased outputs.\nAlthough recent advances have reduced this sensitivity, the problem remains\nunresolved. This paper investigates the extent of order sensitivity in LLMs\nwhose internal components are hidden from users (such as closed-source models\nor those accessed via API calls). We conduct experiments across multiple tasks,\nincluding paraphrasing, relevance judgment, and multiple-choice questions. Our\nresults show that input order significantly affects performance across tasks,\nwith shuffled inputs leading to measurable declines in output accuracy.\nFew-shot prompting demonstrates mixed effectiveness and offers partial\nmitigation; however, fails to fully resolve the problem. These findings\nhighlight persistent risks, particularly in high-stakes applications, and point\nto the need for more robust LLMs or improved input-handling techniques in\nfuture development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to diverse applications,\nensuring their reliability under varying input conditions is crucial. One key\nissue affecting this reliability is order sensitivity, wherein slight\nvariations in the input arrangement can lead to inconsistent or biased outputs.\nAlthough recent advances have reduced this sensitivity, the problem remains\nunresolved. This paper investigates the extent of order sensitivity in LLMs\nwhose internal components are hidden from users (such as closed-source models\nor those accessed via API calls). We conduct experiments across multiple tasks,\nincluding paraphrasing, relevance judgment, and multiple-choice questions. Our\nresults show that input order significantly affects performance across tasks,\nwith shuffled inputs leading to measurable declines in output accuracy.\nFew-shot prompting demonstrates mixed effectiveness and offers partial\nmitigation; however, fails to fully resolve the problem. These findings\nhighlight persistent risks, particularly in high-stakes applications, and point\nto the need for more robust LLMs or improved input-handling techniques in\nfuture development."
                },
                "authors": [
                    {
                        "name": "Bryan Guan"
                    },
                    {
                        "name": "Tanya Roosta"
                    },
                    {
                        "name": "Peyman Passban"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "arxiv_comment": "The first 3 authors have contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06184v1",
                "updated": "2025-05-09T16:51:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    51,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:51:24Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    51,
                    24,
                    4,
                    129,
                    0
                ],
                "title": "From Millions of Tweets to Actionable Insights: Leveraging LLMs for User\n  Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Millions of Tweets to Actionable Insights: Leveraging LLMs for User\n  Profiling"
                },
                "summary": "Social media user profiling through content analysis is crucial for tasks\nlike misinformation detection, engagement prediction, hate speech monitoring,\nand user behavior modeling. However, existing profiling techniques, including\ntweet summarization, attribute-based profiling, and latent representation\nlearning, face significant limitations: they often lack transferability,\nproduce non-interpretable features, require large labeled datasets, or rely on\nrigid predefined categories that limit adaptability. We introduce a novel large\nlanguage model (LLM)-based approach that leverages domain-defining statements,\nwhich serve as key characteristics outlining the important pillars of a domain\nas foundations for profiling. Our two-stage method first employs\nsemi-supervised filtering with a domain-specific knowledge base, then generates\nboth abstractive (synthesized descriptions) and extractive (representative\ntweet selections) user profiles. By harnessing LLMs' inherent knowledge with\nminimal human validation, our approach is adaptable across domains while\nreducing the need for large labeled datasets. Our method generates\ninterpretable natural language user profiles, condensing extensive user data\ninto a scale that unlocks LLMs' reasoning and knowledge capabilities for\ndownstream social network tasks. We contribute a Persian political Twitter (X)\ndataset and an LLM-based evaluation framework with human validation.\nExperimental results show our method significantly outperforms state-of-the-art\nLLM-based and traditional methods by 9.8%, demonstrating its effectiveness in\ncreating flexible, adaptable, and interpretable user profiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media user profiling through content analysis is crucial for tasks\nlike misinformation detection, engagement prediction, hate speech monitoring,\nand user behavior modeling. However, existing profiling techniques, including\ntweet summarization, attribute-based profiling, and latent representation\nlearning, face significant limitations: they often lack transferability,\nproduce non-interpretable features, require large labeled datasets, or rely on\nrigid predefined categories that limit adaptability. We introduce a novel large\nlanguage model (LLM)-based approach that leverages domain-defining statements,\nwhich serve as key characteristics outlining the important pillars of a domain\nas foundations for profiling. Our two-stage method first employs\nsemi-supervised filtering with a domain-specific knowledge base, then generates\nboth abstractive (synthesized descriptions) and extractive (representative\ntweet selections) user profiles. By harnessing LLMs' inherent knowledge with\nminimal human validation, our approach is adaptable across domains while\nreducing the need for large labeled datasets. Our method generates\ninterpretable natural language user profiles, condensing extensive user data\ninto a scale that unlocks LLMs' reasoning and knowledge capabilities for\ndownstream social network tasks. We contribute a Persian political Twitter (X)\ndataset and an LLM-based evaluation framework with human validation.\nExperimental results show our method significantly outperforms state-of-the-art\nLLM-based and traditional methods by 9.8%, demonstrating its effectiveness in\ncreating flexible, adaptable, and interpretable user profiles."
                },
                "authors": [
                    {
                        "name": "Vahid Rahimzadeh"
                    },
                    {
                        "name": "Ali Hamzehpour"
                    },
                    {
                        "name": "Azadeh Shakery"
                    },
                    {
                        "name": "Masoud Asadpour"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Asadpour"
                },
                "author": "Masoud Asadpour",
                "arxiv_comment": "Accepted at MisD @ AAAI ICWSM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06178v1",
                "updated": "2025-05-09T16:45:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    45,
                    43,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:45:43Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    45,
                    43,
                    4,
                    129,
                    0
                ],
                "title": "A Large Language Model-Enhanced Q-learning for Capacitated Vehicle\n  Routing Problem with Time Windows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model-Enhanced Q-learning for Capacitated Vehicle\n  Routing Problem with Time Windows"
                },
                "summary": "The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a\nclassic NP-hard combinatorial optimization problem widely applied in logistics\ndistribution and transportation management. Its complexity stems from the\nconstraints of vehicle capacity and time windows, which pose significant\nchallenges to traditional approaches. Advances in Large Language Models (LLMs)\nprovide new possibilities for finding approximate solutions to CVRPTW. This\npaper proposes a novel LLM-enhanced Q-learning framework to address the CVRPTW\nwith real-time emergency constraints. Our solution introduces an adaptive\ntwo-phase training mechanism that transitions from the LLM-guided exploration\nphase to the autonomous optimization phase of Q-network. To ensure reliability,\nwe design a three-tier self-correction mechanism based on the Chain-of-Thought\n(CoT) for LLMs: syntactic validation, semantic verification, and physical\nconstraint enforcement. In addition, we also prioritized replay of the\nexperience generated by LLMs to amplify the regulatory role of LLMs in the\narchitecture. Experimental results demonstrate that our framework achieves a\n7.3\\% average reduction in cost compared to traditional Q-learning, with fewer\ntraining steps required for convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a\nclassic NP-hard combinatorial optimization problem widely applied in logistics\ndistribution and transportation management. Its complexity stems from the\nconstraints of vehicle capacity and time windows, which pose significant\nchallenges to traditional approaches. Advances in Large Language Models (LLMs)\nprovide new possibilities for finding approximate solutions to CVRPTW. This\npaper proposes a novel LLM-enhanced Q-learning framework to address the CVRPTW\nwith real-time emergency constraints. Our solution introduces an adaptive\ntwo-phase training mechanism that transitions from the LLM-guided exploration\nphase to the autonomous optimization phase of Q-network. To ensure reliability,\nwe design a three-tier self-correction mechanism based on the Chain-of-Thought\n(CoT) for LLMs: syntactic validation, semantic verification, and physical\nconstraint enforcement. In addition, we also prioritized replay of the\nexperience generated by LLMs to amplify the regulatory role of LLMs in the\narchitecture. Experimental results demonstrate that our framework achieves a\n7.3\\% average reduction in cost compared to traditional Q-learning, with fewer\ntraining steps required for convergence."
                },
                "authors": [
                    {
                        "name": "Linjiang Cao"
                    },
                    {
                        "name": "Maonan Wang"
                    },
                    {
                        "name": "Xi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Xi Xiong"
                },
                "author": "Xi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06176v1",
                "updated": "2025-05-09T16:38:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    38,
                    27,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:38:27Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    38,
                    27,
                    4,
                    129,
                    0
                ],
                "title": "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills"
                },
                "summary": "Retouching is an essential task in post-manipulation of raw photographs.\nGenerative editing, guided by text or strokes, provides a new tool accessible\nto users but can easily change the identity of the original objects in\nunacceptable and unpredictable ways. In contrast, although traditional\nprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,\nLightroom), are conservative, they are still preferred by professionals.\nUnfortunately, professional quality retouching involves many individual\nprocedural editing operations that is challenging to plan for most novices. In\nthis paper, we ask if a multimodal large language model (MLLM) can be taught to\ncritique raw photographs, suggest suitable remedies, and finally realize them\nwith a given set of pre-authored procedural image operations. We demonstrate\nthat MLLMs can be first made aware of the underlying image processing\noperations, by training them to solve specially designed visual puzzles.\nSubsequently, such an operation-aware MLLM can both plan and propose edit\nsequences. To facilitate training, given a set of expert-edited photos, we\nsynthesize a reasoning dataset by procedurally manipulating the expert edits\nand then grounding a pretrained LLM on the visual adjustments, to synthesize\nreasoning for finetuning. The proposed retouching operations are, by\nconstruction, understandable by the users, preserve object details and\nresolution, and can be optionally overridden. We evaluate our setup on a\nvariety of test examples and show advantages, in terms of explainability and\nidentity preservation, over existing generative and other procedural\nalternatives. Code, data, models, and supplementary results can be found via\nour project website at https://monetgpt.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retouching is an essential task in post-manipulation of raw photographs.\nGenerative editing, guided by text or strokes, provides a new tool accessible\nto users but can easily change the identity of the original objects in\nunacceptable and unpredictable ways. In contrast, although traditional\nprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,\nLightroom), are conservative, they are still preferred by professionals.\nUnfortunately, professional quality retouching involves many individual\nprocedural editing operations that is challenging to plan for most novices. In\nthis paper, we ask if a multimodal large language model (MLLM) can be taught to\ncritique raw photographs, suggest suitable remedies, and finally realize them\nwith a given set of pre-authored procedural image operations. We demonstrate\nthat MLLMs can be first made aware of the underlying image processing\noperations, by training them to solve specially designed visual puzzles.\nSubsequently, such an operation-aware MLLM can both plan and propose edit\nsequences. To facilitate training, given a set of expert-edited photos, we\nsynthesize a reasoning dataset by procedurally manipulating the expert edits\nand then grounding a pretrained LLM on the visual adjustments, to synthesize\nreasoning for finetuning. The proposed retouching operations are, by\nconstruction, understandable by the users, preserve object details and\nresolution, and can be optionally overridden. We evaluate our setup on a\nvariety of test examples and show advantages, in terms of explainability and\nidentity preservation, over existing generative and other procedural\nalternatives. Code, data, models, and supplementary results can be found via\nour project website at https://monetgpt.github.io."
                },
                "authors": [
                    {
                        "name": "Niladri Shekhar Dutt"
                    },
                    {
                        "name": "Duygu Ceylan"
                    },
                    {
                        "name": "Niloy J. Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Niloy J. Mitra"
                },
                "author": "Niloy J. Mitra",
                "arxiv_doi": "10.1145/3730926",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3730926",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.06176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGGRAPH 2025 [ACM Transactions on Graphics]; Project\n  website: https://monetgpt.github.io",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06175v1",
                "updated": "2025-05-09T16:29:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    29,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:29:29Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    29,
                    29,
                    4,
                    129,
                    0
                ],
                "title": "Turbo-ICL: In-Context Learning-Based Turbo Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turbo-ICL: In-Context Learning-Based Turbo Equalization"
                },
                "summary": "This paper introduces a novel in-context learning (ICL) framework, inspired\nby large language models (LLMs), for soft-input soft-output channel\nequalization in coded multiple-input multiple-output (MIMO) systems. The\nproposed approach learns to infer posterior symbol distributions directly from\na prompt of pilot signals and decoder feedback. A key innovation is the use of\nprompt augmentation to incorporate extrinsic information from the decoder\noutput as additional context, enabling the ICL model to refine its symbol\nestimates iteratively across turbo decoding iterations. Two model variants,\nbased on Transformer and state-space architectures, are developed and\nevaluated. Extensive simulations demonstrate that, when traditional linear\nassumptions break down, e.g., in the presence of low-resolution quantization,\nICL equalizers consistently outperform conventional model-based baselines, even\nwhen the latter are provided with perfect channel state information. Results\nalso highlight the advantage of Transformer-based models under limited training\ndiversity, as well as the efficiency of state-space models in\nresource-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel in-context learning (ICL) framework, inspired\nby large language models (LLMs), for soft-input soft-output channel\nequalization in coded multiple-input multiple-output (MIMO) systems. The\nproposed approach learns to infer posterior symbol distributions directly from\na prompt of pilot signals and decoder feedback. A key innovation is the use of\nprompt augmentation to incorporate extrinsic information from the decoder\noutput as additional context, enabling the ICL model to refine its symbol\nestimates iteratively across turbo decoding iterations. Two model variants,\nbased on Transformer and state-space architectures, are developed and\nevaluated. Extensive simulations demonstrate that, when traditional linear\nassumptions break down, e.g., in the presence of low-resolution quantization,\nICL equalizers consistently outperform conventional model-based baselines, even\nwhen the latter are provided with perfect channel state information. Results\nalso highlight the advantage of Transformer-based models under limited training\ndiversity, as well as the efficiency of state-space models in\nresource-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Zihang Song"
                    },
                    {
                        "name": "Matteo Zecchin"
                    },
                    {
                        "name": "Bipin Rajendran"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08049v2",
                "updated": "2025-05-09T16:20:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    20,
                    52,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-10T18:08:16Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    18,
                    8,
                    16,
                    3,
                    100,
                    0
                ],
                "title": "Patch distribution modeling framework adaptive cosine estimator\n  (PaDiM-ACE) for anomaly detection and localization in synthetic aperture\n  radar imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patch distribution modeling framework adaptive cosine estimator\n  (PaDiM-ACE) for anomaly detection and localization in synthetic aperture\n  radar imagery"
                },
                "summary": "This work presents a new approach to anomaly detection and localization in\nsynthetic aperture radar imagery (SAR), expanding upon the existing patch\ndistribution modeling framework (PaDiM). We introduce the adaptive cosine\nestimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at\ninference, an unbounded metric. ACE instead uses the cosine similarity metric,\nproviding bounded anomaly detection scores. The proposed method is evaluated\nacross multiple SAR datasets, with performance metrics including the area under\nthe receiver operating curve (AUROC) at the image and pixel level, aiming for\nincreased performance in anomaly detection and localization of SAR imagery. The\ncode is publicly available:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a new approach to anomaly detection and localization in\nsynthetic aperture radar imagery (SAR), expanding upon the existing patch\ndistribution modeling framework (PaDiM). We introduce the adaptive cosine\nestimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at\ninference, an unbounded metric. ACE instead uses the cosine similarity metric,\nproviding bounded anomaly detection scores. The proposed method is evaluated\nacross multiple SAR datasets, with performance metrics including the area under\nthe receiver operating curve (AUROC) at the image and pixel level, aiming for\nincreased performance in anomaly detection and localization of SAR imagery. The\ncode is publicly available:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE."
                },
                "authors": [
                    {
                        "name": "Angelina Ibarra"
                    },
                    {
                        "name": "Joshua Peeples"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Peeples"
                },
                "author": "Joshua Peeples",
                "arxiv_comment": "Accepted to SPIE, Defense and Commercial Sensing, Algorithms for\n  Synthetic Aperture Radar Imagery XXXII (April 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06150v1",
                "updated": "2025-05-09T16:02:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    2,
                    23,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:02:23Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    2,
                    23,
                    4,
                    129,
                    0
                ],
                "title": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed\n  Compute Budgets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed\n  Compute Budgets"
                },
                "summary": "We introduce a scaling law for fine-tuning large language models (LLMs) under\nfixed compute budgets that explicitly accounts for data composition.\nConventional approaches measure training data solely by total tokens, yet the\nnumber of examples and their average token length -- what we term \\emph{dataset\nvolume} -- play a decisive role in model performance. Our formulation is tuned\nfollowing established procedures. Experiments on the BRICC dataset\n\\cite{salavati2024reducing} and subsets of the MMLU dataset\n\\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple\nsubsampling strategies, reveal that data composition significantly affects\ntoken efficiency. These results motivate refined scaling laws for practical LLM\nfine-tuning in resource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a scaling law for fine-tuning large language models (LLMs) under\nfixed compute budgets that explicitly accounts for data composition.\nConventional approaches measure training data solely by total tokens, yet the\nnumber of examples and their average token length -- what we term \\emph{dataset\nvolume} -- play a decisive role in model performance. Our formulation is tuned\nfollowing established procedures. Experiments on the BRICC dataset\n\\cite{salavati2024reducing} and subsets of the MMLU dataset\n\\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple\nsubsampling strategies, reveal that data composition significantly affects\ntoken efficiency. These results motivate refined scaling laws for practical LLM\nfine-tuning in resource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Ryan Lagasse"
                    },
                    {
                        "name": "Aidan Kiernans"
                    },
                    {
                        "name": "Avijit Ghosh"
                    },
                    {
                        "name": "Shiri Dori-Hacohen"
                    }
                ],
                "author_detail": {
                    "name": "Shiri Dori-Hacohen"
                },
                "author": "Shiri Dori-Hacohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08598v2",
                "updated": "2025-05-09T16:02:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    2,
                    18,
                    4,
                    129,
                    0
                ],
                "published": "2023-12-14T01:48:58Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    1,
                    48,
                    58,
                    3,
                    348,
                    0
                ],
                "title": "MotherNet: Fast Training and Inference via Hyper-Network Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotherNet: Fast Training and Inference via Hyper-Network Transformers"
                },
                "summary": "Foundation models are transforming machine learning across many modalities,\nwith in-context learning replacing classical model training. Recent work on\ntabular data hints at a similar opportunity to build foundation models for\nclassification for numerical data. However, existing meta-learning approaches\ncan not compete with tree-based methods in terms of inference time. In this\npaper, we propose MotherNet, a hypernetwork architecture trained on synthetic\nclassification tasks that, once prompted with a never-seen-before training set\ngenerates the weights of a trained ``child'' neural-network by in-context\nlearning using a single forward pass. In contrast to most existing\nhypernetworks that are usually trained for relatively constrained multi-task\nsettings, MotherNet can create models for multiclass classification on\narbitrary tabular datasets without any dataset specific gradient descent. The\nchild network generated by MotherNet outperforms neural networks trained using\ngradient descent on small datasets, and is comparable to predictions by TabPFN\nand standard ML methods like Gradient Boosting. Unlike a direct application of\nTabPFN, MotherNet generated networks are highly efficient at inference time. We\nalso demonstrate that HyperFast is unable to perform effective in-context\nlearning on small datasets, and heavily relies on dataset specific fine-tuning\nand hyper-parameter tuning, while MotherNet requires no fine-tuning or\nper-dataset hyper-parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models are transforming machine learning across many modalities,\nwith in-context learning replacing classical model training. Recent work on\ntabular data hints at a similar opportunity to build foundation models for\nclassification for numerical data. However, existing meta-learning approaches\ncan not compete with tree-based methods in terms of inference time. In this\npaper, we propose MotherNet, a hypernetwork architecture trained on synthetic\nclassification tasks that, once prompted with a never-seen-before training set\ngenerates the weights of a trained ``child'' neural-network by in-context\nlearning using a single forward pass. In contrast to most existing\nhypernetworks that are usually trained for relatively constrained multi-task\nsettings, MotherNet can create models for multiclass classification on\narbitrary tabular datasets without any dataset specific gradient descent. The\nchild network generated by MotherNet outperforms neural networks trained using\ngradient descent on small datasets, and is comparable to predictions by TabPFN\nand standard ML methods like Gradient Boosting. Unlike a direct application of\nTabPFN, MotherNet generated networks are highly efficient at inference time. We\nalso demonstrate that HyperFast is unable to perform effective in-context\nlearning on small datasets, and heavily relies on dataset specific fine-tuning\nand hyper-parameter tuning, while MotherNet requires no fine-tuning or\nper-dataset hyper-parameters."
                },
                "authors": [
                    {
                        "name": "Andreas Müller"
                    },
                    {
                        "name": "Carlo Curino"
                    },
                    {
                        "name": "Raghu Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Ramakrishnan"
                },
                "author": "Raghu Ramakrishnan",
                "arxiv_comment": "17 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06149v1",
                "updated": "2025-05-09T16:00:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    0,
                    1,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:00:01Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    0,
                    1,
                    4,
                    129,
                    0
                ],
                "title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A\n  Zero-shot and Few-shot Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A\n  Zero-shot and Few-shot Study"
                },
                "summary": "Despite growing interest in automated hate speech detection, most existing\napproaches overlook the linguistic diversity of online content. Multilingual\ninstruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ\noffer promising capabilities across languages, but their effectiveness in\nidentifying hate speech through zero-shot and few-shot prompting remains\nunderexplored. This work evaluates LLM prompting-based detection across eight\nnon-English languages, utilizing several prompting techniques and comparing\nthem to fine-tuned encoder models. We show that while zero-shot and few-shot\nprompting lag behind fine-tuned encoder models on most of the real-world\nevaluation sets, they achieve better generalization on functional tests for\nhate speech detection. Our study also reveals that prompt design plays a\ncritical role, with each language often requiring customized prompting\ntechniques to maximize performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite growing interest in automated hate speech detection, most existing\napproaches overlook the linguistic diversity of online content. Multilingual\ninstruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ\noffer promising capabilities across languages, but their effectiveness in\nidentifying hate speech through zero-shot and few-shot prompting remains\nunderexplored. This work evaluates LLM prompting-based detection across eight\nnon-English languages, utilizing several prompting techniques and comparing\nthem to fine-tuned encoder models. We show that while zero-shot and few-shot\nprompting lag behind fine-tuned encoder models on most of the real-world\nevaluation sets, they achieve better generalization on functional tests for\nhate speech detection. Our study also reveals that prompt design plays a\ncritical role, with each language often requiring customized prompting\ntechniques to maximize performance."
                },
                "authors": [
                    {
                        "name": "Faeze Ghorbanpour"
                    },
                    {
                        "name": "Daryna Dementieva"
                    },
                    {
                        "name": "Alexander Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fraser"
                },
                "author": "Alexander Fraser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24289v2",
                "updated": "2025-05-09T15:58:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    58,
                    9,
                    4,
                    129,
                    0
                ],
                "published": "2025-03-31T16:36:00Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    36,
                    0,
                    0,
                    90,
                    0
                ],
                "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning"
                },
                "summary": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Jiacheng Lin"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Kun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Kun Qian"
                },
                "author": "Kun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05085v2",
                "updated": "2025-05-09T15:51:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    51,
                    8,
                    4,
                    129,
                    0
                ],
                "published": "2024-08-09T14:16:21Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    16,
                    21,
                    4,
                    222,
                    0
                ],
                "title": "On expected signatures and signature cumulants in semimartingale models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On expected signatures and signature cumulants in semimartingale models"
                },
                "summary": "The concept of signatures and expected signatures is vital in data science,\nespecially for sequential data analysis. The signature transform, a Cartan type\ndevelopment, translates paths into high-dimensional feature vectors, capturing\ntheir intrinsic characteristics. Under natural conditions, the expectation of\nthe signature determines the law of the signature, providing a statistical\nsummary of the data distribution. This property facilitates robust modeling and\ninference in machine learning and stochastic processes. Building on previous\nwork by the present authors [Unified signature cumulants and generalized Magnus\nexpansions, FoM Sigma '22] we here revisit the actual computation of expected\nsignatures, in a general semimartingale setting. Several new formulae are\ngiven. A log-transform of (expected) signatures leads to log-signatures\n(signature cumulants), offering a significant reduction in complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of signatures and expected signatures is vital in data science,\nespecially for sequential data analysis. The signature transform, a Cartan type\ndevelopment, translates paths into high-dimensional feature vectors, capturing\ntheir intrinsic characteristics. Under natural conditions, the expectation of\nthe signature determines the law of the signature, providing a statistical\nsummary of the data distribution. This property facilitates robust modeling and\ninference in machine learning and stochastic processes. Building on previous\nwork by the present authors [Unified signature cumulants and generalized Magnus\nexpansions, FoM Sigma '22] we here revisit the actual computation of expected\nsignatures, in a general semimartingale setting. Several new formulae are\ngiven. A log-transform of (expected) signatures leads to log-signatures\n(signature cumulants), offering a significant reduction in complexity."
                },
                "authors": [
                    {
                        "name": "Peter K. Friz"
                    },
                    {
                        "name": "Paul P. Hager"
                    },
                    {
                        "name": "Nikolas Tapia"
                    }
                ],
                "author_detail": {
                    "name": "Nikolas Tapia"
                },
                "author": "Nikolas Tapia",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2102.03345",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60L10, 60L90, 60E10, 60G44, 60G48, 60G51, 60J76",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04707v2",
                "updated": "2025-05-09T15:48:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    48,
                    27,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-07T18:01:01Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    18,
                    1,
                    1,
                    2,
                    127,
                    0
                ],
                "title": "Physical Conditions of the Ionized Superwind in NGC 253 with VLT/MUSE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Conditions of the Ionized Superwind in NGC 253 with VLT/MUSE"
                },
                "summary": "We present an analysis of the H$\\alpha$-emitting ionized gas in the warm\nphase of the NGC 253 outflow using integral field spectroscopy from the Multi\nUnit Spectroscopic Explorer (MUSE). In each spaxel, we decompose H$\\alpha$, [N\nII], and [S II] emission lines into a system of up to 3 Gaussian components,\naccounting for the velocity contributions due to the disk and both intercepted\nwalls of an outflow cone. In the approaching southern lobe of the outflow, we\nfind maximum deprojected outflow velocities down to ~ -500 km/s. Velocity\ngradients of this outflowing gas range from ~ -350 to -550 km/s/kpc with\nincreasing distance from the nucleus. Additionally, [N II]/H$\\alpha$ and [S\nII]/H$\\alpha$ integrated line ratios are suggestive of shocks as the dominant\nionization source throughout the wind. Electron densities, inferred from the [S\nII] doublet, peak at 2100 cm$^{-3}$ near the nucleus and reach $\\lesssim 50\n$cm$^{-3}$ in the wind. Finally, at an uncertainty of 0.3 dex on the inferred\nmass of $4\\times10^{5}$ M$_{\\odot}$, the mass-outflow rate of the\nH$\\alpha$-emitting gas in the southern outflow lobe is ~ 0.4 M$_{\\odot}$/year.\nThis yields a mass-loading factor of $\\eta$ ~ 0.1 and a ~ 2% starburst energy\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an analysis of the H$\\alpha$-emitting ionized gas in the warm\nphase of the NGC 253 outflow using integral field spectroscopy from the Multi\nUnit Spectroscopic Explorer (MUSE). In each spaxel, we decompose H$\\alpha$, [N\nII], and [S II] emission lines into a system of up to 3 Gaussian components,\naccounting for the velocity contributions due to the disk and both intercepted\nwalls of an outflow cone. In the approaching southern lobe of the outflow, we\nfind maximum deprojected outflow velocities down to ~ -500 km/s. Velocity\ngradients of this outflowing gas range from ~ -350 to -550 km/s/kpc with\nincreasing distance from the nucleus. Additionally, [N II]/H$\\alpha$ and [S\nII]/H$\\alpha$ integrated line ratios are suggestive of shocks as the dominant\nionization source throughout the wind. Electron densities, inferred from the [S\nII] doublet, peak at 2100 cm$^{-3}$ near the nucleus and reach $\\lesssim 50\n$cm$^{-3}$ in the wind. Finally, at an uncertainty of 0.3 dex on the inferred\nmass of $4\\times10^{5}$ M$_{\\odot}$, the mass-outflow rate of the\nH$\\alpha$-emitting gas in the southern outflow lobe is ~ 0.4 M$_{\\odot}$/year.\nThis yields a mass-loading factor of $\\eta$ ~ 0.1 and a ~ 2% starburst energy\nefficiency."
                },
                "authors": [
                    {
                        "name": "Serena A. Cronin"
                    },
                    {
                        "name": "Alberto D. Bolatto"
                    },
                    {
                        "name": "Enrico Congiu"
                    },
                    {
                        "name": "Keaton Donaghue"
                    },
                    {
                        "name": "Kathryn Kreckel"
                    },
                    {
                        "name": "Adam K. Leroy"
                    },
                    {
                        "name": "Rebecca C. Levy"
                    },
                    {
                        "name": "Sylvain Veilleux"
                    },
                    {
                        "name": "Fabian Walter"
                    },
                    {
                        "name": "Lenin Nolasco"
                    }
                ],
                "author_detail": {
                    "name": "Lenin Nolasco"
                },
                "author": "Lenin Nolasco",
                "arxiv_comment": "21 pages, 14 figures, accepted to ApJ, fixed coordinates typo in axes\n  of Fig. 5",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12106v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12106v3",
                "updated": "2025-05-09T15:45:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    45,
                    53,
                    4,
                    129,
                    0
                ],
                "published": "2025-01-21T12:56:47Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    56,
                    47,
                    1,
                    21,
                    0
                ],
                "title": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes"
                },
                "summary": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP."
                },
                "authors": [
                    {
                        "name": "Stefan Lenz"
                    },
                    {
                        "name": "Arsenij Ustjanzew"
                    },
                    {
                        "name": "Marco Jeray"
                    },
                    {
                        "name": "Meike Ressing"
                    },
                    {
                        "name": "Torsten Panholzer"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Panholzer"
                },
                "author": "Torsten Panholzer",
                "arxiv_comment": "53 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12106v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12106v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00307v2",
                "updated": "2025-05-09T15:45:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    45,
                    0,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-01T04:59:05Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    4,
                    59,
                    5,
                    3,
                    121,
                    0
                ],
                "title": "Gateformer: Advancing Multivariate Time Series Forecasting through\n  Temporal and Variate-Wise Attention with Gated Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gateformer: Advancing Multivariate Time Series Forecasting through\n  Temporal and Variate-Wise Attention with Gated Representations"
                },
                "summary": "There has been a recent surge of interest in time series modeling using the\nTransformer architecture. However, forecasting multivariate time series with\nTransformer presents a unique challenge as it requires modeling both temporal\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\nmodels have gained popularity for their flexibility in capturing both\nsequential and cross-variate relationships, it is unclear how to best integrate\nthese two sources of information in the context of the Transformer architecture\nwhile optimizing for both performance and efficiency. We re-purpose the\nTransformer architecture to effectively model both cross-time and cross-variate\ndependencies. Our approach begins by embedding each variate independently into\na variate-wise representation that captures its cross-time dynamics, and then\nmodels cross-variate dependencies through attention mechanisms on these learned\nembeddings. Gating operations in both cross-time and cross-variate modeling\nphases regulate information flow, allowing the model to focus on the most\nrelevant features for accurate predictions. Our method achieves\nstate-of-the-art performance across 13 real-world datasets and can be\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\ndelivering performance improvements up to 20.7\\% over original models. Code is\navailable at this repository: https://github.com/nyuolab/Gateformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a recent surge of interest in time series modeling using the\nTransformer architecture. However, forecasting multivariate time series with\nTransformer presents a unique challenge as it requires modeling both temporal\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\nmodels have gained popularity for their flexibility in capturing both\nsequential and cross-variate relationships, it is unclear how to best integrate\nthese two sources of information in the context of the Transformer architecture\nwhile optimizing for both performance and efficiency. We re-purpose the\nTransformer architecture to effectively model both cross-time and cross-variate\ndependencies. Our approach begins by embedding each variate independently into\na variate-wise representation that captures its cross-time dynamics, and then\nmodels cross-variate dependencies through attention mechanisms on these learned\nembeddings. Gating operations in both cross-time and cross-variate modeling\nphases regulate information flow, allowing the model to focus on the most\nrelevant features for accurate predictions. Our method achieves\nstate-of-the-art performance across 13 real-world datasets and can be\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\ndelivering performance improvements up to 20.7\\% over original models. Code is\navailable at this repository: https://github.com/nyuolab/Gateformer."
                },
                "authors": [
                    {
                        "name": "Yu-Hsiang Lan"
                    },
                    {
                        "name": "Eric K. Oermann"
                    }
                ],
                "author_detail": {
                    "name": "Eric K. Oermann"
                },
                "author": "Eric K. Oermann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11928v3",
                "updated": "2025-05-09T15:43:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    43,
                    7,
                    4,
                    129,
                    0
                ],
                "published": "2024-05-20T10:06:33Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    10,
                    6,
                    33,
                    0,
                    141,
                    0
                ],
                "title": "\"Set It Up!\": Functional Object Arrangement with Compositional\n  Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Set It Up!\": Functional Object Arrangement with Compositional\n  Generative Models"
                },
                "summary": "This paper studies the challenge of developing robots capable of\nunderstanding under-specified instructions for creating functional object\narrangements, such as \"set up a dining table for two\"; previous arrangement\napproaches have focused on much more explicit instructions, such as \"put object\nA on the table.\" We introduce a framework, SetItUp, for learning to interpret\nunder-specified instructions. SetItUp takes a small number of training examples\nand a human-crafted program sketch to uncover arrangement rules for specific\nscene types. By leveraging an intermediate graph-like representation of\nabstract spatial relationships among objects, SetItUp decomposes the\narrangement problem into two subproblems: i) learning the arrangement patterns\nfrom limited data and ii) grounding these abstract relationships into object\nposes. SetItUp leverages large language models (LLMs) to propose the abstract\nspatial relationships among objects in novel scenes as the constraints to be\nsatisfied; then, it composes a library of diffusion models associated with\nthese abstract relationships to find object poses that satisfy the constraints.\nWe validate our framework on a dataset comprising study desks, dining tables,\nand coffee tables, with the results showing superior performance in generating\nphysically plausible, functional, and aesthetically pleasing object\narrangements compared to existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the challenge of developing robots capable of\nunderstanding under-specified instructions for creating functional object\narrangements, such as \"set up a dining table for two\"; previous arrangement\napproaches have focused on much more explicit instructions, such as \"put object\nA on the table.\" We introduce a framework, SetItUp, for learning to interpret\nunder-specified instructions. SetItUp takes a small number of training examples\nand a human-crafted program sketch to uncover arrangement rules for specific\nscene types. By leveraging an intermediate graph-like representation of\nabstract spatial relationships among objects, SetItUp decomposes the\narrangement problem into two subproblems: i) learning the arrangement patterns\nfrom limited data and ii) grounding these abstract relationships into object\nposes. SetItUp leverages large language models (LLMs) to propose the abstract\nspatial relationships among objects in novel scenes as the constraints to be\nsatisfied; then, it composes a library of diffusion models associated with\nthese abstract relationships to find object poses that satisfy the constraints.\nWe validate our framework on a dataset comprising study desks, dining tables,\nand coffee tables, with the results showing superior performance in generating\nphysically plausible, functional, and aesthetically pleasing object\narrangements compared to existing models."
                },
                "authors": [
                    {
                        "name": "Yiqing Xu"
                    },
                    {
                        "name": "Jiayuan Mao"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Tomas Lozáno-Pérez"
                    },
                    {
                        "name": "Leslie Pack Kaelbling"
                    },
                    {
                        "name": "David Hsu"
                    }
                ],
                "author_detail": {
                    "name": "David Hsu"
                },
                "author": "David Hsu",
                "arxiv_comment": "10 pages main paper, 21 pages appendix, RSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06131v1",
                "updated": "2025-05-09T15:39:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    39,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T15:39:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    39,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "ELA-ZSON: Efficient Layout-Aware Zero-Shot Object Navigation Agent with\n  Hierarchical Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELA-ZSON: Efficient Layout-Aware Zero-Shot Object Navigation Agent with\n  Hierarchical Planning"
                },
                "summary": "We introduce ELA-ZSON, an efficient layout-aware zero-shot object navigation\n(ZSON) approach designed for complex multi-room indoor environments.\n  By planning hierarchically leveraging a global topologigal map with layout\ninformation and local imperative approach with detailed scene representation\nmemory, ELA-ZSON achieves both efficient and effective navigation.\n  The process is managed by an LLM-powered agent, ensuring seamless effective\nplanning and navigation, without the need for human interaction, complex\nrewards, or costly training.\n  Our experimental results on the MP3D benchmark achieves 85\\% object\nnavigation success rate (SR) and 79\\% success rate weighted by path length\n(SPL) (over 40\\% point improvement in SR and 60\\% improvement in SPL compared\nto exsisting methods). Furthermore, we validate the robustness of our approach\nthrough virtual agent and real-world robotic deployment, showcasing its\ncapability in practical scenarios. See\nhttps://anonymous.4open.science/r/ELA-ZSON-C67E/ for details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ELA-ZSON, an efficient layout-aware zero-shot object navigation\n(ZSON) approach designed for complex multi-room indoor environments.\n  By planning hierarchically leveraging a global topologigal map with layout\ninformation and local imperative approach with detailed scene representation\nmemory, ELA-ZSON achieves both efficient and effective navigation.\n  The process is managed by an LLM-powered agent, ensuring seamless effective\nplanning and navigation, without the need for human interaction, complex\nrewards, or costly training.\n  Our experimental results on the MP3D benchmark achieves 85\\% object\nnavigation success rate (SR) and 79\\% success rate weighted by path length\n(SPL) (over 40\\% point improvement in SR and 60\\% improvement in SPL compared\nto exsisting methods). Furthermore, we validate the robustness of our approach\nthrough virtual agent and real-world robotic deployment, showcasing its\ncapability in practical scenarios. See\nhttps://anonymous.4open.science/r/ELA-ZSON-C67E/ for details."
                },
                "authors": [
                    {
                        "name": "Jiawei Hou"
                    },
                    {
                        "name": "Yuting Xiao"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Taiping Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Taiping Zeng"
                },
                "author": "Taiping Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09667v2",
                "updated": "2025-05-09T15:39:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    39,
                    35,
                    4,
                    129,
                    0
                ],
                "published": "2025-02-12T19:50:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    19,
                    50,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via\n  LLM-based Centroids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via\n  LLM-based Centroids"
                },
                "summary": "We introduce k-LLMmeans, a novel modification of the k-means algorithm for\ntext clustering that leverages LLM-generated summaries as cluster centroids,\ncapturing semantic nuances often missed by purely numerical averages. This\ndesign preserves the core optimization properties of k-means while enhancing\nsemantic interpretability and avoiding the scalability and instability issues\ntypical of modern LLM-based clustering. Unlike existing methods, our approach\ndoes not increase LLM usage with dataset size and produces transparent\nintermediate outputs. We further extend it with a mini-batch variant for\nefficient, real-time clustering of streaming text. Extensive experiments across\nmultiple datasets, embeddings, and LLMs show that k-LLMmeans consistently\noutperforms k-means and other traditional baselines and achieves results\ncomparable to state-of-the-art LLM-based clustering, with a fraction of the LLM\ncalls. Finally, we present a case study on sequential text streams and\nintroduce a new benchmark dataset constructed from StackExchange to evaluate\ntext-stream clustering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce k-LLMmeans, a novel modification of the k-means algorithm for\ntext clustering that leverages LLM-generated summaries as cluster centroids,\ncapturing semantic nuances often missed by purely numerical averages. This\ndesign preserves the core optimization properties of k-means while enhancing\nsemantic interpretability and avoiding the scalability and instability issues\ntypical of modern LLM-based clustering. Unlike existing methods, our approach\ndoes not increase LLM usage with dataset size and produces transparent\nintermediate outputs. We further extend it with a mini-batch variant for\nefficient, real-time clustering of streaming text. Extensive experiments across\nmultiple datasets, embeddings, and LLMs show that k-LLMmeans consistently\noutperforms k-means and other traditional baselines and achieves results\ncomparable to state-of-the-art LLM-based clustering, with a fraction of the LLM\ncalls. Finally, we present a case study on sequential text streams and\nintroduce a new benchmark dataset constructed from StackExchange to evaluate\ntext-stream clustering methods."
                },
                "authors": [
                    {
                        "name": "Jairo Diaz-Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "Jairo Diaz-Rodriguez"
                },
                "author": "Jairo Diaz-Rodriguez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06122v1",
                "updated": "2025-05-09T15:25:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    25,
                    48,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T15:25:48Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    25,
                    48,
                    4,
                    129,
                    0
                ],
                "title": "Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled\n  Systems via Particle Filter Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled\n  Systems via Particle Filter Reinforcement Learning"
                },
                "summary": "This paper addresses the problem of parameter privacy-preserving data sharing\nin coupled systems, where a data provider shares data with a data user but\nwants to protect its sensitive parameters. The shared data affects not only the\ndata user's decision-making but also the data provider's operations through\nsystem interactions. To trade off control performance and privacy, we propose\nan interaction-aware privacy-preserving data sharing approach. Our approach\ngenerates distorted data by minimizing a combination of (i) mutual information,\nquantifying privacy leakage of sensitive parameters, and (ii) the impact of\ndistorted data on the data provider's control performance, considering the\ninteractions between stakeholders. The optimization problem is formulated into\na Bellman equation and solved by a particle filter reinforcement learning\n(RL)-based approach. Compared to existing RL-based methods, our formulation\nsignificantly reduces history dependency and efficiently handles scenarios with\ncontinuous state space. Validated in a mixed-autonomy platoon scenario, our\nmethod effectively protects sensitive driving behavior parameters of\nhuman-driven vehicles (HDVs) against inference attacks while maintaining\nnegligible impact on fuel efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of parameter privacy-preserving data sharing\nin coupled systems, where a data provider shares data with a data user but\nwants to protect its sensitive parameters. The shared data affects not only the\ndata user's decision-making but also the data provider's operations through\nsystem interactions. To trade off control performance and privacy, we propose\nan interaction-aware privacy-preserving data sharing approach. Our approach\ngenerates distorted data by minimizing a combination of (i) mutual information,\nquantifying privacy leakage of sensitive parameters, and (ii) the impact of\ndistorted data on the data provider's control performance, considering the\ninteractions between stakeholders. The optimization problem is formulated into\na Bellman equation and solved by a particle filter reinforcement learning\n(RL)-based approach. Compared to existing RL-based methods, our formulation\nsignificantly reduces history dependency and efficiently handles scenarios with\ncontinuous state space. Validated in a mixed-autonomy platoon scenario, our\nmethod effectively protects sensitive driving behavior parameters of\nhuman-driven vehicles (HDVs) against inference attacks while maintaining\nnegligible impact on fuel efficiency."
                },
                "authors": [
                    {
                        "name": "Haokun Yu"
                    },
                    {
                        "name": "Jingyuan Zhou"
                    },
                    {
                        "name": "Kaidi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Yang"
                },
                "author": "Kaidi Yang",
                "arxiv_comment": "21 pages, 8 figures, accepted at the 7th Annual Learning for Dynamics\n  and Control (L4DC) Conference, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14362v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14362v3",
                "updated": "2025-05-09T15:24:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    24,
                    2,
                    4,
                    129,
                    0
                ],
                "published": "2024-01-25T18:08:53Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    8,
                    53,
                    3,
                    25,
                    0
                ],
                "title": "The Typing Cure: Experiences with Large Language Model Chatbots for\n  Mental Health Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Typing Cure: Experiences with Large Language Model Chatbots for\n  Mental Health Support"
                },
                "summary": "People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care."
                },
                "authors": [
                    {
                        "name": "Inhwa Song"
                    },
                    {
                        "name": "Sachin R. Pendse"
                    },
                    {
                        "name": "Neha Kumar"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "arxiv_comment": "The first two authors contributed equally to this work; typos\n  corrected and post-review revisions incorporated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14362v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14362v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06121v1",
                "updated": "2025-05-09T15:22:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    22,
                    21,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T15:22:21Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    22,
                    21,
                    4,
                    129,
                    0
                ],
                "title": "Constraints to Lorentz violation and ultrahigh-energy electrons in\n  D-foamy space-times",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints to Lorentz violation and ultrahigh-energy electrons in\n  D-foamy space-times"
                },
                "summary": "We entertain the constraints that the absence of vacuum Cherenkov radiation\nof ultrahigh-energy electrons inferred from LHAASO observations of the Crab\nNebula can impose on generic models in which Lorentz symmetry of the particle\nvacuum is violated, as established by some recent studies in\n\\href{https://doi.org/10.1016/j.physletb.2022.137034}{\\emph{Phys. Lett. B} {\\bf\n829} (2022) 137034}; \\href{https://doi.org/10.1016/j.physletb.2022.137536}{{\\bf\n835} (2022) 137536};\n\\href{https://doi.org/10.1103/PhysRevD.108.063006}{\\emph{Phys. Rev. D} {\\bf108}\n(2023) 063006}. We demonstrate in the present paper, that implementing a\nphenomenological approach to the Lorentz violation, the rates of this vacuum\nprocess are substantial such that one is justified in deriving bounds on the\nviolation scales from simple threshold analysis just as these works did. Albeit\nsuch results are likely effective then, they do not apply in the same form\namong scenarios. Specifically, we show that these Cherenkov constraints are\nnaturally evaded in models of space-time foam inspired from~(supercritical)\nstring theory, involving D-branes as space-time defects in a brane-world\nscenario, in which subluminous energy-dependent refractive indices of light\nhave been suggested. We examine here two specific foam situations and find for\nboth cases~(though, for different reasons) the potentiality that charged quanta\nsuch as electrons do \\emph{not} radiate as they pass through the gravitational\nvacuum `medium' despite moving faster than photons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We entertain the constraints that the absence of vacuum Cherenkov radiation\nof ultrahigh-energy electrons inferred from LHAASO observations of the Crab\nNebula can impose on generic models in which Lorentz symmetry of the particle\nvacuum is violated, as established by some recent studies in\n\\href{https://doi.org/10.1016/j.physletb.2022.137034}{\\emph{Phys. Lett. B} {\\bf\n829} (2022) 137034}; \\href{https://doi.org/10.1016/j.physletb.2022.137536}{{\\bf\n835} (2022) 137536};\n\\href{https://doi.org/10.1103/PhysRevD.108.063006}{\\emph{Phys. Rev. D} {\\bf108}\n(2023) 063006}. We demonstrate in the present paper, that implementing a\nphenomenological approach to the Lorentz violation, the rates of this vacuum\nprocess are substantial such that one is justified in deriving bounds on the\nviolation scales from simple threshold analysis just as these works did. Albeit\nsuch results are likely effective then, they do not apply in the same form\namong scenarios. Specifically, we show that these Cherenkov constraints are\nnaturally evaded in models of space-time foam inspired from~(supercritical)\nstring theory, involving D-branes as space-time defects in a brane-world\nscenario, in which subluminous energy-dependent refractive indices of light\nhave been suggested. We examine here two specific foam situations and find for\nboth cases~(though, for different reasons) the potentiality that charged quanta\nsuch as electrons do \\emph{not} radiate as they pass through the gravitational\nvacuum `medium' despite moving faster than photons."
                },
                "authors": [
                    {
                        "name": "Chengyi Li"
                    },
                    {
                        "name": "Bo-Qiang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Bo-Qiang Ma"
                },
                "author": "Bo-Qiang Ma",
                "arxiv_comment": "35 pages, no figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06120v1",
                "updated": "2025-05-09T15:21:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    21,
                    44,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T15:21:44Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    21,
                    44,
                    4,
                    129,
                    0
                ],
                "title": "LLMs Get Lost In Multi-Turn Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Get Lost In Multi-Turn Conversation"
                },
                "summary": "Large Language Models (LLMs) are conversational interfaces. As such, LLMs\nhave the potential to assist their users not only when they can fully specify\nthe task at hand, but also to help them define, explore, and refine what they\nneed through multi-turn conversational exchange. Although analysis of LLM\nconversation logs has confirmed that underspecification occurs frequently in\nuser instructions, LLM evaluation has predominantly focused on the single-turn,\nfully-specified instruction setting. In this work, we perform large-scale\nsimulation experiments to compare LLM performance in single- and multi-turn\nsettings. Our experiments confirm that all the top open- and closed-weight LLMs\nwe test exhibit significantly lower performance in multi-turn conversations\nthan single-turn, with an average drop of 39% across six generation tasks.\nAnalysis of 200,000+ simulated conversations decomposes the performance\ndegradation into two components: a minor loss in aptitude and a significant\nincrease in unreliability. We find that LLMs often make assumptions in early\nturns and prematurely attempt to generate final solutions, on which they overly\nrely. In simpler terms, we discover that *when LLMs take a wrong turn in a\nconversation, they get lost and do not recover*.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are conversational interfaces. As such, LLMs\nhave the potential to assist their users not only when they can fully specify\nthe task at hand, but also to help them define, explore, and refine what they\nneed through multi-turn conversational exchange. Although analysis of LLM\nconversation logs has confirmed that underspecification occurs frequently in\nuser instructions, LLM evaluation has predominantly focused on the single-turn,\nfully-specified instruction setting. In this work, we perform large-scale\nsimulation experiments to compare LLM performance in single- and multi-turn\nsettings. Our experiments confirm that all the top open- and closed-weight LLMs\nwe test exhibit significantly lower performance in multi-turn conversations\nthan single-turn, with an average drop of 39% across six generation tasks.\nAnalysis of 200,000+ simulated conversations decomposes the performance\ndegradation into two components: a minor loss in aptitude and a significant\nincrease in unreliability. We find that LLMs often make assumptions in early\nturns and prematurely attempt to generate final solutions, on which they overly\nrely. In simpler terms, we discover that *when LLMs take a wrong turn in a\nconversation, they get lost and do not recover*."
                },
                "authors": [
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Hiroaki Hayashi"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Jennifer Neville"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Neville"
                },
                "author": "Jennifer Neville",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.17051v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.17051v4",
                "updated": "2025-05-09T15:20:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    20,
                    57,
                    4,
                    129,
                    0
                ],
                "published": "2023-03-29T22:50:05Z",
                "published_parsed": [
                    2023,
                    3,
                    29,
                    22,
                    50,
                    5,
                    2,
                    88,
                    0
                ],
                "title": "Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning\n  for Volumetric Organ Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning\n  for Volumetric Organ Segmentation"
                },
                "summary": "The recent popularity of foundation models and the pre-train-and-adapt\nparadigm, where a large-scale model is transferred to downstream tasks, is\ngaining attention for volumetric medical image segmentation. However, current\ntransfer learning strategies devoted to full fine-tuning for transfer learning\nmay require significant resources and yield sub-optimal results when the\nlabeled data of the target task is scarce. This makes its applicability in real\nclinical settings challenging since these institutions are usually constrained\non data and computational resources to develop proprietary solutions. To\naddress this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a\nnovel and realistic scenario for adapting medical image segmentation foundation\nmodels. This setting considers the key role of both data- and\nparameter-efficiency during adaptation. Building on a foundation model\npre-trained on open-access CT organ segmentation sources, we propose leveraging\nParameter-Efficient Fine-Tuning and black-box Adapters to address such\nchallenges. Furthermore, novel efficient adaptation methodologies are\nintroduced in this work, which include Spatial black-box Adapters that are more\nappropriate for dense prediction tasks and constrained transductive inference,\nleveraging task-specific prior knowledge. Our comprehensive transfer learning\nexperiments confirm the suitability of foundation models in medical image\nsegmentation and unveil the limitations of popular fine-tuning strategies in\nfew-shot scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent popularity of foundation models and the pre-train-and-adapt\nparadigm, where a large-scale model is transferred to downstream tasks, is\ngaining attention for volumetric medical image segmentation. However, current\ntransfer learning strategies devoted to full fine-tuning for transfer learning\nmay require significant resources and yield sub-optimal results when the\nlabeled data of the target task is scarce. This makes its applicability in real\nclinical settings challenging since these institutions are usually constrained\non data and computational resources to develop proprietary solutions. To\naddress this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a\nnovel and realistic scenario for adapting medical image segmentation foundation\nmodels. This setting considers the key role of both data- and\nparameter-efficiency during adaptation. Building on a foundation model\npre-trained on open-access CT organ segmentation sources, we propose leveraging\nParameter-Efficient Fine-Tuning and black-box Adapters to address such\nchallenges. Furthermore, novel efficient adaptation methodologies are\nintroduced in this work, which include Spatial black-box Adapters that are more\nappropriate for dense prediction tasks and constrained transductive inference,\nleveraging task-specific prior knowledge. Our comprehensive transfer learning\nexperiments confirm the suitability of foundation models in medical image\nsegmentation and unveil the limitations of popular fine-tuning strategies in\nfew-shot scenarios."
                },
                "authors": [
                    {
                        "name": "Julio Silva-Rodríguez"
                    },
                    {
                        "name": "Jose Dolz"
                    },
                    {
                        "name": "Ismail Ben Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Ismail Ben Ayed"
                },
                "author": "Ismail Ben Ayed",
                "arxiv_doi": "10.1016/j.media.2025.103596",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.media.2025.103596",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.17051v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.17051v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in Medical Image Analysis. The pre-trained model and\n  adaptation code is available at: https://github.com/jusiro/fewshot-finetuning",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06117v1",
                "updated": "2025-05-09T15:16:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    16,
                    42,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T15:16:42Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    16,
                    42,
                    4,
                    129,
                    0
                ],
                "title": "Photovoltaic Defect Image Generator with Boundary Alignment Smoothing\n  Constraint for Domain Shift Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photovoltaic Defect Image Generator with Boundary Alignment Smoothing\n  Constraint for Domain Shift Mitigation"
                },
                "summary": "Accurate defect detection of photovoltaic (PV) cells is critical for ensuring\nquality and efficiency in intelligent PV manufacturing systems. However, the\nscarcity of rich defect data poses substantial challenges for effective model\ntraining. While existing methods have explored generative models to augment\ndatasets, they often suffer from instability, limited diversity, and domain\nshifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image\nGenerator based on Stable Diffusion (SD). PDIG leverages the strong priors\nlearned from large-scale datasets to enhance generation quality under limited\ndata. Specifically, we introduce a Semantic Concept Embedding (SCE) module that\nincorporates text-conditioned priors to capture the relational concepts between\ndefect types and their appearances. To further enrich the domain distribution,\nwe design a Lightweight Industrial Style Adaptor (LISA), which injects\nindustrial defect characteristics into the SD model through cross-disentangled\nattention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC)\nmodule, enforcing the quality of generated images via positional consistency\nand spatial smoothing alignment. Extensive experiments demonstrate that PDIG\nachieves superior realism and diversity compared to state-of-the-art methods.\nSpecifically, our approach improves Frechet Inception Distance (FID) by 19.16\npoints over the second-best method and significantly enhances the performance\nof downstream defect detection tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate defect detection of photovoltaic (PV) cells is critical for ensuring\nquality and efficiency in intelligent PV manufacturing systems. However, the\nscarcity of rich defect data poses substantial challenges for effective model\ntraining. While existing methods have explored generative models to augment\ndatasets, they often suffer from instability, limited diversity, and domain\nshifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image\nGenerator based on Stable Diffusion (SD). PDIG leverages the strong priors\nlearned from large-scale datasets to enhance generation quality under limited\ndata. Specifically, we introduce a Semantic Concept Embedding (SCE) module that\nincorporates text-conditioned priors to capture the relational concepts between\ndefect types and their appearances. To further enrich the domain distribution,\nwe design a Lightweight Industrial Style Adaptor (LISA), which injects\nindustrial defect characteristics into the SD model through cross-disentangled\nattention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC)\nmodule, enforcing the quality of generated images via positional consistency\nand spatial smoothing alignment. Extensive experiments demonstrate that PDIG\nachieves superior realism and diversity compared to state-of-the-art methods.\nSpecifically, our approach improves Frechet Inception Distance (FID) by 19.16\npoints over the second-best method and significantly enhances the performance\nof downstream defect detection tasks."
                },
                "authors": [
                    {
                        "name": "Dongying Li"
                    },
                    {
                        "name": "Binyi Su"
                    },
                    {
                        "name": "Hua Zhang"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Haiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haiyong Chen"
                },
                "author": "Haiyong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19346v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19346v5",
                "updated": "2025-05-09T15:06:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    6,
                    1,
                    4,
                    129,
                    0
                ],
                "published": "2024-03-28T12:04:28Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    12,
                    4,
                    28,
                    3,
                    88,
                    0
                ],
                "title": "Large Language Models Are Struggle to Cope with Unreasonability in Math\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Struggle to Cope with Unreasonability in Math\n  Problems"
                },
                "summary": "Recent research have demonstrated LLMs' impressive performance in math and\nreasoning. However, the capacity of LLMs to address math problems under\nunconventional conditions, such as internal inconsistencies and flawed\nassumptions, remains largely unexplored. In this paper, we propose a novel\nbenchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to\nrecognize and respond to unreasonability in math problem. The benchmark\nconsists of a carefully curated collection of unreasonable math questions\nacross diverse types. Based on extensive experiments covering 19 LLMs, we\nobserve that even state-of-the-art models such as GPT-4o achieve only limited\nperformance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone\nto overthinking and unstable. We further explore strategies for improving the\nrecognition of unreasonable inputs, shedding light on both the possibility and\nlimitations of LLMs in this challenging setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research have demonstrated LLMs' impressive performance in math and\nreasoning. However, the capacity of LLMs to address math problems under\nunconventional conditions, such as internal inconsistencies and flawed\nassumptions, remains largely unexplored. In this paper, we propose a novel\nbenchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to\nrecognize and respond to unreasonability in math problem. The benchmark\nconsists of a carefully curated collection of unreasonable math questions\nacross diverse types. Based on extensive experiments covering 19 LLMs, we\nobserve that even state-of-the-art models such as GPT-4o achieve only limited\nperformance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone\nto overthinking and unstable. We further explore strategies for improving the\nrecognition of unreasonable inputs, shedding light on both the possibility and\nlimitations of LLMs in this challenging setting."
                },
                "authors": [
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Zihang Yuan"
                    },
                    {
                        "name": "Rui li"
                    },
                    {
                        "name": "Weilin Luo"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "32 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19346v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19346v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06108v1",
                "updated": "2025-05-09T15:05:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    5,
                    57,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T15:05:57Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    5,
                    57,
                    4,
                    129,
                    0
                ],
                "title": "LLMs Outperform Experts on Challenging Biology Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Outperform Experts on Challenging Biology Benchmarks"
                },
                "summary": "This study systematically evaluates 27 frontier Large Language Models on\neight diverse biology benchmarks spanning molecular biology, genetics, cloning,\nvirology, and biosecurity. Models from major AI developers released between\nNovember 2022 and April 2025 were assessed through ten independent runs per\nbenchmark. The findings reveal dramatic improvements in biological\ncapabilities. Top model performance increased more than 4-fold on the\nchallenging text-only subset of the Virology Capabilities Test over the study\nperiod, with the top model now performing twice as well as expert virologists.\nSeveral models now match or exceed expert-level performance on other\nchallenging benchmarks, including LAB-Bench CloningScenarios and the biology\nsubsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not\nsubstantially improve performance over zero-shot evaluation, while extended\nreasoning features in o3-mini and Claude 3.7 Sonnet typically improved\nperformance as predicted by inference scaling. Benchmarks such as PubMedQA and\nthe MMLU and WMDP biology subsets exhibited performance plateaus well below\n100%, suggesting benchmark saturation and errors in the underlying benchmark\ndata. The analysis highlights the need for more sophisticated evaluation\nmethodologies as AI systems continue to advance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study systematically evaluates 27 frontier Large Language Models on\neight diverse biology benchmarks spanning molecular biology, genetics, cloning,\nvirology, and biosecurity. Models from major AI developers released between\nNovember 2022 and April 2025 were assessed through ten independent runs per\nbenchmark. The findings reveal dramatic improvements in biological\ncapabilities. Top model performance increased more than 4-fold on the\nchallenging text-only subset of the Virology Capabilities Test over the study\nperiod, with the top model now performing twice as well as expert virologists.\nSeveral models now match or exceed expert-level performance on other\nchallenging benchmarks, including LAB-Bench CloningScenarios and the biology\nsubsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not\nsubstantially improve performance over zero-shot evaluation, while extended\nreasoning features in o3-mini and Claude 3.7 Sonnet typically improved\nperformance as predicted by inference scaling. Benchmarks such as PubMedQA and\nthe MMLU and WMDP biology subsets exhibited performance plateaus well below\n100%, suggesting benchmark saturation and errors in the underlying benchmark\ndata. The analysis highlights the need for more sophisticated evaluation\nmethodologies as AI systems continue to advance."
                },
                "authors": [
                    {
                        "name": "Lennart Justen"
                    }
                ],
                "author_detail": {
                    "name": "Lennart Justen"
                },
                "author": "Lennart Justen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06096v1",
                "updated": "2025-05-09T14:44:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    44,
                    7,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T14:44:07Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    44,
                    7,
                    4,
                    129,
                    0
                ],
                "title": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog\n  Generation using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog\n  Generation using LLMs"
                },
                "summary": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%."
                },
                "authors": [
                    {
                        "name": "Sam Bush"
                    },
                    {
                        "name": "Matthew DeLorenzo"
                    },
                    {
                        "name": "Phat Tieu"
                    },
                    {
                        "name": "Jeyavijayan Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Jeyavijayan Rajendran"
                },
                "author": "Jeyavijayan Rajendran",
                "arxiv_comment": "Accepted at DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16218v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16218v4",
                "updated": "2025-05-09T14:33:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    33,
                    58,
                    4,
                    129,
                    0
                ],
                "published": "2024-03-24T16:18:27Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    16,
                    18,
                    27,
                    6,
                    84,
                    0
                ],
                "title": "CoverUp: Effective High Coverage Test Generation for Python",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoverUp: Effective High Coverage Test Generation for Python"
                },
                "summary": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 89% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 89% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components."
                },
                "authors": [
                    {
                        "name": "Juan Altmayer Pizzorno"
                    },
                    {
                        "name": "Emery D. Berger"
                    }
                ],
                "author_detail": {
                    "name": "Emery D. Berger"
                },
                "author": "Emery D. Berger",
                "arxiv_doi": "10.1145/3729398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.16218v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16218v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages; to appear at FSE'25",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06085v1",
                "updated": "2025-05-09T14:29:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    29,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T14:29:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    29,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities"
                },
                "summary": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16."
                },
                "authors": [
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Daniele Cesarini"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "Accepted to the Computational Aspects of Deep Learning Workshop at\n  ISC High Performance 2025. To appear in the ISC High Performance 2025\n  Workshop Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20377v2",
                "updated": "2025-05-09T14:28:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    28,
                    25,
                    4,
                    129,
                    0
                ],
                "published": "2025-03-26T09:56:07Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    56,
                    7,
                    2,
                    85,
                    0
                ],
                "title": "UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network\n  Architecture"
                },
                "summary": "As the Large-scale Language Models (LLMs) continue to scale, the requisite\ncomputational power and bandwidth escalate. To address this, we introduce\nUB-Mesh, a novel AI datacenter network architecture designed to enhance\nscalability, performance, cost-efficiency and availability. Unlike traditional\ndatacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a\nhierarchically localized nD-FullMesh network topology. This design fully\nleverages the data locality of LLM training, prioritizing short-range, direct\ninterconnects to minimize data movement distance and reduce switch usage.\n  Although UB-Mesh's nD-FullMesh topology offers several theoretical\nadvantages, its concrete architecture design, physical implementation and\nnetworking system optimization present new challenges. For the actual\nconstruction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is\nbased on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of\nhardware components that serve as the foundational building blocks, including\nspecifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch\n(HRS), NICs and others. These components are interconnected via a novel Unified\nBus (UB) technique, which enables flexible IO bandwidth allocation and hardware\nresource pooling. For networking system optimization, we propose advanced\nrouting mechanism named All-Path-Routing (APR) to efficiently manage data\ntraffic. These optimizations, combined with topology-aware performance\nenhancements and robust reliability measures like 64+1 backup design, result in\n2.04x higher cost-efficiency, 7.2% higher network availability compared to\ntraditional Clos architecture and 95%+ linearity in various LLM training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the Large-scale Language Models (LLMs) continue to scale, the requisite\ncomputational power and bandwidth escalate. To address this, we introduce\nUB-Mesh, a novel AI datacenter network architecture designed to enhance\nscalability, performance, cost-efficiency and availability. Unlike traditional\ndatacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a\nhierarchically localized nD-FullMesh network topology. This design fully\nleverages the data locality of LLM training, prioritizing short-range, direct\ninterconnects to minimize data movement distance and reduce switch usage.\n  Although UB-Mesh's nD-FullMesh topology offers several theoretical\nadvantages, its concrete architecture design, physical implementation and\nnetworking system optimization present new challenges. For the actual\nconstruction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is\nbased on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of\nhardware components that serve as the foundational building blocks, including\nspecifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch\n(HRS), NICs and others. These components are interconnected via a novel Unified\nBus (UB) technique, which enables flexible IO bandwidth allocation and hardware\nresource pooling. For networking system optimization, we propose advanced\nrouting mechanism named All-Path-Routing (APR) to efficiently manage data\ntraffic. These optimizations, combined with topology-aware performance\nenhancements and robust reliability measures like 64+1 backup design, result in\n2.04x higher cost-efficiency, 7.2% higher network availability compared to\ntraditional Clos architecture and 95%+ linearity in various LLM training tasks."
                },
                "authors": [
                    {
                        "name": "Heng Liao"
                    },
                    {
                        "name": "Bingyang Liu"
                    },
                    {
                        "name": "Xianping Chen"
                    },
                    {
                        "name": "Zhigang Guo"
                    },
                    {
                        "name": "Chuanning Cheng"
                    },
                    {
                        "name": "Jianbing Wang"
                    },
                    {
                        "name": "Xiangyu Chen"
                    },
                    {
                        "name": "Peng Dong"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Wenjie Liu"
                    },
                    {
                        "name": "Zhe Zhou"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Yuhang Gai"
                    },
                    {
                        "name": "Cunle Qian"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhongwu Cheng"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yuli Ma"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wenhua Du"
                    },
                    {
                        "name": "Shizhong Xiao"
                    },
                    {
                        "name": "Chungang Li"
                    },
                    {
                        "name": "Yong Qin"
                    },
                    {
                        "name": "Liudong Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Buyun Wang"
                    },
                    {
                        "name": "Pei Wu"
                    },
                    {
                        "name": "Junen Gao"
                    },
                    {
                        "name": "Xiaochu Li"
                    },
                    {
                        "name": "Jian He"
                    },
                    {
                        "name": "Shizhuan Yan"
                    },
                    {
                        "name": "Bill McColl"
                    }
                ],
                "author_detail": {
                    "name": "Bill McColl"
                },
                "author": "Bill McColl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20797v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20797v3",
                "updated": "2025-05-09T14:03:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    3,
                    38,
                    4,
                    129,
                    0
                ],
                "published": "2024-07-30T13:00:37Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    0,
                    37,
                    1,
                    212,
                    0
                ],
                "title": "Witnessing Disorder in Quantum Magnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Witnessing Disorder in Quantum Magnets"
                },
                "summary": "There are no clean samples in nature. Therefore, when we come to discuss the\nentanglement properties of quantum materials, the effects of disorder must be\ntaken into account. This question is of particular interest for high-entangled\nphases, such as quantum spin liquids, which lie outside the Landau paradigm for\nclassifying phases of matter. In this work, we explore what\nexperimentally-accessible measures, in the form of concurrence, residual tangle\nand quantum Fisher information, can teach us about the entanglement in the\npresence of disorder. As a representative example, we consider the\nTomonaga-Luttinger liquids (TLL) and disorder-driven random singlet (RS) phases\nfound in antiferromagnetic quantum spin chains. Using quantum Fisher\ninformation and residual tangle, we demonstrate that both TLL and RS phases\nexhibit multi-partite entanglement. In the case of the RS phase, we attribute\nthis to entanglement localized below a crossover length scale. We further show\nthat the order of disorder average matters in calculating measures like\nconcurrence, and that this can lead to false inferences when interpreting\nexperiment. Nonetheless, correctly interpreted, these witnesses provide useful\ninformation about the effects of disorder. We explore how information about the\ncentral charge of the TLL can be extracted from the low-temperature behavior of\nconcurrence, and conjecture that this analysis can be extended to the effective\ncentral charge of the RS phase. Finally, we establish how RS and TLL phases can\nbe distinguished through the growth of multi-partite entanglement, as witnessed\nby the equal-time structure factor. These results establish that, used\ncarefully, experiments based on entanglement witnesses can provide important\ninformation about quantum spin systems in the presence of disorder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are no clean samples in nature. Therefore, when we come to discuss the\nentanglement properties of quantum materials, the effects of disorder must be\ntaken into account. This question is of particular interest for high-entangled\nphases, such as quantum spin liquids, which lie outside the Landau paradigm for\nclassifying phases of matter. In this work, we explore what\nexperimentally-accessible measures, in the form of concurrence, residual tangle\nand quantum Fisher information, can teach us about the entanglement in the\npresence of disorder. As a representative example, we consider the\nTomonaga-Luttinger liquids (TLL) and disorder-driven random singlet (RS) phases\nfound in antiferromagnetic quantum spin chains. Using quantum Fisher\ninformation and residual tangle, we demonstrate that both TLL and RS phases\nexhibit multi-partite entanglement. In the case of the RS phase, we attribute\nthis to entanglement localized below a crossover length scale. We further show\nthat the order of disorder average matters in calculating measures like\nconcurrence, and that this can lead to false inferences when interpreting\nexperiment. Nonetheless, correctly interpreted, these witnesses provide useful\ninformation about the effects of disorder. We explore how information about the\ncentral charge of the TLL can be extracted from the low-temperature behavior of\nconcurrence, and conjecture that this analysis can be extended to the effective\ncentral charge of the RS phase. Finally, we establish how RS and TLL phases can\nbe distinguished through the growth of multi-partite entanglement, as witnessed\nby the equal-time structure factor. These results establish that, used\ncarefully, experiments based on entanglement witnesses can provide important\ninformation about quantum spin systems in the presence of disorder."
                },
                "authors": [
                    {
                        "name": "Snigdh Sabharwal"
                    },
                    {
                        "name": "Tokuro Shimokawa"
                    },
                    {
                        "name": "Nic Shannon"
                    }
                ],
                "author_detail": {
                    "name": "Nic Shannon"
                },
                "author": "Nic Shannon",
                "arxiv_comment": "Updated the schematic figure and discussion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20797v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20797v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02847v2",
                "updated": "2025-05-09T13:49:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    49,
                    8,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-01T19:06:10Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    6,
                    10,
                    3,
                    121,
                    0
                ],
                "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models"
                },
                "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents."
                },
                "authors": [
                    {
                        "name": "Bang Zhang"
                    },
                    {
                        "name": "Ruotian Ma"
                    },
                    {
                        "name": "Qingxuan Jiang"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Zheng Xie"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Xiaolong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Li"
                },
                "author": "Xiaolong Li",
                "arxiv_comment": "code: https://github.com/Tencent/digitalhuman/tree/main/SAGE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10088v4",
                "updated": "2025-05-09T13:47:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    47,
                    21,
                    4,
                    129,
                    0
                ],
                "published": "2024-02-01T15:15:25Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    15,
                    15,
                    25,
                    3,
                    32,
                    0
                ],
                "title": "Deep hybrid models: infer and plan in a dynamic world",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep hybrid models: infer and plan in a dynamic world"
                },
                "summary": "To determine an optimal plan for complex tasks, one often deals with dynamic\nand hierarchical relationships between several entities. Traditionally, such\nproblems are tackled with optimal control, which relies on the optimization of\ncost functions; instead, a recent biologically-motivated proposal casts\nplanning and control as an inference process. Active inference assumes that\naction and perception are two complementary aspects of life whereby the role of\nthe former is to fulfill the predictions inferred by the latter. Here, we\npresent an active inference approach that exploits discrete and continuous\nprocessing, based on three features: the representation of potential body\nconfigurations in relation to the objects of interest; the use of hierarchical\nrelationships that enable the agent to easily interpret and flexibly expand its\nbody schema for tool use; the definition of potential trajectories related to\nthe agent's intentions, used to infer and plan with dynamic elements at\ndifferent temporal scales. We evaluate this deep hybrid model on a habitual\ntask: reaching a moving object after having picked a moving tool. We show that\nthe model can tackle the presented task under different conditions. This study\nextends past work on planning as inference and advances an alternative\ndirection to optimal control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To determine an optimal plan for complex tasks, one often deals with dynamic\nand hierarchical relationships between several entities. Traditionally, such\nproblems are tackled with optimal control, which relies on the optimization of\ncost functions; instead, a recent biologically-motivated proposal casts\nplanning and control as an inference process. Active inference assumes that\naction and perception are two complementary aspects of life whereby the role of\nthe former is to fulfill the predictions inferred by the latter. Here, we\npresent an active inference approach that exploits discrete and continuous\nprocessing, based on three features: the representation of potential body\nconfigurations in relation to the objects of interest; the use of hierarchical\nrelationships that enable the agent to easily interpret and flexibly expand its\nbody schema for tool use; the definition of potential trajectories related to\nthe agent's intentions, used to infer and plan with dynamic elements at\ndifferent temporal scales. We evaluate this deep hybrid model on a habitual\ntask: reaching a moving object after having picked a moving tool. We show that\nthe model can tackle the presented task under different conditions. This study\nextends past work on planning as inference and advances an alternative\ndirection to optimal control."
                },
                "authors": [
                    {
                        "name": "Matteo Priorelli"
                    },
                    {
                        "name": "Ivilin Peev Stoianov"
                    }
                ],
                "author_detail": {
                    "name": "Ivilin Peev Stoianov"
                },
                "author": "Ivilin Peev Stoianov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06046v1",
                "updated": "2025-05-09T13:42:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    42,
                    59,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T13:42:59Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    42,
                    59,
                    4,
                    129,
                    0
                ],
                "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information"
                },
                "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics."
                },
                "authors": [
                    {
                        "name": "Joshua Harris"
                    },
                    {
                        "name": "Fan Grayson"
                    },
                    {
                        "name": "Felix Feldman"
                    },
                    {
                        "name": "Timothy Laurence"
                    },
                    {
                        "name": "Toby Nonnenmacher"
                    },
                    {
                        "name": "Oliver Higgins"
                    },
                    {
                        "name": "Leo Loman"
                    },
                    {
                        "name": "Selina Patel"
                    },
                    {
                        "name": "Thomas Finnie"
                    },
                    {
                        "name": "Samuel Collins"
                    },
                    {
                        "name": "Michael Borowitz"
                    }
                ],
                "author_detail": {
                    "name": "Michael Borowitz"
                },
                "author": "Michael Borowitz",
                "arxiv_comment": "24 pages, 10 pages main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06044v1",
                "updated": "2025-05-09T13:40:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    40,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T13:40:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    40,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Shadow-Based Framework for Estimating Transition Disk Geometries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow-Based Framework for Estimating Transition Disk Geometries"
                },
                "summary": "Some transition disks host misaligned inner disks with radii of several au.\nUnderstanding the geometric and physical properties of these misaligned disks\nis essential for advancing terrestrial planet formation models. This study\nintroduces a novel method to infer the three-dimensional structures of both\ninner and outer disks by analyzing non-axisymmetric shadows and the horizon in\noptical and infrared scattered light images of the outer disk. This method was\napplied to the HD 100453 system, in which infrared scattered light images from\nthe Very Large Telescope revealed disk shadows. These results indicate that the\ninner disk is misaligned by $\\sim$70$^{\\circ}$ relative to the outer disk,\nwhich is consistent with the results of previous studies. The aspect ratio of\nthe inner disk surface was estimated to be 0.17, which may reflect the surface\nheight of the optically thick dusty component due to vertical lofting by MHD\nwinds or turbulence. In addition, the surface height distribution of the outer\ndisk was characterized, providing novel insights into its vertical structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some transition disks host misaligned inner disks with radii of several au.\nUnderstanding the geometric and physical properties of these misaligned disks\nis essential for advancing terrestrial planet formation models. This study\nintroduces a novel method to infer the three-dimensional structures of both\ninner and outer disks by analyzing non-axisymmetric shadows and the horizon in\noptical and infrared scattered light images of the outer disk. This method was\napplied to the HD 100453 system, in which infrared scattered light images from\nthe Very Large Telescope revealed disk shadows. These results indicate that the\ninner disk is misaligned by $\\sim$70$^{\\circ}$ relative to the outer disk,\nwhich is consistent with the results of previous studies. The aspect ratio of\nthe inner disk surface was estimated to be 0.17, which may reflect the surface\nheight of the optically thick dusty component due to vertical lofting by MHD\nwinds or turbulence. In addition, the surface height distribution of the outer\ndisk was characterized, providing novel insights into its vertical structure."
                },
                "authors": [
                    {
                        "name": "Ryuta Orihara"
                    },
                    {
                        "name": "Munetake Momose"
                    }
                ],
                "author_detail": {
                    "name": "Munetake Momose"
                },
                "author": "Munetake Momose",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02835v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02835v2",
                "updated": "2025-05-09T13:39:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    39,
                    58,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-05T17:59:50Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    59,
                    50,
                    0,
                    125,
                    0
                ],
                "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning"
                },
                "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Tianke Zhang"
                    },
                    {
                        "name": "Changyi Liu"
                    },
                    {
                        "name": "Kaiyu Jiang"
                    },
                    {
                        "name": "Kaibing Chen"
                    },
                    {
                        "name": "Kaiyu Tang"
                    },
                    {
                        "name": "Haojie Ding"
                    },
                    {
                        "name": "Jiankang Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "arxiv_comment": "Home page: https://github.com/yfzhang114/r1_reward",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02835v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02835v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06032v1",
                "updated": "2025-05-09T13:26:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    26,
                    21,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T13:26:21Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    26,
                    21,
                    4,
                    129,
                    0
                ],
                "title": "Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in\n  Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in\n  Text Classification"
                },
                "summary": "Reliance on spurious correlations (shortcuts) has been shown to underlie many\nof the successes of language models. Previous work focused on identifying the\ninput elements that impact prediction. We investigate how shortcuts are\nactually processed within the model's decision-making mechanism. We use actor\nnames in movie reviews as controllable shortcuts with known impact on the\noutcome. We use mechanistic interpretability methods and identify specific\nattention heads that focus on shortcuts. These heads gear the model towards a\nlabel before processing the complete input, effectively making premature\ndecisions that bypass contextual analysis. Based on these findings, we\nintroduce Head-based Token Attribution (HTA), which traces intermediate\ndecisions back to input tokens. We show that HTA is effective in detecting\nshortcuts in LLMs and enables targeted mitigation by selectively deactivating\nshortcut-related attention heads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliance on spurious correlations (shortcuts) has been shown to underlie many\nof the successes of language models. Previous work focused on identifying the\ninput elements that impact prediction. We investigate how shortcuts are\nactually processed within the model's decision-making mechanism. We use actor\nnames in movie reviews as controllable shortcuts with known impact on the\noutcome. We use mechanistic interpretability methods and identify specific\nattention heads that focus on shortcuts. These heads gear the model towards a\nlabel before processing the complete input, effectively making premature\ndecisions that bypass contextual analysis. Based on these findings, we\nintroduce Head-based Token Attribution (HTA), which traces intermediate\ndecisions back to input tokens. We show that HTA is effective in detecting\nshortcuts in LLMs and enables targeted mitigation by selectively deactivating\nshortcut-related attention heads."
                },
                "authors": [
                    {
                        "name": "Leon Eshuijs"
                    },
                    {
                        "name": "Shihan Wang"
                    },
                    {
                        "name": "Antske Fokkens"
                    }
                ],
                "author_detail": {
                    "name": "Antske Fokkens"
                },
                "author": "Antske Fokkens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06027v1",
                "updated": "2025-05-09T13:19:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    19,
                    9,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T13:19:09Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    19,
                    9,
                    4,
                    129,
                    0
                ],
                "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target\n  Self-Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target\n  Self-Distillation"
                },
                "summary": "This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning."
                },
                "authors": [
                    {
                        "name": "Stefan Vasilev"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Seyyed Hadi Hashemi"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "16 pages, 6 figures, 5 tables, under review at ACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06020v1",
                "updated": "2025-05-09T13:08:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    8,
                    27,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T13:08:27Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    8,
                    27,
                    4,
                    129,
                    0
                ],
                "title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for\n  Visual Art Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArtRAG: Retrieval-Augmented Generation with Structured Context for\n  Visual Art Understanding"
                },
                "summary": "Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Ivona Najdenkoska"
                    },
                    {
                        "name": "Hongyi Zhu"
                    },
                    {
                        "name": "Stevan Rudinac"
                    },
                    {
                        "name": "Monika Kackovic"
                    },
                    {
                        "name": "Nachoem Wijnberg"
                    },
                    {
                        "name": "Marcel Worring"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Worring"
                },
                "author": "Marcel Worring",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06004v1",
                "updated": "2025-05-09T12:35:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    12,
                    35,
                    26,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T12:35:26Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    12,
                    35,
                    26,
                    4,
                    129,
                    0
                ],
                "title": "Exploring the Feasibility of Multilingual Grammatical Error Correction\n  with a Single LLM up to 9B parameters: A Comparative Study of 17 Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Feasibility of Multilingual Grammatical Error Correction\n  with a Single LLM up to 9B parameters: A Comparative Study of 17 Models"
                },
                "summary": "Recent language models can successfully solve various language-related tasks,\nand many understand inputs stated in different languages. In this paper, we\nexplore the performance of 17 popular models used to correct grammatical issues\nin texts stated in English, German, Italian, and Swedish when using a single\nmodel to correct texts in all those languages. We analyze the outputs generated\nby these models, focusing on decreasing the number of grammatical errors while\nkeeping the changes small. The conclusions drawn help us understand what\nproblems occur among those models and which models can be recommended for\nmultilingual grammatical error correction tasks. We list six models that\nimprove grammatical correctness in all four languages and show that Gemma 9B is\ncurrently the best performing one for the languages considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent language models can successfully solve various language-related tasks,\nand many understand inputs stated in different languages. In this paper, we\nexplore the performance of 17 popular models used to correct grammatical issues\nin texts stated in English, German, Italian, and Swedish when using a single\nmodel to correct texts in all those languages. We analyze the outputs generated\nby these models, focusing on decreasing the number of grammatical errors while\nkeeping the changes small. The conclusions drawn help us understand what\nproblems occur among those models and which models can be recommended for\nmultilingual grammatical error correction tasks. We list six models that\nimprove grammatical correctness in all four languages and show that Gemma 9B is\ncurrently the best performing one for the languages considered."
                },
                "authors": [
                    {
                        "name": "Dawid Wisniewski"
                    },
                    {
                        "name": "Antoni Solarski"
                    },
                    {
                        "name": "Artur Nowakowski"
                    }
                ],
                "author_detail": {
                    "name": "Artur Nowakowski"
                },
                "author": "Artur Nowakowski",
                "arxiv_comment": "Accepted at MTSummit 2025 (The 20th Machine Translation Summit)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06002v1",
                "updated": "2025-05-09T12:34:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    12,
                    34,
                    10,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T12:34:10Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    12,
                    34,
                    10,
                    4,
                    129,
                    0
                ],
                "title": "Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for\n  Few-shot Action Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for\n  Few-shot Action Recognition"
                },
                "summary": "Large-scale pre-trained models have achieved remarkable success in language\nand image tasks, leading an increasing number of studies to explore the\napplication of pre-trained image models, such as CLIP, in the domain of\nfew-shot action recognition (FSAR). However, current methods generally suffer\nfrom several problems: 1) Direct fine-tuning often undermines the\ngeneralization capability of the pre-trained model; 2) The exploration of\ntask-specific information is insufficient in the visual tasks; 3) The semantic\norder information is typically overlooked during text modeling; 4) Existing\ncross-modal alignment techniques ignore the temporal coupling of multimodal\ninformation. To address these, we propose Task-Adapter++, a parameter-efficient\ndual adaptation method for both image and text encoders. Specifically, to make\nfull use of the variations across different few-shot learning tasks, we design\na task-specific adaptation for the image encoder so that the most\ndiscriminative information can be well noticed during feature extraction.\nFurthermore, we leverage large language models (LLMs) to generate detailed\nsequential sub-action descriptions for each action class, and introduce\nsemantic order adapters into the text encoder to effectively model the\nsequential relationships between these sub-actions. Finally, we develop an\ninnovative fine-grained cross-modal alignment strategy that actively maps\nvisual features to reside in the same temporal stage as semantic descriptions.\nExtensive experiments fully demonstrate the effectiveness and superiority of\nthe proposed method, which achieves state-of-the-art performance on 5\nbenchmarks consistently. The code is open-sourced at\nhttps://github.com/Jaulin-Bage/Task-Adapter-pp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models have achieved remarkable success in language\nand image tasks, leading an increasing number of studies to explore the\napplication of pre-trained image models, such as CLIP, in the domain of\nfew-shot action recognition (FSAR). However, current methods generally suffer\nfrom several problems: 1) Direct fine-tuning often undermines the\ngeneralization capability of the pre-trained model; 2) The exploration of\ntask-specific information is insufficient in the visual tasks; 3) The semantic\norder information is typically overlooked during text modeling; 4) Existing\ncross-modal alignment techniques ignore the temporal coupling of multimodal\ninformation. To address these, we propose Task-Adapter++, a parameter-efficient\ndual adaptation method for both image and text encoders. Specifically, to make\nfull use of the variations across different few-shot learning tasks, we design\na task-specific adaptation for the image encoder so that the most\ndiscriminative information can be well noticed during feature extraction.\nFurthermore, we leverage large language models (LLMs) to generate detailed\nsequential sub-action descriptions for each action class, and introduce\nsemantic order adapters into the text encoder to effectively model the\nsequential relationships between these sub-actions. Finally, we develop an\ninnovative fine-grained cross-modal alignment strategy that actively maps\nvisual features to reside in the same temporal stage as semantic descriptions.\nExtensive experiments fully demonstrate the effectiveness and superiority of\nthe proposed method, which achieves state-of-the-art performance on 5\nbenchmarks consistently. The code is open-sourced at\nhttps://github.com/Jaulin-Bage/Task-Adapter-pp."
                },
                "authors": [
                    {
                        "name": "Congqi Cao"
                    },
                    {
                        "name": "Peiheng Han"
                    },
                    {
                        "name": "Yueran zhang"
                    },
                    {
                        "name": "Yating Yu"
                    },
                    {
                        "name": "Qinyi Lv"
                    },
                    {
                        "name": "Lingtong Min"
                    },
                    {
                        "name": "Yanning zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanning zhang"
                },
                "author": "Yanning zhang",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2408.00249",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05979v1",
                "updated": "2025-05-09T12:06:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    12,
                    6,
                    46,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T12:06:46Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    12,
                    6,
                    46,
                    4,
                    129,
                    0
                ],
                "title": "Mixtures of multivariate linear asymmetric Laplace regressions with\n  multiple asymmetric Laplace covariates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixtures of multivariate linear asymmetric Laplace regressions with\n  multiple asymmetric Laplace covariates"
                },
                "summary": "In response to the challenge of accommodating non-Gaussian behaviour in data,\nthe shifted asymmetric Laplace (SAL) cluster-weighted model (SALCWM) is\nintroduced as a model-based method for jointly clustering responses and random\ncovariates that exhibit skewness. Within each cluster, the multivariate SAL\ndistribution is assumed for both the covariates and the responses given the\ncovariates. To mitigate the effect of possible atypical observations, a\nheavy-tailed extension, the contaminated SALCWM (cSALCWM), is also proposed. In\naddition to the SALCWM parameters, each mixture component has a parameter\ncontrolling the proportion of outliers, one controlling the proportion of\nleverage points, one specifying the degree of outlierness, and another\nspecifying the degree of leverage. The cSALCWM has the added benefit that once\nthe model parameters are estimated and the observations are assigned to\ncomponents, a more refined intra-group classification in typical points, (mild)\noutliers, good leverage, and bad leverage points can be directly obtained. An\nexpectation-conditional maximization algorithm is developed for efficient\nmaximum likelihood parameter estimation under this framework. Theoretical\nidentifiability conditions are established, and empirical results from\nsimulation studies and validation via real-world applications demonstrate that\nthe cSALCWM not only preserves the modelling strengths of the SALCWM but also\nsignificantly enhances outlier detection and overall inference reliability. The\nmethodology proposed in this paper has been implemented in an \\texttt{R}\npackage, which is publicly available at https://github.com/arnootto/ALCWM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the challenge of accommodating non-Gaussian behaviour in data,\nthe shifted asymmetric Laplace (SAL) cluster-weighted model (SALCWM) is\nintroduced as a model-based method for jointly clustering responses and random\ncovariates that exhibit skewness. Within each cluster, the multivariate SAL\ndistribution is assumed for both the covariates and the responses given the\ncovariates. To mitigate the effect of possible atypical observations, a\nheavy-tailed extension, the contaminated SALCWM (cSALCWM), is also proposed. In\naddition to the SALCWM parameters, each mixture component has a parameter\ncontrolling the proportion of outliers, one controlling the proportion of\nleverage points, one specifying the degree of outlierness, and another\nspecifying the degree of leverage. The cSALCWM has the added benefit that once\nthe model parameters are estimated and the observations are assigned to\ncomponents, a more refined intra-group classification in typical points, (mild)\noutliers, good leverage, and bad leverage points can be directly obtained. An\nexpectation-conditional maximization algorithm is developed for efficient\nmaximum likelihood parameter estimation under this framework. Theoretical\nidentifiability conditions are established, and empirical results from\nsimulation studies and validation via real-world applications demonstrate that\nthe cSALCWM not only preserves the modelling strengths of the SALCWM but also\nsignificantly enhances outlier detection and overall inference reliability. The\nmethodology proposed in this paper has been implemented in an \\texttt{R}\npackage, which is publicly available at https://github.com/arnootto/ALCWM."
                },
                "authors": [
                    {
                        "name": "Arnoldus F. Otto"
                    },
                    {
                        "name": "Andriëtte Bekker"
                    },
                    {
                        "name": "Antonio Punzo"
                    },
                    {
                        "name": "Johannes T. Ferreira"
                    },
                    {
                        "name": "Cristina Tortora"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Tortora"
                },
                "arxiv_affiliation": "Department of Mathematics and Statistics, San José State University, California, United States of America",
                "author": "Cristina Tortora",
                "arxiv_comment": "30 pages, 4 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04430v3",
                "updated": "2025-05-09T11:25:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    11,
                    25,
                    57,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-06T10:01:15Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    10,
                    1,
                    15,
                    6,
                    96,
                    0
                ],
                "title": "AGITB: A Signal-Level Benchmark for Evaluating Artificial General\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGITB: A Signal-Level Benchmark for Evaluating Artificial General\n  Intelligence"
                },
                "summary": "Despite remarkable progress in machine learning, current AI systems continue\nto fall short of true human-like intelligence. While Large Language Models\n(LLMs) excel in pattern recognition and response generation, they lack genuine\nunderstanding - an essential hallmark of Artificial General Intelligence (AGI).\nExisting AGI evaluation methods fail to offer a practical, gradual, and\ninformative metric. This paper introduces the Artificial General Intelligence\nTest Bed (AGITB), comprising twelve rigorous tests that form a\nsignal-processing-level foundation for the potential emergence of cognitive\ncapabilities. AGITB evaluates intelligence through a model's ability to predict\nbinary signals across time without relying on symbolic representations or\npretraining. Unlike high-level tests grounded in language or perception, AGITB\nfocuses on core computational invariants reflective of biological intelligence,\nsuch as determinism, sensitivity, and generalisation. The test bed assumes no\nprior bias, operates independently of semantic meaning, and ensures\nunsolvability through brute force or memorization. While humans pass AGITB by\ndesign, no current AI system has met its criteria, making AGITB a compelling\nbenchmark for guiding and recognizing progress toward AGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite remarkable progress in machine learning, current AI systems continue\nto fall short of true human-like intelligence. While Large Language Models\n(LLMs) excel in pattern recognition and response generation, they lack genuine\nunderstanding - an essential hallmark of Artificial General Intelligence (AGI).\nExisting AGI evaluation methods fail to offer a practical, gradual, and\ninformative metric. This paper introduces the Artificial General Intelligence\nTest Bed (AGITB), comprising twelve rigorous tests that form a\nsignal-processing-level foundation for the potential emergence of cognitive\ncapabilities. AGITB evaluates intelligence through a model's ability to predict\nbinary signals across time without relying on symbolic representations or\npretraining. Unlike high-level tests grounded in language or perception, AGITB\nfocuses on core computational invariants reflective of biological intelligence,\nsuch as determinism, sensitivity, and generalisation. The test bed assumes no\nprior bias, operates independently of semantic meaning, and ensures\nunsolvability through brute force or memorization. While humans pass AGITB by\ndesign, no current AI system has met its criteria, making AGITB a compelling\nbenchmark for guiding and recognizing progress toward AGI."
                },
                "authors": [
                    {
                        "name": "Matej Šprogar"
                    }
                ],
                "author_detail": {
                    "name": "Matej Šprogar"
                },
                "author": "Matej Šprogar",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19716v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19716v2",
                "updated": "2025-05-09T11:18:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    11,
                    18,
                    8,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-28T12:09:10Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    9,
                    10,
                    0,
                    118,
                    0
                ],
                "title": "QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds"
                },
                "summary": "Grasping has been a long-standing challenge in facilitating the final\ninterface between a robot and the environment. As environments and tasks become\ncomplicated, the need to embed higher intelligence to infer from the\nsurroundings and act on them has become necessary. Although most methods\nutilize techniques to estimate grasp pose by treating the problem via pure\nsampling-based approaches in the six-degree-of-freedom space or as a learning\nproblem, they usually fail in real-life settings owing to poor generalization\nacross domains. In addition, the time taken to generate the grasp plan and the\nlack of repeatability, owing to sampling inefficiency and the probabilistic\nnature of existing grasp planning approaches, severely limits their application\nin real-world tasks. This paper presents a lightweight analytical approach\ntowards robotic grasp planning, particularly antipodal grasps, with little to\nno sampling in the six-degree-of-freedom space. The proposed grasp planning\nalgorithm is formulated as an optimization problem towards estimating grasp\npoints on the object surface instead of directly estimating the end-effector\npose. To this extent, a soft-region-growing algorithm is presented for\neffective plane segmentation, even in the case of curved surfaces. An\noptimization-based quality metric is then used for the evaluation of grasp\npoints to ensure indirect force closure. The proposed grasp framework is\ncompared with the existing state-of-the-art grasp planning approach, Grasp pose\ndetection (GPD), as a baseline over multiple simulated objects. The\neffectiveness of the proposed approach in comparison to GPD is also evaluated\nin a real-world setting using image and point-cloud data, with the planned\ngrasps being executed using a ROBOTIQ gripper and UR5 manipulator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasping has been a long-standing challenge in facilitating the final\ninterface between a robot and the environment. As environments and tasks become\ncomplicated, the need to embed higher intelligence to infer from the\nsurroundings and act on them has become necessary. Although most methods\nutilize techniques to estimate grasp pose by treating the problem via pure\nsampling-based approaches in the six-degree-of-freedom space or as a learning\nproblem, they usually fail in real-life settings owing to poor generalization\nacross domains. In addition, the time taken to generate the grasp plan and the\nlack of repeatability, owing to sampling inefficiency and the probabilistic\nnature of existing grasp planning approaches, severely limits their application\nin real-world tasks. This paper presents a lightweight analytical approach\ntowards robotic grasp planning, particularly antipodal grasps, with little to\nno sampling in the six-degree-of-freedom space. The proposed grasp planning\nalgorithm is formulated as an optimization problem towards estimating grasp\npoints on the object surface instead of directly estimating the end-effector\npose. To this extent, a soft-region-growing algorithm is presented for\neffective plane segmentation, even in the case of curved surfaces. An\noptimization-based quality metric is then used for the evaluation of grasp\npoints to ensure indirect force closure. The proposed grasp framework is\ncompared with the existing state-of-the-art grasp planning approach, Grasp pose\ndetection (GPD), as a baseline over multiple simulated objects. The\neffectiveness of the proposed approach in comparison to GPD is also evaluated\nin a real-world setting using image and point-cloud data, with the planned\ngrasps being executed using a ROBOTIQ gripper and UR5 manipulator."
                },
                "authors": [
                    {
                        "name": "Navin Sriram Ravie"
                    },
                    {
                        "name": "Keerthi Vasan M"
                    },
                    {
                        "name": "Asokan Thondiyath"
                    },
                    {
                        "name": "Bijo Sebastian"
                    }
                ],
                "author_detail": {
                    "name": "Bijo Sebastian"
                },
                "author": "Bijo Sebastian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19716v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05950v1",
                "updated": "2025-05-09T10:53:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    53,
                    47,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T10:53:47Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    53,
                    47,
                    4,
                    129,
                    0
                ],
                "title": "FloE: On-the-Fly MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FloE: On-the-Fly MoE Inference"
                },
                "summary": "With the widespread adoption of Mixture-of-Experts (MoE) models, there is a\ngrowing demand for efficient inference on memory-constrained devices. While\noffloading expert parameters to CPU memory and loading activated experts on\ndemand has emerged as a potential solution, the large size of activated experts\noverburdens the limited PCIe bandwidth, hindering the effectiveness in\nlatency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly\nMoE inference system on memory-constrained GPUs. FloE is built on the insight\nthat there exists substantial untapped redundancy within sparsely activated\nexperts. It employs various compression techniques on the expert's internal\nparameter matrices to reduce the data movement load, combined with low-cost\nsparse prediction, achieving perceptible inference acceleration in wall-clock\ntime on resource-constrained devices. Empirically, FloE achieves a 9.3x\ncompression of parameters per expert in Mixtral-8x7B; enables deployment on a\nGPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and\ndelivers a 48.7x inference speedup compared to DeepSpeed-MII on a single\nGeForce RTX 3090.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of Mixture-of-Experts (MoE) models, there is a\ngrowing demand for efficient inference on memory-constrained devices. While\noffloading expert parameters to CPU memory and loading activated experts on\ndemand has emerged as a potential solution, the large size of activated experts\noverburdens the limited PCIe bandwidth, hindering the effectiveness in\nlatency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly\nMoE inference system on memory-constrained GPUs. FloE is built on the insight\nthat there exists substantial untapped redundancy within sparsely activated\nexperts. It employs various compression techniques on the expert's internal\nparameter matrices to reduce the data movement load, combined with low-cost\nsparse prediction, achieving perceptible inference acceleration in wall-clock\ntime on resource-constrained devices. Empirically, FloE achieves a 9.3x\ncompression of parameters per expert in Mixtral-8x7B; enables deployment on a\nGPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and\ndelivers a 48.7x inference speedup compared to DeepSpeed-MII on a single\nGeForce RTX 3090."
                },
                "authors": [
                    {
                        "name": "Yuxin Zhou"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Yiping Wang"
                    },
                    {
                        "name": "Zhongle Xie"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Lidan Shou"
                    }
                ],
                "author_detail": {
                    "name": "Lidan Shou"
                },
                "author": "Lidan Shou",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05949v1",
                "updated": "2025-05-09T10:51:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    51,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T10:51:29Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    51,
                    29,
                    4,
                    129,
                    0
                ],
                "title": "NeoQA: Evidence-based Question Answering with Generated News Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeoQA: Evidence-based Question Answering with Generated News Events"
                },
                "summary": "Evaluating Retrieval-Augmented Generation (RAG) in large language models\n(LLMs) is challenging because benchmarks can quickly become stale. Questions\ninitially requiring retrieval may become answerable from pretraining knowledge\nas newer models incorporate more recent information during pretraining, making\nit difficult to distinguish evidence-based reasoning from recall. We introduce\nNeoQA (News Events for Out-of-training Question Answering), a benchmark\ndesigned to address this issue. To construct NeoQA, we generated timelines and\nknowledge bases of fictional news events and entities along with news articles\nand Q\\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring\nthat no prior evidence exists in their training data. We propose our dataset as\na new platform for evaluating evidence-based question answering, as it requires\nLLMs to generate responses exclusively from retrieved evidence and only when\nsufficient evidence is available. NeoQA enables controlled evaluation across\nvarious evidence scenarios, including cases with missing or misleading details.\nOur findings indicate that LLMs struggle to distinguish subtle mismatches\nbetween questions and evidence, and suffer from short-cut reasoning when key\ninformation required to answer a question is missing from the evidence,\nunderscoring key limitations in evidence-based reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Retrieval-Augmented Generation (RAG) in large language models\n(LLMs) is challenging because benchmarks can quickly become stale. Questions\ninitially requiring retrieval may become answerable from pretraining knowledge\nas newer models incorporate more recent information during pretraining, making\nit difficult to distinguish evidence-based reasoning from recall. We introduce\nNeoQA (News Events for Out-of-training Question Answering), a benchmark\ndesigned to address this issue. To construct NeoQA, we generated timelines and\nknowledge bases of fictional news events and entities along with news articles\nand Q\\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring\nthat no prior evidence exists in their training data. We propose our dataset as\na new platform for evaluating evidence-based question answering, as it requires\nLLMs to generate responses exclusively from retrieved evidence and only when\nsufficient evidence is available. NeoQA enables controlled evaluation across\nvarious evidence scenarios, including cases with missing or misleading details.\nOur findings indicate that LLMs struggle to distinguish subtle mismatches\nbetween questions and evidence, and suffer from short-cut reasoning when key\ninformation required to answer a question is missing from the evidence,\nunderscoring key limitations in evidence-based reasoning."
                },
                "authors": [
                    {
                        "name": "Max Glockner"
                    },
                    {
                        "name": "Xiang Jiang"
                    },
                    {
                        "name": "Leonardo F. R. Ribeiro"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Markus Dreyer"
                    }
                ],
                "author_detail": {
                    "name": "Markus Dreyer"
                },
                "author": "Markus Dreyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05946v1",
                "updated": "2025-05-09T10:43:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    43,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T10:43:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    43,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Elastic Weight Consolidation for Full-Parameter Continual Pre-Training\n  of Gemma2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic Weight Consolidation for Full-Parameter Continual Pre-Training\n  of Gemma2"
                },
                "summary": "This technical report describes an experiment on autoregressive pre-training\nof Gemma2 2 billion parameter large language model (LLM) with 10\\% on the\nLithuanian language component of CulturaX from the point of view of continual\nlearning. We apply elastic weight consolidation (EWC) to the full set of the\nmodel's parameters and investigate language understanding benchmarks,\nconsisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande\nsets (both in English and Lithuanian versions), and perplexity benchmarks. We\nempirically demonstrate that EWC regularisation allows us not only to mitigate\ncatastrophic forgetting effects but also that it is potentially beneficial for\nlearning of the new task with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report describes an experiment on autoregressive pre-training\nof Gemma2 2 billion parameter large language model (LLM) with 10\\% on the\nLithuanian language component of CulturaX from the point of view of continual\nlearning. We apply elastic weight consolidation (EWC) to the full set of the\nmodel's parameters and investigate language understanding benchmarks,\nconsisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande\nsets (both in English and Lithuanian versions), and perplexity benchmarks. We\nempirically demonstrate that EWC regularisation allows us not only to mitigate\ncatastrophic forgetting effects but also that it is potentially beneficial for\nlearning of the new task with LLMs."
                },
                "authors": [
                    {
                        "name": "Vytenis Šliogeris"
                    },
                    {
                        "name": "Povilas Daniušis"
                    },
                    {
                        "name": "Artūras Nakvosas"
                    }
                ],
                "author_detail": {
                    "name": "Artūras Nakvosas"
                },
                "author": "Artūras Nakvosas",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05937v1",
                "updated": "2025-05-09T10:27:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    27,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T10:27:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    27,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "MER-CLIP: AU-Guided Vision-Language Alignment for Micro-Expression\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MER-CLIP: AU-Guided Vision-Language Alignment for Micro-Expression\n  Recognition"
                },
                "summary": "As a critical psychological stress response, micro-expressions (MEs) are\nfleeting and subtle facial movements revealing genuine emotions. Automatic ME\nrecognition (MER) holds valuable applications in fields such as criminal\ninvestigation and psychological diagnosis. The Facial Action Coding System\n(FACS) encodes expressions by identifying activations of specific facial action\nunits (AUs), serving as a key reference for ME analysis. However, current MER\nmethods typically limit AU utilization to defining regions of interest (ROIs)\nor relying on specific prior knowledge, often resulting in limited performance\nand poor generalization. To address this, we integrate the CLIP model's\npowerful cross-modal semantic alignment capability into MER and propose a novel\napproach namely MER-CLIP. Specifically, we convert AU labels into detailed\ntextual descriptions of facial muscle movements, guiding fine-grained\nspatiotemporal ME learning by aligning visual dynamics and textual AU-based\nrepresentations. Additionally, we introduce an Emotion Inference Module to\ncapture the nuanced relationships between ME patterns and emotions with\nhigher-level semantic understanding. To mitigate overfitting caused by the\nscarcity of ME data, we put forward LocalStaticFaceMix, an effective data\naugmentation strategy blending facial images to enhance facial diversity while\npreserving critical ME features. Finally, comprehensive experiments on four\nbenchmark ME datasets confirm the superiority of MER-CLIP. Notably, UF1 scores\non CAS(ME)3 reach 0.7832, 0.6544, and 0.4997 for 3-, 4-, and 7-class\nclassification tasks, significantly outperforming previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a critical psychological stress response, micro-expressions (MEs) are\nfleeting and subtle facial movements revealing genuine emotions. Automatic ME\nrecognition (MER) holds valuable applications in fields such as criminal\ninvestigation and psychological diagnosis. The Facial Action Coding System\n(FACS) encodes expressions by identifying activations of specific facial action\nunits (AUs), serving as a key reference for ME analysis. However, current MER\nmethods typically limit AU utilization to defining regions of interest (ROIs)\nor relying on specific prior knowledge, often resulting in limited performance\nand poor generalization. To address this, we integrate the CLIP model's\npowerful cross-modal semantic alignment capability into MER and propose a novel\napproach namely MER-CLIP. Specifically, we convert AU labels into detailed\ntextual descriptions of facial muscle movements, guiding fine-grained\nspatiotemporal ME learning by aligning visual dynamics and textual AU-based\nrepresentations. Additionally, we introduce an Emotion Inference Module to\ncapture the nuanced relationships between ME patterns and emotions with\nhigher-level semantic understanding. To mitigate overfitting caused by the\nscarcity of ME data, we put forward LocalStaticFaceMix, an effective data\naugmentation strategy blending facial images to enhance facial diversity while\npreserving critical ME features. Finally, comprehensive experiments on four\nbenchmark ME datasets confirm the superiority of MER-CLIP. Notably, UF1 scores\non CAS(ME)3 reach 0.7832, 0.6544, and 0.4997 for 3-, 4-, and 7-class\nclassification tasks, significantly outperforming previous methods."
                },
                "authors": [
                    {
                        "name": "Shifeng Liu"
                    },
                    {
                        "name": "Xinglong Mao"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Peiming Li"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16070v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16070v3",
                "updated": "2025-05-09T10:18:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    18,
                    27,
                    4,
                    129,
                    0
                ],
                "published": "2025-01-27T14:19:56Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    19,
                    56,
                    0,
                    27,
                    0
                ],
                "title": "Generalizing Egocentric Temporal Neighborhoods to probe for spatial\n  correlations in temporal networks and infer their topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing Egocentric Temporal Neighborhoods to probe for spatial\n  correlations in temporal networks and infer their topology"
                },
                "summary": "Motifs are thought to be some fundamental components of social face-to-face\ninteraction temporal networks. However, the motifs previously considered are\neither limited to a handful of nodes and edges, or do not include triangles,\nwhich are thought to be of critical relevance to understand the dynamics of\nsocial systems. Thus, we introduce a new class of motifs, that include these\ntriangles, are not limited in their number of nodes or edges, and yet can be\nmined efficiently in any temporal network. Referring to these motifs as the\nedge-centered motifs, we show analytically how they subsume the Egocentric\nTemporal Neighborhoods motifs of the literature. We also confirm in empirical\ndata that the edge-centered motifs bring relevant information with respect to\nthe Egocentric motifs by using a principle of maximum entropy. Then, we show\nhow mining for the edge-centered motifs in a network can be used to probe for\nspatial correlations in the underlying dynamics that have produced that\nnetwork. We deduce an approximate formula for the distribution of the\nedge-centered motifs in empirical networks of social face-to-face interactions.\nIn the last section of this paper, we explore how the statistics of the\nedge-centered motifs can be used to infer the complete topology of the network\nthey were sampled from. This leads to the needs of mathematical development,\nthat we inaugurate here under the name of graph tiling theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motifs are thought to be some fundamental components of social face-to-face\ninteraction temporal networks. However, the motifs previously considered are\neither limited to a handful of nodes and edges, or do not include triangles,\nwhich are thought to be of critical relevance to understand the dynamics of\nsocial systems. Thus, we introduce a new class of motifs, that include these\ntriangles, are not limited in their number of nodes or edges, and yet can be\nmined efficiently in any temporal network. Referring to these motifs as the\nedge-centered motifs, we show analytically how they subsume the Egocentric\nTemporal Neighborhoods motifs of the literature. We also confirm in empirical\ndata that the edge-centered motifs bring relevant information with respect to\nthe Egocentric motifs by using a principle of maximum entropy. Then, we show\nhow mining for the edge-centered motifs in a network can be used to probe for\nspatial correlations in the underlying dynamics that have produced that\nnetwork. We deduce an approximate formula for the distribution of the\nedge-centered motifs in empirical networks of social face-to-face interactions.\nIn the last section of this paper, we explore how the statistics of the\nedge-centered motifs can be used to infer the complete topology of the network\nthey were sampled from. This leads to the needs of mathematical development,\nthat we inaugurate here under the name of graph tiling theory."
                },
                "authors": [
                    {
                        "name": "Didier Le Bail"
                    }
                ],
                "author_detail": {
                    "name": "Didier Le Bail"
                },
                "author": "Didier Le Bail",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16070v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16070v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17410v2",
                "updated": "2025-05-09T10:02:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    2,
                    55,
                    4,
                    129,
                    0
                ],
                "published": "2024-02-27T11:01:58Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    11,
                    1,
                    58,
                    1,
                    58,
                    0
                ],
                "title": "Image space formalism of convolutional neural networks for k-space\n  interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image space formalism of convolutional neural networks for k-space\n  interpolation"
                },
                "summary": "Purpose: Noise resilience in image reconstructions by scan-specific robust\nartificial neural networks for k-space interpolation (RAKI) is linked to\nnonlinear activations in k-space. To gain a deeper understanding of this\nrelationship, an image space formalism of RAKI is introduced for analyzing\nnoise propagation analytically, identifying and characterizing image\nreconstruction features and to describe the role of nonlinear activations in a\nhuman readable manner. Methods: The image space formalism for RAKI inference is\nemployed by expressing nonlinear activations in k-space as element-wise\nmultiplications with activation masks, which transform into convolutions in\nimage space. Jacobians of the de-aliased, coil-combined image relative to the\naliased coil images can be expressed algebraically, and thus, the noise\namplification is quantified analytically (g-factor maps). We analyze the role\nof nonlinearity for noise resilience by controlling the degree of nonlinearity\nin the reconstruction model via the negative slope parameter in leaky ReLU.\nResults: The analytical g-factor maps correspond with those obtained from Monte\nCarlo simulations and from an auto differentiation approach for in vivo brain\nimages. Apparent blurring and contrast loss artifacts are identified as\nimplications of enhanced noise resilience. These residual artifacts can be\ntraded against noise resilience by adjusting the degree of nonlinearity in the\nmodel (Tikhonov-like regularization) in case of limited training data. The\ninspection of image space activations reveals an autocorrelation pattern\nleading to a potential center artifact. Conclusion: The image space formalism\nof RAKI provides the means for analytical quantitative noisepropagation\nanalysis and human-readable visualization of the effects of the nonlinear\nactivation functions in k-space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Noise resilience in image reconstructions by scan-specific robust\nartificial neural networks for k-space interpolation (RAKI) is linked to\nnonlinear activations in k-space. To gain a deeper understanding of this\nrelationship, an image space formalism of RAKI is introduced for analyzing\nnoise propagation analytically, identifying and characterizing image\nreconstruction features and to describe the role of nonlinear activations in a\nhuman readable manner. Methods: The image space formalism for RAKI inference is\nemployed by expressing nonlinear activations in k-space as element-wise\nmultiplications with activation masks, which transform into convolutions in\nimage space. Jacobians of the de-aliased, coil-combined image relative to the\naliased coil images can be expressed algebraically, and thus, the noise\namplification is quantified analytically (g-factor maps). We analyze the role\nof nonlinearity for noise resilience by controlling the degree of nonlinearity\nin the reconstruction model via the negative slope parameter in leaky ReLU.\nResults: The analytical g-factor maps correspond with those obtained from Monte\nCarlo simulations and from an auto differentiation approach for in vivo brain\nimages. Apparent blurring and contrast loss artifacts are identified as\nimplications of enhanced noise resilience. These residual artifacts can be\ntraded against noise resilience by adjusting the degree of nonlinearity in the\nmodel (Tikhonov-like regularization) in case of limited training data. The\ninspection of image space activations reveals an autocorrelation pattern\nleading to a potential center artifact. Conclusion: The image space formalism\nof RAKI provides the means for analytical quantitative noisepropagation\nanalysis and human-readable visualization of the effects of the nonlinear\nactivation functions in k-space."
                },
                "authors": [
                    {
                        "name": "Peter Dawood"
                    },
                    {
                        "name": "Felix Breuer"
                    },
                    {
                        "name": "Istvan Homolya"
                    },
                    {
                        "name": "Maximilian Gram"
                    },
                    {
                        "name": "Peter M. Jakob"
                    },
                    {
                        "name": "Moritz Zaiss"
                    },
                    {
                        "name": "Martin Blaimer"
                    }
                ],
                "author_detail": {
                    "name": "Martin Blaimer"
                },
                "author": "Martin Blaimer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00717v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00717v3",
                "updated": "2025-05-09T09:59:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    59,
                    54,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-17T15:01:01Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    1,
                    1,
                    3,
                    107,
                    0
                ],
                "title": "Thinning-Stable Point Processes as a Model for Spatial Burstiness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinning-Stable Point Processes as a Model for Spatial Burstiness"
                },
                "summary": "In modern telecommunications, spatial burstiness of data traffic\n  poses challenges to traditional Poisson-based models. This paper\n  describes application of thinning-stable point processes,\n  which provide a more appropriate framework for modeling bursty\n  spatial data. We discuss their properties, representation, inference\n  methods, and applications, demonstrating the advantages over\n  classical approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern telecommunications, spatial burstiness of data traffic\n  poses challenges to traditional Poisson-based models. This paper\n  describes application of thinning-stable point processes,\n  which provide a more appropriate framework for modeling bursty\n  spatial data. We discuss their properties, representation, inference\n  methods, and applications, demonstrating the advantages over\n  classical approaches."
                },
                "authors": [
                    {
                        "name": "Sergei Zuyev"
                    }
                ],
                "author_detail": {
                    "name": "Sergei Zuyev"
                },
                "author": "Sergei Zuyev",
                "arxiv_comment": "7 pages, 2 figures. Accepted for WiOpt+25 Conference",
                "arxiv_journal_ref": "WiOpt-2025 Conference paper, 26-29th of May, 2025, Link\\\"oping,\n  Sweden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00717v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00717v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05922v1",
                "updated": "2025-05-09T09:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    54,
                    7,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T09:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    54,
                    7,
                    4,
                    129,
                    0
                ],
                "title": "CAPE: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPE: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy"
                },
                "summary": "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works."
                },
                "authors": [
                    {
                        "name": "Haoqi Wu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yan"
                },
                "author": "Qiang Yan",
                "arxiv_comment": "to be published in ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05920v1",
                "updated": "2025-05-09T09:46:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    46,
                    56,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T09:46:56Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    46,
                    56,
                    4,
                    129,
                    0
                ],
                "title": "Privacy-Preserving Credit Card Approval Using Homomorphic SVM: Toward\n  Secure Inference in FinTech Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Credit Card Approval Using Homomorphic SVM: Toward\n  Secure Inference in FinTech Applications"
                },
                "summary": "The growing use of machine learning in cloud environments raises critical\nconcerns about data security and privacy, especially in finance. Fully\nHomomorphic Encryption (FHE) offers a solution by enabling computations on\nencrypted data, but its high computational cost limits practicality. In this\npaper, we propose PP-FinTech, a privacy-preserving scheme for financial\napplications that employs a CKKS-based encrypted soft-margin SVM, enhanced with\na hybrid kernel for modeling non-linear patterns and an adaptive thresholding\nmechanism for robust encrypted classification. Experiments on the Credit Card\nApproval dataset demonstrate comparable performance to the plaintext models,\nhighlighting PP-FinTech's ability to balance privacy, and efficiency in secure\nfinancial ML systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of machine learning in cloud environments raises critical\nconcerns about data security and privacy, especially in finance. Fully\nHomomorphic Encryption (FHE) offers a solution by enabling computations on\nencrypted data, but its high computational cost limits practicality. In this\npaper, we propose PP-FinTech, a privacy-preserving scheme for financial\napplications that employs a CKKS-based encrypted soft-margin SVM, enhanced with\na hybrid kernel for modeling non-linear patterns and an adaptive thresholding\nmechanism for robust encrypted classification. Experiments on the Credit Card\nApproval dataset demonstrate comparable performance to the plaintext models,\nhighlighting PP-FinTech's ability to balance privacy, and efficiency in secure\nfinancial ML systems."
                },
                "authors": [
                    {
                        "name": "Faneela"
                    },
                    {
                        "name": "Baraq Ghaleb"
                    },
                    {
                        "name": "Jawad Ahmad"
                    },
                    {
                        "name": "William J. Buchanan"
                    },
                    {
                        "name": "Sana Ullah Jan"
                    }
                ],
                "author_detail": {
                    "name": "Sana Ullah Jan"
                },
                "author": "Sana Ullah Jan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11963v2",
                "updated": "2025-05-09T09:23:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    23,
                    22,
                    4,
                    129,
                    0
                ],
                "published": "2024-07-16T17:59:06Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    17,
                    59,
                    6,
                    1,
                    198,
                    0
                ],
                "title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense\n  Context?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense\n  Context?"
                },
                "summary": "The capability of large language models to handle long-context information is\ncrucial across various real-world applications. Existing evaluation methods\noften rely either on real-world long texts, making it difficult to exclude the\ninfluence of models' inherent knowledge, or introduce irrelevant filler content\nto artificially achieve target lengths, reducing assessment effectiveness. To\naddress these limitations, we introduce NeedleBench, a synthetic framework for\nassessing retrieval and reasoning performance in bilingual long-context tasks\nwith adaptive context lengths. NeedleBench systematically embeds key data\npoints at varying depths to rigorously test model capabilities. Tasks are\ncategorized into two scenarios: information-sparse, featuring minimal relevant\ndetails within extensive irrelevant text to simulate simple retrieval tasks;\nand information-dense (the Ancestral Trace Challenge), where relevant\ninformation is continuously distributed throughout the context to simulate\ncomplex reasoning tasks. Our experiments reveal that although recent reasoning\nmodels like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning, they\nstruggle with continuous retrieval and reasoning in information-dense\nscenarios, even at shorter context lengths. We also characterize a phenomenon\ntermed 'under-thinking', where models prematurely conclude reasoning despite\navailable information. NeedleBench thus provides critical insights and targeted\ntools essential for evaluating and improving LLMs' long-context capabilities.\nAll resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capability of large language models to handle long-context information is\ncrucial across various real-world applications. Existing evaluation methods\noften rely either on real-world long texts, making it difficult to exclude the\ninfluence of models' inherent knowledge, or introduce irrelevant filler content\nto artificially achieve target lengths, reducing assessment effectiveness. To\naddress these limitations, we introduce NeedleBench, a synthetic framework for\nassessing retrieval and reasoning performance in bilingual long-context tasks\nwith adaptive context lengths. NeedleBench systematically embeds key data\npoints at varying depths to rigorously test model capabilities. Tasks are\ncategorized into two scenarios: information-sparse, featuring minimal relevant\ndetails within extensive irrelevant text to simulate simple retrieval tasks;\nand information-dense (the Ancestral Trace Challenge), where relevant\ninformation is continuously distributed throughout the context to simulate\ncomplex reasoning tasks. Our experiments reveal that although recent reasoning\nmodels like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning, they\nstruggle with continuous retrieval and reasoning in information-dense\nscenarios, even at shorter context lengths. We also characterize a phenomenon\ntermed 'under-thinking', where models prematurely conclude reasoning despite\navailable information. NeedleBench thus provides critical insights and targeted\ntools essential for evaluating and improving LLMs' long-context capabilities.\nAll resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass."
                },
                "authors": [
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "v2: updated with tested models and Multi-Needle Reasoning\n  implementation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12345v2",
                "updated": "2025-05-09T09:12:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    12,
                    39,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-15T16:58:11Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    58,
                    11,
                    1,
                    105,
                    0
                ],
                "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Urban Science: Scaling Causal Inference with Large Language\n  Models"
                },
                "summary": "Urban causal research is essential for understanding the complex dynamics of\ncities and informing evidence-based policies. However, it is challenged by the\ninefficiency and bias of hypothesis generation, barriers to multimodal data\ncomplexity, and the methodological fragility of causal experimentation. Recent\nadvances in large language models (LLMs) present an opportunity to rethink how\nurban causal analysis is conducted. This Perspective examines current urban\ncausal research by analyzing taxonomies that categorize research topics, data\nsources, and methodological approaches to identify structural gaps. We then\nintroduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four\ndistinct modular agents responsible for hypothesis generation, data\nengineering, experiment design and execution, and results interpretation with\npolicy recommendations. We propose evaluation criteria for rigor and\ntransparency and reflect on implications for human-AI collaboration, equity,\nand accountability. We call for a new research agenda that embraces\nAI-augmented workflows not as replacements for human expertise but as tools to\nbroaden participation, improve reproducibility, and unlock more inclusive forms\nof urban causal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban causal research is essential for understanding the complex dynamics of\ncities and informing evidence-based policies. However, it is challenged by the\ninefficiency and bias of hypothesis generation, barriers to multimodal data\ncomplexity, and the methodological fragility of causal experimentation. Recent\nadvances in large language models (LLMs) present an opportunity to rethink how\nurban causal analysis is conducted. This Perspective examines current urban\ncausal research by analyzing taxonomies that categorize research topics, data\nsources, and methodological approaches to identify structural gaps. We then\nintroduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four\ndistinct modular agents responsible for hypothesis generation, data\nengineering, experiment design and execution, and results interpretation with\npolicy recommendations. We propose evaluation criteria for rigor and\ntransparency and reflect on implications for human-AI collaboration, equity,\nand accountability. We call for a new research agenda that embraces\nAI-augmented workflows not as replacements for human expertise but as tools to\nbroaden participation, improve reproducibility, and unlock more inclusive forms\nof urban causal reasoning."
                },
                "authors": [
                    {
                        "name": "Yutong Xia"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Yunhan Zheng"
                    },
                    {
                        "name": "Yihong Tang"
                    },
                    {
                        "name": "Dingyi Zhuang"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Shenhao Wang"
                    },
                    {
                        "name": "Cathy Wu"
                    },
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Roger Zimmermann"
                    },
                    {
                        "name": "Jinhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jinhua Zhao"
                },
                "author": "Jinhua Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05901v1",
                "updated": "2025-05-09T09:09:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    9,
                    8,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T09:09:08Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    9,
                    8,
                    4,
                    129,
                    0
                ],
                "title": "Examining the Source of Defects from a Mechanical Perspective for 3D\n  Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Source of Defects from a Mechanical Perspective for 3D\n  Anomaly Detection"
                },
                "summary": "In this paper, we go beyond identifying anomalies only in structural terms\nand think about better anomaly detection motivated by anomaly causes. Most\nanomalies are regarded as the result of unpredictable defective forces from\ninternal and external sources, and their opposite forces are sought to correct\nthe anomalies. We introduced a Mechanics Complementary framework for 3D anomaly\ndetection (MC4AD) to generate internal and external Corrective forces for each\npoint. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to\nsimulate various anomalies. Then, we present a Corrective Force Prediction\nNetwork (CFP-Net) with complementary representations for point-level\nrepresentation to simulate the different contributions of internal and external\ncorrective forces. A combined loss was proposed, including a new symmetric loss\nand an overall loss, to constrain the corrective forces properly. As a\nhighlight, we consider 3D anomaly detection in industry more comprehensively,\ncreating a hierarchical quality control strategy based on a three-way decision\nand contributing a dataset named Anomaly-IntraVariance with intraclass variance\nto evaluate the model. On the proposed and existing five datasets, we obtained\nnine state-of-the-art performers with the minimum parameters and the fastest\ninference speed. The source is available at\nhttps://github.com/hzzzzzhappy/MC4AD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we go beyond identifying anomalies only in structural terms\nand think about better anomaly detection motivated by anomaly causes. Most\nanomalies are regarded as the result of unpredictable defective forces from\ninternal and external sources, and their opposite forces are sought to correct\nthe anomalies. We introduced a Mechanics Complementary framework for 3D anomaly\ndetection (MC4AD) to generate internal and external Corrective forces for each\npoint. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to\nsimulate various anomalies. Then, we present a Corrective Force Prediction\nNetwork (CFP-Net) with complementary representations for point-level\nrepresentation to simulate the different contributions of internal and external\ncorrective forces. A combined loss was proposed, including a new symmetric loss\nand an overall loss, to constrain the corrective forces properly. As a\nhighlight, we consider 3D anomaly detection in industry more comprehensively,\ncreating a hierarchical quality control strategy based on a three-way decision\nand contributing a dataset named Anomaly-IntraVariance with intraclass variance\nto evaluate the model. On the proposed and existing five datasets, we obtained\nnine state-of-the-art performers with the minimum parameters and the fastest\ninference speed. The source is available at\nhttps://github.com/hzzzzzhappy/MC4AD"
                },
                "authors": [
                    {
                        "name": "Hanzhe Liang"
                    },
                    {
                        "name": "Aoran Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Can Gao"
                    },
                    {
                        "name": "Jinbao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinbao Wang"
                },
                "author": "Jinbao Wang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00103v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00103v3",
                "updated": "2025-05-09T09:02:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    2,
                    22,
                    4,
                    129,
                    0
                ],
                "published": "2024-07-31T18:25:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    18,
                    25,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation\n  Extraction on an Academic Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation\n  Extraction on an Academic Budget"
                },
                "summary": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in\nNatural Language Processing, serving as critical components in a wide range of\napplications. In this paper, we propose ReLiK, a Retriever-Reader architecture\nfor both EL and RE, where, given an input text, the Retriever module undertakes\nthe identification of candidate entities or relations that could potentially\nappear within the text. Subsequently, the Reader module is tasked to discern\nthe pertinent retrieved entities or relations and establish their alignment\nwith the corresponding textual spans. Notably, we put forward an innovative\ninput representation that incorporates the candidate entities or relations\nalongside the text, making it possible to link entities or extract relations in\na single forward pass and to fully leverage pre-trained language models\ncontextualization capabilities, in contrast with previous\nRetriever-Reader-based methods, which require a forward pass for each\ncandidate. Our formulation of EL and RE achieves state-of-the-art performance\nin both in-domain and out-of-domain benchmarks while using academic budget\ntraining and with up to 40x inference speed compared to competitors. Finally,\nwe show how our architecture can be used seamlessly for Information Extraction\n(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared\nReader that simultaneously extracts entities and relations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in\nNatural Language Processing, serving as critical components in a wide range of\napplications. In this paper, we propose ReLiK, a Retriever-Reader architecture\nfor both EL and RE, where, given an input text, the Retriever module undertakes\nthe identification of candidate entities or relations that could potentially\nappear within the text. Subsequently, the Reader module is tasked to discern\nthe pertinent retrieved entities or relations and establish their alignment\nwith the corresponding textual spans. Notably, we put forward an innovative\ninput representation that incorporates the candidate entities or relations\nalongside the text, making it possible to link entities or extract relations in\na single forward pass and to fully leverage pre-trained language models\ncontextualization capabilities, in contrast with previous\nRetriever-Reader-based methods, which require a forward pass for each\ncandidate. Our formulation of EL and RE achieves state-of-the-art performance\nin both in-domain and out-of-domain benchmarks while using academic budget\ntraining and with up to 40x inference speed compared to competitors. Finally,\nwe show how our architecture can be used seamlessly for Information Extraction\n(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared\nReader that simultaneously extracts entities and relations."
                },
                "authors": [
                    {
                        "name": "Riccardo Orlando"
                    },
                    {
                        "name": "Pere-Lluis Huguet Cabot"
                    },
                    {
                        "name": "Edoardo Barba"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "Findings of the Association for Computational Linguistics ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00103v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00103v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03119v2",
                "updated": "2025-05-09T08:49:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    8,
                    49,
                    26,
                    4,
                    129,
                    0
                ],
                "published": "2025-01-06T16:27:53Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    27,
                    53,
                    0,
                    6,
                    0
                ],
                "title": "From Models to Network Topologies: A Topology Inference Attack in\n  Decentralized Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Models to Network Topologies: A Topology Inference Attack in\n  Decentralized Federated Learning"
                },
                "summary": "Federated Learning (FL) is widely recognized as a privacy-preserving machine\nlearning paradigm due to its model-sharing mechanism that avoids direct data\nexchange. Nevertheless, model training leaves exploitable traces that can be\nused to infer sensitive information. In Decentralized FL (DFL), the topology,\ndefining how participants are connected, plays a crucial role in shaping the\nmodel's privacy, robustness, and convergence. However, the topology introduces\nan unexplored vulnerability: attackers can exploit it to infer participant\nrelationships and launch targeted attacks. This work uncovers the hidden risks\nof DFL topologies by proposing a novel Topology Inference Attack that infers\nthe topology solely from model behavior. A taxonomy of topology inference\nattacks is introduced, categorizing them by the attacker's capabilities and\nknowledge. Practical attack strategies are designed for various scenarios, and\nexperiments are conducted to identify key factors influencing attack success.\nThe results demonstrate that analyzing only the model of each node can\naccurately infer the DFL topology, highlighting a critical privacy risk in DFL\nsystems. These findings offer valuable insights for improving privacy\npreservation in DFL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is widely recognized as a privacy-preserving machine\nlearning paradigm due to its model-sharing mechanism that avoids direct data\nexchange. Nevertheless, model training leaves exploitable traces that can be\nused to infer sensitive information. In Decentralized FL (DFL), the topology,\ndefining how participants are connected, plays a crucial role in shaping the\nmodel's privacy, robustness, and convergence. However, the topology introduces\nan unexplored vulnerability: attackers can exploit it to infer participant\nrelationships and launch targeted attacks. This work uncovers the hidden risks\nof DFL topologies by proposing a novel Topology Inference Attack that infers\nthe topology solely from model behavior. A taxonomy of topology inference\nattacks is introduced, categorizing them by the attacker's capabilities and\nknowledge. Practical attack strategies are designed for various scenarios, and\nexperiments are conducted to identify key factors influencing attack success.\nThe results demonstrate that analyzing only the model of each node can\naccurately infer the DFL topology, highlighting a critical privacy risk in DFL\nsystems. These findings offer valuable insights for improving privacy\npreservation in DFL environments."
                },
                "authors": [
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Yuanzhe Gao"
                    },
                    {
                        "name": "Alberto Huertas Celdran"
                    },
                    {
                        "name": "Gerome Bovet"
                    },
                    {
                        "name": "Burkhard Stiller"
                    }
                ],
                "author_detail": {
                    "name": "Burkhard Stiller"
                },
                "author": "Burkhard Stiller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05875v1",
                "updated": "2025-05-09T08:34:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    8,
                    34,
                    14,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T08:34:14Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    8,
                    34,
                    14,
                    4,
                    129,
                    0
                ],
                "title": "Inferring charge noise source locations from correlations in spin qubits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring charge noise source locations from correlations in spin qubits"
                },
                "summary": "We investigate low-frequency noise in a spin-qubit device made in\nisotopically purified Si/Si-Ge. Observing sizable cross-correlations among\nenergy fluctuations of different qubits, we conclude that these fluctuations\nare dominated by charge noise. At low frequencies, the noise spectra do not fit\nwell a power law; rather, we can recognize a few individual two-level\nfluctuators (TLFs). We demonstrate that the noise cross-correlations allow one\nto get information on the spatial location of such individual TLFs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate low-frequency noise in a spin-qubit device made in\nisotopically purified Si/Si-Ge. Observing sizable cross-correlations among\nenergy fluctuations of different qubits, we conclude that these fluctuations\nare dominated by charge noise. At low frequencies, the noise spectra do not fit\nwell a power law; rather, we can recognize a few individual two-level\nfluctuators (TLFs). We demonstrate that the noise cross-correlations allow one\nto get information on the spatial location of such individual TLFs."
                },
                "authors": [
                    {
                        "name": "Juan S. Rojas-Arias"
                    },
                    {
                        "name": "Akito Noiri"
                    },
                    {
                        "name": "Jun Yoneda"
                    },
                    {
                        "name": "Peter Stano"
                    },
                    {
                        "name": "Takashi Nakajima"
                    },
                    {
                        "name": "Kenta Takeda"
                    },
                    {
                        "name": "Takashi Kobayashi"
                    },
                    {
                        "name": "Giordano Scappucci"
                    },
                    {
                        "name": "Seigo Tarucha"
                    },
                    {
                        "name": "Daniel Loss"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Loss"
                },
                "author": "Daniel Loss",
                "arxiv_comment": "Main text: 6 pages, 4 figures. Supplemental material: 6 pages, 7\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22567v2",
                "updated": "2025-05-09T08:29:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    8,
                    29,
                    30,
                    4,
                    129,
                    0
                ],
                "published": "2025-03-28T16:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    14,
                    6,
                    4,
                    87,
                    0
                ],
                "title": "Benchmarking Ultra-Low-Power $μ$NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Ultra-Low-Power $μ$NPUs"
                },
                "summary": "Efficient on-device neural network (NN) inference has various advantages over\ncloud-based processing, including predictable latency, enhanced privacy,\ngreater reliability, and reduced operating costs for vendors. This has sparked\nthe recent rapid development of microcontroller-scale NN accelerators, often\nreferred to as neural processing units ($\\mu$NPUs), designed specifically for\nultra-low-power applications.\n  In this paper we present the first comparative evaluation of a number of\ncommercially-available $\\mu$NPUs, as well as the first independent benchmarks\nfor several of these platforms. We develop and open-source a model compilation\nframework to enable consistent benchmarking of quantized models across diverse\n$\\mu$NPU hardware. Our benchmark targets end-to-end performance and includes\nmodel inference latency, power consumption, and memory overhead, alongside\nother factors. The resulting analysis uncovers both expected performance trends\nas well as surprising disparities between hardware specifications and actual\nperformance, including $\\mu$NPUs exhibiting unexpected scaling behaviors with\nincreasing model complexity. Our framework provides a foundation for further\nevaluation of $\\mu$NPU platforms alongside valuable insights for both hardware\ndesigners and software developers in this rapidly evolving space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient on-device neural network (NN) inference has various advantages over\ncloud-based processing, including predictable latency, enhanced privacy,\ngreater reliability, and reduced operating costs for vendors. This has sparked\nthe recent rapid development of microcontroller-scale NN accelerators, often\nreferred to as neural processing units ($\\mu$NPUs), designed specifically for\nultra-low-power applications.\n  In this paper we present the first comparative evaluation of a number of\ncommercially-available $\\mu$NPUs, as well as the first independent benchmarks\nfor several of these platforms. We develop and open-source a model compilation\nframework to enable consistent benchmarking of quantized models across diverse\n$\\mu$NPU hardware. Our benchmark targets end-to-end performance and includes\nmodel inference latency, power consumption, and memory overhead, alongside\nother factors. The resulting analysis uncovers both expected performance trends\nas well as surprising disparities between hardware specifications and actual\nperformance, including $\\mu$NPUs exhibiting unexpected scaling behaviors with\nincreasing model complexity. Our framework provides a foundation for further\nevaluation of $\\mu$NPU platforms alongside valuable insights for both hardware\ndesigners and software developers in this rapidly evolving space."
                },
                "authors": [
                    {
                        "name": "Josh Millar"
                    },
                    {
                        "name": "Yushan Huang"
                    },
                    {
                        "name": "Sarab Sethi"
                    },
                    {
                        "name": "Hamed Haddadi"
                    },
                    {
                        "name": "Anil Madhavapeddy"
                    }
                ],
                "author_detail": {
                    "name": "Anil Madhavapeddy"
                },
                "author": "Anil Madhavapeddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05863v1",
                "updated": "2025-05-09T07:57:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    57,
                    10,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T07:57:10Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    57,
                    10,
                    4,
                    129,
                    0
                ],
                "title": "Evolutionary ecology of words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary ecology of words"
                },
                "summary": "We propose a model for the evolutionary ecology of words as one attempt to\nextend evolutionary game theory and agent-based models by utilizing the rich\nlinguistic expressions of Large Language Models (LLMs). Our model enables the\nemergence and evolution of diverse and infinite options for interactions among\nagents. Within the population, each agent possesses a short word (or phrase)\ngenerated by an LLM and moves within a spatial environment. When agents become\nadjacent, the outcome of their interaction is determined by the LLM based on\nthe relationship between their words, with the loser's word being replaced by\nthe winner's. Word mutations, also based on LLM outputs, may occur. We\nconducted preliminary experiments assuming that ``strong animal species\" would\nsurvive. The results showed that from an initial population consisting of\nwell-known species, many species emerged both gradually and in a punctuated\nequilibrium manner. Each trial demonstrated the unique evolution of diverse\npopulations, with one type of large species becoming dominant, such as\nterrestrial animals, marine life, or extinct species, which were ecologically\nspecialized and adapted ones across diverse extreme habitats. We also conducted\na long-term experiment with a large population, demonstrating the emergence and\ncoexistence of diverse species.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a model for the evolutionary ecology of words as one attempt to\nextend evolutionary game theory and agent-based models by utilizing the rich\nlinguistic expressions of Large Language Models (LLMs). Our model enables the\nemergence and evolution of diverse and infinite options for interactions among\nagents. Within the population, each agent possesses a short word (or phrase)\ngenerated by an LLM and moves within a spatial environment. When agents become\nadjacent, the outcome of their interaction is determined by the LLM based on\nthe relationship between their words, with the loser's word being replaced by\nthe winner's. Word mutations, also based on LLM outputs, may occur. We\nconducted preliminary experiments assuming that ``strong animal species\" would\nsurvive. The results showed that from an initial population consisting of\nwell-known species, many species emerged both gradually and in a punctuated\nequilibrium manner. Each trial demonstrated the unique evolution of diverse\npopulations, with one type of large species becoming dominant, such as\nterrestrial animals, marine life, or extinct species, which were ecologically\nspecialized and adapted ones across diverse extreme habitats. We also conducted\na long-term experiment with a large population, demonstrating the emergence and\ncoexistence of diverse species."
                },
                "authors": [
                    {
                        "name": "Reiji Suzuki"
                    },
                    {
                        "name": "Takaya Arita"
                    }
                ],
                "author_detail": {
                    "name": "Takaya Arita"
                },
                "author": "Takaya Arita",
                "arxiv_doi": "10.1109/ALIFE-CIS64968.2025.10979831",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ALIFE-CIS64968.2025.10979831",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 5 figures. Preprint of the paper published in Proceedings of\n  2025 IEEE Symposium on Computational Intelligence in Artificial Life and\n  Cooperative Intelligent Systems (ALIFE-CIS)",
                "arxiv_journal_ref": "Proceedings of 2025 IEEE Symposium on Computational Intelligence\n  in Artificial Life and Cooperative Intelligent Systems (ALIFE-CIS), pp. 1-7\n  (2025)",
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92B20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05440v2",
                "updated": "2025-05-09T07:47:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    47,
                    44,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-08T17:31:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    31,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation"
                },
                "summary": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\n\\textbf{EcoAgent}, an \\textbf{E}dge-\\textbf{C}loud c\\textbf{O}llaborative\nmulti-agent framework for mobile automation. EcoAgent features a closed-loop\ncollaboration among a cloud-based Planning Agent and two edge-based agents: the\nExecution Agent for action execution and the Observation Agent for verifying\noutcomes. The Observation Agent uses a Pre-Understanding Module to compress\nscreen images into concise text, reducing token usage and communication\noverhead. In case of failure, the Planning Agent retrieves screen history\nthrough a Memory Module and replans via a Reflection Module. Experiments on\nAndroidWorld show that EcoAgent achieves task success rates comparable to\ncloud-based mobile agents while significantly reducing MLLM token consumption,\nenabling efficient and practical mobile automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\n\\textbf{EcoAgent}, an \\textbf{E}dge-\\textbf{C}loud c\\textbf{O}llaborative\nmulti-agent framework for mobile automation. EcoAgent features a closed-loop\ncollaboration among a cloud-based Planning Agent and two edge-based agents: the\nExecution Agent for action execution and the Observation Agent for verifying\noutcomes. The Observation Agent uses a Pre-Understanding Module to compress\nscreen images into concise text, reducing token usage and communication\noverhead. In case of failure, the Planning Agent retrieves screen history\nthrough a Memory Module and replans via a Reflection Module. Experiments on\nAndroidWorld show that EcoAgent achieves task success rates comparable to\ncloud-based mobile agents while significantly reducing MLLM token consumption,\nenabling efficient and practical mobile automation."
                },
                "authors": [
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Xavier Hu"
                    },
                    {
                        "name": "Yurun Chen"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05849v1",
                "updated": "2025-05-09T07:40:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    40,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T07:40:17Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    40,
                    17,
                    4,
                    129,
                    0
                ],
                "title": "AgentXploit: End-to-End Redteaming of Black-Box AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentXploit: End-to-End Redteaming of Black-Box AI Agents"
                },
                "summary": "The strong planning and reasoning capabilities of Large Language Models\n(LLMs) have fostered the development of agent-based systems capable of\nleveraging external tools and interacting with increasingly complex\nenvironments. However, these powerful features also introduce a critical\nsecurity risk: indirect prompt injection, a sophisticated attack vector that\ncompromises the core of these agents, the LLM, by manipulating contextual\ninformation rather than direct user prompts. In this work, we propose a generic\nblack-box fuzzing framework, AgentXploit, designed to automatically discover\nand exploit indirect prompt injection vulnerabilities across diverse LLM\nagents. Our approach starts by constructing a high-quality initial seed corpus,\nthen employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)\nto iteratively refine inputs, thereby maximizing the likelihood of uncovering\nagent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo\nand VWA-adv, where it achieves 71% and 70% success rates against agents based\non o3-mini and GPT-4o, respectively, nearly doubling the performance of\nbaseline attacks. Moreover, AgentXploit exhibits strong transferability across\nunseen tasks and internal LLMs, as well as promising results against defenses.\nBeyond benchmark evaluations, we apply our attacks in real-world environments,\nsuccessfully misleading agents to navigate to arbitrary URLs, including\nmalicious sites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong planning and reasoning capabilities of Large Language Models\n(LLMs) have fostered the development of agent-based systems capable of\nleveraging external tools and interacting with increasingly complex\nenvironments. However, these powerful features also introduce a critical\nsecurity risk: indirect prompt injection, a sophisticated attack vector that\ncompromises the core of these agents, the LLM, by manipulating contextual\ninformation rather than direct user prompts. In this work, we propose a generic\nblack-box fuzzing framework, AgentXploit, designed to automatically discover\nand exploit indirect prompt injection vulnerabilities across diverse LLM\nagents. Our approach starts by constructing a high-quality initial seed corpus,\nthen employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)\nto iteratively refine inputs, thereby maximizing the likelihood of uncovering\nagent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo\nand VWA-adv, where it achieves 71% and 70% success rates against agents based\non o3-mini and GPT-4o, respectively, nearly doubling the performance of\nbaseline attacks. Moreover, AgentXploit exhibits strong transferability across\nunseen tasks and internal LLMs, as well as promising results against defenses.\nBeyond benchmark evaluations, we apply our attacks in real-world environments,\nsuccessfully misleading agents to navigate to arbitrary URLs, including\nmalicious sites."
                },
                "authors": [
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Vincent Siu"
                    },
                    {
                        "name": "Zhe Ye"
                    },
                    {
                        "name": "Tianneng Shi"
                    },
                    {
                        "name": "Yuzhou Nie"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Chenguang Wang"
                    },
                    {
                        "name": "Wenbo Guo"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08222v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08222v3",
                "updated": "2025-05-09T07:11:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    11,
                    50,
                    4,
                    129,
                    0
                ],
                "published": "2024-09-26T03:42:05Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    42,
                    5,
                    3,
                    270,
                    0
                ],
                "title": "Variational Source-Channel Coding for Semantic Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Source-Channel Coding for Semantic Communication"
                },
                "summary": "Semantic communication technology emerges as a pivotal bridge connecting AI\nwith classical communication. The current semantic communication systems are\ngenerally modeled as an Auto-Encoder (AE). AE lacks a deep integration of AI\nprinciples with communication strategies due to its inability to effectively\ncapture channel dynamics. This gap makes it difficult to justify the need for\njoint source-channel coding (JSCC) and to explain why performance improves.\nThis paper begins by exploring lossless and lossy communication, highlighting\nthat the inclusion of data distortion distinguishes semantic communication from\nclassical communication. It breaks the conditions for the separation theorem to\nhold and explains why the amount of data transferred by semantic communication\nis less. Therefore, employing JSCC becomes imperative for achieving optimal\nsemantic communication. Moreover, a Variational Source-Channel Coding (VSCC)\nmethod is proposed for constructing semantic communication systems based on\ndata distortion theory, integrating variational inference and channel\ncharacteristics. Using a deep learning network, we develop a semantic\ncommunication system employing the VSCC method and demonstrate its capability\nfor semantic transmission. We also establish semantic communication systems of\nequivalent complexity employing the AE method and the VAE method. Experimental\nresults reveal that the VSCC model offers superior interpretability compared to\nAE model, as it clearly captures the semantic features of the transmitted data,\nrepresented as the variance of latent variables in our experiments. In\naddition, VSCC model exhibits superior semantic transmission capabilities\ncompared to VAE model. At the same level of data distortion evaluated by PSNR,\nVSCC model exhibits stronger human interpretability, which can be partially\nassessed by SSIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic communication technology emerges as a pivotal bridge connecting AI\nwith classical communication. The current semantic communication systems are\ngenerally modeled as an Auto-Encoder (AE). AE lacks a deep integration of AI\nprinciples with communication strategies due to its inability to effectively\ncapture channel dynamics. This gap makes it difficult to justify the need for\njoint source-channel coding (JSCC) and to explain why performance improves.\nThis paper begins by exploring lossless and lossy communication, highlighting\nthat the inclusion of data distortion distinguishes semantic communication from\nclassical communication. It breaks the conditions for the separation theorem to\nhold and explains why the amount of data transferred by semantic communication\nis less. Therefore, employing JSCC becomes imperative for achieving optimal\nsemantic communication. Moreover, a Variational Source-Channel Coding (VSCC)\nmethod is proposed for constructing semantic communication systems based on\ndata distortion theory, integrating variational inference and channel\ncharacteristics. Using a deep learning network, we develop a semantic\ncommunication system employing the VSCC method and demonstrate its capability\nfor semantic transmission. We also establish semantic communication systems of\nequivalent complexity employing the AE method and the VAE method. Experimental\nresults reveal that the VSCC model offers superior interpretability compared to\nAE model, as it clearly captures the semantic features of the transmitted data,\nrepresented as the variance of latent variables in our experiments. In\naddition, VSCC model exhibits superior semantic transmission capabilities\ncompared to VAE model. At the same level of data distortion evaluated by PSNR,\nVSCC model exhibits stronger human interpretability, which can be partially\nassessed by SSIM."
                },
                "authors": [
                    {
                        "name": "Yulong Feng"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Liujun Hu"
                    },
                    {
                        "name": "Guanghui Yu"
                    },
                    {
                        "name": "Xiangyang Duan"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Duan"
                },
                "author": "Xiangyang Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08222v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08222v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05832v1",
                "updated": "2025-05-09T07:00:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    0,
                    11,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T07:00:11Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    0,
                    11,
                    4,
                    129,
                    0
                ],
                "title": "Augmented Body Communicator: Enhancing daily body expression for people\n  with upper limb limitations through LLM and a robotic arm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Body Communicator: Enhancing daily body expression for people\n  with upper limb limitations through LLM and a robotic arm"
                },
                "summary": "Individuals with upper limb movement limitations face challenges in\ninteracting with others. Although robotic arms are currently used primarily for\nfunctional tasks, there is considerable potential to explore ways to enhance\nusers' body language capabilities during social interactions. This paper\nintroduces an Augmented Body Communicator system that integrates robotic arms\nand a large language model. Through the incorporation of kinetic memory,\ndisabled users and their supporters can collaboratively design actions for the\nrobot arm. The LLM system then provides suggestions on the most suitable action\nbased on contextual cues during interactions. The system underwent thorough\nuser testing with six participants who have conditions affecting upper limb\nmobility. Results indicate that the system improves users' ability to express\nthemselves. Based on our findings, we offer recommendations for developing\nrobotic arms that support disabled individuals with body language capabilities\nand functional tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individuals with upper limb movement limitations face challenges in\ninteracting with others. Although robotic arms are currently used primarily for\nfunctional tasks, there is considerable potential to explore ways to enhance\nusers' body language capabilities during social interactions. This paper\nintroduces an Augmented Body Communicator system that integrates robotic arms\nand a large language model. Through the incorporation of kinetic memory,\ndisabled users and their supporters can collaboratively design actions for the\nrobot arm. The LLM system then provides suggestions on the most suitable action\nbased on contextual cues during interactions. The system underwent thorough\nuser testing with six participants who have conditions affecting upper limb\nmobility. Results indicate that the system improves users' ability to express\nthemselves. Based on our findings, we offer recommendations for developing\nrobotic arms that support disabled individuals with body language capabilities\nand functional tasks."
                },
                "authors": [
                    {
                        "name": "Songchen Zhou"
                    },
                    {
                        "name": "Mark Armstrong"
                    },
                    {
                        "name": "Giulia Barbareschi"
                    },
                    {
                        "name": "Toshihiro Ajioka"
                    },
                    {
                        "name": "Zheng Hu"
                    },
                    {
                        "name": "Ryoichi Ando"
                    },
                    {
                        "name": "Kentaro Yoshifuji"
                    },
                    {
                        "name": "Masatane Muto"
                    },
                    {
                        "name": "Kouta Minamizawa"
                    }
                ],
                "author_detail": {
                    "name": "Kouta Minamizawa"
                },
                "author": "Kouta Minamizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01409v2",
                "updated": "2025-05-09T06:59:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    59,
                    53,
                    4,
                    129,
                    0
                ],
                "published": "2025-03-03T11:02:42Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    11,
                    2,
                    42,
                    0,
                    62,
                    0
                ],
                "title": "Magnetar structure in non-linear electrodynamics with mixed\n  poloidal-toroidal fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetar structure in non-linear electrodynamics with mixed\n  poloidal-toroidal fields"
                },
                "summary": "Magnetars have inferred polar field strengths in excess of the Schwinger\nlimit, where non-linear electromagnetic effects can be significant. Their\ninternal fields may be even stronger, suggesting that Maxwellian\ncharacterizations of hydromagnetic structure may require revision. A\ngeneralized Grad-Shafranov equation, describing static and axisymmetric fluid\nstars with mixed poloidal-toroidal fields, is introduced and subsequently\nsolved in a perturbative scheme to calculate quadrupolar deformations. In the\nBorn-Infeld theory, we show that the toroidal field has a maximum strength set\nby the scale parameter, $b$, implying an upper limit to the stellar\nprolateness, $|\\epsilon_{\\rm max}| \\sim 10^{-5} \\left(b/10^{16}\\text{\nG}\\right)^2$, that is independent of field specifics. Observations of magnetar\nphenomena that are interpreted as evidence for ellipticity, such as precession,\ncan thus implicitly constrain post-Maxwellian parameters in a way that\ncomplements terrestrial experiments. Toroidal ceilings also have implications\nfor dynamo theory and gravitational waves, which we revisit together with field\nevolution in crusts abiding by beyond-Maxwell physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetars have inferred polar field strengths in excess of the Schwinger\nlimit, where non-linear electromagnetic effects can be significant. Their\ninternal fields may be even stronger, suggesting that Maxwellian\ncharacterizations of hydromagnetic structure may require revision. A\ngeneralized Grad-Shafranov equation, describing static and axisymmetric fluid\nstars with mixed poloidal-toroidal fields, is introduced and subsequently\nsolved in a perturbative scheme to calculate quadrupolar deformations. In the\nBorn-Infeld theory, we show that the toroidal field has a maximum strength set\nby the scale parameter, $b$, implying an upper limit to the stellar\nprolateness, $|\\epsilon_{\\rm max}| \\sim 10^{-5} \\left(b/10^{16}\\text{\nG}\\right)^2$, that is independent of field specifics. Observations of magnetar\nphenomena that are interpreted as evidence for ellipticity, such as precession,\ncan thus implicitly constrain post-Maxwellian parameters in a way that\ncomplements terrestrial experiments. Toroidal ceilings also have implications\nfor dynamo theory and gravitational waves, which we revisit together with field\nevolution in crusts abiding by beyond-Maxwell physics."
                },
                "authors": [
                    {
                        "name": "Arthur G. Suvorov"
                    },
                    {
                        "name": "José A. Pons"
                    }
                ],
                "author_detail": {
                    "name": "José A. Pons"
                },
                "author": "José A. Pons",
                "arxiv_doi": "10.1093/mnras/staf704",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf704",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures, 1 table. Matches published version",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16386v2",
                "updated": "2025-05-09T06:53:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    53,
                    36,
                    4,
                    129,
                    0
                ],
                "published": "2024-10-21T18:01:11Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    18,
                    1,
                    11,
                    0,
                    295,
                    0
                ],
                "title": "LEGO-Learn: Label-Efficient Graph Open-Set Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEGO-Learn: Label-Efficient Graph Open-Set Learning"
                },
                "summary": "How can we train graph-based models to recognize unseen classes while keeping\nlabeling costs low? Graph open-set learning (GOL) and out-of-distribution (OOD)\ndetection aim to address this challenge by training models that can accurately\nclassify known, in-distribution (ID) classes while identifying and handling\npreviously unseen classes during inference. It is critical for high-stakes,\nreal-world applications where models frequently encounter unexpected data,\nincluding finance, security, and healthcare. However, current GOL methods\nassume access to many labeled ID samples, which is unrealistic for large-scale\ngraphs due to high annotation costs. In this paper, we propose LEGO-Learn\n(Label-Efficient Graph Open-set Learning), a novel framework that tackles\nopen-set node classification on graphs within a given label budget by selecting\nthe most informative ID nodes. LEGO-Learn employs a GNN-based filter to\nidentify and exclude potential OOD nodes and then select highly informative ID\nnodes for labeling using the K-Medoids algorithm. To prevent the filter from\ndiscarding valuable ID examples, we introduce a classifier that differentiates\nbetween the C known ID classes and an additional class representing OOD nodes\n(hence, a C+1 classifier). This classifier uses a weighted cross-entropy loss\nto balance the removal of OOD nodes while retaining informative ID nodes.\nExperimental results on four real-world datasets demonstrate that LEGO-Learn\nsignificantly outperforms leading methods, with up to a 6.62% improvement in ID\nclassification accuracy and a 7.49% increase in AUROC for OOD detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we train graph-based models to recognize unseen classes while keeping\nlabeling costs low? Graph open-set learning (GOL) and out-of-distribution (OOD)\ndetection aim to address this challenge by training models that can accurately\nclassify known, in-distribution (ID) classes while identifying and handling\npreviously unseen classes during inference. It is critical for high-stakes,\nreal-world applications where models frequently encounter unexpected data,\nincluding finance, security, and healthcare. However, current GOL methods\nassume access to many labeled ID samples, which is unrealistic for large-scale\ngraphs due to high annotation costs. In this paper, we propose LEGO-Learn\n(Label-Efficient Graph Open-set Learning), a novel framework that tackles\nopen-set node classification on graphs within a given label budget by selecting\nthe most informative ID nodes. LEGO-Learn employs a GNN-based filter to\nidentify and exclude potential OOD nodes and then select highly informative ID\nnodes for labeling using the K-Medoids algorithm. To prevent the filter from\ndiscarding valuable ID examples, we introduce a classifier that differentiates\nbetween the C known ID classes and an additional class representing OOD nodes\n(hence, a C+1 classifier). This classifier uses a weighted cross-entropy loss\nto balance the removal of OOD nodes while retaining informative ID nodes.\nExperimental results on four real-world datasets demonstrate that LEGO-Learn\nsignificantly outperforms leading methods, with up to a 6.62% improvement in ID\nclassification accuracy and a 7.49% increase in AUROC for OOD detection."
                },
                "authors": [
                    {
                        "name": "Haoyan Xu"
                    },
                    {
                        "name": "Kay Liu"
                    },
                    {
                        "name": "Zhengtao Yao"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Mengyuan Li"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05821v1",
                "updated": "2025-05-09T06:47:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    47,
                    16,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T06:47:16Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    47,
                    16,
                    4,
                    129,
                    0
                ],
                "title": "EMU: Cross-correlating EMU Pilot Survey 1 with Dark Energy Survey to\n  constrain the radio galaxy redshift distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMU: Cross-correlating EMU Pilot Survey 1 with Dark Energy Survey to\n  constrain the radio galaxy redshift distribution"
                },
                "summary": "Radio continuum galaxy surveys can provide a relatively fast map of the\nprojected distribution of structure in the Universe, at the cost of lacking\ninformation about the radial distribution. We can use these surveys to learn\nabout the growth of structure and the fundamental physics of the Universe, but\ndoing so requires extra information to be provided in the modelling of the\nredshift distribution, $dN/dz$. In this work, we show how the cross-correlation\nof the two dimensional radio continuum map with another galaxy map (in this\ncase a photometric optical extragalactic survey), with a known redshift\ndistribution, can be used to determine the redshift distribution through\nstatistical inference. We use data from the Evolutionary Map of the Universe\n(EMU) Pilot Survey 1 and cross-correlate it with optical data from the Dark\nEnergy Survey to fit the parameters of our $dN/dz$ model. We show that the\nrecovered distribution fits the data much better than current simulated $dN/dz$\nmodels available in the literature, and peaks at a much higher redshift. These\nresults will have significance for future cosmological analyses with\nlarge-scale radio continuum surveys such as the full EMU, or with the SKAO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio continuum galaxy surveys can provide a relatively fast map of the\nprojected distribution of structure in the Universe, at the cost of lacking\ninformation about the radial distribution. We can use these surveys to learn\nabout the growth of structure and the fundamental physics of the Universe, but\ndoing so requires extra information to be provided in the modelling of the\nredshift distribution, $dN/dz$. In this work, we show how the cross-correlation\nof the two dimensional radio continuum map with another galaxy map (in this\ncase a photometric optical extragalactic survey), with a known redshift\ndistribution, can be used to determine the redshift distribution through\nstatistical inference. We use data from the Evolutionary Map of the Universe\n(EMU) Pilot Survey 1 and cross-correlate it with optical data from the Dark\nEnergy Survey to fit the parameters of our $dN/dz$ model. We show that the\nrecovered distribution fits the data much better than current simulated $dN/dz$\nmodels available in the literature, and peaks at a much higher redshift. These\nresults will have significance for future cosmological analyses with\nlarge-scale radio continuum surveys such as the full EMU, or with the SKAO."
                },
                "authors": [
                    {
                        "name": "Chandra Shekhar Saraf"
                    },
                    {
                        "name": "David Parkinson"
                    },
                    {
                        "name": "Jacobo Asorey"
                    },
                    {
                        "name": "Catherine L. Hale"
                    },
                    {
                        "name": "Benedict Bahr-Kalus"
                    },
                    {
                        "name": "Maciej Bilicki"
                    },
                    {
                        "name": "Stefano Camera"
                    },
                    {
                        "name": "Andrew M. Hopkins"
                    },
                    {
                        "name": "Konstantinos Tanidis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Tanidis"
                },
                "author": "Konstantinos Tanidis",
                "arxiv_comment": "15 pages, 10 figures, 4 tables. Prepared for submission to PASA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04852v2",
                "updated": "2025-05-09T06:32:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    32,
                    8,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-07T23:30:27Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    23,
                    30,
                    27,
                    2,
                    127,
                    0
                ],
                "title": "PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer\n  Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer\n  Rust"
                },
                "summary": "There has been a growing interest in translating C code to Rust due to Rust's\nrobust memory and thread safety guarantees. Tools such as C2RUST enable\nsyntax-guided transpilation from C to semantically equivalent Rust code.\nHowever, the resulting Rust programs often rely heavily on unsafe\nconstructs--particularly raw pointers--which undermines Rust's safety\nguarantees. This paper aims to improve the memory safety of Rust programs\ngenerated by C2RUST by eliminating raw pointers. Specifically, we propose a\npeephole raw pointer rewriting technique that lifts raw pointers in individual\nfunctions to appropriate Rust data structures. Technically, PR2 employs\ndecision-tree-based prompting to guide the pointer lifting process.\nAdditionally, it leverages code change analysis to guide the repair of errors\nintroduced during rewriting, effectively addressing errors encountered during\ncompilation and test case execution. We implement PR2 as a prototype and\nevaluate it using gpt-4o-mini on 28 real-world C projects. The results show\nthat PR2 successfully eliminates 13.22% of local raw pointers across these\nprojects, significantly enhancing the safety of the translated Rust code. On\naverage, PR2 completes the transformation of a project in 5.44 hours, at an\naverage cost of $1.46.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a growing interest in translating C code to Rust due to Rust's\nrobust memory and thread safety guarantees. Tools such as C2RUST enable\nsyntax-guided transpilation from C to semantically equivalent Rust code.\nHowever, the resulting Rust programs often rely heavily on unsafe\nconstructs--particularly raw pointers--which undermines Rust's safety\nguarantees. This paper aims to improve the memory safety of Rust programs\ngenerated by C2RUST by eliminating raw pointers. Specifically, we propose a\npeephole raw pointer rewriting technique that lifts raw pointers in individual\nfunctions to appropriate Rust data structures. Technically, PR2 employs\ndecision-tree-based prompting to guide the pointer lifting process.\nAdditionally, it leverages code change analysis to guide the repair of errors\nintroduced during rewriting, effectively addressing errors encountered during\ncompilation and test case execution. We implement PR2 as a prototype and\nevaluate it using gpt-4o-mini on 28 real-world C projects. The results show\nthat PR2 successfully eliminates 13.22% of local raw pointers across these\nprojects, significantly enhancing the safety of the translated Rust code. On\naverage, PR2 completes the transformation of a project in 5.44 hours, at an\naverage cost of $1.46."
                },
                "authors": [
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chengpeng Wang"
                    },
                    {
                        "name": "Pengxiang Huang"
                    },
                    {
                        "name": "Xuwei Liu"
                    },
                    {
                        "name": "Mingwei Zheng"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05698v2",
                "updated": "2025-05-09T06:15:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    15,
                    28,
                    4,
                    129,
                    0
                ],
                "published": "2023-12-09T22:31:20Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    22,
                    31,
                    20,
                    5,
                    343,
                    0
                ],
                "title": "Unsupervised Multi-modal Feature Alignment for Time Series\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Multi-modal Feature Alignment for Time Series\n  Representation Learning"
                },
                "summary": "In recent times, the field of unsupervised representation learning (URL) for\ntime series data has garnered significant interest due to its remarkable\nadaptability across diverse downstream applications. Unsupervised learning\ngoals differ from downstream tasks, making it tricky to ensure downstream task\nutility by focusing only on temporal feature characterization. Researchers have\nproposed multiple transformations to extract discriminative patterns implied in\ninformative time series, trying to fill the gap. Despite the introduction of a\nvariety of feature engineering techniques, e.g. spectral domain, wavelet\ntransformed features, features in image form and symbolic features etc. the\nutilization of intricate feature fusion methods and dependence on heterogeneous\nfeatures during inference hampers the scalability of the solutions. To address\nthis, our study introduces an innovative approach that focuses on aligning and\nbinding time series representations encoded from different modalities, inspired\nby spectral graph theory, thereby guiding the neural encoder to uncover latent\npattern associations among these multi-modal features. In contrast to\nconventional methods that fuse features from multiple modalities, our proposed\napproach simplifies the neural architecture by retaining a single time series\nencoder, consequently leading to preserved scalability. We further demonstrate\nand prove mechanisms for the encoder to maintain better inductive bias. In our\nexperimental evaluation, we validated the proposed method on a diverse set of\ntime series datasets from various domains. Our approach outperforms existing\nstate-of-the-art URL methods across diverse downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent times, the field of unsupervised representation learning (URL) for\ntime series data has garnered significant interest due to its remarkable\nadaptability across diverse downstream applications. Unsupervised learning\ngoals differ from downstream tasks, making it tricky to ensure downstream task\nutility by focusing only on temporal feature characterization. Researchers have\nproposed multiple transformations to extract discriminative patterns implied in\ninformative time series, trying to fill the gap. Despite the introduction of a\nvariety of feature engineering techniques, e.g. spectral domain, wavelet\ntransformed features, features in image form and symbolic features etc. the\nutilization of intricate feature fusion methods and dependence on heterogeneous\nfeatures during inference hampers the scalability of the solutions. To address\nthis, our study introduces an innovative approach that focuses on aligning and\nbinding time series representations encoded from different modalities, inspired\nby spectral graph theory, thereby guiding the neural encoder to uncover latent\npattern associations among these multi-modal features. In contrast to\nconventional methods that fuse features from multiple modalities, our proposed\napproach simplifies the neural architecture by retaining a single time series\nencoder, consequently leading to preserved scalability. We further demonstrate\nand prove mechanisms for the encoder to maintain better inductive bias. In our\nexperimental evaluation, we validated the proposed method on a diverse set of\ntime series datasets from various domains. Our approach outperforms existing\nstate-of-the-art URL methods across diverse downstream tasks."
                },
                "authors": [
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Donghua Yang"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Xiyang Zhang"
                    },
                    {
                        "name": "Jianfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Huang"
                },
                "author": "Jianfeng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00290v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00290v5",
                "updated": "2025-05-09T05:37:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    37,
                    54,
                    4,
                    129,
                    0
                ],
                "published": "2025-02-01T03:18:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    18,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "Estimating LLM Uncertainty with Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating LLM Uncertainty with Evidence"
                },
                "summary": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise."
                },
                "authors": [
                    {
                        "name": "Huan Ma"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00290v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00290v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05800v1",
                "updated": "2025-05-09T05:32:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    32,
                    40,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T05:32:40Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    32,
                    40,
                    4,
                    129,
                    0
                ],
                "title": "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language\n  Action Models for Unseen Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language\n  Action Models for Unseen Tasks"
                },
                "summary": "Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint\nspace trajectory of a robot manipulator. Robots must possess semantic and\nvisual perception abilities to transform real-world mappings of their workspace\ninto the low-level control necessary for object manipulation. Recent work has\ndemonstrated the capabilities of fine-tuning large Vision-Language Models\n(VLMs) to learn the mapping between RGB images, language instructions, and\njoint space control. These models typically take as input RGB images of the\nworkspace and language instructions, and are trained on large datasets of\nteleoperated robot demonstrations. In this work, we explore methods to improve\nthe scene context awareness of a popular recent Vision-Language-Action model by\nintegrating chain-of-thought reasoning, depth perception, and task-oriented\nregion of interest detection. Our experiments in the LIBERO simulation\nenvironment show that our proposed model, 3D-CAVLA, improves the success rate\nacross various LIBERO task suites, achieving an average success rate of\n98.1$\\%$. We also evaluate the zero-shot capabilities of our method,\ndemonstrating that 3D scene awareness leads to robust learning and adaptation\nfor completely unseen tasks. 3D-CAVLA achieves an absolute improvement of\n8.8$\\%$ on unseen tasks. We will open-source our code and the unseen tasks\ndataset to promote community-driven research here: https://3d-cavla.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint\nspace trajectory of a robot manipulator. Robots must possess semantic and\nvisual perception abilities to transform real-world mappings of their workspace\ninto the low-level control necessary for object manipulation. Recent work has\ndemonstrated the capabilities of fine-tuning large Vision-Language Models\n(VLMs) to learn the mapping between RGB images, language instructions, and\njoint space control. These models typically take as input RGB images of the\nworkspace and language instructions, and are trained on large datasets of\nteleoperated robot demonstrations. In this work, we explore methods to improve\nthe scene context awareness of a popular recent Vision-Language-Action model by\nintegrating chain-of-thought reasoning, depth perception, and task-oriented\nregion of interest detection. Our experiments in the LIBERO simulation\nenvironment show that our proposed model, 3D-CAVLA, improves the success rate\nacross various LIBERO task suites, achieving an average success rate of\n98.1$\\%$. We also evaluate the zero-shot capabilities of our method,\ndemonstrating that 3D scene awareness leads to robust learning and adaptation\nfor completely unseen tasks. 3D-CAVLA achieves an absolute improvement of\n8.8$\\%$ on unseen tasks. We will open-source our code and the unseen tasks\ndataset to promote community-driven research here: https://3d-cavla.github.io"
                },
                "authors": [
                    {
                        "name": "Vineet Bhat"
                    },
                    {
                        "name": "Yu-Hsiang Lan"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Farshad Khorrami"
                    }
                ],
                "author_detail": {
                    "name": "Farshad Khorrami"
                },
                "author": "Farshad Khorrami",
                "arxiv_comment": "Accepted at the 1st Workshop on 3D LLM/VLA, CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14851v2",
                "updated": "2025-05-09T05:26:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    26,
                    43,
                    4,
                    129,
                    0
                ],
                "published": "2025-01-24T15:49:10Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    49,
                    10,
                    4,
                    24,
                    0
                ],
                "title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning\n  in Large Language Models"
                },
                "summary": "Logical reasoning is a critical component of Large Language Models (LLMs),\nand substantial research efforts in recent years have aimed to enhance their\ndeductive reasoning capabilities. However, existing deductive reasoning\nbenchmarks, which are crucial for evaluating and advancing LLMs, are inadequate\ndue to their lack of task complexity, presence of prior knowledge as a\nconfounder, and superficial error analysis. To address these deficiencies, we\nintroduce JustLogic, a synthetically generated deductive reasoning benchmark\ndesigned for rigorous evaluation of LLMs. JustLogic is (i) highly complex,\ncapable of generating a diverse range of linguistic patterns, vocabulary, and\nargument structures; (ii) prior knowledge independent, eliminating the\nadvantage of models possessing prior knowledge and ensuring that only deductive\nreasoning is used to answer questions; and (iii) capable of in-depth error\nanalysis on the heterogeneous effects of reasoning depth and argument form on\nmodel accuracy. Our experimental results on JustLogic reveal that (i)\nstate-of-the-art (SOTA) reasoning LLMs perform on par or better than the human\naverage but significantly worse than the human ceiling, and (ii) SOTA\nnon-reasoning models still underperform the human average. All code and data\nare available at https://github.com/michaelchen-lab/JustLogic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reasoning is a critical component of Large Language Models (LLMs),\nand substantial research efforts in recent years have aimed to enhance their\ndeductive reasoning capabilities. However, existing deductive reasoning\nbenchmarks, which are crucial for evaluating and advancing LLMs, are inadequate\ndue to their lack of task complexity, presence of prior knowledge as a\nconfounder, and superficial error analysis. To address these deficiencies, we\nintroduce JustLogic, a synthetically generated deductive reasoning benchmark\ndesigned for rigorous evaluation of LLMs. JustLogic is (i) highly complex,\ncapable of generating a diverse range of linguistic patterns, vocabulary, and\nargument structures; (ii) prior knowledge independent, eliminating the\nadvantage of models possessing prior knowledge and ensuring that only deductive\nreasoning is used to answer questions; and (iii) capable of in-depth error\nanalysis on the heterogeneous effects of reasoning depth and argument form on\nmodel accuracy. Our experimental results on JustLogic reveal that (i)\nstate-of-the-art (SOTA) reasoning LLMs perform on par or better than the human\naverage but significantly worse than the human ceiling, and (ii) SOTA\nnon-reasoning models still underperform the human average. All code and data\nare available at https://github.com/michaelchen-lab/JustLogic"
                },
                "authors": [
                    {
                        "name": "Michael K. Chen"
                    },
                    {
                        "name": "Xikun Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05794v1",
                "updated": "2025-05-09T05:19:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    19,
                    14,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T05:19:14Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    19,
                    14,
                    4,
                    129,
                    0
                ],
                "title": "What Is Next for LLMs? Next-Generation AI Computing Hardware Using\n  Photonic Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Is Next for LLMs? Next-Generation AI Computing Hardware Using\n  Photonic Chips"
                },
                "summary": "Large language models (LLMs) are rapidly pushing the limits of contemporary\ncomputing hardware. For example, training GPT-3 has been estimated to consume\naround 1300 MWh of electricity, and projections suggest future models may\nrequire city-scale (gigawatt) power budgets. These demands motivate exploration\nof computing paradigms beyond conventional von Neumann architectures. This\nreview surveys emerging photonic hardware optimized for next-generation\ngenerative AI computing. We discuss integrated photonic neural network\narchitectures (e.g., Mach-Zehnder interferometer meshes, lasers,\nwavelength-multiplexed microring resonators) that perform ultrafast matrix\noperations. We also examine promising alternative neuromorphic devices,\nincluding spiking neural network circuits and hybrid spintronic-photonic\nsynapses, which combine memory and processing. The integration of\ntwo-dimensional materials (graphene, TMDCs) into silicon photonic platforms is\nreviewed for tunable modulators and on-chip synaptic elements.\nTransformer-based LLM architectures (self-attention and feed-forward layers)\nare analyzed in this context, identifying strategies and challenges for mapping\ndynamic matrix multiplications onto these novel hardware substrates. We then\ndissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and\nLLaMA, highlighting their architectural similarities and differences. We\nsynthesize state-of-the-art components, algorithms, and integration methods,\nhighlighting key advances and open issues in scaling such systems to mega-sized\nLLM models. We find that photonic computing systems could potentially surpass\nelectronic processors by orders of magnitude in throughput and energy\nefficiency, but require breakthroughs in memory, especially for long-context\nwindows and long token sequences, and in storage of ultra-large datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly pushing the limits of contemporary\ncomputing hardware. For example, training GPT-3 has been estimated to consume\naround 1300 MWh of electricity, and projections suggest future models may\nrequire city-scale (gigawatt) power budgets. These demands motivate exploration\nof computing paradigms beyond conventional von Neumann architectures. This\nreview surveys emerging photonic hardware optimized for next-generation\ngenerative AI computing. We discuss integrated photonic neural network\narchitectures (e.g., Mach-Zehnder interferometer meshes, lasers,\nwavelength-multiplexed microring resonators) that perform ultrafast matrix\noperations. We also examine promising alternative neuromorphic devices,\nincluding spiking neural network circuits and hybrid spintronic-photonic\nsynapses, which combine memory and processing. The integration of\ntwo-dimensional materials (graphene, TMDCs) into silicon photonic platforms is\nreviewed for tunable modulators and on-chip synaptic elements.\nTransformer-based LLM architectures (self-attention and feed-forward layers)\nare analyzed in this context, identifying strategies and challenges for mapping\ndynamic matrix multiplications onto these novel hardware substrates. We then\ndissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and\nLLaMA, highlighting their architectural similarities and differences. We\nsynthesize state-of-the-art components, algorithms, and integration methods,\nhighlighting key advances and open issues in scaling such systems to mega-sized\nLLM models. We find that photonic computing systems could potentially surpass\nelectronic processors by orders of magnitude in throughput and energy\nefficiency, but require breakthroughs in memory, especially for long-context\nwindows and long token sequences, and in storage of ultra-large datasets."
                },
                "authors": [
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Wenjie Wei"
                    },
                    {
                        "name": "Qi Xin"
                    },
                    {
                        "name": "Xiaoli Liu"
                    },
                    {
                        "name": "Sixuan Mao"
                    },
                    {
                        "name": "Erik Ma"
                    },
                    {
                        "name": "Zijian Chen"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Zhaoyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoyu Zhang"
                },
                "author": "Zhaoyu Zhang",
                "arxiv_comment": "36 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05787v1",
                "updated": "2025-05-09T05:11:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    11,
                    19,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T05:11:19Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    11,
                    19,
                    4,
                    129,
                    0
                ],
                "title": "Demystifying Diffusion Policies: Action Memorization and Simple Lookup\n  Table Alternatives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Diffusion Policies: Action Memorization and Simple Lookup\n  Table Alternatives"
                },
                "summary": "Diffusion policies have demonstrated remarkable dexterity and robustness in\nintricate, high-dimensional robot manipulation tasks, while training from a\nsmall number of demonstrations. However, the reason for this performance\nremains a mystery. In this paper, we offer a surprising hypothesis: diffusion\npolicies essentially memorize an action lookup table -- and this is beneficial.\nWe posit that, at runtime, diffusion policies find the closest training image\nto the test image in a latent space, and recall the associated training action\nsequence, offering reactivity without the need for action generalization. This\nis effective in the sparse data regime, where there is not enough data density\nfor the model to learn action generalization. We support this claim with\nsystematic empirical evidence. Even when conditioned on wildly out of\ndistribution (OOD) images of cats and dogs, the Diffusion Policy still outputs\nan action sequence from the training data. With this insight, we propose a\nsimple policy, the Action Lookup Table (ALT), as a lightweight alternative to\nthe Diffusion Policy. Our ALT policy uses a contrastive image encoder as a hash\nfunction to index the closest corresponding training action sequence,\nexplicitly performing the computation that the Diffusion Policy implicitly\nlearns. We show empirically that for relatively small datasets, ALT matches the\nperformance of a diffusion model, while requiring only 0.0034 of the inference\ntime and 0.0085 of the memory footprint, allowing for much faster closed-loop\ninference with resource constrained robots. We also train our ALT policy to\ngive an explicit OOD flag when the distance between the runtime image is too\nfar in the latent space from the training images, giving a simple but effective\nruntime monitor. More information can be found at:\nhttps://stanfordmsl.github.io/alt/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion policies have demonstrated remarkable dexterity and robustness in\nintricate, high-dimensional robot manipulation tasks, while training from a\nsmall number of demonstrations. However, the reason for this performance\nremains a mystery. In this paper, we offer a surprising hypothesis: diffusion\npolicies essentially memorize an action lookup table -- and this is beneficial.\nWe posit that, at runtime, diffusion policies find the closest training image\nto the test image in a latent space, and recall the associated training action\nsequence, offering reactivity without the need for action generalization. This\nis effective in the sparse data regime, where there is not enough data density\nfor the model to learn action generalization. We support this claim with\nsystematic empirical evidence. Even when conditioned on wildly out of\ndistribution (OOD) images of cats and dogs, the Diffusion Policy still outputs\nan action sequence from the training data. With this insight, we propose a\nsimple policy, the Action Lookup Table (ALT), as a lightweight alternative to\nthe Diffusion Policy. Our ALT policy uses a contrastive image encoder as a hash\nfunction to index the closest corresponding training action sequence,\nexplicitly performing the computation that the Diffusion Policy implicitly\nlearns. We show empirically that for relatively small datasets, ALT matches the\nperformance of a diffusion model, while requiring only 0.0034 of the inference\ntime and 0.0085 of the memory footprint, allowing for much faster closed-loop\ninference with resource constrained robots. We also train our ALT policy to\ngive an explicit OOD flag when the distance between the runtime image is too\nfar in the latent space from the training images, giving a simple but effective\nruntime monitor. More information can be found at:\nhttps://stanfordmsl.github.io/alt/."
                },
                "authors": [
                    {
                        "name": "Chengyang He"
                    },
                    {
                        "name": "Xu Liu"
                    },
                    {
                        "name": "Gadiel Sznaier Camps"
                    },
                    {
                        "name": "Guillaume Sartoretti"
                    },
                    {
                        "name": "Mac Schwager"
                    }
                ],
                "author_detail": {
                    "name": "Mac Schwager"
                },
                "author": "Mac Schwager",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05786v1",
                "updated": "2025-05-09T05:02:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    2,
                    21,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T05:02:21Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    2,
                    21,
                    4,
                    129,
                    0
                ],
                "title": "A Day in Their Shoes: Using LLM-Based Perspective-Taking Interactive\n  Fiction to Reduce Stigma Toward Dirty Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Day in Their Shoes: Using LLM-Based Perspective-Taking Interactive\n  Fiction to Reduce Stigma Toward Dirty Work"
                },
                "summary": "Occupations referred to as \"dirty work\" often face entrenched social stigma,\nwhich adversely affects the mental health of workers in these fields and\nimpedes occupational equity. In this study, we propose a novel Interactive\nFiction (IF) framework powered by Large Language Models (LLMs) to encourage\nperspective-taking and reduce biases against these stigmatized yet essential\nroles. Through an experiment with participants (n = 100) across four such\noccupations, we observed a significant increase in participants' understanding\nof these occupations, as well as a high level of empathy and a strong sense of\nconnection to individuals in these roles. Additionally, qualitative interviews\nwith participants (n = 15) revealed that the LLM-based perspective-taking IF\nenhanced immersion, deepened emotional resonance and empathy toward \"dirty\nwork,\" and allowed participants to experience a sense of professional\nfulfillment in these occupations. However, participants also highlighted\nongoing challenges, such as limited contextual details generated by the LLM and\nthe unintentional reinforcement of existing stereotypes. Overall, our findings\nunderscore that an LLM-based perspective-taking IF framework offers a promising\nand scalable strategy for mitigating stigma and promoting social equity in\nmarginalized professions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occupations referred to as \"dirty work\" often face entrenched social stigma,\nwhich adversely affects the mental health of workers in these fields and\nimpedes occupational equity. In this study, we propose a novel Interactive\nFiction (IF) framework powered by Large Language Models (LLMs) to encourage\nperspective-taking and reduce biases against these stigmatized yet essential\nroles. Through an experiment with participants (n = 100) across four such\noccupations, we observed a significant increase in participants' understanding\nof these occupations, as well as a high level of empathy and a strong sense of\nconnection to individuals in these roles. Additionally, qualitative interviews\nwith participants (n = 15) revealed that the LLM-based perspective-taking IF\nenhanced immersion, deepened emotional resonance and empathy toward \"dirty\nwork,\" and allowed participants to experience a sense of professional\nfulfillment in these occupations. However, participants also highlighted\nongoing challenges, such as limited contextual details generated by the LLM and\nthe unintentional reinforcement of existing stereotypes. Overall, our findings\nunderscore that an LLM-based perspective-taking IF framework offers a promising\nand scalable strategy for mitigating stigma and promoting social equity in\nmarginalized professions."
                },
                "authors": [
                    {
                        "name": "Xiangzhe Yuan"
                    },
                    {
                        "name": "Jiajun Wang"
                    },
                    {
                        "name": "Qian Wan"
                    },
                    {
                        "name": "Siying Hu"
                    }
                ],
                "author_detail": {
                    "name": "Siying Hu"
                },
                "author": "Siying Hu",
                "arxiv_doi": "10.1145/3715275.3732090",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715275.3732090",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.05786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Conference paper for FAccT '25",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05026v2",
                "updated": "2025-05-09T04:56:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    56,
                    44,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-08T08:00:32Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    8,
                    0,
                    32,
                    3,
                    128,
                    0
                ],
                "title": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness"
                },
                "summary": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating user interface (UI) design effectiveness extends beyond aesthetics\nto influencing user behavior, a principle central to Design Persuasiveness. A/B\ntesting is the predominant method for determining which UI variations drive\nhigher user engagement, but it is costly and time-consuming. While recent\nVision-Language Models (VLMs) can process automated UI analysis, current\napproaches focus on isolated design attributes rather than comparative\npersuasiveness-the key factor in optimizing user interactions. To address this,\nwe introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design\nPersuasiveness Assessment task, featuring 300 real-world UI image pairs labeled\nwith A/B test results and expert rationales. Additionally, we propose G-FOCUS,\na novel inference-time reasoning strategy that enhances VLM-based\npersuasiveness assessment by reducing position bias and improving evaluation\naccuracy. Experimental results show that G-FOCUS surpasses existing inference\nstrategies in consistency and accuracy for pairwise UI evaluation. Through\npromoting VLM-driven evaluation of UI persuasiveness, our work offers an\napproach to complement A/B testing, propelling progress in scalable UI\npreference modeling and design optimization. Code and data will be released\npublicly."
                },
                "authors": [
                    {
                        "name": "Jaehyun Jeon"
                    },
                    {
                        "name": "Jang Han Yoon"
                    },
                    {
                        "name": "Min Soo Kim"
                    },
                    {
                        "name": "Sumin Shim"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Hanbin Kim"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "31 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21223v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21223v3",
                "updated": "2025-05-09T04:51:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    51,
                    10,
                    4,
                    129,
                    0
                ],
                "published": "2025-03-27T07:28:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    28,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "Rethinking Graph Structure Learning in the Era of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Graph Structure Learning in the Era of LLMs"
                },
                "summary": "Recently, the emergence of LLMs has prompted researchers to integrate\nlanguage descriptions into graphs, aiming to enhance model encoding\ncapabilities from a data-centric perspective. This graph representation is\ncalled text-attributed graphs (TAGs). A review of prior advancements highlights\nthat graph structure learning (GSL) is a pivotal technique for improving data\nutility, making it highly relevant to efficient TAG learning. However, most GSL\nmethods are tailored for traditional graphs without textual information,\nunderscoring the necessity of developing a new GSL paradigm. Despite clear\nmotivations, it remains challenging: (1) How can we define a reasonable\noptimization objective for GSL in the era of LLMs, considering the massive\nparameters in LLM? (2) How can we design an efficient model architecture that\nenables seamless integration of LLM for this optimization objective? For\nQuestion 1, we reformulate existing GSL optimization objectives as a tree\noptimization framework, shifting the focus from obtaining a well-trained edge\npredictor to a language-aware tree sampler. For Question 2, we propose\ndecoupled and training-free model design principles for LLM integration,\nshifting the focus from computation-intensive fine-tuning to more efficient\ninference. Based on this, we propose Large Language and Tree Assistant (LLaTA),\nwhich leverages tree-based LLM in-context learning to enhance the understanding\nof topology and text, enabling reliable inference and generating improved graph\nstructure. Extensive experiments on 10 datasets demonstrate that LLaTA enjoys\nflexibility-incorporated with any backbone; scalability-outperforms other\nLLM-enhanced graph learning methods; effectiveness-achieves SOTA predictive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the emergence of LLMs has prompted researchers to integrate\nlanguage descriptions into graphs, aiming to enhance model encoding\ncapabilities from a data-centric perspective. This graph representation is\ncalled text-attributed graphs (TAGs). A review of prior advancements highlights\nthat graph structure learning (GSL) is a pivotal technique for improving data\nutility, making it highly relevant to efficient TAG learning. However, most GSL\nmethods are tailored for traditional graphs without textual information,\nunderscoring the necessity of developing a new GSL paradigm. Despite clear\nmotivations, it remains challenging: (1) How can we define a reasonable\noptimization objective for GSL in the era of LLMs, considering the massive\nparameters in LLM? (2) How can we design an efficient model architecture that\nenables seamless integration of LLM for this optimization objective? For\nQuestion 1, we reformulate existing GSL optimization objectives as a tree\noptimization framework, shifting the focus from obtaining a well-trained edge\npredictor to a language-aware tree sampler. For Question 2, we propose\ndecoupled and training-free model design principles for LLM integration,\nshifting the focus from computation-intensive fine-tuning to more efficient\ninference. Based on this, we propose Large Language and Tree Assistant (LLaTA),\nwhich leverages tree-based LLM in-context learning to enhance the understanding\nof topology and text, enabling reliable inference and generating improved graph\nstructure. Extensive experiments on 10 datasets demonstrate that LLaTA enjoys\nflexibility-incorporated with any backbone; scalability-outperforms other\nLLM-enhanced graph learning methods; effectiveness-achieves SOTA predictive\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhihan Zhang"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Zhu Lei"
                    },
                    {
                        "name": "Guang Zeng"
                    },
                    {
                        "name": "Ronghua Li"
                    },
                    {
                        "name": "Guoren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoren Wang"
                },
                "author": "Guoren Wang",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21223v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21223v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17500v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17500v4",
                "updated": "2025-05-09T04:37:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    37,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2024-06-25T12:36:21Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    12,
                    36,
                    21,
                    1,
                    177,
                    0
                ],
                "title": "Using iterated local alignment to aggregate trajectory data into a\n  traffic flow map",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using iterated local alignment to aggregate trajectory data into a\n  traffic flow map"
                },
                "summary": "Vehicle trajectories, with their detailed geolocations, are a promising data\nsource to compute traffic flow maps at scales ranging from the city/regional\nlevel to the road level. The main obstacle is that trajectory data are prone to\nmeasurement noise. While this is negligible for city level large-scale flow\naggregation, it poses substantial difficulties for road level small-scale\naggregation. To overcome these difficulties, we introduce innovative local\nalignment algorithms, where we infer road segments to serve as local reference\nsegments, and proceed to align nearby road segments to them. We deploy these\nalgorithms in an iterative workflow to compute locally aligned flow maps. By\napplying this workflow to synthetic and empirical trajectories, we verify that\nour locally aligned flow maps provide high levels of accuracy and spatial\nresolution of flow aggregation at multiple scales for static and interactive\nmaps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle trajectories, with their detailed geolocations, are a promising data\nsource to compute traffic flow maps at scales ranging from the city/regional\nlevel to the road level. The main obstacle is that trajectory data are prone to\nmeasurement noise. While this is negligible for city level large-scale flow\naggregation, it poses substantial difficulties for road level small-scale\naggregation. To overcome these difficulties, we introduce innovative local\nalignment algorithms, where we infer road segments to serve as local reference\nsegments, and proceed to align nearby road segments to them. We deploy these\nalgorithms in an iterative workflow to compute locally aligned flow maps. By\napplying this workflow to synthetic and empirical trajectories, we verify that\nour locally aligned flow maps provide high levels of accuracy and spatial\nresolution of flow aggregation at multiple scales for static and interactive\nmaps."
                },
                "authors": [
                    {
                        "name": "Tarn Duong"
                    }
                ],
                "author_detail": {
                    "name": "Tarn Duong"
                },
                "author": "Tarn Duong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17500v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17500v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P30, 62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05773v1",
                "updated": "2025-05-09T04:20:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    20,
                    47,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:20:47Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    20,
                    47,
                    4,
                    129,
                    0
                ],
                "title": "Human-Robot Collaboration for the Remote Control of Mobile Humanoid\n  Robots with Torso-Arm Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Robot Collaboration for the Remote Control of Mobile Humanoid\n  Robots with Torso-Arm Coordination"
                },
                "summary": "Recently, many humanoid robots have been increasingly deployed in various\nfacilities, including hospitals and assisted living environments, where they\nare often remotely controlled by human operators. Their kinematic redundancy\nenhances reachability and manipulability, enabling them to navigate complex,\ncluttered environments and perform a wide range of tasks. However, this\nredundancy also presents significant control challenges, particularly in\ncoordinating the movements of the robot's macro-micro structure (torso and\narms). Therefore, we propose various human-robot collaborative (HRC) methods\nfor coordinating the torso and arm of remotely controlled mobile humanoid\nrobots, aiming to balance autonomy and human input to enhance system efficiency\nand task execution. The proposed methods include human-initiated approaches,\nwhere users manually control torso movements, and robot-initiated approaches,\nwhich autonomously coordinate torso and arm based on factors such as\nreachability, task goal, or inferred human intent. We conducted a user study\nwith N=17 participants to compare the proposed approaches in terms of task\nperformance, manipulability, and energy efficiency, and analyzed which methods\nwere preferred by participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, many humanoid robots have been increasingly deployed in various\nfacilities, including hospitals and assisted living environments, where they\nare often remotely controlled by human operators. Their kinematic redundancy\nenhances reachability and manipulability, enabling them to navigate complex,\ncluttered environments and perform a wide range of tasks. However, this\nredundancy also presents significant control challenges, particularly in\ncoordinating the movements of the robot's macro-micro structure (torso and\narms). Therefore, we propose various human-robot collaborative (HRC) methods\nfor coordinating the torso and arm of remotely controlled mobile humanoid\nrobots, aiming to balance autonomy and human input to enhance system efficiency\nand task execution. The proposed methods include human-initiated approaches,\nwhere users manually control torso movements, and robot-initiated approaches,\nwhich autonomously coordinate torso and arm based on factors such as\nreachability, task goal, or inferred human intent. We conducted a user study\nwith N=17 participants to compare the proposed approaches in terms of task\nperformance, manipulability, and energy efficiency, and analyzed which methods\nwere preferred by participants."
                },
                "authors": [
                    {
                        "name": "Nikita Boguslavskii"
                    },
                    {
                        "name": "Lorena Maria Genua"
                    },
                    {
                        "name": "Zhi Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Li"
                },
                "author": "Zhi Li",
                "arxiv_comment": "This work has been accepted for publication in 2025 IEEE\n  International Conference on Robotics and Automation (ICRA 2025). The final\n  published version will be available via IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05772v1",
                "updated": "2025-05-09T04:17:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:17:05Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM"
                },
                "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."
                },
                "authors": [
                    {
                        "name": "Zehao Fan"
                    },
                    {
                        "name": "Garrett Gagnon"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Liu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liu Liu"
                },
                "author": "Liu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05771v1",
                "updated": "2025-05-09T04:15:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    15,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:15:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    15,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Statistical methods for cost-effectiveness analysis of left-truncated\n  censored survival data with treatment delays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical methods for cost-effectiveness analysis of left-truncated\n  censored survival data with treatment delays"
                },
                "summary": "The incremental cost-effectiveness ratio (ICER) and incremental net benefit\n(INB) are widely used for cost-effectiveness analysis. We develop methods for\nestimation and inference for the ICER and INB which use the semiparametric\nstratified Cox proportional hazard model, allowing for adjustment for risk\nfactors. Since in public health settings, patients often begin treatment after\nthey become eligible, we account for delay times in treatment initiation.\nExcellent finite sample properties of the proposed estimator are demonstrated\nin an extensive simulation study under different delay scenarios. We apply the\nproposed method to evaluate the cost-effectiveness of switching treatments\namong AIDS patients in Tanzania.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The incremental cost-effectiveness ratio (ICER) and incremental net benefit\n(INB) are widely used for cost-effectiveness analysis. We develop methods for\nestimation and inference for the ICER and INB which use the semiparametric\nstratified Cox proportional hazard model, allowing for adjustment for risk\nfactors. Since in public health settings, patients often begin treatment after\nthey become eligible, we account for delay times in treatment initiation.\nExcellent finite sample properties of the proposed estimator are demonstrated\nin an extensive simulation study under different delay scenarios. We apply the\nproposed method to evaluate the cost-effectiveness of switching treatments\namong AIDS patients in Tanzania."
                },
                "authors": [
                    {
                        "name": "Polyna Khudyakov"
                    },
                    {
                        "name": "Li Xu"
                    },
                    {
                        "name": "Ce Yang"
                    },
                    {
                        "name": "Donna Spiegelman"
                    },
                    {
                        "name": "Molin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Molin Wang"
                },
                "author": "Molin Wang",
                "arxiv_comment": "24 pages, 4 figures, has Supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08471v2",
                "updated": "2025-05-09T04:03:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    3,
                    8,
                    4,
                    129,
                    0
                ],
                "published": "2024-10-11T02:47:35Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    47,
                    35,
                    4,
                    285,
                    0
                ],
                "title": "Opacity Enforcement by Edit Functions Under Incomparable Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opacity Enforcement by Edit Functions Under Incomparable Observations"
                },
                "summary": "As an information-flow privacy property, opacity characterizes whether a\nmalicious external observer (referred to as an intruder) is able to infer the\nsecret behavior of a system. This paper addresses the problem of opacity\nenforcement using edit functions in discrete event systems modeled by partially\nobserved deterministic finite automata. A defender uses the edit function as an\ninterface at the output of a system to manipulate actual observations through\ninsertion, substitution, and deletion operations so that the intruder will be\nprevented from inferring the secret behavior of the system. Unlike existing\nwork which usually assumes that the observation capabilities of the intruder\nand the defender are identical, we consider a more general setting where they\nmay observe incomparable subsets of events generated by the system.To\ncharacterize whether the defender has the ability to enforce opacity of the\nsystem under this setting, the notion of \\emph{$ic$-enforceability} is\nintroduced. Then, the opacity enforcement problem is transformed to a\ntwo-player game, with imperfect information between the system and the\ndefender, which can be used to determine a feasible decision-making strategy\nfor the defender. Within the game scheme, an edit mechanism is constructed to\nenumerate all feasible edit actions following system behavior. We further show\nthat an $ic$-enforcing edit function (if one exists) can be synthesized from\nthe edit mechanism to enforce opacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an information-flow privacy property, opacity characterizes whether a\nmalicious external observer (referred to as an intruder) is able to infer the\nsecret behavior of a system. This paper addresses the problem of opacity\nenforcement using edit functions in discrete event systems modeled by partially\nobserved deterministic finite automata. A defender uses the edit function as an\ninterface at the output of a system to manipulate actual observations through\ninsertion, substitution, and deletion operations so that the intruder will be\nprevented from inferring the secret behavior of the system. Unlike existing\nwork which usually assumes that the observation capabilities of the intruder\nand the defender are identical, we consider a more general setting where they\nmay observe incomparable subsets of events generated by the system.To\ncharacterize whether the defender has the ability to enforce opacity of the\nsystem under this setting, the notion of \\emph{$ic$-enforceability} is\nintroduced. Then, the opacity enforcement problem is transformed to a\ntwo-player game, with imperfect information between the system and the\ndefender, which can be used to determine a feasible decision-making strategy\nfor the defender. Within the game scheme, an edit mechanism is constructed to\nenumerate all feasible edit actions following system behavior. We further show\nthat an $ic$-enforcing edit function (if one exists) can be synthesized from\nthe edit mechanism to enforce opacity."
                },
                "authors": [
                    {
                        "name": "Wei Duan"
                    },
                    {
                        "name": "Ruotian Liu"
                    },
                    {
                        "name": "Maria Pia Fanti"
                    },
                    {
                        "name": "Christoforos N. Hadjicostis"
                    },
                    {
                        "name": "Zhiwu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwu Li"
                },
                "author": "Zhiwu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21463v2",
                "updated": "2025-05-09T03:59:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    59,
                    15,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-30T09:38:17Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    9,
                    38,
                    17,
                    2,
                    120,
                    0
                ],
                "title": "RWKV-X: A Linear Complexity Hybrid Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RWKV-X: A Linear Complexity Hybrid Language Model"
                },
                "summary": "In this paper, we introduce RWKV-X, a novel hybrid architecture that combines\nthe efficiency of RWKV for short-range modeling with a sparse attention\nmechanism designed to capture long-range context. Unlike previous hybrid\napproaches that rely on full attention layers and retain quadratic complexity,\nRWKV-X achieves linear-time complexity in training and constant-time complexity\nin inference decoding. We demonstrate that RWKV-X, when continually pretrained\non 64K-token sequences, achieves near-perfect accuracy on the 64K passkey\nretrieval benchmark. It consistently outperforms prior RWKV-7 models on\nlong-context benchmarks, while maintaining strong performance on short-context\ntasks. These results highlight RWKV-X as a scalable and efficient backbone for\ngeneral-purpose language modeling, capable of decoding sequences up to 1\nmillion tokens with stable speed and memory usage. To facilitate further\nresearch and analysis, we have made the checkpoints and the associated code\npublicly accessible at: https://github.com/howard-hou/RWKV-X.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce RWKV-X, a novel hybrid architecture that combines\nthe efficiency of RWKV for short-range modeling with a sparse attention\nmechanism designed to capture long-range context. Unlike previous hybrid\napproaches that rely on full attention layers and retain quadratic complexity,\nRWKV-X achieves linear-time complexity in training and constant-time complexity\nin inference decoding. We demonstrate that RWKV-X, when continually pretrained\non 64K-token sequences, achieves near-perfect accuracy on the 64K passkey\nretrieval benchmark. It consistently outperforms prior RWKV-7 models on\nlong-context benchmarks, while maintaining strong performance on short-context\ntasks. These results highlight RWKV-X as a scalable and efficient backbone for\ngeneral-purpose language modeling, capable of decoding sequences up to 1\nmillion tokens with stable speed and memory usage. To facilitate further\nresearch and analysis, we have made the checkpoints and the associated code\npublicly accessible at: https://github.com/howard-hou/RWKV-X."
                },
                "authors": [
                    {
                        "name": "Haowen Hou"
                    },
                    {
                        "name": "Zhiyi Huang"
                    },
                    {
                        "name": "Kaifeng Tan"
                    },
                    {
                        "name": "Rongchang Lu"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "12 pages, typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05762v1",
                "updated": "2025-05-09T03:52:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    52,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T03:52:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    52,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Multi-Agent Systems for Robotic Autonomy with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Systems for Robotic Autonomy with LLMs"
                },
                "summary": "Since the advent of Large Language Models (LLMs), various research based on\nsuch models have maintained significant academic attention and impact,\nespecially in AI and robotics. In this paper, we propose a multi-agent\nframework with LLMs to construct an integrated system for robotic task\nanalysis, mechanical design, and path generation. The framework includes three\ncore agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer.\nOutputs are formatted as multimodal results, such as code files or technical\nreports, for stronger understandability and usability. To evaluate\ngeneralizability comparatively, we conducted experiments with models from both\nGPT and DeepSeek. Results demonstrate that the proposed system can design\nfeasible robots with control strategies when appropriate task inputs are\nprovided, exhibiting substantial potential for enhancing the efficiency and\naccessibility of robotic system development in research and industrial\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of Large Language Models (LLMs), various research based on\nsuch models have maintained significant academic attention and impact,\nespecially in AI and robotics. In this paper, we propose a multi-agent\nframework with LLMs to construct an integrated system for robotic task\nanalysis, mechanical design, and path generation. The framework includes three\ncore agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer.\nOutputs are formatted as multimodal results, such as code files or technical\nreports, for stronger understandability and usability. To evaluate\ngeneralizability comparatively, we conducted experiments with models from both\nGPT and DeepSeek. Results demonstrate that the proposed system can design\nfeasible robots with control strategies when appropriate task inputs are\nprovided, exhibiting substantial potential for enhancing the efficiency and\naccessibility of robotic system development in research and industrial\napplications."
                },
                "authors": [
                    {
                        "name": "Junhong Chen"
                    },
                    {
                        "name": "Ziqi Yang"
                    },
                    {
                        "name": "Haoyuan G Xu"
                    },
                    {
                        "name": "Dandan Zhang"
                    },
                    {
                        "name": "George Mylonas"
                    }
                ],
                "author_detail": {
                    "name": "George Mylonas"
                },
                "author": "George Mylonas",
                "arxiv_comment": "11 pages, 2 figures, 5 tables, submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05758v1",
                "updated": "2025-05-09T03:38:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    38,
                    31,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T03:38:31Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    38,
                    31,
                    4,
                    129,
                    0
                ],
                "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal\n  Reasoning"
                },
                "summary": "Formal reasoning and automated theorem proving constitute a challenging\nsubfield of machine learning, in which machines are tasked with proving\nmathematical theorems using formal languages like Lean. A formal verification\nsystem can check whether a formal proof is correct or not almost\ninstantaneously, but generating a completely correct formal proof with large\nlanguage models (LLMs) remains a formidable task. The usual approach in the\nliterature is to prompt the LLM many times (up to several thousands) until one\nof the generated proofs passes the verification system. In this work, we\npresent APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a\nmodular, model-agnostic pipeline that combines the strengths of the Lean\ncompiler with an LLM's reasoning abilities to achieve better proof-generation\nresults at a low sampling budget. Apollo directs a fully automated process in\nwhich the LLM generates proofs for theorems, a set of agents analyze the\nproofs, fix the syntax errors, identify the mistakes in the proofs using Lean,\nisolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on\neach remaining goal with a low top-K budget. The repaired sub-proofs are\nrecombined and reverified, iterating up to a user-controlled maximum number of\nattempts. On the miniF2F benchmark, we establish a new state-of-the-art\naccuracy of 75.0% among 7B-parameter models while keeping the sampling budget\nbelow one thousand. Moreover, Apollo raises the state-of-the-art accuracy for\nGoedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few\nhundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%\naccuracy. Our results demonstrate that targeted, compiler-guided repair of LLM\noutputs yields dramatic gains in both efficiency and correctness, suggesting a\ngeneral paradigm for scalable automated theorem proving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal reasoning and automated theorem proving constitute a challenging\nsubfield of machine learning, in which machines are tasked with proving\nmathematical theorems using formal languages like Lean. A formal verification\nsystem can check whether a formal proof is correct or not almost\ninstantaneously, but generating a completely correct formal proof with large\nlanguage models (LLMs) remains a formidable task. The usual approach in the\nliterature is to prompt the LLM many times (up to several thousands) until one\nof the generated proofs passes the verification system. In this work, we\npresent APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a\nmodular, model-agnostic pipeline that combines the strengths of the Lean\ncompiler with an LLM's reasoning abilities to achieve better proof-generation\nresults at a low sampling budget. Apollo directs a fully automated process in\nwhich the LLM generates proofs for theorems, a set of agents analyze the\nproofs, fix the syntax errors, identify the mistakes in the proofs using Lean,\nisolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on\neach remaining goal with a low top-K budget. The repaired sub-proofs are\nrecombined and reverified, iterating up to a user-controlled maximum number of\nattempts. On the miniF2F benchmark, we establish a new state-of-the-art\naccuracy of 75.0% among 7B-parameter models while keeping the sampling budget\nbelow one thousand. Moreover, Apollo raises the state-of-the-art accuracy for\nGoedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few\nhundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%\naccuracy. Our results demonstrate that targeted, compiler-guided repair of LLM\noutputs yields dramatic gains in both efficiency and correctness, suggesting a\ngeneral paradigm for scalable automated theorem proving."
                },
                "authors": [
                    {
                        "name": "Azim Ospanov"
                    },
                    {
                        "name": "Roozbeh Yousefzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Roozbeh Yousefzadeh"
                },
                "author": "Roozbeh Yousefzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05756v1",
                "updated": "2025-05-09T03:32:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    32,
                    18,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T03:32:18Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    32,
                    18,
                    4,
                    129,
                    0
                ],
                "title": "Evolutionary thoughts: integration of large language models and\n  evolutionary algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary thoughts: integration of large language models and\n  evolutionary algorithms"
                },
                "summary": "Large Language Models (LLMs) have unveiled remarkable capabilities in\nunderstanding and generating both natural language and code, but LLM reasoning\nis prone to hallucination and struggle with complex, novel scenarios, often\ngetting stuck on partial or incorrect solutions. However, the inherent ability\nof Evolutionary Algorithms (EAs) to explore extensive and complex search spaces\nmakes them particularly effective in scenarios where traditional optimization\nmethodologies may falter. However, EAs explore a vast search space when applied\nto complex problems.\n  To address the computational bottleneck of evaluating large populations,\nparticularly crucial for complex evolutionary tasks, we introduce a highly\nefficient evaluation framework. This implementation maintains compatibility\nwith existing primitive definitions, ensuring the generation of valid\nindividuals.\n  Using LLMs, we propose an enhanced evolutionary search strategy that enables\na more focused exploration of expansive solution spaces. LLMs facilitate the\ngeneration of superior candidate solutions, as evidenced by empirical results\ndemonstrating their efficacy in producing improved outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have unveiled remarkable capabilities in\nunderstanding and generating both natural language and code, but LLM reasoning\nis prone to hallucination and struggle with complex, novel scenarios, often\ngetting stuck on partial or incorrect solutions. However, the inherent ability\nof Evolutionary Algorithms (EAs) to explore extensive and complex search spaces\nmakes them particularly effective in scenarios where traditional optimization\nmethodologies may falter. However, EAs explore a vast search space when applied\nto complex problems.\n  To address the computational bottleneck of evaluating large populations,\nparticularly crucial for complex evolutionary tasks, we introduce a highly\nefficient evaluation framework. This implementation maintains compatibility\nwith existing primitive definitions, ensuring the generation of valid\nindividuals.\n  Using LLMs, we propose an enhanced evolutionary search strategy that enables\na more focused exploration of expansive solution spaces. LLMs facilitate the\ngeneration of superior candidate solutions, as evidenced by empirical results\ndemonstrating their efficacy in producing improved outcomes."
                },
                "authors": [
                    {
                        "name": "Antonio Jimeno Yepes"
                    },
                    {
                        "name": "Pieter Barnard"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Barnard"
                },
                "author": "Pieter Barnard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09831v2",
                "updated": "2025-05-09T03:11:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    11,
                    55,
                    4,
                    129,
                    0
                ],
                "published": "2024-06-14T08:40:58Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    40,
                    58,
                    4,
                    166,
                    0
                ],
                "title": "Recent Advances in Federated Learning Driven Large Language Models: A\n  Survey on Architecture, Performance, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Federated Learning Driven Large Language Models: A\n  Survey on Architecture, Performance, and Security"
                },
                "summary": "Federated Learning (FL) offers a promising paradigm for training Large\nLanguage Models (LLMs) in a decentralized manner while preserving data privacy\nand minimizing communication overhead. This survey examines recent advancements\nin FL-driven LLMs, with a particular emphasis on architectural designs,\nperformance optimization, and security concerns, including the emerging area of\nmachine unlearning. In this context, machine unlearning refers to the\nsystematic removal of specific data contributions from trained models to comply\nwith privacy regulations such as the Right to be Forgotten. We review a range\nof strategies enabling unlearning in federated LLMs, including\nperturbation-based methods, model decomposition, and incremental retraining,\nwhile evaluating their trade-offs in terms of efficiency, privacy guarantees,\nand model utility. Through selected case studies and empirical evaluations, we\nanalyze how these methods perform in practical FL scenarios. This survey\nidentifies critical research directions toward developing secure, adaptable,\nand high-performing federated LLM systems for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) offers a promising paradigm for training Large\nLanguage Models (LLMs) in a decentralized manner while preserving data privacy\nand minimizing communication overhead. This survey examines recent advancements\nin FL-driven LLMs, with a particular emphasis on architectural designs,\nperformance optimization, and security concerns, including the emerging area of\nmachine unlearning. In this context, machine unlearning refers to the\nsystematic removal of specific data contributions from trained models to comply\nwith privacy regulations such as the Right to be Forgotten. We review a range\nof strategies enabling unlearning in federated LLMs, including\nperturbation-based methods, model decomposition, and incremental retraining,\nwhile evaluating their trade-offs in terms of efficiency, privacy guarantees,\nand model utility. Through selected case studies and empirical evaluations, we\nanalyze how these methods perform in practical FL scenarios. This survey\nidentifies critical research directions toward developing secure, adaptable,\nand high-performing federated LLM systems for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Tianqing Zhu"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Shui Yu"
                    },
                    {
                        "name": "Wanlei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wanlei Zhou"
                },
                "author": "Wanlei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00679v2",
                "updated": "2025-05-09T03:10:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    10,
                    7,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-01T17:39:02Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    39,
                    2,
                    3,
                    121,
                    0
                ],
                "title": "Steering Large Language Models with Register Analysis for Arbitrary\n  Style Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Large Language Models with Register Analysis for Arbitrary\n  Style Transfer"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies."
                },
                "authors": [
                    {
                        "name": "Xinchen Yang"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05744v1",
                "updated": "2025-05-09T02:57:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    2,
                    57,
                    39,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T02:57:39Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    2,
                    57,
                    39,
                    4,
                    129,
                    0
                ],
                "title": "Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data\n  Classification"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable ability in solving complex\ntasks, making them a promising tool for enhancing tabular learning. However,\nexisting LLM-based methods suffer from high resource requirements, suboptimal\ndemonstration selection, and limited interpretability, which largely hinder\ntheir prediction performance and application in the real world. To overcome\nthese problems, we propose a novel in-context learning framework for tabular\nprediction. The core idea is to leverage the explanations generated by LLMs to\nguide a smaller, locally deployable Surrogate Language Model (SLM) to make\ninterpretable tabular predictions. Specifically, our framework mainly involves\nthree stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to\ngenerate explanations for question-answer pairs in candidate demonstrations,\nproviding insights into the reasoning behind the answer. (ii) Post Hoc\nExplanation-Guided Demonstrations Selection, which utilizes explanations\ngenerated by LLMs to guide the process of demonstration selection from\ncandidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM\nPrediction, which utilizes the demonstrations obtained in step (ii) as\nin-context and merges corresponding explanations as rationales to improve the\nperformance of SLM and guide the model to generate interpretable outputs.\nExperimental results highlight the framework's effectiveness, with an average\naccuracy improvement of 5.31% across various tabular datasets in diverse\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable ability in solving complex\ntasks, making them a promising tool for enhancing tabular learning. However,\nexisting LLM-based methods suffer from high resource requirements, suboptimal\ndemonstration selection, and limited interpretability, which largely hinder\ntheir prediction performance and application in the real world. To overcome\nthese problems, we propose a novel in-context learning framework for tabular\nprediction. The core idea is to leverage the explanations generated by LLMs to\nguide a smaller, locally deployable Surrogate Language Model (SLM) to make\ninterpretable tabular predictions. Specifically, our framework mainly involves\nthree stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to\ngenerate explanations for question-answer pairs in candidate demonstrations,\nproviding insights into the reasoning behind the answer. (ii) Post Hoc\nExplanation-Guided Demonstrations Selection, which utilizes explanations\ngenerated by LLMs to guide the process of demonstration selection from\ncandidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM\nPrediction, which utilizes the demonstrations obtained in step (ii) as\nin-context and merges corresponding explanations as rationales to improve the\nperformance of SLM and guide the model to generate interpretable outputs.\nExperimental results highlight the framework's effectiveness, with an average\naccuracy improvement of 5.31% across various tabular datasets in diverse\ndomains."
                },
                "authors": [
                    {
                        "name": "Ruxue Shi"
                    },
                    {
                        "name": "Hengrui Gu"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05736v1",
                "updated": "2025-05-09T02:28:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    2,
                    28,
                    41,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T02:28:41Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    2,
                    28,
                    41,
                    4,
                    129,
                    0
                ],
                "title": "Multimodal Integrated Knowledge Transfer to Large Language Models\n  through Preference Optimization with Biomedical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Integrated Knowledge Transfer to Large Language Models\n  through Preference Optimization with Biomedical Applications"
                },
                "summary": "The scarcity of high-quality multimodal biomedical data limits the ability to\neffectively fine-tune pretrained Large Language Models (LLMs) for specialized\nbiomedical tasks. To address this challenge, we introduce MINT (Multimodal\nIntegrated kNowledge Transfer), a framework that aligns unimodal large decoder\nmodels with domain-specific decision patterns from multimodal biomedical data\nthrough preference optimization. While MINT supports different optimization\ntechniques, we primarily implement it with the Odds Ratio Preference\nOptimization (ORPO) framework as its backbone. This strategy enables the\naligned LLMs to perform predictive tasks using text-only or image-only inputs\nwhile retaining knowledge learnt from multimodal data. MINT leverages an\nupstream multimodal machine learning (MML) model trained on high-quality\nmultimodal data to transfer domain-specific insights to downstream text-only or\nimage-only LLMs. We demonstrate its effectiveness through two key applications:\n(1) Rare genetic disease prediction from texts, where MINT uses a multimodal\nencoder model, trained on facial photos and clinical notes, to generate a\npreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite\nrelying on text input only, the MINT-derived model outperforms models trained\nwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue\ntype classification using cell nucleus images, where MINT uses a\nvision-language foundation model as the preference generator, containing\nknowledge learnt from both text and histopathological images to align\ndownstream image-only models. The resulting MINT-derived model significantly\nimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue type\nclassification. In summary, MINT provides an effective strategy to align\nunimodal LLMs with high-quality multimodal expertise through preference\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of high-quality multimodal biomedical data limits the ability to\neffectively fine-tune pretrained Large Language Models (LLMs) for specialized\nbiomedical tasks. To address this challenge, we introduce MINT (Multimodal\nIntegrated kNowledge Transfer), a framework that aligns unimodal large decoder\nmodels with domain-specific decision patterns from multimodal biomedical data\nthrough preference optimization. While MINT supports different optimization\ntechniques, we primarily implement it with the Odds Ratio Preference\nOptimization (ORPO) framework as its backbone. This strategy enables the\naligned LLMs to perform predictive tasks using text-only or image-only inputs\nwhile retaining knowledge learnt from multimodal data. MINT leverages an\nupstream multimodal machine learning (MML) model trained on high-quality\nmultimodal data to transfer domain-specific insights to downstream text-only or\nimage-only LLMs. We demonstrate its effectiveness through two key applications:\n(1) Rare genetic disease prediction from texts, where MINT uses a multimodal\nencoder model, trained on facial photos and clinical notes, to generate a\npreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite\nrelying on text input only, the MINT-derived model outperforms models trained\nwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue\ntype classification using cell nucleus images, where MINT uses a\nvision-language foundation model as the preference generator, containing\nknowledge learnt from both text and histopathological images to align\ndownstream image-only models. The resulting MINT-derived model significantly\nimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue type\nclassification. In summary, MINT provides an effective strategy to align\nunimodal LLMs with high-quality multimodal expertise through preference\noptimization."
                },
                "authors": [
                    {
                        "name": "Da Wu"
                    },
                    {
                        "name": "Zhanliang Wang"
                    },
                    {
                        "name": "Quan Nguyen"
                    },
                    {
                        "name": "Zhuoran Xu"
                    },
                    {
                        "name": "Kai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Wang"
                },
                "author": "Kai Wang",
                "arxiv_comment": "First Draft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05713v1",
                "updated": "2025-05-09T01:24:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    1,
                    24,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T01:24:24Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    1,
                    24,
                    24,
                    4,
                    129,
                    0
                ],
                "title": "Understanding Stragglers in Large Model Training Using What-if Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Stragglers in Large Model Training Using What-if Analysis"
                },
                "summary": "Large language model (LLM) training is one of the most demanding distributed\ncomputations today, often requiring thousands of GPUs with frequent\nsynchronization across machines. Such a workload pattern makes it susceptible\nto stragglers, where the training can be stalled by few slow workers. At\nByteDance we find stragglers are not trivially always caused by hardware\nfailures, but can arise from multiple complex factors. This work aims to\npresent a comprehensive study on the straggler issues in LLM training, using a\nfive-month trace collected from our ByteDance LLM training cluster. The core\nmethodology is what-if analysis that simulates the scenario without any\nstragglers and contrasts with the actual case. We use this method to study the\nfollowing questions: (1) how often do stragglers affect training jobs, and what\neffect do they have on job performance; (2) do stragglers exhibit temporal or\nspatial patterns; and (3) what are the potential root causes for stragglers?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) training is one of the most demanding distributed\ncomputations today, often requiring thousands of GPUs with frequent\nsynchronization across machines. Such a workload pattern makes it susceptible\nto stragglers, where the training can be stalled by few slow workers. At\nByteDance we find stragglers are not trivially always caused by hardware\nfailures, but can arise from multiple complex factors. This work aims to\npresent a comprehensive study on the straggler issues in LLM training, using a\nfive-month trace collected from our ByteDance LLM training cluster. The core\nmethodology is what-if analysis that simulates the scenario without any\nstragglers and contrasts with the actual case. We use this method to study the\nfollowing questions: (1) how often do stragglers affect training jobs, and what\neffect do they have on job performance; (2) do stragglers exhibit temporal or\nspatial patterns; and (3) what are the potential root causes for stragglers?"
                },
                "authors": [
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Zuquan Song"
                    },
                    {
                        "name": "Sida Zhao"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Zhanghan Wang"
                    },
                    {
                        "name": "Chenyuan Wang"
                    },
                    {
                        "name": "Zuocheng Shi"
                    },
                    {
                        "name": "Xiang Shi"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Zherui Liu"
                    },
                    {
                        "name": "Shuguang Wang"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Xiu Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17480v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17480v3",
                "updated": "2025-05-09T01:21:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    1,
                    21,
                    11,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-24T12:15:46Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    15,
                    46,
                    3,
                    114,
                    0
                ],
                "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and\n  Scrubbing in Unauthorized Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Attacks to Large Language Model Watermarks: Spoofing and\n  Scrubbing in Unauthorized Knowledge Distillation"
                },
                "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable."
                },
                "authors": [
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Shunfan Zheng"
                    },
                    {
                        "name": "Linlin Wang"
                    },
                    {
                        "name": "Xiaoling Wang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17480v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17480v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05712v1",
                "updated": "2025-05-09T01:19:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    1,
                    19,
                    1,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T01:19:01Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    1,
                    19,
                    1,
                    4,
                    129,
                    0
                ],
                "title": "LLM-Text Watermarking based on Lagrange Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Text Watermarking based on Lagrange Interpolation"
                },
                "summary": "The rapid advancement of LLMs (Large Language Models) has established them as\na foundational technology for many AI and ML powered human computer\ninteractions. A critical challenge in this context is the attribution of\nLLM-generated text, either to the specific language model used or to the\nindividual user who generated it. This is essential for combating\nmisinformation, fake news, misinterpretation, and plagiarism. One of the key\ntechniques for addressing this issue is watermarking.\n  This work presents a watermarking scheme for LLM-generated text based on\nLagrange interpolation, which enables the recovery of a secret author identity\neven when the text has been heavily redacted by an adversary. The core idea is\nto embed a continuous sequence of points (x, f(x)) that lie on a single\nstraight line. The x-coordinates are generated pseudorandomly using either an\nLFSR (when security is not a priority) or a cryptographically secure NFSR for\nhigh-security applications. The scheme efficiency and resilience to adversarial\nmodifications are analysed. Experimental results show that the proposed method\nis highly effective, allowing the recovery of the author identity when as few\nas three points survive adversarial manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of LLMs (Large Language Models) has established them as\na foundational technology for many AI and ML powered human computer\ninteractions. A critical challenge in this context is the attribution of\nLLM-generated text, either to the specific language model used or to the\nindividual user who generated it. This is essential for combating\nmisinformation, fake news, misinterpretation, and plagiarism. One of the key\ntechniques for addressing this issue is watermarking.\n  This work presents a watermarking scheme for LLM-generated text based on\nLagrange interpolation, which enables the recovery of a secret author identity\neven when the text has been heavily redacted by an adversary. The core idea is\nto embed a continuous sequence of points (x, f(x)) that lie on a single\nstraight line. The x-coordinates are generated pseudorandomly using either an\nLFSR (when security is not a priority) or a cryptographically secure NFSR for\nhigh-security applications. The scheme efficiency and resilience to adversarial\nmodifications are analysed. Experimental results show that the proposed method\nis highly effective, allowing the recovery of the author identity when as few\nas three points survive adversarial manipulation."
                },
                "authors": [
                    {
                        "name": "Jarosław Janas"
                    },
                    {
                        "name": "Paweł Morawiecki"
                    },
                    {
                        "name": "Josef Pieprzyk"
                    }
                ],
                "author_detail": {
                    "name": "Josef Pieprzyk"
                },
                "author": "Josef Pieprzyk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05704v1",
                "updated": "2025-05-09T00:39:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    39,
                    43,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T00:39:43Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    39,
                    43,
                    4,
                    129,
                    0
                ],
                "title": "Assessing Robustness to Spurious Correlations in Post-Training Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Robustness to Spurious Correlations in Post-Training Language\n  Models"
                },
                "summary": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations."
                },
                "authors": [
                    {
                        "name": "Julia Shuieh"
                    },
                    {
                        "name": "Prasann Singhal"
                    },
                    {
                        "name": "Apaar Shanker"
                    },
                    {
                        "name": "John Heyer"
                    },
                    {
                        "name": "George Pu"
                    },
                    {
                        "name": "Samuel Denton"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Denton"
                },
                "author": "Samuel Denton",
                "arxiv_comment": "ICLR '25 Workshop on Spurious Correlation and Shortcut Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v3",
                "updated": "2025-05-09T00:31:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    31,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20364v2",
                "updated": "2025-05-09T00:25:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    25,
                    9,
                    4,
                    129,
                    0
                ],
                "published": "2025-02-27T18:35:39Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    35,
                    39,
                    3,
                    58,
                    0
                ],
                "title": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with\n  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix\n  Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with\n  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix\n  Factorization"
                },
                "summary": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI."
                },
                "authors": [
                    {
                        "name": "Ryan C. Barron"
                    },
                    {
                        "name": "Maksim E. Eren"
                    },
                    {
                        "name": "Olga M. Serafimova"
                    },
                    {
                        "name": "Cynthia Matuszek"
                    },
                    {
                        "name": "Boian S. Alexandrov"
                    }
                ],
                "author_detail": {
                    "name": "Boian S. Alexandrov"
                },
                "author": "Boian S. Alexandrov",
                "arxiv_comment": "10 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.06188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06188v2",
                "updated": "2025-05-09T17:25:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    17,
                    25,
                    34,
                    4,
                    129,
                    0
                ],
                "published": "2024-07-08T17:59:36Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    17,
                    59,
                    36,
                    0,
                    190,
                    0
                ],
                "title": "CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation"
                },
                "summary": "While recent advances in text-to-motion generation have shown promising\nresults, they typically assume all individuals are grouped as a single unit.\nScaling these methods to handle larger crowds and ensuring that individuals\nrespond appropriately to specific events remains a significant challenge. This\nis primarily due to the complexities of scene planning, which involves\norganizing groups, planning their activities, and coordinating interactions,\nand controllable motion generation. In this paper, we present CrowdMoGen, the\nfirst zero-shot framework for collective motion generation, which effectively\ngroups individuals and generates event-aligned motion sequences from text\nprompts. 1) Being limited by the available datasets for training an effective\nscene planning module in a supervised manner, we instead propose a crowd scene\nplanner that leverages pre-trained large language models (LLMs) to organize\nindividuals into distinct groups. While LLMs offer high-level guidance for\ngroup divisions, they lack the low-level understanding of human motion. To\naddress this, we further propose integrating an SMPL-based joint prior to\ngenerate context-appropriate activities, which consists of both joint\ntrajectories and textual descriptions. 2) Secondly, to incorporate the assigned\nactivities into the generative network, we introduce a collective motion\ngenerator that integrates the activities into a transformer-based network in a\njoint-wise manner, maintaining the spatial constraints during the multi-step\ndenoising process. Extensive experiments demonstrate that CrowdMoGen\nsignificantly outperforms previous approaches, delivering realistic,\nevent-driven motion sequences that are spatially coherent. As the first\nframework of collective motion generation, CrowdMoGen has the potential to\nadvance applications in urban simulation, crowd planning, and other large-scale\ninteractive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent advances in text-to-motion generation have shown promising\nresults, they typically assume all individuals are grouped as a single unit.\nScaling these methods to handle larger crowds and ensuring that individuals\nrespond appropriately to specific events remains a significant challenge. This\nis primarily due to the complexities of scene planning, which involves\norganizing groups, planning their activities, and coordinating interactions,\nand controllable motion generation. In this paper, we present CrowdMoGen, the\nfirst zero-shot framework for collective motion generation, which effectively\ngroups individuals and generates event-aligned motion sequences from text\nprompts. 1) Being limited by the available datasets for training an effective\nscene planning module in a supervised manner, we instead propose a crowd scene\nplanner that leverages pre-trained large language models (LLMs) to organize\nindividuals into distinct groups. While LLMs offer high-level guidance for\ngroup divisions, they lack the low-level understanding of human motion. To\naddress this, we further propose integrating an SMPL-based joint prior to\ngenerate context-appropriate activities, which consists of both joint\ntrajectories and textual descriptions. 2) Secondly, to incorporate the assigned\nactivities into the generative network, we introduce a collective motion\ngenerator that integrates the activities into a transformer-based network in a\njoint-wise manner, maintaining the spatial constraints during the multi-step\ndenoising process. Extensive experiments demonstrate that CrowdMoGen\nsignificantly outperforms previous approaches, delivering realistic,\nevent-driven motion sequences that are spatially coherent. As the first\nframework of collective motion generation, CrowdMoGen has the potential to\nadvance applications in urban simulation, crowd planning, and other large-scale\ninteractive environments."
                },
                "authors": [
                    {
                        "name": "Yukang Cao"
                    },
                    {
                        "name": "Xinying Guo"
                    },
                    {
                        "name": "Mingyuan Zhang"
                    },
                    {
                        "name": "Haozhe Xie"
                    },
                    {
                        "name": "Chenyang Gu"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "Project page: https://yukangcao.github.io/CrowdMoGen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05423v2",
                "updated": "2025-05-09T17:02:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    17,
                    2,
                    1,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-08T17:12:56Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    12,
                    56,
                    3,
                    128,
                    0
                ],
                "title": "LiTransProQA: an LLM-based Literary Translation evaluation metric with\n  Professional Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiTransProQA: an LLM-based Literary Translation evaluation metric with\n  Professional Question Answering"
                },
                "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce LiTransProQA, a novel, reference-free,\nLLM-based question-answering framework designed specifically for literary\ntranslation evaluation. LiTransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, LiTransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art metrics by over 15\npoints in adequacy assessments. Incorporating professional translator insights\nas weights further improves performance, highlighting the value of translator\ninputs. Notably, LiTransProQA approaches human-level evaluation performance\ncomparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce LiTransProQA, a novel, reference-free,\nLLM-based question-answering framework designed specifically for literary\ntranslation evaluation. LiTransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, LiTransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art metrics by over 15\npoints in adequacy assessments. Incorporating professional translator insights\nas weights further improves performance, highlighting the value of translator\ninputs. Notably, LiTransProQA approaches human-level evaluation performance\ncomparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Lieve Macken"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "Update WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04134v2",
                "updated": "2025-05-09T16:58:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    58,
                    59,
                    4,
                    129,
                    0
                ],
                "published": "2025-02-06T15:14:02Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    14,
                    2,
                    3,
                    37,
                    0
                ],
                "title": "The Order Effect: Investigating Prompt Sensitivity to Input Order in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Order Effect: Investigating Prompt Sensitivity to Input Order in\n  LLMs"
                },
                "summary": "As large language models (LLMs) become integral to diverse applications,\nensuring their reliability under varying input conditions is crucial. One key\nissue affecting this reliability is order sensitivity, wherein slight\nvariations in the input arrangement can lead to inconsistent or biased outputs.\nAlthough recent advances have reduced this sensitivity, the problem remains\nunresolved. This paper investigates the extent of order sensitivity in LLMs\nwhose internal components are hidden from users (such as closed-source models\nor those accessed via API calls). We conduct experiments across multiple tasks,\nincluding paraphrasing, relevance judgment, and multiple-choice questions. Our\nresults show that input order significantly affects performance across tasks,\nwith shuffled inputs leading to measurable declines in output accuracy.\nFew-shot prompting demonstrates mixed effectiveness and offers partial\nmitigation; however, fails to fully resolve the problem. These findings\nhighlight persistent risks, particularly in high-stakes applications, and point\nto the need for more robust LLMs or improved input-handling techniques in\nfuture development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to diverse applications,\nensuring their reliability under varying input conditions is crucial. One key\nissue affecting this reliability is order sensitivity, wherein slight\nvariations in the input arrangement can lead to inconsistent or biased outputs.\nAlthough recent advances have reduced this sensitivity, the problem remains\nunresolved. This paper investigates the extent of order sensitivity in LLMs\nwhose internal components are hidden from users (such as closed-source models\nor those accessed via API calls). We conduct experiments across multiple tasks,\nincluding paraphrasing, relevance judgment, and multiple-choice questions. Our\nresults show that input order significantly affects performance across tasks,\nwith shuffled inputs leading to measurable declines in output accuracy.\nFew-shot prompting demonstrates mixed effectiveness and offers partial\nmitigation; however, fails to fully resolve the problem. These findings\nhighlight persistent risks, particularly in high-stakes applications, and point\nto the need for more robust LLMs or improved input-handling techniques in\nfuture development."
                },
                "authors": [
                    {
                        "name": "Bryan Guan"
                    },
                    {
                        "name": "Tanya Roosta"
                    },
                    {
                        "name": "Peyman Passban"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "arxiv_comment": "The first 3 authors have contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06184v1",
                "updated": "2025-05-09T16:51:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    51,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:51:24Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    51,
                    24,
                    4,
                    129,
                    0
                ],
                "title": "From Millions of Tweets to Actionable Insights: Leveraging LLMs for User\n  Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Millions of Tweets to Actionable Insights: Leveraging LLMs for User\n  Profiling"
                },
                "summary": "Social media user profiling through content analysis is crucial for tasks\nlike misinformation detection, engagement prediction, hate speech monitoring,\nand user behavior modeling. However, existing profiling techniques, including\ntweet summarization, attribute-based profiling, and latent representation\nlearning, face significant limitations: they often lack transferability,\nproduce non-interpretable features, require large labeled datasets, or rely on\nrigid predefined categories that limit adaptability. We introduce a novel large\nlanguage model (LLM)-based approach that leverages domain-defining statements,\nwhich serve as key characteristics outlining the important pillars of a domain\nas foundations for profiling. Our two-stage method first employs\nsemi-supervised filtering with a domain-specific knowledge base, then generates\nboth abstractive (synthesized descriptions) and extractive (representative\ntweet selections) user profiles. By harnessing LLMs' inherent knowledge with\nminimal human validation, our approach is adaptable across domains while\nreducing the need for large labeled datasets. Our method generates\ninterpretable natural language user profiles, condensing extensive user data\ninto a scale that unlocks LLMs' reasoning and knowledge capabilities for\ndownstream social network tasks. We contribute a Persian political Twitter (X)\ndataset and an LLM-based evaluation framework with human validation.\nExperimental results show our method significantly outperforms state-of-the-art\nLLM-based and traditional methods by 9.8%, demonstrating its effectiveness in\ncreating flexible, adaptable, and interpretable user profiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media user profiling through content analysis is crucial for tasks\nlike misinformation detection, engagement prediction, hate speech monitoring,\nand user behavior modeling. However, existing profiling techniques, including\ntweet summarization, attribute-based profiling, and latent representation\nlearning, face significant limitations: they often lack transferability,\nproduce non-interpretable features, require large labeled datasets, or rely on\nrigid predefined categories that limit adaptability. We introduce a novel large\nlanguage model (LLM)-based approach that leverages domain-defining statements,\nwhich serve as key characteristics outlining the important pillars of a domain\nas foundations for profiling. Our two-stage method first employs\nsemi-supervised filtering with a domain-specific knowledge base, then generates\nboth abstractive (synthesized descriptions) and extractive (representative\ntweet selections) user profiles. By harnessing LLMs' inherent knowledge with\nminimal human validation, our approach is adaptable across domains while\nreducing the need for large labeled datasets. Our method generates\ninterpretable natural language user profiles, condensing extensive user data\ninto a scale that unlocks LLMs' reasoning and knowledge capabilities for\ndownstream social network tasks. We contribute a Persian political Twitter (X)\ndataset and an LLM-based evaluation framework with human validation.\nExperimental results show our method significantly outperforms state-of-the-art\nLLM-based and traditional methods by 9.8%, demonstrating its effectiveness in\ncreating flexible, adaptable, and interpretable user profiles."
                },
                "authors": [
                    {
                        "name": "Vahid Rahimzadeh"
                    },
                    {
                        "name": "Ali Hamzehpour"
                    },
                    {
                        "name": "Azadeh Shakery"
                    },
                    {
                        "name": "Masoud Asadpour"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Asadpour"
                },
                "author": "Masoud Asadpour",
                "arxiv_comment": "Accepted at MisD @ AAAI ICWSM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06178v1",
                "updated": "2025-05-09T16:45:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    45,
                    43,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:45:43Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    45,
                    43,
                    4,
                    129,
                    0
                ],
                "title": "A Large Language Model-Enhanced Q-learning for Capacitated Vehicle\n  Routing Problem with Time Windows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model-Enhanced Q-learning for Capacitated Vehicle\n  Routing Problem with Time Windows"
                },
                "summary": "The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a\nclassic NP-hard combinatorial optimization problem widely applied in logistics\ndistribution and transportation management. Its complexity stems from the\nconstraints of vehicle capacity and time windows, which pose significant\nchallenges to traditional approaches. Advances in Large Language Models (LLMs)\nprovide new possibilities for finding approximate solutions to CVRPTW. This\npaper proposes a novel LLM-enhanced Q-learning framework to address the CVRPTW\nwith real-time emergency constraints. Our solution introduces an adaptive\ntwo-phase training mechanism that transitions from the LLM-guided exploration\nphase to the autonomous optimization phase of Q-network. To ensure reliability,\nwe design a three-tier self-correction mechanism based on the Chain-of-Thought\n(CoT) for LLMs: syntactic validation, semantic verification, and physical\nconstraint enforcement. In addition, we also prioritized replay of the\nexperience generated by LLMs to amplify the regulatory role of LLMs in the\narchitecture. Experimental results demonstrate that our framework achieves a\n7.3\\% average reduction in cost compared to traditional Q-learning, with fewer\ntraining steps required for convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a\nclassic NP-hard combinatorial optimization problem widely applied in logistics\ndistribution and transportation management. Its complexity stems from the\nconstraints of vehicle capacity and time windows, which pose significant\nchallenges to traditional approaches. Advances in Large Language Models (LLMs)\nprovide new possibilities for finding approximate solutions to CVRPTW. This\npaper proposes a novel LLM-enhanced Q-learning framework to address the CVRPTW\nwith real-time emergency constraints. Our solution introduces an adaptive\ntwo-phase training mechanism that transitions from the LLM-guided exploration\nphase to the autonomous optimization phase of Q-network. To ensure reliability,\nwe design a three-tier self-correction mechanism based on the Chain-of-Thought\n(CoT) for LLMs: syntactic validation, semantic verification, and physical\nconstraint enforcement. In addition, we also prioritized replay of the\nexperience generated by LLMs to amplify the regulatory role of LLMs in the\narchitecture. Experimental results demonstrate that our framework achieves a\n7.3\\% average reduction in cost compared to traditional Q-learning, with fewer\ntraining steps required for convergence."
                },
                "authors": [
                    {
                        "name": "Linjiang Cao"
                    },
                    {
                        "name": "Maonan Wang"
                    },
                    {
                        "name": "Xi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Xi Xiong"
                },
                "author": "Xi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06176v1",
                "updated": "2025-05-09T16:38:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    38,
                    27,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:38:27Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    38,
                    27,
                    4,
                    129,
                    0
                ],
                "title": "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills"
                },
                "summary": "Retouching is an essential task in post-manipulation of raw photographs.\nGenerative editing, guided by text or strokes, provides a new tool accessible\nto users but can easily change the identity of the original objects in\nunacceptable and unpredictable ways. In contrast, although traditional\nprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,\nLightroom), are conservative, they are still preferred by professionals.\nUnfortunately, professional quality retouching involves many individual\nprocedural editing operations that is challenging to plan for most novices. In\nthis paper, we ask if a multimodal large language model (MLLM) can be taught to\ncritique raw photographs, suggest suitable remedies, and finally realize them\nwith a given set of pre-authored procedural image operations. We demonstrate\nthat MLLMs can be first made aware of the underlying image processing\noperations, by training them to solve specially designed visual puzzles.\nSubsequently, such an operation-aware MLLM can both plan and propose edit\nsequences. To facilitate training, given a set of expert-edited photos, we\nsynthesize a reasoning dataset by procedurally manipulating the expert edits\nand then grounding a pretrained LLM on the visual adjustments, to synthesize\nreasoning for finetuning. The proposed retouching operations are, by\nconstruction, understandable by the users, preserve object details and\nresolution, and can be optionally overridden. We evaluate our setup on a\nvariety of test examples and show advantages, in terms of explainability and\nidentity preservation, over existing generative and other procedural\nalternatives. Code, data, models, and supplementary results can be found via\nour project website at https://monetgpt.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retouching is an essential task in post-manipulation of raw photographs.\nGenerative editing, guided by text or strokes, provides a new tool accessible\nto users but can easily change the identity of the original objects in\nunacceptable and unpredictable ways. In contrast, although traditional\nprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,\nLightroom), are conservative, they are still preferred by professionals.\nUnfortunately, professional quality retouching involves many individual\nprocedural editing operations that is challenging to plan for most novices. In\nthis paper, we ask if a multimodal large language model (MLLM) can be taught to\ncritique raw photographs, suggest suitable remedies, and finally realize them\nwith a given set of pre-authored procedural image operations. We demonstrate\nthat MLLMs can be first made aware of the underlying image processing\noperations, by training them to solve specially designed visual puzzles.\nSubsequently, such an operation-aware MLLM can both plan and propose edit\nsequences. To facilitate training, given a set of expert-edited photos, we\nsynthesize a reasoning dataset by procedurally manipulating the expert edits\nand then grounding a pretrained LLM on the visual adjustments, to synthesize\nreasoning for finetuning. The proposed retouching operations are, by\nconstruction, understandable by the users, preserve object details and\nresolution, and can be optionally overridden. We evaluate our setup on a\nvariety of test examples and show advantages, in terms of explainability and\nidentity preservation, over existing generative and other procedural\nalternatives. Code, data, models, and supplementary results can be found via\nour project website at https://monetgpt.github.io."
                },
                "authors": [
                    {
                        "name": "Niladri Shekhar Dutt"
                    },
                    {
                        "name": "Duygu Ceylan"
                    },
                    {
                        "name": "Niloy J. Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Niloy J. Mitra"
                },
                "author": "Niloy J. Mitra",
                "arxiv_doi": "10.1145/3730926",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3730926",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.06176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGGRAPH 2025 [ACM Transactions on Graphics]; Project\n  website: https://monetgpt.github.io",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06175v1",
                "updated": "2025-05-09T16:29:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    29,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:29:29Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    29,
                    29,
                    4,
                    129,
                    0
                ],
                "title": "Turbo-ICL: In-Context Learning-Based Turbo Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turbo-ICL: In-Context Learning-Based Turbo Equalization"
                },
                "summary": "This paper introduces a novel in-context learning (ICL) framework, inspired\nby large language models (LLMs), for soft-input soft-output channel\nequalization in coded multiple-input multiple-output (MIMO) systems. The\nproposed approach learns to infer posterior symbol distributions directly from\na prompt of pilot signals and decoder feedback. A key innovation is the use of\nprompt augmentation to incorporate extrinsic information from the decoder\noutput as additional context, enabling the ICL model to refine its symbol\nestimates iteratively across turbo decoding iterations. Two model variants,\nbased on Transformer and state-space architectures, are developed and\nevaluated. Extensive simulations demonstrate that, when traditional linear\nassumptions break down, e.g., in the presence of low-resolution quantization,\nICL equalizers consistently outperform conventional model-based baselines, even\nwhen the latter are provided with perfect channel state information. Results\nalso highlight the advantage of Transformer-based models under limited training\ndiversity, as well as the efficiency of state-space models in\nresource-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel in-context learning (ICL) framework, inspired\nby large language models (LLMs), for soft-input soft-output channel\nequalization in coded multiple-input multiple-output (MIMO) systems. The\nproposed approach learns to infer posterior symbol distributions directly from\na prompt of pilot signals and decoder feedback. A key innovation is the use of\nprompt augmentation to incorporate extrinsic information from the decoder\noutput as additional context, enabling the ICL model to refine its symbol\nestimates iteratively across turbo decoding iterations. Two model variants,\nbased on Transformer and state-space architectures, are developed and\nevaluated. Extensive simulations demonstrate that, when traditional linear\nassumptions break down, e.g., in the presence of low-resolution quantization,\nICL equalizers consistently outperform conventional model-based baselines, even\nwhen the latter are provided with perfect channel state information. Results\nalso highlight the advantage of Transformer-based models under limited training\ndiversity, as well as the efficiency of state-space models in\nresource-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Zihang Song"
                    },
                    {
                        "name": "Matteo Zecchin"
                    },
                    {
                        "name": "Bipin Rajendran"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06150v1",
                "updated": "2025-05-09T16:02:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    2,
                    23,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:02:23Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    2,
                    23,
                    4,
                    129,
                    0
                ],
                "title": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed\n  Compute Budgets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed\n  Compute Budgets"
                },
                "summary": "We introduce a scaling law for fine-tuning large language models (LLMs) under\nfixed compute budgets that explicitly accounts for data composition.\nConventional approaches measure training data solely by total tokens, yet the\nnumber of examples and their average token length -- what we term \\emph{dataset\nvolume} -- play a decisive role in model performance. Our formulation is tuned\nfollowing established procedures. Experiments on the BRICC dataset\n\\cite{salavati2024reducing} and subsets of the MMLU dataset\n\\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple\nsubsampling strategies, reveal that data composition significantly affects\ntoken efficiency. These results motivate refined scaling laws for practical LLM\nfine-tuning in resource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a scaling law for fine-tuning large language models (LLMs) under\nfixed compute budgets that explicitly accounts for data composition.\nConventional approaches measure training data solely by total tokens, yet the\nnumber of examples and their average token length -- what we term \\emph{dataset\nvolume} -- play a decisive role in model performance. Our formulation is tuned\nfollowing established procedures. Experiments on the BRICC dataset\n\\cite{salavati2024reducing} and subsets of the MMLU dataset\n\\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple\nsubsampling strategies, reveal that data composition significantly affects\ntoken efficiency. These results motivate refined scaling laws for practical LLM\nfine-tuning in resource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Ryan Lagasse"
                    },
                    {
                        "name": "Aidan Kiernans"
                    },
                    {
                        "name": "Avijit Ghosh"
                    },
                    {
                        "name": "Shiri Dori-Hacohen"
                    }
                ],
                "author_detail": {
                    "name": "Shiri Dori-Hacohen"
                },
                "author": "Shiri Dori-Hacohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06149v1",
                "updated": "2025-05-09T16:00:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    0,
                    1,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T16:00:01Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    16,
                    0,
                    1,
                    4,
                    129,
                    0
                ],
                "title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A\n  Zero-shot and Few-shot Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A\n  Zero-shot and Few-shot Study"
                },
                "summary": "Despite growing interest in automated hate speech detection, most existing\napproaches overlook the linguistic diversity of online content. Multilingual\ninstruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ\noffer promising capabilities across languages, but their effectiveness in\nidentifying hate speech through zero-shot and few-shot prompting remains\nunderexplored. This work evaluates LLM prompting-based detection across eight\nnon-English languages, utilizing several prompting techniques and comparing\nthem to fine-tuned encoder models. We show that while zero-shot and few-shot\nprompting lag behind fine-tuned encoder models on most of the real-world\nevaluation sets, they achieve better generalization on functional tests for\nhate speech detection. Our study also reveals that prompt design plays a\ncritical role, with each language often requiring customized prompting\ntechniques to maximize performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite growing interest in automated hate speech detection, most existing\napproaches overlook the linguistic diversity of online content. Multilingual\ninstruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ\noffer promising capabilities across languages, but their effectiveness in\nidentifying hate speech through zero-shot and few-shot prompting remains\nunderexplored. This work evaluates LLM prompting-based detection across eight\nnon-English languages, utilizing several prompting techniques and comparing\nthem to fine-tuned encoder models. We show that while zero-shot and few-shot\nprompting lag behind fine-tuned encoder models on most of the real-world\nevaluation sets, they achieve better generalization on functional tests for\nhate speech detection. Our study also reveals that prompt design plays a\ncritical role, with each language often requiring customized prompting\ntechniques to maximize performance."
                },
                "authors": [
                    {
                        "name": "Faeze Ghorbanpour"
                    },
                    {
                        "name": "Daryna Dementieva"
                    },
                    {
                        "name": "Alexander Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fraser"
                },
                "author": "Alexander Fraser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24289v2",
                "updated": "2025-05-09T15:58:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    58,
                    9,
                    4,
                    129,
                    0
                ],
                "published": "2025-03-31T16:36:00Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    16,
                    36,
                    0,
                    0,
                    90,
                    0
                ],
                "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rec-R1: Bridging Generative Large Language Models and User-Centric\n  Recommendation Systems via Reinforcement Learning"
                },
                "summary": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting."
                },
                "authors": [
                    {
                        "name": "Jiacheng Lin"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Kun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Kun Qian"
                },
                "author": "Kun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2111.00187v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2111.00187v4",
                "updated": "2025-05-09T15:48:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    48,
                    20,
                    4,
                    129,
                    0
                ],
                "published": "2021-10-30T06:41:17Z",
                "published_parsed": [
                    2021,
                    10,
                    30,
                    6,
                    41,
                    17,
                    5,
                    303,
                    0
                ],
                "title": "Interoperable and scalable echosounder data processing with Echopype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoperable and scalable echosounder data processing with Echopype"
                },
                "summary": "Echosounders are high-frequency sonar systems used to sense fish and\nzooplankton underwater. Their deployment on a variety of ocean observing\nplatforms is generating vast amounts of data at an unprecedented speed from the\noceans. Efficient and integrative analysis of these data, whether across\ndifferent echosounder instruments or in combination with other oceanographic\ndatasets, is crucial for understanding marine ecosystem response to the rapidly\nchanging climate. Here we present Echopype, an open-source Python software\nlibrary designed to address this need. By standardizing data as labeled,\nmulti-dimensional arrays encoded in the widely embraced netCDF data model\nfollowing a community convention, Echopype enhances the interoperability of\nechosounder data, making it easier to explore and use. By leveraging scientific\nPython libraries optimized for distributed computing, Echopype achieves\ncomputational scalability, enabling efficient processing in both local and\ncloud computing environments. Echopype's modularized package structure further\nprovides a unified framework for expanding support for additional instrument\nraw data formats and incorporating new analysis functionalities. We plan to\ncontinue developing Echopype by supporting and collaborating with the\nechosounder user community, and envision that the growth of this package will\ncatalyze the integration of echosounder data into broader regional and global\nocean observation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echosounders are high-frequency sonar systems used to sense fish and\nzooplankton underwater. Their deployment on a variety of ocean observing\nplatforms is generating vast amounts of data at an unprecedented speed from the\noceans. Efficient and integrative analysis of these data, whether across\ndifferent echosounder instruments or in combination with other oceanographic\ndatasets, is crucial for understanding marine ecosystem response to the rapidly\nchanging climate. Here we present Echopype, an open-source Python software\nlibrary designed to address this need. By standardizing data as labeled,\nmulti-dimensional arrays encoded in the widely embraced netCDF data model\nfollowing a community convention, Echopype enhances the interoperability of\nechosounder data, making it easier to explore and use. By leveraging scientific\nPython libraries optimized for distributed computing, Echopype achieves\ncomputational scalability, enabling efficient processing in both local and\ncloud computing environments. Echopype's modularized package structure further\nprovides a unified framework for expanding support for additional instrument\nraw data formats and incorporating new analysis functionalities. We plan to\ncontinue developing Echopype by supporting and collaborating with the\nechosounder user community, and envision that the growth of this package will\ncatalyze the integration of echosounder data into broader regional and global\nocean observation strategies."
                },
                "authors": [
                    {
                        "name": "Wu-Jung Lee"
                    },
                    {
                        "name": "Landung Setiawan"
                    },
                    {
                        "name": "Caesar Tuguinay"
                    },
                    {
                        "name": "Emilio Mayorga"
                    },
                    {
                        "name": "Valentina Staneva"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Staneva"
                },
                "author": "Valentina Staneva",
                "arxiv_doi": "10.1093/icesjms/fsae133",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/icesjms/fsae133",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2111.00187v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2111.00187v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Update to the final accepted manuscript",
                "arxiv_journal_ref": "ICES Journal of Marine Science. Volume 81, Issue 10, Pages\n  1941-1951 (2024)",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12106v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12106v3",
                "updated": "2025-05-09T15:45:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    45,
                    53,
                    4,
                    129,
                    0
                ],
                "published": "2025-01-21T12:56:47Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    56,
                    47,
                    1,
                    21,
                    0
                ],
                "title": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes"
                },
                "summary": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP."
                },
                "authors": [
                    {
                        "name": "Stefan Lenz"
                    },
                    {
                        "name": "Arsenij Ustjanzew"
                    },
                    {
                        "name": "Marco Jeray"
                    },
                    {
                        "name": "Meike Ressing"
                    },
                    {
                        "name": "Torsten Panholzer"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Panholzer"
                },
                "author": "Torsten Panholzer",
                "arxiv_comment": "53 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12106v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12106v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00307v2",
                "updated": "2025-05-09T15:45:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    45,
                    0,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-01T04:59:05Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    4,
                    59,
                    5,
                    3,
                    121,
                    0
                ],
                "title": "Gateformer: Advancing Multivariate Time Series Forecasting through\n  Temporal and Variate-Wise Attention with Gated Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gateformer: Advancing Multivariate Time Series Forecasting through\n  Temporal and Variate-Wise Attention with Gated Representations"
                },
                "summary": "There has been a recent surge of interest in time series modeling using the\nTransformer architecture. However, forecasting multivariate time series with\nTransformer presents a unique challenge as it requires modeling both temporal\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\nmodels have gained popularity for their flexibility in capturing both\nsequential and cross-variate relationships, it is unclear how to best integrate\nthese two sources of information in the context of the Transformer architecture\nwhile optimizing for both performance and efficiency. We re-purpose the\nTransformer architecture to effectively model both cross-time and cross-variate\ndependencies. Our approach begins by embedding each variate independently into\na variate-wise representation that captures its cross-time dynamics, and then\nmodels cross-variate dependencies through attention mechanisms on these learned\nembeddings. Gating operations in both cross-time and cross-variate modeling\nphases regulate information flow, allowing the model to focus on the most\nrelevant features for accurate predictions. Our method achieves\nstate-of-the-art performance across 13 real-world datasets and can be\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\ndelivering performance improvements up to 20.7\\% over original models. Code is\navailable at this repository: https://github.com/nyuolab/Gateformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a recent surge of interest in time series modeling using the\nTransformer architecture. However, forecasting multivariate time series with\nTransformer presents a unique challenge as it requires modeling both temporal\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\nmodels have gained popularity for their flexibility in capturing both\nsequential and cross-variate relationships, it is unclear how to best integrate\nthese two sources of information in the context of the Transformer architecture\nwhile optimizing for both performance and efficiency. We re-purpose the\nTransformer architecture to effectively model both cross-time and cross-variate\ndependencies. Our approach begins by embedding each variate independently into\na variate-wise representation that captures its cross-time dynamics, and then\nmodels cross-variate dependencies through attention mechanisms on these learned\nembeddings. Gating operations in both cross-time and cross-variate modeling\nphases regulate information flow, allowing the model to focus on the most\nrelevant features for accurate predictions. Our method achieves\nstate-of-the-art performance across 13 real-world datasets and can be\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\ndelivering performance improvements up to 20.7\\% over original models. Code is\navailable at this repository: https://github.com/nyuolab/Gateformer."
                },
                "authors": [
                    {
                        "name": "Yu-Hsiang Lan"
                    },
                    {
                        "name": "Eric K. Oermann"
                    }
                ],
                "author_detail": {
                    "name": "Eric K. Oermann"
                },
                "author": "Eric K. Oermann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11928v3",
                "updated": "2025-05-09T15:43:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    43,
                    7,
                    4,
                    129,
                    0
                ],
                "published": "2024-05-20T10:06:33Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    10,
                    6,
                    33,
                    0,
                    141,
                    0
                ],
                "title": "\"Set It Up!\": Functional Object Arrangement with Compositional\n  Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Set It Up!\": Functional Object Arrangement with Compositional\n  Generative Models"
                },
                "summary": "This paper studies the challenge of developing robots capable of\nunderstanding under-specified instructions for creating functional object\narrangements, such as \"set up a dining table for two\"; previous arrangement\napproaches have focused on much more explicit instructions, such as \"put object\nA on the table.\" We introduce a framework, SetItUp, for learning to interpret\nunder-specified instructions. SetItUp takes a small number of training examples\nand a human-crafted program sketch to uncover arrangement rules for specific\nscene types. By leveraging an intermediate graph-like representation of\nabstract spatial relationships among objects, SetItUp decomposes the\narrangement problem into two subproblems: i) learning the arrangement patterns\nfrom limited data and ii) grounding these abstract relationships into object\nposes. SetItUp leverages large language models (LLMs) to propose the abstract\nspatial relationships among objects in novel scenes as the constraints to be\nsatisfied; then, it composes a library of diffusion models associated with\nthese abstract relationships to find object poses that satisfy the constraints.\nWe validate our framework on a dataset comprising study desks, dining tables,\nand coffee tables, with the results showing superior performance in generating\nphysically plausible, functional, and aesthetically pleasing object\narrangements compared to existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the challenge of developing robots capable of\nunderstanding under-specified instructions for creating functional object\narrangements, such as \"set up a dining table for two\"; previous arrangement\napproaches have focused on much more explicit instructions, such as \"put object\nA on the table.\" We introduce a framework, SetItUp, for learning to interpret\nunder-specified instructions. SetItUp takes a small number of training examples\nand a human-crafted program sketch to uncover arrangement rules for specific\nscene types. By leveraging an intermediate graph-like representation of\nabstract spatial relationships among objects, SetItUp decomposes the\narrangement problem into two subproblems: i) learning the arrangement patterns\nfrom limited data and ii) grounding these abstract relationships into object\nposes. SetItUp leverages large language models (LLMs) to propose the abstract\nspatial relationships among objects in novel scenes as the constraints to be\nsatisfied; then, it composes a library of diffusion models associated with\nthese abstract relationships to find object poses that satisfy the constraints.\nWe validate our framework on a dataset comprising study desks, dining tables,\nand coffee tables, with the results showing superior performance in generating\nphysically plausible, functional, and aesthetically pleasing object\narrangements compared to existing models."
                },
                "authors": [
                    {
                        "name": "Yiqing Xu"
                    },
                    {
                        "name": "Jiayuan Mao"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Tomas Lozáno-Pérez"
                    },
                    {
                        "name": "Leslie Pack Kaelbling"
                    },
                    {
                        "name": "David Hsu"
                    }
                ],
                "author_detail": {
                    "name": "David Hsu"
                },
                "author": "David Hsu",
                "arxiv_comment": "10 pages main paper, 21 pages appendix, RSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06131v1",
                "updated": "2025-05-09T15:39:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    39,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T15:39:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    39,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "ELA-ZSON: Efficient Layout-Aware Zero-Shot Object Navigation Agent with\n  Hierarchical Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELA-ZSON: Efficient Layout-Aware Zero-Shot Object Navigation Agent with\n  Hierarchical Planning"
                },
                "summary": "We introduce ELA-ZSON, an efficient layout-aware zero-shot object navigation\n(ZSON) approach designed for complex multi-room indoor environments.\n  By planning hierarchically leveraging a global topologigal map with layout\ninformation and local imperative approach with detailed scene representation\nmemory, ELA-ZSON achieves both efficient and effective navigation.\n  The process is managed by an LLM-powered agent, ensuring seamless effective\nplanning and navigation, without the need for human interaction, complex\nrewards, or costly training.\n  Our experimental results on the MP3D benchmark achieves 85\\% object\nnavigation success rate (SR) and 79\\% success rate weighted by path length\n(SPL) (over 40\\% point improvement in SR and 60\\% improvement in SPL compared\nto exsisting methods). Furthermore, we validate the robustness of our approach\nthrough virtual agent and real-world robotic deployment, showcasing its\ncapability in practical scenarios. See\nhttps://anonymous.4open.science/r/ELA-ZSON-C67E/ for details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ELA-ZSON, an efficient layout-aware zero-shot object navigation\n(ZSON) approach designed for complex multi-room indoor environments.\n  By planning hierarchically leveraging a global topologigal map with layout\ninformation and local imperative approach with detailed scene representation\nmemory, ELA-ZSON achieves both efficient and effective navigation.\n  The process is managed by an LLM-powered agent, ensuring seamless effective\nplanning and navigation, without the need for human interaction, complex\nrewards, or costly training.\n  Our experimental results on the MP3D benchmark achieves 85\\% object\nnavigation success rate (SR) and 79\\% success rate weighted by path length\n(SPL) (over 40\\% point improvement in SR and 60\\% improvement in SPL compared\nto exsisting methods). Furthermore, we validate the robustness of our approach\nthrough virtual agent and real-world robotic deployment, showcasing its\ncapability in practical scenarios. See\nhttps://anonymous.4open.science/r/ELA-ZSON-C67E/ for details."
                },
                "authors": [
                    {
                        "name": "Jiawei Hou"
                    },
                    {
                        "name": "Yuting Xiao"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Taiping Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Taiping Zeng"
                },
                "author": "Taiping Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09667v2",
                "updated": "2025-05-09T15:39:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    39,
                    35,
                    4,
                    129,
                    0
                ],
                "published": "2025-02-12T19:50:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    19,
                    50,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via\n  LLM-based Centroids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via\n  LLM-based Centroids"
                },
                "summary": "We introduce k-LLMmeans, a novel modification of the k-means algorithm for\ntext clustering that leverages LLM-generated summaries as cluster centroids,\ncapturing semantic nuances often missed by purely numerical averages. This\ndesign preserves the core optimization properties of k-means while enhancing\nsemantic interpretability and avoiding the scalability and instability issues\ntypical of modern LLM-based clustering. Unlike existing methods, our approach\ndoes not increase LLM usage with dataset size and produces transparent\nintermediate outputs. We further extend it with a mini-batch variant for\nefficient, real-time clustering of streaming text. Extensive experiments across\nmultiple datasets, embeddings, and LLMs show that k-LLMmeans consistently\noutperforms k-means and other traditional baselines and achieves results\ncomparable to state-of-the-art LLM-based clustering, with a fraction of the LLM\ncalls. Finally, we present a case study on sequential text streams and\nintroduce a new benchmark dataset constructed from StackExchange to evaluate\ntext-stream clustering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce k-LLMmeans, a novel modification of the k-means algorithm for\ntext clustering that leverages LLM-generated summaries as cluster centroids,\ncapturing semantic nuances often missed by purely numerical averages. This\ndesign preserves the core optimization properties of k-means while enhancing\nsemantic interpretability and avoiding the scalability and instability issues\ntypical of modern LLM-based clustering. Unlike existing methods, our approach\ndoes not increase LLM usage with dataset size and produces transparent\nintermediate outputs. We further extend it with a mini-batch variant for\nefficient, real-time clustering of streaming text. Extensive experiments across\nmultiple datasets, embeddings, and LLMs show that k-LLMmeans consistently\noutperforms k-means and other traditional baselines and achieves results\ncomparable to state-of-the-art LLM-based clustering, with a fraction of the LLM\ncalls. Finally, we present a case study on sequential text streams and\nintroduce a new benchmark dataset constructed from StackExchange to evaluate\ntext-stream clustering methods."
                },
                "authors": [
                    {
                        "name": "Jairo Diaz-Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "Jairo Diaz-Rodriguez"
                },
                "author": "Jairo Diaz-Rodriguez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16688v2",
                "updated": "2025-05-09T15:36:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    36,
                    42,
                    4,
                    129,
                    0
                ],
                "published": "2023-10-25T14:50:15Z",
                "published_parsed": [
                    2023,
                    10,
                    25,
                    14,
                    50,
                    15,
                    2,
                    298,
                    0
                ],
                "title": "Learning-based adaption of robotic friction models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based adaption of robotic friction models"
                },
                "summary": "In the Fourth Industrial Revolution, wherein artificial intelligence and the\nautomation of machines occupy a central role, the deployment of robots is\nindispensable. However, the manufacturing process using robots, especially in\ncollaboration with humans, is highly intricate. In particular, modeling the\nfriction torque in robotic joints is a longstanding problem due to the lack of\na good mathematical description. This motivates the usage of data-driven\nmethods in recent works. However, model-based and data-driven models often\nexhibit limitations in their ability to generalize beyond the specific dynamics\nthey were trained on, as we demonstrate in this paper. To address this\nchallenge, we introduce a novel approach based on residual learning, which aims\nto adapt an existing friction model to new dynamics using as little data as\npossible. We validate our approach by training a base neural network on a\nsymmetric friction data set to learn an accurate relation between the velocity\nand the friction torque. Subsequently, to adapt to more complex asymmetric\nsettings, we train a second network on a small dataset, focusing on predicting\nthe residual of the initial network's output. By combining the output of both\nnetworks in a suitable manner, our proposed estimator outperforms the\nconventional model-based approach, an extended LuGre model, and the base neural\nnetwork significantly. Furthermore, we evaluate our method on trajectories\ninvolving external loads and still observe a substantial improvement,\napproximately 60-70%, over the conventional approach. Our method does not rely\non data with external load during training, eliminating the need for external\ntorque sensors. This demonstrates the generalization capability of our\napproach, even with a small amount of data--less than a minute--enabling\nadaptation to diverse scenarios based on prior knowledge about friction in\ndifferent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Fourth Industrial Revolution, wherein artificial intelligence and the\nautomation of machines occupy a central role, the deployment of robots is\nindispensable. However, the manufacturing process using robots, especially in\ncollaboration with humans, is highly intricate. In particular, modeling the\nfriction torque in robotic joints is a longstanding problem due to the lack of\na good mathematical description. This motivates the usage of data-driven\nmethods in recent works. However, model-based and data-driven models often\nexhibit limitations in their ability to generalize beyond the specific dynamics\nthey were trained on, as we demonstrate in this paper. To address this\nchallenge, we introduce a novel approach based on residual learning, which aims\nto adapt an existing friction model to new dynamics using as little data as\npossible. We validate our approach by training a base neural network on a\nsymmetric friction data set to learn an accurate relation between the velocity\nand the friction torque. Subsequently, to adapt to more complex asymmetric\nsettings, we train a second network on a small dataset, focusing on predicting\nthe residual of the initial network's output. By combining the output of both\nnetworks in a suitable manner, our proposed estimator outperforms the\nconventional model-based approach, an extended LuGre model, and the base neural\nnetwork significantly. Furthermore, we evaluate our method on trajectories\ninvolving external loads and still observe a substantial improvement,\napproximately 60-70%, over the conventional approach. Our method does not rely\non data with external load during training, eliminating the need for external\ntorque sensors. This demonstrates the generalization capability of our\napproach, even with a small amount of data--less than a minute--enabling\nadaptation to diverse scenarios based on prior knowledge about friction in\ndifferent settings."
                },
                "authors": [
                    {
                        "name": "Philipp Scholl"
                    },
                    {
                        "name": "Maged Iskandar"
                    },
                    {
                        "name": "Sebastian Wolf"
                    },
                    {
                        "name": "Jinoh Lee"
                    },
                    {
                        "name": "Aras Bacho"
                    },
                    {
                        "name": "Alexander Dietrich"
                    },
                    {
                        "name": "Alin Albu-Schäffer"
                    },
                    {
                        "name": "Gitta Kutyniok"
                    }
                ],
                "author_detail": {
                    "name": "Gitta Kutyniok"
                },
                "author": "Gitta Kutyniok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14362v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14362v3",
                "updated": "2025-05-09T15:24:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    24,
                    2,
                    4,
                    129,
                    0
                ],
                "published": "2024-01-25T18:08:53Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    8,
                    53,
                    3,
                    25,
                    0
                ],
                "title": "The Typing Cure: Experiences with Large Language Model Chatbots for\n  Mental Health Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Typing Cure: Experiences with Large Language Model Chatbots for\n  Mental Health Support"
                },
                "summary": "People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care."
                },
                "authors": [
                    {
                        "name": "Inhwa Song"
                    },
                    {
                        "name": "Sachin R. Pendse"
                    },
                    {
                        "name": "Neha Kumar"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "arxiv_comment": "The first two authors contributed equally to this work; typos\n  corrected and post-review revisions incorporated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14362v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14362v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06120v1",
                "updated": "2025-05-09T15:21:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    21,
                    44,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T15:21:44Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    21,
                    44,
                    4,
                    129,
                    0
                ],
                "title": "LLMs Get Lost In Multi-Turn Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Get Lost In Multi-Turn Conversation"
                },
                "summary": "Large Language Models (LLMs) are conversational interfaces. As such, LLMs\nhave the potential to assist their users not only when they can fully specify\nthe task at hand, but also to help them define, explore, and refine what they\nneed through multi-turn conversational exchange. Although analysis of LLM\nconversation logs has confirmed that underspecification occurs frequently in\nuser instructions, LLM evaluation has predominantly focused on the single-turn,\nfully-specified instruction setting. In this work, we perform large-scale\nsimulation experiments to compare LLM performance in single- and multi-turn\nsettings. Our experiments confirm that all the top open- and closed-weight LLMs\nwe test exhibit significantly lower performance in multi-turn conversations\nthan single-turn, with an average drop of 39% across six generation tasks.\nAnalysis of 200,000+ simulated conversations decomposes the performance\ndegradation into two components: a minor loss in aptitude and a significant\nincrease in unreliability. We find that LLMs often make assumptions in early\nturns and prematurely attempt to generate final solutions, on which they overly\nrely. In simpler terms, we discover that *when LLMs take a wrong turn in a\nconversation, they get lost and do not recover*.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are conversational interfaces. As such, LLMs\nhave the potential to assist their users not only when they can fully specify\nthe task at hand, but also to help them define, explore, and refine what they\nneed through multi-turn conversational exchange. Although analysis of LLM\nconversation logs has confirmed that underspecification occurs frequently in\nuser instructions, LLM evaluation has predominantly focused on the single-turn,\nfully-specified instruction setting. In this work, we perform large-scale\nsimulation experiments to compare LLM performance in single- and multi-turn\nsettings. Our experiments confirm that all the top open- and closed-weight LLMs\nwe test exhibit significantly lower performance in multi-turn conversations\nthan single-turn, with an average drop of 39% across six generation tasks.\nAnalysis of 200,000+ simulated conversations decomposes the performance\ndegradation into two components: a minor loss in aptitude and a significant\nincrease in unreliability. We find that LLMs often make assumptions in early\nturns and prematurely attempt to generate final solutions, on which they overly\nrely. In simpler terms, we discover that *when LLMs take a wrong turn in a\nconversation, they get lost and do not recover*."
                },
                "authors": [
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Hiroaki Hayashi"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Jennifer Neville"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer Neville"
                },
                "author": "Jennifer Neville",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06111v1",
                "updated": "2025-05-09T15:11:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    11,
                    13,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T15:11:13Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    11,
                    13,
                    4,
                    129,
                    0
                ],
                "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions"
                },
                "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning."
                },
                "authors": [
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Yanting Yang"
                    },
                    {
                        "name": "Jisong Cai"
                    },
                    {
                        "name": "Shenyuan Gao"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Hongyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Li"
                },
                "author": "Hongyang Li",
                "arxiv_comment": "Accepted to RSS 2025. Code is available at\n  https://github.com/OpenDriveLab/UniVLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19346v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19346v5",
                "updated": "2025-05-09T15:06:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    6,
                    1,
                    4,
                    129,
                    0
                ],
                "published": "2024-03-28T12:04:28Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    12,
                    4,
                    28,
                    3,
                    88,
                    0
                ],
                "title": "Large Language Models Are Struggle to Cope with Unreasonability in Math\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Struggle to Cope with Unreasonability in Math\n  Problems"
                },
                "summary": "Recent research have demonstrated LLMs' impressive performance in math and\nreasoning. However, the capacity of LLMs to address math problems under\nunconventional conditions, such as internal inconsistencies and flawed\nassumptions, remains largely unexplored. In this paper, we propose a novel\nbenchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to\nrecognize and respond to unreasonability in math problem. The benchmark\nconsists of a carefully curated collection of unreasonable math questions\nacross diverse types. Based on extensive experiments covering 19 LLMs, we\nobserve that even state-of-the-art models such as GPT-4o achieve only limited\nperformance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone\nto overthinking and unstable. We further explore strategies for improving the\nrecognition of unreasonable inputs, shedding light on both the possibility and\nlimitations of LLMs in this challenging setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research have demonstrated LLMs' impressive performance in math and\nreasoning. However, the capacity of LLMs to address math problems under\nunconventional conditions, such as internal inconsistencies and flawed\nassumptions, remains largely unexplored. In this paper, we propose a novel\nbenchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to\nrecognize and respond to unreasonability in math problem. The benchmark\nconsists of a carefully curated collection of unreasonable math questions\nacross diverse types. Based on extensive experiments covering 19 LLMs, we\nobserve that even state-of-the-art models such as GPT-4o achieve only limited\nperformance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone\nto overthinking and unstable. We further explore strategies for improving the\nrecognition of unreasonable inputs, shedding light on both the possibility and\nlimitations of LLMs in this challenging setting."
                },
                "authors": [
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Zihang Yuan"
                    },
                    {
                        "name": "Rui li"
                    },
                    {
                        "name": "Weilin Luo"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "32 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19346v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19346v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06108v1",
                "updated": "2025-05-09T15:05:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    5,
                    57,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T15:05:57Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    5,
                    57,
                    4,
                    129,
                    0
                ],
                "title": "LLMs Outperform Experts on Challenging Biology Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Outperform Experts on Challenging Biology Benchmarks"
                },
                "summary": "This study systematically evaluates 27 frontier Large Language Models on\neight diverse biology benchmarks spanning molecular biology, genetics, cloning,\nvirology, and biosecurity. Models from major AI developers released between\nNovember 2022 and April 2025 were assessed through ten independent runs per\nbenchmark. The findings reveal dramatic improvements in biological\ncapabilities. Top model performance increased more than 4-fold on the\nchallenging text-only subset of the Virology Capabilities Test over the study\nperiod, with the top model now performing twice as well as expert virologists.\nSeveral models now match or exceed expert-level performance on other\nchallenging benchmarks, including LAB-Bench CloningScenarios and the biology\nsubsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not\nsubstantially improve performance over zero-shot evaluation, while extended\nreasoning features in o3-mini and Claude 3.7 Sonnet typically improved\nperformance as predicted by inference scaling. Benchmarks such as PubMedQA and\nthe MMLU and WMDP biology subsets exhibited performance plateaus well below\n100%, suggesting benchmark saturation and errors in the underlying benchmark\ndata. The analysis highlights the need for more sophisticated evaluation\nmethodologies as AI systems continue to advance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study systematically evaluates 27 frontier Large Language Models on\neight diverse biology benchmarks spanning molecular biology, genetics, cloning,\nvirology, and biosecurity. Models from major AI developers released between\nNovember 2022 and April 2025 were assessed through ten independent runs per\nbenchmark. The findings reveal dramatic improvements in biological\ncapabilities. Top model performance increased more than 4-fold on the\nchallenging text-only subset of the Virology Capabilities Test over the study\nperiod, with the top model now performing twice as well as expert virologists.\nSeveral models now match or exceed expert-level performance on other\nchallenging benchmarks, including LAB-Bench CloningScenarios and the biology\nsubsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not\nsubstantially improve performance over zero-shot evaluation, while extended\nreasoning features in o3-mini and Claude 3.7 Sonnet typically improved\nperformance as predicted by inference scaling. Benchmarks such as PubMedQA and\nthe MMLU and WMDP biology subsets exhibited performance plateaus well below\n100%, suggesting benchmark saturation and errors in the underlying benchmark\ndata. The analysis highlights the need for more sophisticated evaluation\nmethodologies as AI systems continue to advance."
                },
                "authors": [
                    {
                        "name": "Lennart Justen"
                    }
                ],
                "author_detail": {
                    "name": "Lennart Justen"
                },
                "author": "Lennart Justen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06096v1",
                "updated": "2025-05-09T14:44:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    44,
                    7,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T14:44:07Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    44,
                    7,
                    4,
                    129,
                    0
                ],
                "title": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog\n  Generation using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog\n  Generation using LLMs"
                },
                "summary": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%."
                },
                "authors": [
                    {
                        "name": "Sam Bush"
                    },
                    {
                        "name": "Matthew DeLorenzo"
                    },
                    {
                        "name": "Phat Tieu"
                    },
                    {
                        "name": "Jeyavijayan Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Jeyavijayan Rajendran"
                },
                "author": "Jeyavijayan Rajendran",
                "arxiv_comment": "Accepted at DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14411v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14411v3",
                "updated": "2025-05-09T14:35:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    35,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-19T21:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    21,
                    58,
                    0,
                    5,
                    109,
                    0
                ],
                "title": "Planet as a Brain: Towards Internet of AgentSites based on AIOS Server",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planet as a Brain: Towards Internet of AgentSites based on AIOS Server"
                },
                "summary": "The internet is undergoing a historical transformation from the \"Internet of\nWebsites\" to the \"Internet of AgentSites.\" While traditional Websites served as\nthe foundation for information hosting and dissemination, a new frontier is\nemerging where AgentSites serve as the hubs of the internet, where each\nAgentSite hosts one or more AI agents that receive tasks, address them, and\ndeliver actionable solutions, marking a significant shift in the digital\nlandscape and representing the next generation of online ecosystems. Under this\nvision, AIOS, the AI Agent Operating System, serves as the server for the\ndevelopment, deployment and execution of AI agents, which is a fundamental\ninfrastructure for the Internet of Agentsites.\n  In this paper, we introduce AIOS Server, a runtime framework to host agents\nand enable global-scale collaboration among decentralized agents. AIOS Server\nprovides a communication protocol leveraging the Model Context Protocol (MCP)\nand JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node\noperates as a server to host and execute agents, while supporting peer-to-peer\ncoordination without reliance on centralized orchestration. Based on AIOS\nServer, we further present the world's first practically deployed Internet of\nAgentsites (AIOS-IoA), including AgentHub for agent registration and discovery\nand AgentChat for interactive communication, at https://planet.aios.foundation.\nThe agent discovery mechanism based on Distributed Hash Tables (DHT) and a\nGossip protocol serves as the search engine for the internet of agentsites.\nThis work provides a practical foundation for building the Internet of\nAgentsites-a new paradigm where autonomous agents become first-class citizens\nof the web. The implementation is available at\nhttps://github.com/agiresearch/AIOS.Server and is integrated into the AIOS main\nbranch at https://github.com/agiresearch/AIOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The internet is undergoing a historical transformation from the \"Internet of\nWebsites\" to the \"Internet of AgentSites.\" While traditional Websites served as\nthe foundation for information hosting and dissemination, a new frontier is\nemerging where AgentSites serve as the hubs of the internet, where each\nAgentSite hosts one or more AI agents that receive tasks, address them, and\ndeliver actionable solutions, marking a significant shift in the digital\nlandscape and representing the next generation of online ecosystems. Under this\nvision, AIOS, the AI Agent Operating System, serves as the server for the\ndevelopment, deployment and execution of AI agents, which is a fundamental\ninfrastructure for the Internet of Agentsites.\n  In this paper, we introduce AIOS Server, a runtime framework to host agents\nand enable global-scale collaboration among decentralized agents. AIOS Server\nprovides a communication protocol leveraging the Model Context Protocol (MCP)\nand JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node\noperates as a server to host and execute agents, while supporting peer-to-peer\ncoordination without reliance on centralized orchestration. Based on AIOS\nServer, we further present the world's first practically deployed Internet of\nAgentsites (AIOS-IoA), including AgentHub for agent registration and discovery\nand AgentChat for interactive communication, at https://planet.aios.foundation.\nThe agent discovery mechanism based on Distributed Hash Tables (DHT) and a\nGossip protocol serves as the search engine for the internet of agentsites.\nThis work provides a practical foundation for building the Internet of\nAgentsites-a new paradigm where autonomous agents become first-class citizens\nof the web. The implementation is available at\nhttps://github.com/agiresearch/AIOS.Server and is integrated into the AIOS main\nbranch at https://github.com/agiresearch/AIOS."
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14411v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14411v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16218v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16218v4",
                "updated": "2025-05-09T14:33:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    33,
                    58,
                    4,
                    129,
                    0
                ],
                "published": "2024-03-24T16:18:27Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    16,
                    18,
                    27,
                    6,
                    84,
                    0
                ],
                "title": "CoverUp: Effective High Coverage Test Generation for Python",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoverUp: Effective High Coverage Test Generation for Python"
                },
                "summary": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 89% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 89% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components."
                },
                "authors": [
                    {
                        "name": "Juan Altmayer Pizzorno"
                    },
                    {
                        "name": "Emery D. Berger"
                    }
                ],
                "author_detail": {
                    "name": "Emery D. Berger"
                },
                "author": "Emery D. Berger",
                "arxiv_doi": "10.1145/3729398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.16218v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16218v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages; to appear at FSE'25",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06085v1",
                "updated": "2025-05-09T14:29:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    29,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T14:29:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    29,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities"
                },
                "summary": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16."
                },
                "authors": [
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Daniele Cesarini"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "Accepted to the Computational Aspects of Deep Learning Workshop at\n  ISC High Performance 2025. To appear in the ISC High Performance 2025\n  Workshop Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20377v2",
                "updated": "2025-05-09T14:28:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    28,
                    25,
                    4,
                    129,
                    0
                ],
                "published": "2025-03-26T09:56:07Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    56,
                    7,
                    2,
                    85,
                    0
                ],
                "title": "UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network\n  Architecture"
                },
                "summary": "As the Large-scale Language Models (LLMs) continue to scale, the requisite\ncomputational power and bandwidth escalate. To address this, we introduce\nUB-Mesh, a novel AI datacenter network architecture designed to enhance\nscalability, performance, cost-efficiency and availability. Unlike traditional\ndatacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a\nhierarchically localized nD-FullMesh network topology. This design fully\nleverages the data locality of LLM training, prioritizing short-range, direct\ninterconnects to minimize data movement distance and reduce switch usage.\n  Although UB-Mesh's nD-FullMesh topology offers several theoretical\nadvantages, its concrete architecture design, physical implementation and\nnetworking system optimization present new challenges. For the actual\nconstruction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is\nbased on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of\nhardware components that serve as the foundational building blocks, including\nspecifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch\n(HRS), NICs and others. These components are interconnected via a novel Unified\nBus (UB) technique, which enables flexible IO bandwidth allocation and hardware\nresource pooling. For networking system optimization, we propose advanced\nrouting mechanism named All-Path-Routing (APR) to efficiently manage data\ntraffic. These optimizations, combined with topology-aware performance\nenhancements and robust reliability measures like 64+1 backup design, result in\n2.04x higher cost-efficiency, 7.2% higher network availability compared to\ntraditional Clos architecture and 95%+ linearity in various LLM training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the Large-scale Language Models (LLMs) continue to scale, the requisite\ncomputational power and bandwidth escalate. To address this, we introduce\nUB-Mesh, a novel AI datacenter network architecture designed to enhance\nscalability, performance, cost-efficiency and availability. Unlike traditional\ndatacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a\nhierarchically localized nD-FullMesh network topology. This design fully\nleverages the data locality of LLM training, prioritizing short-range, direct\ninterconnects to minimize data movement distance and reduce switch usage.\n  Although UB-Mesh's nD-FullMesh topology offers several theoretical\nadvantages, its concrete architecture design, physical implementation and\nnetworking system optimization present new challenges. For the actual\nconstruction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is\nbased on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of\nhardware components that serve as the foundational building blocks, including\nspecifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch\n(HRS), NICs and others. These components are interconnected via a novel Unified\nBus (UB) technique, which enables flexible IO bandwidth allocation and hardware\nresource pooling. For networking system optimization, we propose advanced\nrouting mechanism named All-Path-Routing (APR) to efficiently manage data\ntraffic. These optimizations, combined with topology-aware performance\nenhancements and robust reliability measures like 64+1 backup design, result in\n2.04x higher cost-efficiency, 7.2% higher network availability compared to\ntraditional Clos architecture and 95%+ linearity in various LLM training tasks."
                },
                "authors": [
                    {
                        "name": "Heng Liao"
                    },
                    {
                        "name": "Bingyang Liu"
                    },
                    {
                        "name": "Xianping Chen"
                    },
                    {
                        "name": "Zhigang Guo"
                    },
                    {
                        "name": "Chuanning Cheng"
                    },
                    {
                        "name": "Jianbing Wang"
                    },
                    {
                        "name": "Xiangyu Chen"
                    },
                    {
                        "name": "Peng Dong"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Wenjie Liu"
                    },
                    {
                        "name": "Zhe Zhou"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Yuhang Gai"
                    },
                    {
                        "name": "Cunle Qian"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhongwu Cheng"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yuli Ma"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wenhua Du"
                    },
                    {
                        "name": "Shizhong Xiao"
                    },
                    {
                        "name": "Chungang Li"
                    },
                    {
                        "name": "Yong Qin"
                    },
                    {
                        "name": "Liudong Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Buyun Wang"
                    },
                    {
                        "name": "Pei Wu"
                    },
                    {
                        "name": "Junen Gao"
                    },
                    {
                        "name": "Xiaochu Li"
                    },
                    {
                        "name": "Jian He"
                    },
                    {
                        "name": "Shizhuan Yan"
                    },
                    {
                        "name": "Bill McColl"
                    }
                ],
                "author_detail": {
                    "name": "Bill McColl"
                },
                "author": "Bill McColl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06064v1",
                "updated": "2025-05-09T14:00:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    0,
                    5,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T14:00:05Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    0,
                    5,
                    4,
                    129,
                    0
                ],
                "title": "Context Informed Incremental Learning Improves Myoelectric Control\n  Performance in Virtual Reality Object Manipulation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Informed Incremental Learning Improves Myoelectric Control\n  Performance in Virtual Reality Object Manipulation Tasks"
                },
                "summary": "Electromyography (EMG)-based gesture recognition is a promising approach for\ndesigning intuitive human-computer interfaces. However, while these systems\ntypically perform well in controlled laboratory settings, their usability in\nreal-world applications is compromised by declining performance during\nreal-time control. This decline is largely due to goal-directed behaviors that\nare not captured in static, offline scenarios. To address this issue, we use\n\\textit{Context Informed Incremental Learning} (CIIL) - marking its first\ndeployment in an object-manipulation scenario - to continuously adapt the\nclassifier using contextual cues. Nine participants without upper limb\ndifferences completed a functional task in a virtual reality (VR) environment\ninvolving transporting objects with life-like grips. We compared two scenarios:\none where the classifier was adapted in real-time using contextual information,\nand the other using a traditional open-loop approach without adaptation. The\nCIIL-based approach not only enhanced task success rates and efficiency, but\nalso reduced the perceived workload by 7.1 %, despite causing a 5.8 % reduction\nin offline classification accuracy. This study highlights the potential of\nreal-time contextualized adaptation to enhance user experience and usability of\nEMG-based systems for practical, goal-oriented applications, crucial elements\ntowards their long-term adoption. The source code for this study is available\nat: https://github.com/BiomedicalITS/ciil-emg-vr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electromyography (EMG)-based gesture recognition is a promising approach for\ndesigning intuitive human-computer interfaces. However, while these systems\ntypically perform well in controlled laboratory settings, their usability in\nreal-world applications is compromised by declining performance during\nreal-time control. This decline is largely due to goal-directed behaviors that\nare not captured in static, offline scenarios. To address this issue, we use\n\\textit{Context Informed Incremental Learning} (CIIL) - marking its first\ndeployment in an object-manipulation scenario - to continuously adapt the\nclassifier using contextual cues. Nine participants without upper limb\ndifferences completed a functional task in a virtual reality (VR) environment\ninvolving transporting objects with life-like grips. We compared two scenarios:\none where the classifier was adapted in real-time using contextual information,\nand the other using a traditional open-loop approach without adaptation. The\nCIIL-based approach not only enhanced task success rates and efficiency, but\nalso reduced the perceived workload by 7.1 %, despite causing a 5.8 % reduction\nin offline classification accuracy. This study highlights the potential of\nreal-time contextualized adaptation to enhance user experience and usability of\nEMG-based systems for practical, goal-oriented applications, crucial elements\ntowards their long-term adoption. The source code for this study is available\nat: https://github.com/BiomedicalITS/ciil-emg-vr."
                },
                "authors": [
                    {
                        "name": "Gabriel Gagné"
                    },
                    {
                        "name": "Anisha Azad"
                    },
                    {
                        "name": "Thomas Labbé"
                    },
                    {
                        "name": "Evan Campbell"
                    },
                    {
                        "name": "Xavier Isabel"
                    },
                    {
                        "name": "Erik Scheme"
                    },
                    {
                        "name": "Ulysse Côté-Allard"
                    },
                    {
                        "name": "Benoit Gosselin"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Gosselin"
                },
                "author": "Benoit Gosselin",
                "arxiv_comment": "5 pages, 6 figures, 3 tables, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02847v2",
                "updated": "2025-05-09T13:49:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    49,
                    8,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-01T19:06:10Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    6,
                    10,
                    3,
                    121,
                    0
                ],
                "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in\n  Large Language Models"
                },
                "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents."
                },
                "authors": [
                    {
                        "name": "Bang Zhang"
                    },
                    {
                        "name": "Ruotian Ma"
                    },
                    {
                        "name": "Qingxuan Jiang"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Zheng Xie"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Xiaolong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Li"
                },
                "author": "Xiaolong Li",
                "arxiv_comment": "code: https://github.com/Tencent/digitalhuman/tree/main/SAGE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06046v1",
                "updated": "2025-05-09T13:42:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    42,
                    59,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T13:42:59Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    42,
                    59,
                    4,
                    129,
                    0
                ],
                "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information"
                },
                "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics."
                },
                "authors": [
                    {
                        "name": "Joshua Harris"
                    },
                    {
                        "name": "Fan Grayson"
                    },
                    {
                        "name": "Felix Feldman"
                    },
                    {
                        "name": "Timothy Laurence"
                    },
                    {
                        "name": "Toby Nonnenmacher"
                    },
                    {
                        "name": "Oliver Higgins"
                    },
                    {
                        "name": "Leo Loman"
                    },
                    {
                        "name": "Selina Patel"
                    },
                    {
                        "name": "Thomas Finnie"
                    },
                    {
                        "name": "Samuel Collins"
                    },
                    {
                        "name": "Michael Borowitz"
                    }
                ],
                "author_detail": {
                    "name": "Michael Borowitz"
                },
                "author": "Michael Borowitz",
                "arxiv_comment": "24 pages, 10 pages main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06032v1",
                "updated": "2025-05-09T13:26:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    26,
                    21,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T13:26:21Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    26,
                    21,
                    4,
                    129,
                    0
                ],
                "title": "Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in\n  Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in\n  Text Classification"
                },
                "summary": "Reliance on spurious correlations (shortcuts) has been shown to underlie many\nof the successes of language models. Previous work focused on identifying the\ninput elements that impact prediction. We investigate how shortcuts are\nactually processed within the model's decision-making mechanism. We use actor\nnames in movie reviews as controllable shortcuts with known impact on the\noutcome. We use mechanistic interpretability methods and identify specific\nattention heads that focus on shortcuts. These heads gear the model towards a\nlabel before processing the complete input, effectively making premature\ndecisions that bypass contextual analysis. Based on these findings, we\nintroduce Head-based Token Attribution (HTA), which traces intermediate\ndecisions back to input tokens. We show that HTA is effective in detecting\nshortcuts in LLMs and enables targeted mitigation by selectively deactivating\nshortcut-related attention heads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliance on spurious correlations (shortcuts) has been shown to underlie many\nof the successes of language models. Previous work focused on identifying the\ninput elements that impact prediction. We investigate how shortcuts are\nactually processed within the model's decision-making mechanism. We use actor\nnames in movie reviews as controllable shortcuts with known impact on the\noutcome. We use mechanistic interpretability methods and identify specific\nattention heads that focus on shortcuts. These heads gear the model towards a\nlabel before processing the complete input, effectively making premature\ndecisions that bypass contextual analysis. Based on these findings, we\nintroduce Head-based Token Attribution (HTA), which traces intermediate\ndecisions back to input tokens. We show that HTA is effective in detecting\nshortcuts in LLMs and enables targeted mitigation by selectively deactivating\nshortcut-related attention heads."
                },
                "authors": [
                    {
                        "name": "Leon Eshuijs"
                    },
                    {
                        "name": "Shihan Wang"
                    },
                    {
                        "name": "Antske Fokkens"
                    }
                ],
                "author_detail": {
                    "name": "Antske Fokkens"
                },
                "author": "Antske Fokkens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06027v1",
                "updated": "2025-05-09T13:19:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    19,
                    9,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T13:19:09Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    19,
                    9,
                    4,
                    129,
                    0
                ],
                "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target\n  Self-Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target\n  Self-Distillation"
                },
                "summary": "This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning."
                },
                "authors": [
                    {
                        "name": "Stefan Vasilev"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Seyyed Hadi Hashemi"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "16 pages, 6 figures, 5 tables, under review at ACL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06004v1",
                "updated": "2025-05-09T12:35:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    12,
                    35,
                    26,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T12:35:26Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    12,
                    35,
                    26,
                    4,
                    129,
                    0
                ],
                "title": "Exploring the Feasibility of Multilingual Grammatical Error Correction\n  with a Single LLM up to 9B parameters: A Comparative Study of 17 Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Feasibility of Multilingual Grammatical Error Correction\n  with a Single LLM up to 9B parameters: A Comparative Study of 17 Models"
                },
                "summary": "Recent language models can successfully solve various language-related tasks,\nand many understand inputs stated in different languages. In this paper, we\nexplore the performance of 17 popular models used to correct grammatical issues\nin texts stated in English, German, Italian, and Swedish when using a single\nmodel to correct texts in all those languages. We analyze the outputs generated\nby these models, focusing on decreasing the number of grammatical errors while\nkeeping the changes small. The conclusions drawn help us understand what\nproblems occur among those models and which models can be recommended for\nmultilingual grammatical error correction tasks. We list six models that\nimprove grammatical correctness in all four languages and show that Gemma 9B is\ncurrently the best performing one for the languages considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent language models can successfully solve various language-related tasks,\nand many understand inputs stated in different languages. In this paper, we\nexplore the performance of 17 popular models used to correct grammatical issues\nin texts stated in English, German, Italian, and Swedish when using a single\nmodel to correct texts in all those languages. We analyze the outputs generated\nby these models, focusing on decreasing the number of grammatical errors while\nkeeping the changes small. The conclusions drawn help us understand what\nproblems occur among those models and which models can be recommended for\nmultilingual grammatical error correction tasks. We list six models that\nimprove grammatical correctness in all four languages and show that Gemma 9B is\ncurrently the best performing one for the languages considered."
                },
                "authors": [
                    {
                        "name": "Dawid Wisniewski"
                    },
                    {
                        "name": "Antoni Solarski"
                    },
                    {
                        "name": "Artur Nowakowski"
                    }
                ],
                "author_detail": {
                    "name": "Artur Nowakowski"
                },
                "author": "Artur Nowakowski",
                "arxiv_comment": "Accepted at MTSummit 2025 (The 20th Machine Translation Summit)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06002v1",
                "updated": "2025-05-09T12:34:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    12,
                    34,
                    10,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T12:34:10Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    12,
                    34,
                    10,
                    4,
                    129,
                    0
                ],
                "title": "Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for\n  Few-shot Action Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for\n  Few-shot Action Recognition"
                },
                "summary": "Large-scale pre-trained models have achieved remarkable success in language\nand image tasks, leading an increasing number of studies to explore the\napplication of pre-trained image models, such as CLIP, in the domain of\nfew-shot action recognition (FSAR). However, current methods generally suffer\nfrom several problems: 1) Direct fine-tuning often undermines the\ngeneralization capability of the pre-trained model; 2) The exploration of\ntask-specific information is insufficient in the visual tasks; 3) The semantic\norder information is typically overlooked during text modeling; 4) Existing\ncross-modal alignment techniques ignore the temporal coupling of multimodal\ninformation. To address these, we propose Task-Adapter++, a parameter-efficient\ndual adaptation method for both image and text encoders. Specifically, to make\nfull use of the variations across different few-shot learning tasks, we design\na task-specific adaptation for the image encoder so that the most\ndiscriminative information can be well noticed during feature extraction.\nFurthermore, we leverage large language models (LLMs) to generate detailed\nsequential sub-action descriptions for each action class, and introduce\nsemantic order adapters into the text encoder to effectively model the\nsequential relationships between these sub-actions. Finally, we develop an\ninnovative fine-grained cross-modal alignment strategy that actively maps\nvisual features to reside in the same temporal stage as semantic descriptions.\nExtensive experiments fully demonstrate the effectiveness and superiority of\nthe proposed method, which achieves state-of-the-art performance on 5\nbenchmarks consistently. The code is open-sourced at\nhttps://github.com/Jaulin-Bage/Task-Adapter-pp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models have achieved remarkable success in language\nand image tasks, leading an increasing number of studies to explore the\napplication of pre-trained image models, such as CLIP, in the domain of\nfew-shot action recognition (FSAR). However, current methods generally suffer\nfrom several problems: 1) Direct fine-tuning often undermines the\ngeneralization capability of the pre-trained model; 2) The exploration of\ntask-specific information is insufficient in the visual tasks; 3) The semantic\norder information is typically overlooked during text modeling; 4) Existing\ncross-modal alignment techniques ignore the temporal coupling of multimodal\ninformation. To address these, we propose Task-Adapter++, a parameter-efficient\ndual adaptation method for both image and text encoders. Specifically, to make\nfull use of the variations across different few-shot learning tasks, we design\na task-specific adaptation for the image encoder so that the most\ndiscriminative information can be well noticed during feature extraction.\nFurthermore, we leverage large language models (LLMs) to generate detailed\nsequential sub-action descriptions for each action class, and introduce\nsemantic order adapters into the text encoder to effectively model the\nsequential relationships between these sub-actions. Finally, we develop an\ninnovative fine-grained cross-modal alignment strategy that actively maps\nvisual features to reside in the same temporal stage as semantic descriptions.\nExtensive experiments fully demonstrate the effectiveness and superiority of\nthe proposed method, which achieves state-of-the-art performance on 5\nbenchmarks consistently. The code is open-sourced at\nhttps://github.com/Jaulin-Bage/Task-Adapter-pp."
                },
                "authors": [
                    {
                        "name": "Congqi Cao"
                    },
                    {
                        "name": "Peiheng Han"
                    },
                    {
                        "name": "Yueran zhang"
                    },
                    {
                        "name": "Yating Yu"
                    },
                    {
                        "name": "Qinyi Lv"
                    },
                    {
                        "name": "Lingtong Min"
                    },
                    {
                        "name": "Yanning zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanning zhang"
                },
                "author": "Yanning zhang",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2408.00249",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04430v3",
                "updated": "2025-05-09T11:25:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    11,
                    25,
                    57,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-06T10:01:15Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    10,
                    1,
                    15,
                    6,
                    96,
                    0
                ],
                "title": "AGITB: A Signal-Level Benchmark for Evaluating Artificial General\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGITB: A Signal-Level Benchmark for Evaluating Artificial General\n  Intelligence"
                },
                "summary": "Despite remarkable progress in machine learning, current AI systems continue\nto fall short of true human-like intelligence. While Large Language Models\n(LLMs) excel in pattern recognition and response generation, they lack genuine\nunderstanding - an essential hallmark of Artificial General Intelligence (AGI).\nExisting AGI evaluation methods fail to offer a practical, gradual, and\ninformative metric. This paper introduces the Artificial General Intelligence\nTest Bed (AGITB), comprising twelve rigorous tests that form a\nsignal-processing-level foundation for the potential emergence of cognitive\ncapabilities. AGITB evaluates intelligence through a model's ability to predict\nbinary signals across time without relying on symbolic representations or\npretraining. Unlike high-level tests grounded in language or perception, AGITB\nfocuses on core computational invariants reflective of biological intelligence,\nsuch as determinism, sensitivity, and generalisation. The test bed assumes no\nprior bias, operates independently of semantic meaning, and ensures\nunsolvability through brute force or memorization. While humans pass AGITB by\ndesign, no current AI system has met its criteria, making AGITB a compelling\nbenchmark for guiding and recognizing progress toward AGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite remarkable progress in machine learning, current AI systems continue\nto fall short of true human-like intelligence. While Large Language Models\n(LLMs) excel in pattern recognition and response generation, they lack genuine\nunderstanding - an essential hallmark of Artificial General Intelligence (AGI).\nExisting AGI evaluation methods fail to offer a practical, gradual, and\ninformative metric. This paper introduces the Artificial General Intelligence\nTest Bed (AGITB), comprising twelve rigorous tests that form a\nsignal-processing-level foundation for the potential emergence of cognitive\ncapabilities. AGITB evaluates intelligence through a model's ability to predict\nbinary signals across time without relying on symbolic representations or\npretraining. Unlike high-level tests grounded in language or perception, AGITB\nfocuses on core computational invariants reflective of biological intelligence,\nsuch as determinism, sensitivity, and generalisation. The test bed assumes no\nprior bias, operates independently of semantic meaning, and ensures\nunsolvability through brute force or memorization. While humans pass AGITB by\ndesign, no current AI system has met its criteria, making AGITB a compelling\nbenchmark for guiding and recognizing progress toward AGI."
                },
                "authors": [
                    {
                        "name": "Matej Šprogar"
                    }
                ],
                "author_detail": {
                    "name": "Matej Šprogar"
                },
                "author": "Matej Šprogar",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05950v1",
                "updated": "2025-05-09T10:53:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    53,
                    47,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T10:53:47Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    53,
                    47,
                    4,
                    129,
                    0
                ],
                "title": "FloE: On-the-Fly MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FloE: On-the-Fly MoE Inference"
                },
                "summary": "With the widespread adoption of Mixture-of-Experts (MoE) models, there is a\ngrowing demand for efficient inference on memory-constrained devices. While\noffloading expert parameters to CPU memory and loading activated experts on\ndemand has emerged as a potential solution, the large size of activated experts\noverburdens the limited PCIe bandwidth, hindering the effectiveness in\nlatency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly\nMoE inference system on memory-constrained GPUs. FloE is built on the insight\nthat there exists substantial untapped redundancy within sparsely activated\nexperts. It employs various compression techniques on the expert's internal\nparameter matrices to reduce the data movement load, combined with low-cost\nsparse prediction, achieving perceptible inference acceleration in wall-clock\ntime on resource-constrained devices. Empirically, FloE achieves a 9.3x\ncompression of parameters per expert in Mixtral-8x7B; enables deployment on a\nGPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and\ndelivers a 48.7x inference speedup compared to DeepSpeed-MII on a single\nGeForce RTX 3090.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of Mixture-of-Experts (MoE) models, there is a\ngrowing demand for efficient inference on memory-constrained devices. While\noffloading expert parameters to CPU memory and loading activated experts on\ndemand has emerged as a potential solution, the large size of activated experts\noverburdens the limited PCIe bandwidth, hindering the effectiveness in\nlatency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly\nMoE inference system on memory-constrained GPUs. FloE is built on the insight\nthat there exists substantial untapped redundancy within sparsely activated\nexperts. It employs various compression techniques on the expert's internal\nparameter matrices to reduce the data movement load, combined with low-cost\nsparse prediction, achieving perceptible inference acceleration in wall-clock\ntime on resource-constrained devices. Empirically, FloE achieves a 9.3x\ncompression of parameters per expert in Mixtral-8x7B; enables deployment on a\nGPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and\ndelivers a 48.7x inference speedup compared to DeepSpeed-MII on a single\nGeForce RTX 3090."
                },
                "authors": [
                    {
                        "name": "Yuxin Zhou"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Yiping Wang"
                    },
                    {
                        "name": "Zhongle Xie"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Lidan Shou"
                    }
                ],
                "author_detail": {
                    "name": "Lidan Shou"
                },
                "author": "Lidan Shou",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05949v1",
                "updated": "2025-05-09T10:51:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    51,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T10:51:29Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    51,
                    29,
                    4,
                    129,
                    0
                ],
                "title": "NeoQA: Evidence-based Question Answering with Generated News Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeoQA: Evidence-based Question Answering with Generated News Events"
                },
                "summary": "Evaluating Retrieval-Augmented Generation (RAG) in large language models\n(LLMs) is challenging because benchmarks can quickly become stale. Questions\ninitially requiring retrieval may become answerable from pretraining knowledge\nas newer models incorporate more recent information during pretraining, making\nit difficult to distinguish evidence-based reasoning from recall. We introduce\nNeoQA (News Events for Out-of-training Question Answering), a benchmark\ndesigned to address this issue. To construct NeoQA, we generated timelines and\nknowledge bases of fictional news events and entities along with news articles\nand Q\\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring\nthat no prior evidence exists in their training data. We propose our dataset as\na new platform for evaluating evidence-based question answering, as it requires\nLLMs to generate responses exclusively from retrieved evidence and only when\nsufficient evidence is available. NeoQA enables controlled evaluation across\nvarious evidence scenarios, including cases with missing or misleading details.\nOur findings indicate that LLMs struggle to distinguish subtle mismatches\nbetween questions and evidence, and suffer from short-cut reasoning when key\ninformation required to answer a question is missing from the evidence,\nunderscoring key limitations in evidence-based reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Retrieval-Augmented Generation (RAG) in large language models\n(LLMs) is challenging because benchmarks can quickly become stale. Questions\ninitially requiring retrieval may become answerable from pretraining knowledge\nas newer models incorporate more recent information during pretraining, making\nit difficult to distinguish evidence-based reasoning from recall. We introduce\nNeoQA (News Events for Out-of-training Question Answering), a benchmark\ndesigned to address this issue. To construct NeoQA, we generated timelines and\nknowledge bases of fictional news events and entities along with news articles\nand Q\\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring\nthat no prior evidence exists in their training data. We propose our dataset as\na new platform for evaluating evidence-based question answering, as it requires\nLLMs to generate responses exclusively from retrieved evidence and only when\nsufficient evidence is available. NeoQA enables controlled evaluation across\nvarious evidence scenarios, including cases with missing or misleading details.\nOur findings indicate that LLMs struggle to distinguish subtle mismatches\nbetween questions and evidence, and suffer from short-cut reasoning when key\ninformation required to answer a question is missing from the evidence,\nunderscoring key limitations in evidence-based reasoning."
                },
                "authors": [
                    {
                        "name": "Max Glockner"
                    },
                    {
                        "name": "Xiang Jiang"
                    },
                    {
                        "name": "Leonardo F. R. Ribeiro"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Markus Dreyer"
                    }
                ],
                "author_detail": {
                    "name": "Markus Dreyer"
                },
                "author": "Markus Dreyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05375v2",
                "updated": "2025-05-09T10:51:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    51,
                    13,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-08T16:09:40Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    16,
                    9,
                    40,
                    3,
                    128,
                    0
                ],
                "title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural\n  Networks"
                },
                "summary": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN."
                },
                "authors": [
                    {
                        "name": "Kejie Zhao"
                    },
                    {
                        "name": "Wenjia Hua"
                    },
                    {
                        "name": "Aiersi Tuerhong"
                    },
                    {
                        "name": "Luziwei Leng"
                    },
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Qinghai Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qinghai Guo"
                },
                "author": "Qinghai Guo",
                "arxiv_comment": "Accepted by IJCNN 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, including reprinting/republishing this material for advertising or\n  promotional purposes, collecting new collected works for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05946v1",
                "updated": "2025-05-09T10:43:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    43,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T10:43:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    10,
                    43,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Elastic Weight Consolidation for Full-Parameter Continual Pre-Training\n  of Gemma2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic Weight Consolidation for Full-Parameter Continual Pre-Training\n  of Gemma2"
                },
                "summary": "This technical report describes an experiment on autoregressive pre-training\nof Gemma2 2 billion parameter large language model (LLM) with 10\\% on the\nLithuanian language component of CulturaX from the point of view of continual\nlearning. We apply elastic weight consolidation (EWC) to the full set of the\nmodel's parameters and investigate language understanding benchmarks,\nconsisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande\nsets (both in English and Lithuanian versions), and perplexity benchmarks. We\nempirically demonstrate that EWC regularisation allows us not only to mitigate\ncatastrophic forgetting effects but also that it is potentially beneficial for\nlearning of the new task with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report describes an experiment on autoregressive pre-training\nof Gemma2 2 billion parameter large language model (LLM) with 10\\% on the\nLithuanian language component of CulturaX from the point of view of continual\nlearning. We apply elastic weight consolidation (EWC) to the full set of the\nmodel's parameters and investigate language understanding benchmarks,\nconsisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande\nsets (both in English and Lithuanian versions), and perplexity benchmarks. We\nempirically demonstrate that EWC regularisation allows us not only to mitigate\ncatastrophic forgetting effects but also that it is potentially beneficial for\nlearning of the new task with LLMs."
                },
                "authors": [
                    {
                        "name": "Vytenis Šliogeris"
                    },
                    {
                        "name": "Povilas Daniušis"
                    },
                    {
                        "name": "Artūras Nakvosas"
                    }
                ],
                "author_detail": {
                    "name": "Artūras Nakvosas"
                },
                "author": "Artūras Nakvosas",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05922v1",
                "updated": "2025-05-09T09:54:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    54,
                    7,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T09:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    54,
                    7,
                    4,
                    129,
                    0
                ],
                "title": "CAPE: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPE: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy"
                },
                "summary": "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works."
                },
                "authors": [
                    {
                        "name": "Haoqi Wu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yan"
                },
                "author": "Qiang Yan",
                "arxiv_comment": "to be published in ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11963v2",
                "updated": "2025-05-09T09:23:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    23,
                    22,
                    4,
                    129,
                    0
                ],
                "published": "2024-07-16T17:59:06Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    17,
                    59,
                    6,
                    1,
                    198,
                    0
                ],
                "title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense\n  Context?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense\n  Context?"
                },
                "summary": "The capability of large language models to handle long-context information is\ncrucial across various real-world applications. Existing evaluation methods\noften rely either on real-world long texts, making it difficult to exclude the\ninfluence of models' inherent knowledge, or introduce irrelevant filler content\nto artificially achieve target lengths, reducing assessment effectiveness. To\naddress these limitations, we introduce NeedleBench, a synthetic framework for\nassessing retrieval and reasoning performance in bilingual long-context tasks\nwith adaptive context lengths. NeedleBench systematically embeds key data\npoints at varying depths to rigorously test model capabilities. Tasks are\ncategorized into two scenarios: information-sparse, featuring minimal relevant\ndetails within extensive irrelevant text to simulate simple retrieval tasks;\nand information-dense (the Ancestral Trace Challenge), where relevant\ninformation is continuously distributed throughout the context to simulate\ncomplex reasoning tasks. Our experiments reveal that although recent reasoning\nmodels like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning, they\nstruggle with continuous retrieval and reasoning in information-dense\nscenarios, even at shorter context lengths. We also characterize a phenomenon\ntermed 'under-thinking', where models prematurely conclude reasoning despite\navailable information. NeedleBench thus provides critical insights and targeted\ntools essential for evaluating and improving LLMs' long-context capabilities.\nAll resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capability of large language models to handle long-context information is\ncrucial across various real-world applications. Existing evaluation methods\noften rely either on real-world long texts, making it difficult to exclude the\ninfluence of models' inherent knowledge, or introduce irrelevant filler content\nto artificially achieve target lengths, reducing assessment effectiveness. To\naddress these limitations, we introduce NeedleBench, a synthetic framework for\nassessing retrieval and reasoning performance in bilingual long-context tasks\nwith adaptive context lengths. NeedleBench systematically embeds key data\npoints at varying depths to rigorously test model capabilities. Tasks are\ncategorized into two scenarios: information-sparse, featuring minimal relevant\ndetails within extensive irrelevant text to simulate simple retrieval tasks;\nand information-dense (the Ancestral Trace Challenge), where relevant\ninformation is continuously distributed throughout the context to simulate\ncomplex reasoning tasks. Our experiments reveal that although recent reasoning\nmodels like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning, they\nstruggle with continuous retrieval and reasoning in information-dense\nscenarios, even at shorter context lengths. We also characterize a phenomenon\ntermed 'under-thinking', where models prematurely conclude reasoning despite\navailable information. NeedleBench thus provides critical insights and targeted\ntools essential for evaluating and improving LLMs' long-context capabilities.\nAll resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass."
                },
                "authors": [
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "v2: updated with tested models and Multi-Needle Reasoning\n  implementation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12345v2",
                "updated": "2025-05-09T09:12:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    12,
                    39,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-15T16:58:11Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    58,
                    11,
                    1,
                    105,
                    0
                ],
                "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Urban Science: Scaling Causal Inference with Large Language\n  Models"
                },
                "summary": "Urban causal research is essential for understanding the complex dynamics of\ncities and informing evidence-based policies. However, it is challenged by the\ninefficiency and bias of hypothesis generation, barriers to multimodal data\ncomplexity, and the methodological fragility of causal experimentation. Recent\nadvances in large language models (LLMs) present an opportunity to rethink how\nurban causal analysis is conducted. This Perspective examines current urban\ncausal research by analyzing taxonomies that categorize research topics, data\nsources, and methodological approaches to identify structural gaps. We then\nintroduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four\ndistinct modular agents responsible for hypothesis generation, data\nengineering, experiment design and execution, and results interpretation with\npolicy recommendations. We propose evaluation criteria for rigor and\ntransparency and reflect on implications for human-AI collaboration, equity,\nand accountability. We call for a new research agenda that embraces\nAI-augmented workflows not as replacements for human expertise but as tools to\nbroaden participation, improve reproducibility, and unlock more inclusive forms\nof urban causal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban causal research is essential for understanding the complex dynamics of\ncities and informing evidence-based policies. However, it is challenged by the\ninefficiency and bias of hypothesis generation, barriers to multimodal data\ncomplexity, and the methodological fragility of causal experimentation. Recent\nadvances in large language models (LLMs) present an opportunity to rethink how\nurban causal analysis is conducted. This Perspective examines current urban\ncausal research by analyzing taxonomies that categorize research topics, data\nsources, and methodological approaches to identify structural gaps. We then\nintroduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four\ndistinct modular agents responsible for hypothesis generation, data\nengineering, experiment design and execution, and results interpretation with\npolicy recommendations. We propose evaluation criteria for rigor and\ntransparency and reflect on implications for human-AI collaboration, equity,\nand accountability. We call for a new research agenda that embraces\nAI-augmented workflows not as replacements for human expertise but as tools to\nbroaden participation, improve reproducibility, and unlock more inclusive forms\nof urban causal reasoning."
                },
                "authors": [
                    {
                        "name": "Yutong Xia"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Yunhan Zheng"
                    },
                    {
                        "name": "Yihong Tang"
                    },
                    {
                        "name": "Dingyi Zhuang"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Shenhao Wang"
                    },
                    {
                        "name": "Cathy Wu"
                    },
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Roger Zimmermann"
                    },
                    {
                        "name": "Jinhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jinhua Zhao"
                },
                "author": "Jinhua Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05863v1",
                "updated": "2025-05-09T07:57:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    57,
                    10,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T07:57:10Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    57,
                    10,
                    4,
                    129,
                    0
                ],
                "title": "Evolutionary ecology of words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary ecology of words"
                },
                "summary": "We propose a model for the evolutionary ecology of words as one attempt to\nextend evolutionary game theory and agent-based models by utilizing the rich\nlinguistic expressions of Large Language Models (LLMs). Our model enables the\nemergence and evolution of diverse and infinite options for interactions among\nagents. Within the population, each agent possesses a short word (or phrase)\ngenerated by an LLM and moves within a spatial environment. When agents become\nadjacent, the outcome of their interaction is determined by the LLM based on\nthe relationship between their words, with the loser's word being replaced by\nthe winner's. Word mutations, also based on LLM outputs, may occur. We\nconducted preliminary experiments assuming that ``strong animal species\" would\nsurvive. The results showed that from an initial population consisting of\nwell-known species, many species emerged both gradually and in a punctuated\nequilibrium manner. Each trial demonstrated the unique evolution of diverse\npopulations, with one type of large species becoming dominant, such as\nterrestrial animals, marine life, or extinct species, which were ecologically\nspecialized and adapted ones across diverse extreme habitats. We also conducted\na long-term experiment with a large population, demonstrating the emergence and\ncoexistence of diverse species.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a model for the evolutionary ecology of words as one attempt to\nextend evolutionary game theory and agent-based models by utilizing the rich\nlinguistic expressions of Large Language Models (LLMs). Our model enables the\nemergence and evolution of diverse and infinite options for interactions among\nagents. Within the population, each agent possesses a short word (or phrase)\ngenerated by an LLM and moves within a spatial environment. When agents become\nadjacent, the outcome of their interaction is determined by the LLM based on\nthe relationship between their words, with the loser's word being replaced by\nthe winner's. Word mutations, also based on LLM outputs, may occur. We\nconducted preliminary experiments assuming that ``strong animal species\" would\nsurvive. The results showed that from an initial population consisting of\nwell-known species, many species emerged both gradually and in a punctuated\nequilibrium manner. Each trial demonstrated the unique evolution of diverse\npopulations, with one type of large species becoming dominant, such as\nterrestrial animals, marine life, or extinct species, which were ecologically\nspecialized and adapted ones across diverse extreme habitats. We also conducted\na long-term experiment with a large population, demonstrating the emergence and\ncoexistence of diverse species."
                },
                "authors": [
                    {
                        "name": "Reiji Suzuki"
                    },
                    {
                        "name": "Takaya Arita"
                    }
                ],
                "author_detail": {
                    "name": "Takaya Arita"
                },
                "author": "Takaya Arita",
                "arxiv_doi": "10.1109/ALIFE-CIS64968.2025.10979831",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ALIFE-CIS64968.2025.10979831",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 5 figures. Preprint of the paper published in Proceedings of\n  2025 IEEE Symposium on Computational Intelligence in Artificial Life and\n  Cooperative Intelligent Systems (ALIFE-CIS)",
                "arxiv_journal_ref": "Proceedings of 2025 IEEE Symposium on Computational Intelligence\n  in Artificial Life and Cooperative Intelligent Systems (ALIFE-CIS), pp. 1-7\n  (2025)",
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92B20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05857v1",
                "updated": "2025-05-09T07:51:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    51,
                    36,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T07:51:36Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    51,
                    36,
                    4,
                    129,
                    0
                ],
                "title": "Mixed-Integer Optimization for Responsible Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Integer Optimization for Responsible Machine Learning"
                },
                "summary": "In the last few decades, Machine Learning (ML) has achieved significant\nsuccess across domains ranging from healthcare, sustainability, and the social\nsciences, to criminal justice and finance. But its deployment in increasingly\nsophisticated, critical, and sensitive areas affecting individuals, the groups\nthey belong to, and society as a whole raises critical concerns around\nfairness, transparency, robustness, and privacy, among others. As the\ncomplexity and scale of ML systems and of the settings in which they are\ndeployed grow, so does the need for responsible ML methods that address these\nchallenges while providing guaranteed performance in deployment.\n  Mixed-integer optimization (MIO) offers a powerful framework for embedding\nresponsible ML considerations directly into the learning process while\nmaintaining performance. For example, it enables learning of inherently\ntransparent models that can conveniently incorporate fairness or other domain\nspecific constraints. This tutorial paper provides an accessible and\ncomprehensive introduction to this topic discussing both theoretical and\npractical aspects. It outlines some of the core principles of responsible ML,\ntheir importance in applications, and the practical utility of MIO for building\nML models that align with these principles. Through examples and mathematical\nformulations, it illustrates practical strategies and available tools for\nefficiently solving MIO problems for responsible ML. It concludes with a\ndiscussion on current limitations and open research questions, providing\nsuggestions for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few decades, Machine Learning (ML) has achieved significant\nsuccess across domains ranging from healthcare, sustainability, and the social\nsciences, to criminal justice and finance. But its deployment in increasingly\nsophisticated, critical, and sensitive areas affecting individuals, the groups\nthey belong to, and society as a whole raises critical concerns around\nfairness, transparency, robustness, and privacy, among others. As the\ncomplexity and scale of ML systems and of the settings in which they are\ndeployed grow, so does the need for responsible ML methods that address these\nchallenges while providing guaranteed performance in deployment.\n  Mixed-integer optimization (MIO) offers a powerful framework for embedding\nresponsible ML considerations directly into the learning process while\nmaintaining performance. For example, it enables learning of inherently\ntransparent models that can conveniently incorporate fairness or other domain\nspecific constraints. This tutorial paper provides an accessible and\ncomprehensive introduction to this topic discussing both theoretical and\npractical aspects. It outlines some of the core principles of responsible ML,\ntheir importance in applications, and the practical utility of MIO for building\nML models that align with these principles. Through examples and mathematical\nformulations, it illustrates practical strategies and available tools for\nefficiently solving MIO problems for responsible ML. It concludes with a\ndiscussion on current limitations and open research questions, providing\nsuggestions for future work."
                },
                "authors": [
                    {
                        "name": "Nathan Justin"
                    },
                    {
                        "name": "Qingshi Sun"
                    },
                    {
                        "name": "Andrés Gómez"
                    },
                    {
                        "name": "Phebe Vayanos"
                    }
                ],
                "author_detail": {
                    "name": "Phebe Vayanos"
                },
                "author": "Phebe Vayanos",
                "arxiv_comment": "56 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05440v2",
                "updated": "2025-05-09T07:47:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    47,
                    44,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-08T17:31:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    31,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation"
                },
                "summary": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\n\\textbf{EcoAgent}, an \\textbf{E}dge-\\textbf{C}loud c\\textbf{O}llaborative\nmulti-agent framework for mobile automation. EcoAgent features a closed-loop\ncollaboration among a cloud-based Planning Agent and two edge-based agents: the\nExecution Agent for action execution and the Observation Agent for verifying\noutcomes. The Observation Agent uses a Pre-Understanding Module to compress\nscreen images into concise text, reducing token usage and communication\noverhead. In case of failure, the Planning Agent retrieves screen history\nthrough a Memory Module and replans via a Reflection Module. Experiments on\nAndroidWorld show that EcoAgent achieves task success rates comparable to\ncloud-based mobile agents while significantly reducing MLLM token consumption,\nenabling efficient and practical mobile automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\n\\textbf{EcoAgent}, an \\textbf{E}dge-\\textbf{C}loud c\\textbf{O}llaborative\nmulti-agent framework for mobile automation. EcoAgent features a closed-loop\ncollaboration among a cloud-based Planning Agent and two edge-based agents: the\nExecution Agent for action execution and the Observation Agent for verifying\noutcomes. The Observation Agent uses a Pre-Understanding Module to compress\nscreen images into concise text, reducing token usage and communication\noverhead. In case of failure, the Planning Agent retrieves screen history\nthrough a Memory Module and replans via a Reflection Module. Experiments on\nAndroidWorld show that EcoAgent achieves task success rates comparable to\ncloud-based mobile agents while significantly reducing MLLM token consumption,\nenabling efficient and practical mobile automation."
                },
                "authors": [
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Xavier Hu"
                    },
                    {
                        "name": "Yurun Chen"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05849v1",
                "updated": "2025-05-09T07:40:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    40,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T07:40:17Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    40,
                    17,
                    4,
                    129,
                    0
                ],
                "title": "AgentXploit: End-to-End Redteaming of Black-Box AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentXploit: End-to-End Redteaming of Black-Box AI Agents"
                },
                "summary": "The strong planning and reasoning capabilities of Large Language Models\n(LLMs) have fostered the development of agent-based systems capable of\nleveraging external tools and interacting with increasingly complex\nenvironments. However, these powerful features also introduce a critical\nsecurity risk: indirect prompt injection, a sophisticated attack vector that\ncompromises the core of these agents, the LLM, by manipulating contextual\ninformation rather than direct user prompts. In this work, we propose a generic\nblack-box fuzzing framework, AgentXploit, designed to automatically discover\nand exploit indirect prompt injection vulnerabilities across diverse LLM\nagents. Our approach starts by constructing a high-quality initial seed corpus,\nthen employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)\nto iteratively refine inputs, thereby maximizing the likelihood of uncovering\nagent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo\nand VWA-adv, where it achieves 71% and 70% success rates against agents based\non o3-mini and GPT-4o, respectively, nearly doubling the performance of\nbaseline attacks. Moreover, AgentXploit exhibits strong transferability across\nunseen tasks and internal LLMs, as well as promising results against defenses.\nBeyond benchmark evaluations, we apply our attacks in real-world environments,\nsuccessfully misleading agents to navigate to arbitrary URLs, including\nmalicious sites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong planning and reasoning capabilities of Large Language Models\n(LLMs) have fostered the development of agent-based systems capable of\nleveraging external tools and interacting with increasingly complex\nenvironments. However, these powerful features also introduce a critical\nsecurity risk: indirect prompt injection, a sophisticated attack vector that\ncompromises the core of these agents, the LLM, by manipulating contextual\ninformation rather than direct user prompts. In this work, we propose a generic\nblack-box fuzzing framework, AgentXploit, designed to automatically discover\nand exploit indirect prompt injection vulnerabilities across diverse LLM\nagents. Our approach starts by constructing a high-quality initial seed corpus,\nthen employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)\nto iteratively refine inputs, thereby maximizing the likelihood of uncovering\nagent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo\nand VWA-adv, where it achieves 71% and 70% success rates against agents based\non o3-mini and GPT-4o, respectively, nearly doubling the performance of\nbaseline attacks. Moreover, AgentXploit exhibits strong transferability across\nunseen tasks and internal LLMs, as well as promising results against defenses.\nBeyond benchmark evaluations, we apply our attacks in real-world environments,\nsuccessfully misleading agents to navigate to arbitrary URLs, including\nmalicious sites."
                },
                "authors": [
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Vincent Siu"
                    },
                    {
                        "name": "Zhe Ye"
                    },
                    {
                        "name": "Tianneng Shi"
                    },
                    {
                        "name": "Yuzhou Nie"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Chenguang Wang"
                    },
                    {
                        "name": "Wenbo Guo"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05832v1",
                "updated": "2025-05-09T07:00:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    0,
                    11,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T07:00:11Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    0,
                    11,
                    4,
                    129,
                    0
                ],
                "title": "Augmented Body Communicator: Enhancing daily body expression for people\n  with upper limb limitations through LLM and a robotic arm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Body Communicator: Enhancing daily body expression for people\n  with upper limb limitations through LLM and a robotic arm"
                },
                "summary": "Individuals with upper limb movement limitations face challenges in\ninteracting with others. Although robotic arms are currently used primarily for\nfunctional tasks, there is considerable potential to explore ways to enhance\nusers' body language capabilities during social interactions. This paper\nintroduces an Augmented Body Communicator system that integrates robotic arms\nand a large language model. Through the incorporation of kinetic memory,\ndisabled users and their supporters can collaboratively design actions for the\nrobot arm. The LLM system then provides suggestions on the most suitable action\nbased on contextual cues during interactions. The system underwent thorough\nuser testing with six participants who have conditions affecting upper limb\nmobility. Results indicate that the system improves users' ability to express\nthemselves. Based on our findings, we offer recommendations for developing\nrobotic arms that support disabled individuals with body language capabilities\nand functional tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individuals with upper limb movement limitations face challenges in\ninteracting with others. Although robotic arms are currently used primarily for\nfunctional tasks, there is considerable potential to explore ways to enhance\nusers' body language capabilities during social interactions. This paper\nintroduces an Augmented Body Communicator system that integrates robotic arms\nand a large language model. Through the incorporation of kinetic memory,\ndisabled users and their supporters can collaboratively design actions for the\nrobot arm. The LLM system then provides suggestions on the most suitable action\nbased on contextual cues during interactions. The system underwent thorough\nuser testing with six participants who have conditions affecting upper limb\nmobility. Results indicate that the system improves users' ability to express\nthemselves. Based on our findings, we offer recommendations for developing\nrobotic arms that support disabled individuals with body language capabilities\nand functional tasks."
                },
                "authors": [
                    {
                        "name": "Songchen Zhou"
                    },
                    {
                        "name": "Mark Armstrong"
                    },
                    {
                        "name": "Giulia Barbareschi"
                    },
                    {
                        "name": "Toshihiro Ajioka"
                    },
                    {
                        "name": "Zheng Hu"
                    },
                    {
                        "name": "Ryoichi Ando"
                    },
                    {
                        "name": "Kentaro Yoshifuji"
                    },
                    {
                        "name": "Masatane Muto"
                    },
                    {
                        "name": "Kouta Minamizawa"
                    }
                ],
                "author_detail": {
                    "name": "Kouta Minamizawa"
                },
                "author": "Kouta Minamizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05831v1",
                "updated": "2025-05-09T06:58:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    58,
                    43,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T06:58:43Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    58,
                    43,
                    4,
                    129,
                    0
                ],
                "title": "Oh F**k! How Do People Feel about Robots that Leverage Profanity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oh F**k! How Do People Feel about Robots that Leverage Profanity?"
                },
                "summary": "Profanity is nearly as old as language itself, and cursing has become\nparticularly ubiquitous within the last century. At the same time, robots in\npersonal and service applications are often overly polite, even though past\nwork demonstrates the potential benefits of robot norm-breaking. Thus, we\nbecame curious about robots using curse words in error scenarios as a means for\nimproving social perceptions by human users. We investigated this idea using\nthree phases of exploratory work: an online video-based study (N = 76) with a\nstudent pool, an online video-based study (N = 98) in the general U.S.\npopulation, and an in-person proof-of-concept deployment (N = 52) in a campus\nspace, each of which included the following conditions: no-speech,\nnon-expletive error response, and expletive error response. A surprising result\nin the outcomes for all three studies was that although verbal acknowledgment\nof an error was typically beneficial (as expected based on prior work), few\nsignificant differences appeared between the non-expletive and expletive error\nacknowledgment conditions (counter to our expectations). Within the cultural\ncontext of our work, the U.S., it seems that many users would likely not mind\nif robots curse, and may even find it relatable and humorous. This work signals\na promising and mischievous design space that challenges typical robot\ncharacter design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profanity is nearly as old as language itself, and cursing has become\nparticularly ubiquitous within the last century. At the same time, robots in\npersonal and service applications are often overly polite, even though past\nwork demonstrates the potential benefits of robot norm-breaking. Thus, we\nbecame curious about robots using curse words in error scenarios as a means for\nimproving social perceptions by human users. We investigated this idea using\nthree phases of exploratory work: an online video-based study (N = 76) with a\nstudent pool, an online video-based study (N = 98) in the general U.S.\npopulation, and an in-person proof-of-concept deployment (N = 52) in a campus\nspace, each of which included the following conditions: no-speech,\nnon-expletive error response, and expletive error response. A surprising result\nin the outcomes for all three studies was that although verbal acknowledgment\nof an error was typically beneficial (as expected based on prior work), few\nsignificant differences appeared between the non-expletive and expletive error\nacknowledgment conditions (counter to our expectations). Within the cultural\ncontext of our work, the U.S., it seems that many users would likely not mind\nif robots curse, and may even find it relatable and humorous. This work signals\na promising and mischievous design space that challenges typical robot\ncharacter design."
                },
                "authors": [
                    {
                        "name": "Madison R. Shippy"
                    },
                    {
                        "name": "Brian J. Zhang"
                    },
                    {
                        "name": "Naomi T. Fitter"
                    }
                ],
                "author_detail": {
                    "name": "Naomi T. Fitter"
                },
                "author": "Naomi T. Fitter",
                "arxiv_comment": "Under review for the 2025 IEEE RO-MAN Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05829v1",
                "updated": "2025-05-09T06:56:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T06:56:17Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition"
                },
                "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Keyi Li"
                    },
                    {
                        "name": "Yifan Jia"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Yufei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ma"
                },
                "author": "Yufei Ma",
                "arxiv_comment": "accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04852v2",
                "updated": "2025-05-09T06:32:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    32,
                    8,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-07T23:30:27Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    23,
                    30,
                    27,
                    2,
                    127,
                    0
                ],
                "title": "PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer\n  Rust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer\n  Rust"
                },
                "summary": "There has been a growing interest in translating C code to Rust due to Rust's\nrobust memory and thread safety guarantees. Tools such as C2RUST enable\nsyntax-guided transpilation from C to semantically equivalent Rust code.\nHowever, the resulting Rust programs often rely heavily on unsafe\nconstructs--particularly raw pointers--which undermines Rust's safety\nguarantees. This paper aims to improve the memory safety of Rust programs\ngenerated by C2RUST by eliminating raw pointers. Specifically, we propose a\npeephole raw pointer rewriting technique that lifts raw pointers in individual\nfunctions to appropriate Rust data structures. Technically, PR2 employs\ndecision-tree-based prompting to guide the pointer lifting process.\nAdditionally, it leverages code change analysis to guide the repair of errors\nintroduced during rewriting, effectively addressing errors encountered during\ncompilation and test case execution. We implement PR2 as a prototype and\nevaluate it using gpt-4o-mini on 28 real-world C projects. The results show\nthat PR2 successfully eliminates 13.22% of local raw pointers across these\nprojects, significantly enhancing the safety of the translated Rust code. On\naverage, PR2 completes the transformation of a project in 5.44 hours, at an\naverage cost of $1.46.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a growing interest in translating C code to Rust due to Rust's\nrobust memory and thread safety guarantees. Tools such as C2RUST enable\nsyntax-guided transpilation from C to semantically equivalent Rust code.\nHowever, the resulting Rust programs often rely heavily on unsafe\nconstructs--particularly raw pointers--which undermines Rust's safety\nguarantees. This paper aims to improve the memory safety of Rust programs\ngenerated by C2RUST by eliminating raw pointers. Specifically, we propose a\npeephole raw pointer rewriting technique that lifts raw pointers in individual\nfunctions to appropriate Rust data structures. Technically, PR2 employs\ndecision-tree-based prompting to guide the pointer lifting process.\nAdditionally, it leverages code change analysis to guide the repair of errors\nintroduced during rewriting, effectively addressing errors encountered during\ncompilation and test case execution. We implement PR2 as a prototype and\nevaluate it using gpt-4o-mini on 28 real-world C projects. The results show\nthat PR2 successfully eliminates 13.22% of local raw pointers across these\nprojects, significantly enhancing the safety of the translated Rust code. On\naverage, PR2 completes the transformation of a project in 5.44 hours, at an\naverage cost of $1.46."
                },
                "authors": [
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chengpeng Wang"
                    },
                    {
                        "name": "Pengxiang Huang"
                    },
                    {
                        "name": "Xuwei Liu"
                    },
                    {
                        "name": "Mingwei Zheng"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00290v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00290v5",
                "updated": "2025-05-09T05:37:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    37,
                    54,
                    4,
                    129,
                    0
                ],
                "published": "2025-02-01T03:18:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    18,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "Estimating LLM Uncertainty with Evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating LLM Uncertainty with Evidence"
                },
                "summary": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise."
                },
                "authors": [
                    {
                        "name": "Huan Ma"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00290v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00290v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05800v1",
                "updated": "2025-05-09T05:32:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    32,
                    40,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T05:32:40Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    32,
                    40,
                    4,
                    129,
                    0
                ],
                "title": "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language\n  Action Models for Unseen Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language\n  Action Models for Unseen Tasks"
                },
                "summary": "Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint\nspace trajectory of a robot manipulator. Robots must possess semantic and\nvisual perception abilities to transform real-world mappings of their workspace\ninto the low-level control necessary for object manipulation. Recent work has\ndemonstrated the capabilities of fine-tuning large Vision-Language Models\n(VLMs) to learn the mapping between RGB images, language instructions, and\njoint space control. These models typically take as input RGB images of the\nworkspace and language instructions, and are trained on large datasets of\nteleoperated robot demonstrations. In this work, we explore methods to improve\nthe scene context awareness of a popular recent Vision-Language-Action model by\nintegrating chain-of-thought reasoning, depth perception, and task-oriented\nregion of interest detection. Our experiments in the LIBERO simulation\nenvironment show that our proposed model, 3D-CAVLA, improves the success rate\nacross various LIBERO task suites, achieving an average success rate of\n98.1$\\%$. We also evaluate the zero-shot capabilities of our method,\ndemonstrating that 3D scene awareness leads to robust learning and adaptation\nfor completely unseen tasks. 3D-CAVLA achieves an absolute improvement of\n8.8$\\%$ on unseen tasks. We will open-source our code and the unseen tasks\ndataset to promote community-driven research here: https://3d-cavla.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint\nspace trajectory of a robot manipulator. Robots must possess semantic and\nvisual perception abilities to transform real-world mappings of their workspace\ninto the low-level control necessary for object manipulation. Recent work has\ndemonstrated the capabilities of fine-tuning large Vision-Language Models\n(VLMs) to learn the mapping between RGB images, language instructions, and\njoint space control. These models typically take as input RGB images of the\nworkspace and language instructions, and are trained on large datasets of\nteleoperated robot demonstrations. In this work, we explore methods to improve\nthe scene context awareness of a popular recent Vision-Language-Action model by\nintegrating chain-of-thought reasoning, depth perception, and task-oriented\nregion of interest detection. Our experiments in the LIBERO simulation\nenvironment show that our proposed model, 3D-CAVLA, improves the success rate\nacross various LIBERO task suites, achieving an average success rate of\n98.1$\\%$. We also evaluate the zero-shot capabilities of our method,\ndemonstrating that 3D scene awareness leads to robust learning and adaptation\nfor completely unseen tasks. 3D-CAVLA achieves an absolute improvement of\n8.8$\\%$ on unseen tasks. We will open-source our code and the unseen tasks\ndataset to promote community-driven research here: https://3d-cavla.github.io"
                },
                "authors": [
                    {
                        "name": "Vineet Bhat"
                    },
                    {
                        "name": "Yu-Hsiang Lan"
                    },
                    {
                        "name": "Prashanth Krishnamurthy"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Farshad Khorrami"
                    }
                ],
                "author_detail": {
                    "name": "Farshad Khorrami"
                },
                "author": "Farshad Khorrami",
                "arxiv_comment": "Accepted at the 1st Workshop on 3D LLM/VLA, CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05799v1",
                "updated": "2025-05-09T05:32:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    32,
                    21,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T05:32:21Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    32,
                    21,
                    4,
                    129,
                    0
                ],
                "title": "MxMoE: Mixed-precision Quantization for MoE with Accuracy and\n  Performance Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MxMoE: Mixed-precision Quantization for MoE with Accuracy and\n  Performance Co-Design"
                },
                "summary": "Mixture-of-Experts (MoE) models face deployment challenges due to their large\nparameter counts and computational demands. We explore quantization for MoE\nmodels and highlight two key insights: 1) linear blocks exhibit varying\nquantization sensitivity, and 2) divergent expert activation frequencies create\nheterogeneous computational characteristics. Based on these observations, we\nintroduce MxMoE, a mixed-precision optimization framework for MoE models that\nconsiders both algorithmic and system perspectives. MxMoE navigates the design\nspace defined by parameter sensitivity, expert activation dynamics, and\nhardware resources to derive efficient mixed-precision configurations.\nAdditionally, MxMoE automatically generates optimized mixed-precision GroupGEMM\nkernels, enabling parallel execution of GEMMs with different precisions.\nEvaluations show that MxMoE outperforms existing methods, achieving 2.4 lower\nWikitext-2 perplexity than GPTQ at 2.25-bit and delivering up to 3.4x speedup\nover full precision, as well as up to 29.4% speedup over uniform quantization\nat equivalent accuracy with 5-bit weight-activation quantization. Our code is\navailable at https://github.com/cat538/MxMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models face deployment challenges due to their large\nparameter counts and computational demands. We explore quantization for MoE\nmodels and highlight two key insights: 1) linear blocks exhibit varying\nquantization sensitivity, and 2) divergent expert activation frequencies create\nheterogeneous computational characteristics. Based on these observations, we\nintroduce MxMoE, a mixed-precision optimization framework for MoE models that\nconsiders both algorithmic and system perspectives. MxMoE navigates the design\nspace defined by parameter sensitivity, expert activation dynamics, and\nhardware resources to derive efficient mixed-precision configurations.\nAdditionally, MxMoE automatically generates optimized mixed-precision GroupGEMM\nkernels, enabling parallel execution of GEMMs with different precisions.\nEvaluations show that MxMoE outperforms existing methods, achieving 2.4 lower\nWikitext-2 perplexity than GPTQ at 2.25-bit and delivering up to 3.4x speedup\nover full precision, as well as up to 29.4% speedup over uniform quantization\nat equivalent accuracy with 5-bit weight-activation quantization. Our code is\navailable at https://github.com/cat538/MxMoE."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14851v2",
                "updated": "2025-05-09T05:26:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    26,
                    43,
                    4,
                    129,
                    0
                ],
                "published": "2025-01-24T15:49:10Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    49,
                    10,
                    4,
                    24,
                    0
                ],
                "title": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning\n  in Large Language Models"
                },
                "summary": "Logical reasoning is a critical component of Large Language Models (LLMs),\nand substantial research efforts in recent years have aimed to enhance their\ndeductive reasoning capabilities. However, existing deductive reasoning\nbenchmarks, which are crucial for evaluating and advancing LLMs, are inadequate\ndue to their lack of task complexity, presence of prior knowledge as a\nconfounder, and superficial error analysis. To address these deficiencies, we\nintroduce JustLogic, a synthetically generated deductive reasoning benchmark\ndesigned for rigorous evaluation of LLMs. JustLogic is (i) highly complex,\ncapable of generating a diverse range of linguistic patterns, vocabulary, and\nargument structures; (ii) prior knowledge independent, eliminating the\nadvantage of models possessing prior knowledge and ensuring that only deductive\nreasoning is used to answer questions; and (iii) capable of in-depth error\nanalysis on the heterogeneous effects of reasoning depth and argument form on\nmodel accuracy. Our experimental results on JustLogic reveal that (i)\nstate-of-the-art (SOTA) reasoning LLMs perform on par or better than the human\naverage but significantly worse than the human ceiling, and (ii) SOTA\nnon-reasoning models still underperform the human average. All code and data\nare available at https://github.com/michaelchen-lab/JustLogic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reasoning is a critical component of Large Language Models (LLMs),\nand substantial research efforts in recent years have aimed to enhance their\ndeductive reasoning capabilities. However, existing deductive reasoning\nbenchmarks, which are crucial for evaluating and advancing LLMs, are inadequate\ndue to their lack of task complexity, presence of prior knowledge as a\nconfounder, and superficial error analysis. To address these deficiencies, we\nintroduce JustLogic, a synthetically generated deductive reasoning benchmark\ndesigned for rigorous evaluation of LLMs. JustLogic is (i) highly complex,\ncapable of generating a diverse range of linguistic patterns, vocabulary, and\nargument structures; (ii) prior knowledge independent, eliminating the\nadvantage of models possessing prior knowledge and ensuring that only deductive\nreasoning is used to answer questions; and (iii) capable of in-depth error\nanalysis on the heterogeneous effects of reasoning depth and argument form on\nmodel accuracy. Our experimental results on JustLogic reveal that (i)\nstate-of-the-art (SOTA) reasoning LLMs perform on par or better than the human\naverage but significantly worse than the human ceiling, and (ii) SOTA\nnon-reasoning models still underperform the human average. All code and data\nare available at https://github.com/michaelchen-lab/JustLogic"
                },
                "authors": [
                    {
                        "name": "Michael K. Chen"
                    },
                    {
                        "name": "Xikun Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05794v1",
                "updated": "2025-05-09T05:19:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    19,
                    14,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T05:19:14Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    19,
                    14,
                    4,
                    129,
                    0
                ],
                "title": "What Is Next for LLMs? Next-Generation AI Computing Hardware Using\n  Photonic Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Is Next for LLMs? Next-Generation AI Computing Hardware Using\n  Photonic Chips"
                },
                "summary": "Large language models (LLMs) are rapidly pushing the limits of contemporary\ncomputing hardware. For example, training GPT-3 has been estimated to consume\naround 1300 MWh of electricity, and projections suggest future models may\nrequire city-scale (gigawatt) power budgets. These demands motivate exploration\nof computing paradigms beyond conventional von Neumann architectures. This\nreview surveys emerging photonic hardware optimized for next-generation\ngenerative AI computing. We discuss integrated photonic neural network\narchitectures (e.g., Mach-Zehnder interferometer meshes, lasers,\nwavelength-multiplexed microring resonators) that perform ultrafast matrix\noperations. We also examine promising alternative neuromorphic devices,\nincluding spiking neural network circuits and hybrid spintronic-photonic\nsynapses, which combine memory and processing. The integration of\ntwo-dimensional materials (graphene, TMDCs) into silicon photonic platforms is\nreviewed for tunable modulators and on-chip synaptic elements.\nTransformer-based LLM architectures (self-attention and feed-forward layers)\nare analyzed in this context, identifying strategies and challenges for mapping\ndynamic matrix multiplications onto these novel hardware substrates. We then\ndissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and\nLLaMA, highlighting their architectural similarities and differences. We\nsynthesize state-of-the-art components, algorithms, and integration methods,\nhighlighting key advances and open issues in scaling such systems to mega-sized\nLLM models. We find that photonic computing systems could potentially surpass\nelectronic processors by orders of magnitude in throughput and energy\nefficiency, but require breakthroughs in memory, especially for long-context\nwindows and long token sequences, and in storage of ultra-large datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly pushing the limits of contemporary\ncomputing hardware. For example, training GPT-3 has been estimated to consume\naround 1300 MWh of electricity, and projections suggest future models may\nrequire city-scale (gigawatt) power budgets. These demands motivate exploration\nof computing paradigms beyond conventional von Neumann architectures. This\nreview surveys emerging photonic hardware optimized for next-generation\ngenerative AI computing. We discuss integrated photonic neural network\narchitectures (e.g., Mach-Zehnder interferometer meshes, lasers,\nwavelength-multiplexed microring resonators) that perform ultrafast matrix\noperations. We also examine promising alternative neuromorphic devices,\nincluding spiking neural network circuits and hybrid spintronic-photonic\nsynapses, which combine memory and processing. The integration of\ntwo-dimensional materials (graphene, TMDCs) into silicon photonic platforms is\nreviewed for tunable modulators and on-chip synaptic elements.\nTransformer-based LLM architectures (self-attention and feed-forward layers)\nare analyzed in this context, identifying strategies and challenges for mapping\ndynamic matrix multiplications onto these novel hardware substrates. We then\ndissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and\nLLaMA, highlighting their architectural similarities and differences. We\nsynthesize state-of-the-art components, algorithms, and integration methods,\nhighlighting key advances and open issues in scaling such systems to mega-sized\nLLM models. We find that photonic computing systems could potentially surpass\nelectronic processors by orders of magnitude in throughput and energy\nefficiency, but require breakthroughs in memory, especially for long-context\nwindows and long token sequences, and in storage of ultra-large datasets."
                },
                "authors": [
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Wenjie Wei"
                    },
                    {
                        "name": "Qi Xin"
                    },
                    {
                        "name": "Xiaoli Liu"
                    },
                    {
                        "name": "Sixuan Mao"
                    },
                    {
                        "name": "Erik Ma"
                    },
                    {
                        "name": "Zijian Chen"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Zhaoyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoyu Zhang"
                },
                "author": "Zhaoyu Zhang",
                "arxiv_comment": "36 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05786v1",
                "updated": "2025-05-09T05:02:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    2,
                    21,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T05:02:21Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    5,
                    2,
                    21,
                    4,
                    129,
                    0
                ],
                "title": "A Day in Their Shoes: Using LLM-Based Perspective-Taking Interactive\n  Fiction to Reduce Stigma Toward Dirty Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Day in Their Shoes: Using LLM-Based Perspective-Taking Interactive\n  Fiction to Reduce Stigma Toward Dirty Work"
                },
                "summary": "Occupations referred to as \"dirty work\" often face entrenched social stigma,\nwhich adversely affects the mental health of workers in these fields and\nimpedes occupational equity. In this study, we propose a novel Interactive\nFiction (IF) framework powered by Large Language Models (LLMs) to encourage\nperspective-taking and reduce biases against these stigmatized yet essential\nroles. Through an experiment with participants (n = 100) across four such\noccupations, we observed a significant increase in participants' understanding\nof these occupations, as well as a high level of empathy and a strong sense of\nconnection to individuals in these roles. Additionally, qualitative interviews\nwith participants (n = 15) revealed that the LLM-based perspective-taking IF\nenhanced immersion, deepened emotional resonance and empathy toward \"dirty\nwork,\" and allowed participants to experience a sense of professional\nfulfillment in these occupations. However, participants also highlighted\nongoing challenges, such as limited contextual details generated by the LLM and\nthe unintentional reinforcement of existing stereotypes. Overall, our findings\nunderscore that an LLM-based perspective-taking IF framework offers a promising\nand scalable strategy for mitigating stigma and promoting social equity in\nmarginalized professions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occupations referred to as \"dirty work\" often face entrenched social stigma,\nwhich adversely affects the mental health of workers in these fields and\nimpedes occupational equity. In this study, we propose a novel Interactive\nFiction (IF) framework powered by Large Language Models (LLMs) to encourage\nperspective-taking and reduce biases against these stigmatized yet essential\nroles. Through an experiment with participants (n = 100) across four such\noccupations, we observed a significant increase in participants' understanding\nof these occupations, as well as a high level of empathy and a strong sense of\nconnection to individuals in these roles. Additionally, qualitative interviews\nwith participants (n = 15) revealed that the LLM-based perspective-taking IF\nenhanced immersion, deepened emotional resonance and empathy toward \"dirty\nwork,\" and allowed participants to experience a sense of professional\nfulfillment in these occupations. However, participants also highlighted\nongoing challenges, such as limited contextual details generated by the LLM and\nthe unintentional reinforcement of existing stereotypes. Overall, our findings\nunderscore that an LLM-based perspective-taking IF framework offers a promising\nand scalable strategy for mitigating stigma and promoting social equity in\nmarginalized professions."
                },
                "authors": [
                    {
                        "name": "Xiangzhe Yuan"
                    },
                    {
                        "name": "Jiajun Wang"
                    },
                    {
                        "name": "Qian Wan"
                    },
                    {
                        "name": "Siying Hu"
                    }
                ],
                "author_detail": {
                    "name": "Siying Hu"
                },
                "author": "Siying Hu",
                "arxiv_doi": "10.1145/3715275.3732090",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715275.3732090",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.05786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Conference paper for FAccT '25",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21223v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21223v3",
                "updated": "2025-05-09T04:51:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    51,
                    10,
                    4,
                    129,
                    0
                ],
                "published": "2025-03-27T07:28:30Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    28,
                    30,
                    3,
                    86,
                    0
                ],
                "title": "Rethinking Graph Structure Learning in the Era of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Graph Structure Learning in the Era of LLMs"
                },
                "summary": "Recently, the emergence of LLMs has prompted researchers to integrate\nlanguage descriptions into graphs, aiming to enhance model encoding\ncapabilities from a data-centric perspective. This graph representation is\ncalled text-attributed graphs (TAGs). A review of prior advancements highlights\nthat graph structure learning (GSL) is a pivotal technique for improving data\nutility, making it highly relevant to efficient TAG learning. However, most GSL\nmethods are tailored for traditional graphs without textual information,\nunderscoring the necessity of developing a new GSL paradigm. Despite clear\nmotivations, it remains challenging: (1) How can we define a reasonable\noptimization objective for GSL in the era of LLMs, considering the massive\nparameters in LLM? (2) How can we design an efficient model architecture that\nenables seamless integration of LLM for this optimization objective? For\nQuestion 1, we reformulate existing GSL optimization objectives as a tree\noptimization framework, shifting the focus from obtaining a well-trained edge\npredictor to a language-aware tree sampler. For Question 2, we propose\ndecoupled and training-free model design principles for LLM integration,\nshifting the focus from computation-intensive fine-tuning to more efficient\ninference. Based on this, we propose Large Language and Tree Assistant (LLaTA),\nwhich leverages tree-based LLM in-context learning to enhance the understanding\nof topology and text, enabling reliable inference and generating improved graph\nstructure. Extensive experiments on 10 datasets demonstrate that LLaTA enjoys\nflexibility-incorporated with any backbone; scalability-outperforms other\nLLM-enhanced graph learning methods; effectiveness-achieves SOTA predictive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the emergence of LLMs has prompted researchers to integrate\nlanguage descriptions into graphs, aiming to enhance model encoding\ncapabilities from a data-centric perspective. This graph representation is\ncalled text-attributed graphs (TAGs). A review of prior advancements highlights\nthat graph structure learning (GSL) is a pivotal technique for improving data\nutility, making it highly relevant to efficient TAG learning. However, most GSL\nmethods are tailored for traditional graphs without textual information,\nunderscoring the necessity of developing a new GSL paradigm. Despite clear\nmotivations, it remains challenging: (1) How can we define a reasonable\noptimization objective for GSL in the era of LLMs, considering the massive\nparameters in LLM? (2) How can we design an efficient model architecture that\nenables seamless integration of LLM for this optimization objective? For\nQuestion 1, we reformulate existing GSL optimization objectives as a tree\noptimization framework, shifting the focus from obtaining a well-trained edge\npredictor to a language-aware tree sampler. For Question 2, we propose\ndecoupled and training-free model design principles for LLM integration,\nshifting the focus from computation-intensive fine-tuning to more efficient\ninference. Based on this, we propose Large Language and Tree Assistant (LLaTA),\nwhich leverages tree-based LLM in-context learning to enhance the understanding\nof topology and text, enabling reliable inference and generating improved graph\nstructure. Extensive experiments on 10 datasets demonstrate that LLaTA enjoys\nflexibility-incorporated with any backbone; scalability-outperforms other\nLLM-enhanced graph learning methods; effectiveness-achieves SOTA predictive\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhihan Zhang"
                    },
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Zhu Lei"
                    },
                    {
                        "name": "Guang Zeng"
                    },
                    {
                        "name": "Ronghua Li"
                    },
                    {
                        "name": "Guoren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoren Wang"
                },
                "author": "Guoren Wang",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21223v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21223v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05772v1",
                "updated": "2025-05-09T04:17:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:17:05Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM"
                },
                "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."
                },
                "authors": [
                    {
                        "name": "Zehao Fan"
                    },
                    {
                        "name": "Garrett Gagnon"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Liu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liu Liu"
                },
                "author": "Liu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05762v1",
                "updated": "2025-05-09T03:52:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    52,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T03:52:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    52,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Multi-Agent Systems for Robotic Autonomy with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Systems for Robotic Autonomy with LLMs"
                },
                "summary": "Since the advent of Large Language Models (LLMs), various research based on\nsuch models have maintained significant academic attention and impact,\nespecially in AI and robotics. In this paper, we propose a multi-agent\nframework with LLMs to construct an integrated system for robotic task\nanalysis, mechanical design, and path generation. The framework includes three\ncore agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer.\nOutputs are formatted as multimodal results, such as code files or technical\nreports, for stronger understandability and usability. To evaluate\ngeneralizability comparatively, we conducted experiments with models from both\nGPT and DeepSeek. Results demonstrate that the proposed system can design\nfeasible robots with control strategies when appropriate task inputs are\nprovided, exhibiting substantial potential for enhancing the efficiency and\naccessibility of robotic system development in research and industrial\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of Large Language Models (LLMs), various research based on\nsuch models have maintained significant academic attention and impact,\nespecially in AI and robotics. In this paper, we propose a multi-agent\nframework with LLMs to construct an integrated system for robotic task\nanalysis, mechanical design, and path generation. The framework includes three\ncore agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer.\nOutputs are formatted as multimodal results, such as code files or technical\nreports, for stronger understandability and usability. To evaluate\ngeneralizability comparatively, we conducted experiments with models from both\nGPT and DeepSeek. Results demonstrate that the proposed system can design\nfeasible robots with control strategies when appropriate task inputs are\nprovided, exhibiting substantial potential for enhancing the efficiency and\naccessibility of robotic system development in research and industrial\napplications."
                },
                "authors": [
                    {
                        "name": "Junhong Chen"
                    },
                    {
                        "name": "Ziqi Yang"
                    },
                    {
                        "name": "Haoyuan G Xu"
                    },
                    {
                        "name": "Dandan Zhang"
                    },
                    {
                        "name": "George Mylonas"
                    }
                ],
                "author_detail": {
                    "name": "George Mylonas"
                },
                "author": "George Mylonas",
                "arxiv_comment": "11 pages, 2 figures, 5 tables, submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05283v2",
                "updated": "2025-05-09T03:39:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    39,
                    37,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-08T14:27:45Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    14,
                    27,
                    45,
                    3,
                    128,
                    0
                ],
                "title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for\n  Code Large Language Models and Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Development Life Cycle Perspective: A Survey of Benchmarks for\n  Code Large Language Models and Agents"
                },
                "summary": "Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Kaixin Wang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Bin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Bin Shi"
                },
                "author": "Bin Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05758v1",
                "updated": "2025-05-09T03:38:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    38,
                    31,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T03:38:31Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    38,
                    31,
                    4,
                    129,
                    0
                ],
                "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal\n  Reasoning"
                },
                "summary": "Formal reasoning and automated theorem proving constitute a challenging\nsubfield of machine learning, in which machines are tasked with proving\nmathematical theorems using formal languages like Lean. A formal verification\nsystem can check whether a formal proof is correct or not almost\ninstantaneously, but generating a completely correct formal proof with large\nlanguage models (LLMs) remains a formidable task. The usual approach in the\nliterature is to prompt the LLM many times (up to several thousands) until one\nof the generated proofs passes the verification system. In this work, we\npresent APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a\nmodular, model-agnostic pipeline that combines the strengths of the Lean\ncompiler with an LLM's reasoning abilities to achieve better proof-generation\nresults at a low sampling budget. Apollo directs a fully automated process in\nwhich the LLM generates proofs for theorems, a set of agents analyze the\nproofs, fix the syntax errors, identify the mistakes in the proofs using Lean,\nisolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on\neach remaining goal with a low top-K budget. The repaired sub-proofs are\nrecombined and reverified, iterating up to a user-controlled maximum number of\nattempts. On the miniF2F benchmark, we establish a new state-of-the-art\naccuracy of 75.0% among 7B-parameter models while keeping the sampling budget\nbelow one thousand. Moreover, Apollo raises the state-of-the-art accuracy for\nGoedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few\nhundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%\naccuracy. Our results demonstrate that targeted, compiler-guided repair of LLM\noutputs yields dramatic gains in both efficiency and correctness, suggesting a\ngeneral paradigm for scalable automated theorem proving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal reasoning and automated theorem proving constitute a challenging\nsubfield of machine learning, in which machines are tasked with proving\nmathematical theorems using formal languages like Lean. A formal verification\nsystem can check whether a formal proof is correct or not almost\ninstantaneously, but generating a completely correct formal proof with large\nlanguage models (LLMs) remains a formidable task. The usual approach in the\nliterature is to prompt the LLM many times (up to several thousands) until one\nof the generated proofs passes the verification system. In this work, we\npresent APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a\nmodular, model-agnostic pipeline that combines the strengths of the Lean\ncompiler with an LLM's reasoning abilities to achieve better proof-generation\nresults at a low sampling budget. Apollo directs a fully automated process in\nwhich the LLM generates proofs for theorems, a set of agents analyze the\nproofs, fix the syntax errors, identify the mistakes in the proofs using Lean,\nisolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on\neach remaining goal with a low top-K budget. The repaired sub-proofs are\nrecombined and reverified, iterating up to a user-controlled maximum number of\nattempts. On the miniF2F benchmark, we establish a new state-of-the-art\naccuracy of 75.0% among 7B-parameter models while keeping the sampling budget\nbelow one thousand. Moreover, Apollo raises the state-of-the-art accuracy for\nGoedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few\nhundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%\naccuracy. Our results demonstrate that targeted, compiler-guided repair of LLM\noutputs yields dramatic gains in both efficiency and correctness, suggesting a\ngeneral paradigm for scalable automated theorem proving."
                },
                "authors": [
                    {
                        "name": "Azim Ospanov"
                    },
                    {
                        "name": "Roozbeh Yousefzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Roozbeh Yousefzadeh"
                },
                "author": "Roozbeh Yousefzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05756v1",
                "updated": "2025-05-09T03:32:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    32,
                    18,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T03:32:18Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    32,
                    18,
                    4,
                    129,
                    0
                ],
                "title": "Evolutionary thoughts: integration of large language models and\n  evolutionary algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary thoughts: integration of large language models and\n  evolutionary algorithms"
                },
                "summary": "Large Language Models (LLMs) have unveiled remarkable capabilities in\nunderstanding and generating both natural language and code, but LLM reasoning\nis prone to hallucination and struggle with complex, novel scenarios, often\ngetting stuck on partial or incorrect solutions. However, the inherent ability\nof Evolutionary Algorithms (EAs) to explore extensive and complex search spaces\nmakes them particularly effective in scenarios where traditional optimization\nmethodologies may falter. However, EAs explore a vast search space when applied\nto complex problems.\n  To address the computational bottleneck of evaluating large populations,\nparticularly crucial for complex evolutionary tasks, we introduce a highly\nefficient evaluation framework. This implementation maintains compatibility\nwith existing primitive definitions, ensuring the generation of valid\nindividuals.\n  Using LLMs, we propose an enhanced evolutionary search strategy that enables\na more focused exploration of expansive solution spaces. LLMs facilitate the\ngeneration of superior candidate solutions, as evidenced by empirical results\ndemonstrating their efficacy in producing improved outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have unveiled remarkable capabilities in\nunderstanding and generating both natural language and code, but LLM reasoning\nis prone to hallucination and struggle with complex, novel scenarios, often\ngetting stuck on partial or incorrect solutions. However, the inherent ability\nof Evolutionary Algorithms (EAs) to explore extensive and complex search spaces\nmakes them particularly effective in scenarios where traditional optimization\nmethodologies may falter. However, EAs explore a vast search space when applied\nto complex problems.\n  To address the computational bottleneck of evaluating large populations,\nparticularly crucial for complex evolutionary tasks, we introduce a highly\nefficient evaluation framework. This implementation maintains compatibility\nwith existing primitive definitions, ensuring the generation of valid\nindividuals.\n  Using LLMs, we propose an enhanced evolutionary search strategy that enables\na more focused exploration of expansive solution spaces. LLMs facilitate the\ngeneration of superior candidate solutions, as evidenced by empirical results\ndemonstrating their efficacy in producing improved outcomes."
                },
                "authors": [
                    {
                        "name": "Antonio Jimeno Yepes"
                    },
                    {
                        "name": "Pieter Barnard"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Barnard"
                },
                "author": "Pieter Barnard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05751v1",
                "updated": "2025-05-09T03:20:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    20,
                    48,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T03:20:48Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    20,
                    48,
                    4,
                    129,
                    0
                ],
                "title": "Efficient Full-Stack Private Federated Deep Learning with Post-Quantum\n  Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Full-Stack Private Federated Deep Learning with Post-Quantum\n  Security"
                },
                "summary": "Federated learning (FL) enables collaborative model training while preserving\nuser data privacy by keeping data local. Despite these advantages, FL remains\nvulnerable to privacy attacks on user updates and model parameters during\ntraining and deployment. Secure aggregation protocols have been proposed to\nprotect user updates by encrypting them, but these methods often incur high\ncomputational costs and are not resistant to quantum computers. Additionally,\ndifferential privacy (DP) has been used to mitigate privacy leakages, but\nexisting methods focus on secure aggregation or DP, neglecting their potential\nsynergies. To address these gaps, we introduce Beskar, a novel framework that\nprovides post-quantum secure aggregation, optimizes computational overhead for\nFL settings, and defines a comprehensive threat model that accounts for a wide\nspectrum of adversaries. We also integrate DP into different stages of FL\ntraining to enhance privacy protection in diverse scenarios. Our framework\nprovides a detailed analysis of the trade-offs between security, performance,\nand model accuracy, representing the first thorough examination of secure\naggregation protocols combined with various DP approaches for post-quantum\nsecure FL. Beskar aims to address the pressing privacy and security issues FL\nwhile ensuring quantum-safety and robust performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) enables collaborative model training while preserving\nuser data privacy by keeping data local. Despite these advantages, FL remains\nvulnerable to privacy attacks on user updates and model parameters during\ntraining and deployment. Secure aggregation protocols have been proposed to\nprotect user updates by encrypting them, but these methods often incur high\ncomputational costs and are not resistant to quantum computers. Additionally,\ndifferential privacy (DP) has been used to mitigate privacy leakages, but\nexisting methods focus on secure aggregation or DP, neglecting their potential\nsynergies. To address these gaps, we introduce Beskar, a novel framework that\nprovides post-quantum secure aggregation, optimizes computational overhead for\nFL settings, and defines a comprehensive threat model that accounts for a wide\nspectrum of adversaries. We also integrate DP into different stages of FL\ntraining to enhance privacy protection in diverse scenarios. Our framework\nprovides a detailed analysis of the trade-offs between security, performance,\nand model accuracy, representing the first thorough examination of secure\naggregation protocols combined with various DP approaches for post-quantum\nsecure FL. Beskar aims to address the pressing privacy and security issues FL\nwhile ensuring quantum-safety and robust performance."
                },
                "authors": [
                    {
                        "name": "Yiwei Zhang"
                    },
                    {
                        "name": "Rouzbeh Behnia"
                    },
                    {
                        "name": "Attila A. Yavuz"
                    },
                    {
                        "name": "Reza Ebrahimi"
                    },
                    {
                        "name": "Elisa Bertino"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Bertino"
                },
                "author": "Elisa Bertino",
                "arxiv_comment": "Accepted to IEEE TDSC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09831v2",
                "updated": "2025-05-09T03:11:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    11,
                    55,
                    4,
                    129,
                    0
                ],
                "published": "2024-06-14T08:40:58Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    40,
                    58,
                    4,
                    166,
                    0
                ],
                "title": "Recent Advances in Federated Learning Driven Large Language Models: A\n  Survey on Architecture, Performance, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Federated Learning Driven Large Language Models: A\n  Survey on Architecture, Performance, and Security"
                },
                "summary": "Federated Learning (FL) offers a promising paradigm for training Large\nLanguage Models (LLMs) in a decentralized manner while preserving data privacy\nand minimizing communication overhead. This survey examines recent advancements\nin FL-driven LLMs, with a particular emphasis on architectural designs,\nperformance optimization, and security concerns, including the emerging area of\nmachine unlearning. In this context, machine unlearning refers to the\nsystematic removal of specific data contributions from trained models to comply\nwith privacy regulations such as the Right to be Forgotten. We review a range\nof strategies enabling unlearning in federated LLMs, including\nperturbation-based methods, model decomposition, and incremental retraining,\nwhile evaluating their trade-offs in terms of efficiency, privacy guarantees,\nand model utility. Through selected case studies and empirical evaluations, we\nanalyze how these methods perform in practical FL scenarios. This survey\nidentifies critical research directions toward developing secure, adaptable,\nand high-performing federated LLM systems for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) offers a promising paradigm for training Large\nLanguage Models (LLMs) in a decentralized manner while preserving data privacy\nand minimizing communication overhead. This survey examines recent advancements\nin FL-driven LLMs, with a particular emphasis on architectural designs,\nperformance optimization, and security concerns, including the emerging area of\nmachine unlearning. In this context, machine unlearning refers to the\nsystematic removal of specific data contributions from trained models to comply\nwith privacy regulations such as the Right to be Forgotten. We review a range\nof strategies enabling unlearning in federated LLMs, including\nperturbation-based methods, model decomposition, and incremental retraining,\nwhile evaluating their trade-offs in terms of efficiency, privacy guarantees,\nand model utility. Through selected case studies and empirical evaluations, we\nanalyze how these methods perform in practical FL scenarios. This survey\nidentifies critical research directions toward developing secure, adaptable,\nand high-performing federated LLM systems for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Tianqing Zhu"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Shui Yu"
                    },
                    {
                        "name": "Wanlei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wanlei Zhou"
                },
                "author": "Wanlei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00679v2",
                "updated": "2025-05-09T03:10:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    3,
                    10,
                    7,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-01T17:39:02Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    39,
                    2,
                    3,
                    121,
                    0
                ],
                "title": "Steering Large Language Models with Register Analysis for Arbitrary\n  Style Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Large Language Models with Register Analysis for Arbitrary\n  Style Transfer"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies."
                },
                "authors": [
                    {
                        "name": "Xinchen Yang"
                    },
                    {
                        "name": "Marine Carpuat"
                    }
                ],
                "author_detail": {
                    "name": "Marine Carpuat"
                },
                "author": "Marine Carpuat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05744v1",
                "updated": "2025-05-09T02:57:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    2,
                    57,
                    39,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T02:57:39Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    2,
                    57,
                    39,
                    4,
                    129,
                    0
                ],
                "title": "Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data\n  Classification"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable ability in solving complex\ntasks, making them a promising tool for enhancing tabular learning. However,\nexisting LLM-based methods suffer from high resource requirements, suboptimal\ndemonstration selection, and limited interpretability, which largely hinder\ntheir prediction performance and application in the real world. To overcome\nthese problems, we propose a novel in-context learning framework for tabular\nprediction. The core idea is to leverage the explanations generated by LLMs to\nguide a smaller, locally deployable Surrogate Language Model (SLM) to make\ninterpretable tabular predictions. Specifically, our framework mainly involves\nthree stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to\ngenerate explanations for question-answer pairs in candidate demonstrations,\nproviding insights into the reasoning behind the answer. (ii) Post Hoc\nExplanation-Guided Demonstrations Selection, which utilizes explanations\ngenerated by LLMs to guide the process of demonstration selection from\ncandidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM\nPrediction, which utilizes the demonstrations obtained in step (ii) as\nin-context and merges corresponding explanations as rationales to improve the\nperformance of SLM and guide the model to generate interpretable outputs.\nExperimental results highlight the framework's effectiveness, with an average\naccuracy improvement of 5.31% across various tabular datasets in diverse\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable ability in solving complex\ntasks, making them a promising tool for enhancing tabular learning. However,\nexisting LLM-based methods suffer from high resource requirements, suboptimal\ndemonstration selection, and limited interpretability, which largely hinder\ntheir prediction performance and application in the real world. To overcome\nthese problems, we propose a novel in-context learning framework for tabular\nprediction. The core idea is to leverage the explanations generated by LLMs to\nguide a smaller, locally deployable Surrogate Language Model (SLM) to make\ninterpretable tabular predictions. Specifically, our framework mainly involves\nthree stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to\ngenerate explanations for question-answer pairs in candidate demonstrations,\nproviding insights into the reasoning behind the answer. (ii) Post Hoc\nExplanation-Guided Demonstrations Selection, which utilizes explanations\ngenerated by LLMs to guide the process of demonstration selection from\ncandidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM\nPrediction, which utilizes the demonstrations obtained in step (ii) as\nin-context and merges corresponding explanations as rationales to improve the\nperformance of SLM and guide the model to generate interpretable outputs.\nExperimental results highlight the framework's effectiveness, with an average\naccuracy improvement of 5.31% across various tabular datasets in diverse\ndomains."
                },
                "authors": [
                    {
                        "name": "Ruxue Shi"
                    },
                    {
                        "name": "Hengrui Gu"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05736v1",
                "updated": "2025-05-09T02:28:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    2,
                    28,
                    41,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T02:28:41Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    2,
                    28,
                    41,
                    4,
                    129,
                    0
                ],
                "title": "Multimodal Integrated Knowledge Transfer to Large Language Models\n  through Preference Optimization with Biomedical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Integrated Knowledge Transfer to Large Language Models\n  through Preference Optimization with Biomedical Applications"
                },
                "summary": "The scarcity of high-quality multimodal biomedical data limits the ability to\neffectively fine-tune pretrained Large Language Models (LLMs) for specialized\nbiomedical tasks. To address this challenge, we introduce MINT (Multimodal\nIntegrated kNowledge Transfer), a framework that aligns unimodal large decoder\nmodels with domain-specific decision patterns from multimodal biomedical data\nthrough preference optimization. While MINT supports different optimization\ntechniques, we primarily implement it with the Odds Ratio Preference\nOptimization (ORPO) framework as its backbone. This strategy enables the\naligned LLMs to perform predictive tasks using text-only or image-only inputs\nwhile retaining knowledge learnt from multimodal data. MINT leverages an\nupstream multimodal machine learning (MML) model trained on high-quality\nmultimodal data to transfer domain-specific insights to downstream text-only or\nimage-only LLMs. We demonstrate its effectiveness through two key applications:\n(1) Rare genetic disease prediction from texts, where MINT uses a multimodal\nencoder model, trained on facial photos and clinical notes, to generate a\npreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite\nrelying on text input only, the MINT-derived model outperforms models trained\nwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue\ntype classification using cell nucleus images, where MINT uses a\nvision-language foundation model as the preference generator, containing\nknowledge learnt from both text and histopathological images to align\ndownstream image-only models. The resulting MINT-derived model significantly\nimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue type\nclassification. In summary, MINT provides an effective strategy to align\nunimodal LLMs with high-quality multimodal expertise through preference\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of high-quality multimodal biomedical data limits the ability to\neffectively fine-tune pretrained Large Language Models (LLMs) for specialized\nbiomedical tasks. To address this challenge, we introduce MINT (Multimodal\nIntegrated kNowledge Transfer), a framework that aligns unimodal large decoder\nmodels with domain-specific decision patterns from multimodal biomedical data\nthrough preference optimization. While MINT supports different optimization\ntechniques, we primarily implement it with the Odds Ratio Preference\nOptimization (ORPO) framework as its backbone. This strategy enables the\naligned LLMs to perform predictive tasks using text-only or image-only inputs\nwhile retaining knowledge learnt from multimodal data. MINT leverages an\nupstream multimodal machine learning (MML) model trained on high-quality\nmultimodal data to transfer domain-specific insights to downstream text-only or\nimage-only LLMs. We demonstrate its effectiveness through two key applications:\n(1) Rare genetic disease prediction from texts, where MINT uses a multimodal\nencoder model, trained on facial photos and clinical notes, to generate a\npreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite\nrelying on text input only, the MINT-derived model outperforms models trained\nwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue\ntype classification using cell nucleus images, where MINT uses a\nvision-language foundation model as the preference generator, containing\nknowledge learnt from both text and histopathological images to align\ndownstream image-only models. The resulting MINT-derived model significantly\nimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue type\nclassification. In summary, MINT provides an effective strategy to align\nunimodal LLMs with high-quality multimodal expertise through preference\noptimization."
                },
                "authors": [
                    {
                        "name": "Da Wu"
                    },
                    {
                        "name": "Zhanliang Wang"
                    },
                    {
                        "name": "Quan Nguyen"
                    },
                    {
                        "name": "Zhuoran Xu"
                    },
                    {
                        "name": "Kai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Wang"
                },
                "author": "Kai Wang",
                "arxiv_comment": "First Draft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05713v1",
                "updated": "2025-05-09T01:24:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    1,
                    24,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T01:24:24Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    1,
                    24,
                    24,
                    4,
                    129,
                    0
                ],
                "title": "Understanding Stragglers in Large Model Training Using What-if Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Stragglers in Large Model Training Using What-if Analysis"
                },
                "summary": "Large language model (LLM) training is one of the most demanding distributed\ncomputations today, often requiring thousands of GPUs with frequent\nsynchronization across machines. Such a workload pattern makes it susceptible\nto stragglers, where the training can be stalled by few slow workers. At\nByteDance we find stragglers are not trivially always caused by hardware\nfailures, but can arise from multiple complex factors. This work aims to\npresent a comprehensive study on the straggler issues in LLM training, using a\nfive-month trace collected from our ByteDance LLM training cluster. The core\nmethodology is what-if analysis that simulates the scenario without any\nstragglers and contrasts with the actual case. We use this method to study the\nfollowing questions: (1) how often do stragglers affect training jobs, and what\neffect do they have on job performance; (2) do stragglers exhibit temporal or\nspatial patterns; and (3) what are the potential root causes for stragglers?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) training is one of the most demanding distributed\ncomputations today, often requiring thousands of GPUs with frequent\nsynchronization across machines. Such a workload pattern makes it susceptible\nto stragglers, where the training can be stalled by few slow workers. At\nByteDance we find stragglers are not trivially always caused by hardware\nfailures, but can arise from multiple complex factors. This work aims to\npresent a comprehensive study on the straggler issues in LLM training, using a\nfive-month trace collected from our ByteDance LLM training cluster. The core\nmethodology is what-if analysis that simulates the scenario without any\nstragglers and contrasts with the actual case. We use this method to study the\nfollowing questions: (1) how often do stragglers affect training jobs, and what\neffect do they have on job performance; (2) do stragglers exhibit temporal or\nspatial patterns; and (3) what are the potential root causes for stragglers?"
                },
                "authors": [
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Zuquan Song"
                    },
                    {
                        "name": "Sida Zhao"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Zhanghan Wang"
                    },
                    {
                        "name": "Chenyuan Wang"
                    },
                    {
                        "name": "Zuocheng Shi"
                    },
                    {
                        "name": "Xiang Shi"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Zherui Liu"
                    },
                    {
                        "name": "Shuguang Wang"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Xiu Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17480v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17480v3",
                "updated": "2025-05-09T01:21:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    1,
                    21,
                    11,
                    4,
                    129,
                    0
                ],
                "published": "2025-04-24T12:15:46Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    12,
                    15,
                    46,
                    3,
                    114,
                    0
                ],
                "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and\n  Scrubbing in Unauthorized Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Attacks to Large Language Model Watermarks: Spoofing and\n  Scrubbing in Unauthorized Knowledge Distillation"
                },
                "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable."
                },
                "authors": [
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Shunfan Zheng"
                    },
                    {
                        "name": "Linlin Wang"
                    },
                    {
                        "name": "Xiaoling Wang"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17480v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17480v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05712v1",
                "updated": "2025-05-09T01:19:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    1,
                    19,
                    1,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T01:19:01Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    1,
                    19,
                    1,
                    4,
                    129,
                    0
                ],
                "title": "LLM-Text Watermarking based on Lagrange Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Text Watermarking based on Lagrange Interpolation"
                },
                "summary": "The rapid advancement of LLMs (Large Language Models) has established them as\na foundational technology for many AI and ML powered human computer\ninteractions. A critical challenge in this context is the attribution of\nLLM-generated text, either to the specific language model used or to the\nindividual user who generated it. This is essential for combating\nmisinformation, fake news, misinterpretation, and plagiarism. One of the key\ntechniques for addressing this issue is watermarking.\n  This work presents a watermarking scheme for LLM-generated text based on\nLagrange interpolation, which enables the recovery of a secret author identity\neven when the text has been heavily redacted by an adversary. The core idea is\nto embed a continuous sequence of points (x, f(x)) that lie on a single\nstraight line. The x-coordinates are generated pseudorandomly using either an\nLFSR (when security is not a priority) or a cryptographically secure NFSR for\nhigh-security applications. The scheme efficiency and resilience to adversarial\nmodifications are analysed. Experimental results show that the proposed method\nis highly effective, allowing the recovery of the author identity when as few\nas three points survive adversarial manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of LLMs (Large Language Models) has established them as\na foundational technology for many AI and ML powered human computer\ninteractions. A critical challenge in this context is the attribution of\nLLM-generated text, either to the specific language model used or to the\nindividual user who generated it. This is essential for combating\nmisinformation, fake news, misinterpretation, and plagiarism. One of the key\ntechniques for addressing this issue is watermarking.\n  This work presents a watermarking scheme for LLM-generated text based on\nLagrange interpolation, which enables the recovery of a secret author identity\neven when the text has been heavily redacted by an adversary. The core idea is\nto embed a continuous sequence of points (x, f(x)) that lie on a single\nstraight line. The x-coordinates are generated pseudorandomly using either an\nLFSR (when security is not a priority) or a cryptographically secure NFSR for\nhigh-security applications. The scheme efficiency and resilience to adversarial\nmodifications are analysed. Experimental results show that the proposed method\nis highly effective, allowing the recovery of the author identity when as few\nas three points survive adversarial manipulation."
                },
                "authors": [
                    {
                        "name": "Jarosław Janas"
                    },
                    {
                        "name": "Paweł Morawiecki"
                    },
                    {
                        "name": "Josef Pieprzyk"
                    }
                ],
                "author_detail": {
                    "name": "Josef Pieprzyk"
                },
                "author": "Josef Pieprzyk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05707v1",
                "updated": "2025-05-09T00:55:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    55,
                    12,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T00:55:12Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    55,
                    12,
                    4,
                    129,
                    0
                ],
                "title": "Crowding Out The Noise: Algorithmic Collective Action Under Differential\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crowding Out The Noise: Algorithmic Collective Action Under Differential\n  Privacy"
                },
                "summary": "The integration of AI into daily life has generated considerable attention\nand excitement, while also raising concerns about automating algorithmic harms\nand re-entrenching existing social inequities. While the responsible deployment\nof trustworthy AI systems is a worthy goal, there are many possible ways to\nrealize it, from policy and regulation to improved algorithm design and\nevaluation. In fact, since AI trains on social data, there is even a\npossibility for everyday users, citizens, or workers to directly steer its\nbehavior through Algorithmic Collective Action, by deliberately modifying the\ndata they share with a platform to drive its learning process in their favor.\nThis paper considers how these grassroots efforts to influence AI interact with\nmethods already used by AI firms and governments to improve model\ntrustworthiness. In particular, we focus on the setting where the AI firm\ndeploys a differentially private model, motivated by the growing regulatory\nfocus on privacy and data protection. We investigate how the use of\nDifferentially Private Stochastic Gradient Descent (DPSGD) affects the\ncollective's ability to influence the learning process. Our findings show that\nwhile differential privacy contributes to the protection of individual data, it\nintroduces challenges for effective algorithmic collective action. We\ncharacterize lower bounds on the success of algorithmic collective action under\ndifferential privacy as a function of the collective's size and the firm's\nprivacy parameters, and verify these trends experimentally by simulating\ncollective action during the training of deep neural network classifiers across\nseveral datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of AI into daily life has generated considerable attention\nand excitement, while also raising concerns about automating algorithmic harms\nand re-entrenching existing social inequities. While the responsible deployment\nof trustworthy AI systems is a worthy goal, there are many possible ways to\nrealize it, from policy and regulation to improved algorithm design and\nevaluation. In fact, since AI trains on social data, there is even a\npossibility for everyday users, citizens, or workers to directly steer its\nbehavior through Algorithmic Collective Action, by deliberately modifying the\ndata they share with a platform to drive its learning process in their favor.\nThis paper considers how these grassroots efforts to influence AI interact with\nmethods already used by AI firms and governments to improve model\ntrustworthiness. In particular, we focus on the setting where the AI firm\ndeploys a differentially private model, motivated by the growing regulatory\nfocus on privacy and data protection. We investigate how the use of\nDifferentially Private Stochastic Gradient Descent (DPSGD) affects the\ncollective's ability to influence the learning process. Our findings show that\nwhile differential privacy contributes to the protection of individual data, it\nintroduces challenges for effective algorithmic collective action. We\ncharacterize lower bounds on the success of algorithmic collective action under\ndifferential privacy as a function of the collective's size and the firm's\nprivacy parameters, and verify these trends experimentally by simulating\ncollective action during the training of deep neural network classifiers across\nseveral datasets."
                },
                "authors": [
                    {
                        "name": "Rushabh Solanki"
                    },
                    {
                        "name": "Meghana Bhange"
                    },
                    {
                        "name": "Ulrich Aïvodji"
                    },
                    {
                        "name": "Elliot Creager"
                    }
                ],
                "author_detail": {
                    "name": "Elliot Creager"
                },
                "author": "Elliot Creager",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05704v1",
                "updated": "2025-05-09T00:39:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    39,
                    43,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T00:39:43Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    39,
                    43,
                    4,
                    129,
                    0
                ],
                "title": "Assessing Robustness to Spurious Correlations in Post-Training Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Robustness to Spurious Correlations in Post-Training Language\n  Models"
                },
                "summary": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations."
                },
                "authors": [
                    {
                        "name": "Julia Shuieh"
                    },
                    {
                        "name": "Prasann Singhal"
                    },
                    {
                        "name": "Apaar Shanker"
                    },
                    {
                        "name": "John Heyer"
                    },
                    {
                        "name": "George Pu"
                    },
                    {
                        "name": "Samuel Denton"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Denton"
                },
                "author": "Samuel Denton",
                "arxiv_comment": "ICLR '25 Workshop on Spurious Correlation and Shortcut Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05703v1",
                "updated": "2025-05-09T00:35:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    35,
                    14,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T00:35:14Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    35,
                    14,
                    4,
                    129,
                    0
                ],
                "title": "Hybrid Learning: A Novel Combination of Self-Supervised and Supervised\n  Learning for MRI Reconstruction without High-Quality Training Reference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Learning: A Novel Combination of Self-Supervised and Supervised\n  Learning for MRI Reconstruction without High-Quality Training Reference"
                },
                "summary": "Purpose: Deep learning has demonstrated strong potential for MRI\nreconstruction, but conventional supervised learning methods require\nhigh-quality reference images, which are often unavailable in practice.\nSelf-supervised learning offers an alternative, yet its performance degrades at\nhigh acceleration rates. To overcome these limitations, we propose hybrid\nlearning, a novel two-stage training framework that combines self-supervised\nand supervised learning for robust image reconstruction.\n  Methods: Hybrid learning is implemented in two sequential stages. In the\nfirst stage, self-supervised learning is employed to generate improved images\nfrom noisy or undersampled reference data. These enhanced images then serve as\npseudo-ground truths for the second stage, which uses supervised learning to\nrefine reconstruction performance and support higher acceleration rates. We\nevaluated hybrid learning in two representative applications: (1) accelerated\n0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of\nthe brain without access to fully sampled ground truth.\n  Results: For spiral-UTE lung MRI, hybrid learning consistently improved image\nquality over both self-supervised and conventional supervised methods across\ndifferent acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping,\nhybrid learning achieved superior T1 quantification accuracy across a wide\ndynamic range, outperforming self-supervised learning in all tested conditions.\n  Conclusions: Hybrid learning provides a practical and effective solution for\ntraining deep MRI reconstruction networks when only low-quality or incomplete\nreference data are available. It enables improved image quality and accurate\nquantitative mapping across different applications and field strengths,\nrepresenting a promising technique toward broader clinical deployment of deep\nlearning-based MRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Deep learning has demonstrated strong potential for MRI\nreconstruction, but conventional supervised learning methods require\nhigh-quality reference images, which are often unavailable in practice.\nSelf-supervised learning offers an alternative, yet its performance degrades at\nhigh acceleration rates. To overcome these limitations, we propose hybrid\nlearning, a novel two-stage training framework that combines self-supervised\nand supervised learning for robust image reconstruction.\n  Methods: Hybrid learning is implemented in two sequential stages. In the\nfirst stage, self-supervised learning is employed to generate improved images\nfrom noisy or undersampled reference data. These enhanced images then serve as\npseudo-ground truths for the second stage, which uses supervised learning to\nrefine reconstruction performance and support higher acceleration rates. We\nevaluated hybrid learning in two representative applications: (1) accelerated\n0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of\nthe brain without access to fully sampled ground truth.\n  Results: For spiral-UTE lung MRI, hybrid learning consistently improved image\nquality over both self-supervised and conventional supervised methods across\ndifferent acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping,\nhybrid learning achieved superior T1 quantification accuracy across a wide\ndynamic range, outperforming self-supervised learning in all tested conditions.\n  Conclusions: Hybrid learning provides a practical and effective solution for\ntraining deep MRI reconstruction networks when only low-quality or incomplete\nreference data are available. It enables improved image quality and accurate\nquantitative mapping across different applications and field strengths,\nrepresenting a promising technique toward broader clinical deployment of deep\nlearning-based MRI."
                },
                "authors": [
                    {
                        "name": "Haoyang Pei"
                    },
                    {
                        "name": "Ding Xia"
                    },
                    {
                        "name": "Xiang Xu"
                    },
                    {
                        "name": "William Moore"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Hersh Chandarana"
                    },
                    {
                        "name": "Li Feng"
                    }
                ],
                "author_detail": {
                    "name": "Li Feng"
                },
                "author": "Li Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v3",
                "updated": "2025-05-09T00:31:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    31,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20364v2",
                "updated": "2025-05-09T00:25:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    25,
                    9,
                    4,
                    129,
                    0
                ],
                "published": "2025-02-27T18:35:39Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    18,
                    35,
                    39,
                    3,
                    58,
                    0
                ],
                "title": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with\n  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix\n  Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with\n  Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix\n  Factorization"
                },
                "summary": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Generative AI, powered by Large Language Models (LLMs) with\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores\n(VSs), represents a transformative technology applicable to specialized domains\nsuch as legal systems, research, recommender systems, cybersecurity, and global\nsecurity, including proliferation research. This technology excels at inferring\nrelationships within vast unstructured or semi-structured datasets. The legal\ndomain here comprises complex data characterized by extensive, interrelated,\nand semi-structured knowledge systems with complex relations. It comprises\nconstitutions, statutes, regulations, and case law. Extracting insights and\nnavigating the intricate networks of legal documents and their relations is\ncrucial for effective legal research. Here, we introduce a generative AI system\nthat integrates RAG, VS, and KG, constructed via Non-Negative Matrix\nFactorization (NMF), to enhance legal information retrieval and AI reasoning\nand minimize hallucinations. In the legal system, these technologies empower AI\nagents to identify and analyze complex connections among cases, statutes, and\nlegal precedents, uncovering hidden relationships and predicting legal\ntrends-challenging tasks that are essential for ensuring justice and improving\noperational efficiency. Our system employs web scraping techniques to\nsystematically collect legal texts, such as statutes, constitutional\nprovisions, and case law, from publicly accessible platforms like Justia. It\nbridges the gap between traditional keyword-based searches and contextual\nunderstanding by leveraging advanced semantic representations, hierarchical\nrelationships, and latent topic discovery. This framework supports legal\ndocument clustering, summarization, and cross-referencing, for scalable,\ninterpretable, and accurate retrieval for semi-structured data while advancing\ncomputational law and AI."
                },
                "authors": [
                    {
                        "name": "Ryan C. Barron"
                    },
                    {
                        "name": "Maksim E. Eren"
                    },
                    {
                        "name": "Olga M. Serafimova"
                    },
                    {
                        "name": "Cynthia Matuszek"
                    },
                    {
                        "name": "Boian S. Alexandrov"
                    }
                ],
                "author_detail": {
                    "name": "Boian S. Alexandrov"
                },
                "author": "Boian S. Alexandrov",
                "arxiv_comment": "10 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13078v2",
                "updated": "2025-05-08T23:31:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    23,
                    31,
                    39,
                    3,
                    128,
                    0
                ],
                "published": "2024-01-23T20:08:41Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    20,
                    8,
                    41,
                    1,
                    23,
                    0
                ],
                "title": "Open-Source, Cost-Aware Kinematically Feasible Planning for Mobile and\n  Surface Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source, Cost-Aware Kinematically Feasible Planning for Mobile and\n  Surface Robotics"
                },
                "summary": "We present Smac Planner, an openly available, search-based planning framework\nthat addresses the critical need for kinematically feasible path planning\nacross diverse robot platforms. Smac Planner provides high-performance\nimplementations of Cost-Aware A*, Hybrid-A*, and State Lattice planners that\ncan be deployed for Ackermann, legged, and other large non-circular robots. Our\nframework introduces novel \"Cost-Aware\" variations that significantly improve\nperformance in complex environments common to mobile robotics while maintaining\nkinematic feasibility constraints. Integrated as the standard planning system\nwithin the popular ROS 2 Navigation stack, Nav2, Smac Planner now powers\nthousands of robots worldwide across academic research, commercial\napplications, and field deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Smac Planner, an openly available, search-based planning framework\nthat addresses the critical need for kinematically feasible path planning\nacross diverse robot platforms. Smac Planner provides high-performance\nimplementations of Cost-Aware A*, Hybrid-A*, and State Lattice planners that\ncan be deployed for Ackermann, legged, and other large non-circular robots. Our\nframework introduces novel \"Cost-Aware\" variations that significantly improve\nperformance in complex environments common to mobile robotics while maintaining\nkinematic feasibility constraints. Integrated as the standard planning system\nwithin the popular ROS 2 Navigation stack, Nav2, Smac Planner now powers\nthousands of robots worldwide across academic research, commercial\napplications, and field deployments."
                },
                "authors": [
                    {
                        "name": "Steve Macenski"
                    },
                    {
                        "name": "Matthew Booker"
                    },
                    {
                        "name": "Joshua Wallace"
                    },
                    {
                        "name": "Tobias Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Fischer"
                },
                "author": "Tobias Fischer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02550v2",
                "updated": "2025-05-08T22:57:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    22,
                    57,
                    46,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-05T10:39:51Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    39,
                    51,
                    0,
                    125,
                    0
                ],
                "title": "Bielik v3 Small: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bielik v3 Small: Technical Report"
                },
                "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications."
                },
                "authors": [
                    {
                        "name": "Krzysztof Ociepa"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Remigiusz Kinas"
                    },
                    {
                        "name": "Krzysztof Wróbel"
                    },
                    {
                        "name": "Adrian Gwoździej"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Gwoździej"
                },
                "author": "Adrian Gwoździej",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02410v2",
                "updated": "2025-05-08T22:55:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    22,
                    55,
                    18,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-05T07:03:41Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    7,
                    3,
                    41,
                    0,
                    125,
                    0
                ],
                "title": "Bielik 11B v2 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bielik 11B v2 Technical Report"
                },
                "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages."
                },
                "authors": [
                    {
                        "name": "Krzysztof Ociepa"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Krzysztof Wróbel"
                    },
                    {
                        "name": "Adrian Gwoździej"
                    },
                    {
                        "name": "Remigiusz Kinas"
                    }
                ],
                "author_detail": {
                    "name": "Remigiusz Kinas"
                },
                "author": "Remigiusz Kinas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05679v1",
                "updated": "2025-05-08T22:38:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    22,
                    38,
                    10,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T22:38:10Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    22,
                    38,
                    10,
                    3,
                    128,
                    0
                ],
                "title": "From Bias To Improved Prompts: A Case Study of Bias Mitigation of Clone\n  Detection Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Bias To Improved Prompts: A Case Study of Bias Mitigation of Clone\n  Detection Models"
                },
                "summary": "The issue of clone code has persisted in software engineering, primarily\nbecause developers often copy and paste code segments. This common practice has\nelevated the importance of clone code detection, garnering attention from both\nsoftware engineering researchers and industry professionals. Their collective\nconcern arises from the potential negative impacts that clone code can have on\nsoftware quality. The emergence of powerful Generative Large Language Models\n(LLMs) like ChatGPT has exacerbated the clone code problem. These advanced\nmodels possess code generation capabilities that can inadvertently create code\nclones. As a result, the need to detect clone code has become more critical\nthan ever before. In this study, we assess the suitability of LLMs for clone\ncode detection. Our results demonstrate that the Palm model achieved a high F1\nscore of 89.30 for the avatar dataset and 86.41 for the poolC dataset. A known\nissue with LLMs is their susceptibility to prompt bias, where the performance\nof these models fluctuates based on the input prompt provided. In our research,\nwe delve deeper into the reasons behind these fluctuations and propose a\nframework to mitigate prompt bias for clone detection. Our analysis identifies\neight distinct categories of prompt bias, and our devised approach leveraging\nthese biases yields a significant improvement of up to 10.81% in the F1 score.\nThese findings underscore the substantial impact of prompt bias on the\nperformance of LLMs and highlight the potential for leveraging model errors to\nalleviate this bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The issue of clone code has persisted in software engineering, primarily\nbecause developers often copy and paste code segments. This common practice has\nelevated the importance of clone code detection, garnering attention from both\nsoftware engineering researchers and industry professionals. Their collective\nconcern arises from the potential negative impacts that clone code can have on\nsoftware quality. The emergence of powerful Generative Large Language Models\n(LLMs) like ChatGPT has exacerbated the clone code problem. These advanced\nmodels possess code generation capabilities that can inadvertently create code\nclones. As a result, the need to detect clone code has become more critical\nthan ever before. In this study, we assess the suitability of LLMs for clone\ncode detection. Our results demonstrate that the Palm model achieved a high F1\nscore of 89.30 for the avatar dataset and 86.41 for the poolC dataset. A known\nissue with LLMs is their susceptibility to prompt bias, where the performance\nof these models fluctuates based on the input prompt provided. In our research,\nwe delve deeper into the reasons behind these fluctuations and propose a\nframework to mitigate prompt bias for clone detection. Our analysis identifies\neight distinct categories of prompt bias, and our devised approach leveraging\nthese biases yields a significant improvement of up to 10.81% in the F1 score.\nThese findings underscore the substantial impact of prompt bias on the\nperformance of LLMs and highlight the potential for leveraging model errors to\nalleviate this bias."
                },
                "authors": [
                    {
                        "name": "QiHong Chen"
                    },
                    {
                        "name": "Lianghao Jiang"
                    },
                    {
                        "name": "Iftekhar Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Iftekhar Ahmed"
                },
                "author": "Iftekhar Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05678v1",
                "updated": "2025-05-08T22:31:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    22,
                    31,
                    23,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T22:31:23Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    22,
                    31,
                    23,
                    3,
                    128,
                    0
                ],
                "title": "InstanceGen: Image Generation with Instance-level Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstanceGen: Image Generation with Instance-level Instructions"
                },
                "summary": "Despite rapid advancements in the capabilities of generative models,\npretrained text-to-image models still struggle in capturing the semantics\nconveyed by complex prompts that compound multiple objects and instance-level\nattributes. Consequently, we are witnessing growing interests in integrating\nadditional structural constraints, %leveraging additional structural inputs\ntypically in the form of coarse bounding boxes, to better guide the generation\nprocess in such challenging cases. In this work, we take the idea of structural\nguidance a step further by making the observation that contemporary image\ngeneration models can directly provide a plausible \\emph{fine-grained}\nstructural initialization. We propose a technique that couples this image-based\nstructural guidance with LLM-based instance-level instructions, yielding output\nimages that adhere to all parts of the text prompt, including object counts,\ninstance-level attributes, and spatial relations between instances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid advancements in the capabilities of generative models,\npretrained text-to-image models still struggle in capturing the semantics\nconveyed by complex prompts that compound multiple objects and instance-level\nattributes. Consequently, we are witnessing growing interests in integrating\nadditional structural constraints, %leveraging additional structural inputs\ntypically in the form of coarse bounding boxes, to better guide the generation\nprocess in such challenging cases. In this work, we take the idea of structural\nguidance a step further by making the observation that contemporary image\ngeneration models can directly provide a plausible \\emph{fine-grained}\nstructural initialization. We propose a technique that couples this image-based\nstructural guidance with LLM-based instance-level instructions, yielding output\nimages that adhere to all parts of the text prompt, including object counts,\ninstance-level attributes, and spatial relations between instances."
                },
                "authors": [
                    {
                        "name": "Etai Sella"
                    },
                    {
                        "name": "Yanir Kleiman"
                    },
                    {
                        "name": "Hadar Averbuch-Elor"
                    }
                ],
                "author_detail": {
                    "name": "Hadar Averbuch-Elor"
                },
                "author": "Hadar Averbuch-Elor",
                "arxiv_comment": "Project page: https://tau-vailab.github.io/InstanceGen/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05666v1",
                "updated": "2025-05-08T21:54:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    21,
                    54,
                    2,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T21:54:02Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    21,
                    54,
                    2,
                    3,
                    128,
                    0
                ],
                "title": "Lost in OCR Translation? Vision-Based Approaches to Robust Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in OCR Translation? Vision-Based Approaches to Robust Document\n  Retrieval"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become a popular technique for\nenhancing the reliability and utility of Large Language Models (LLMs) by\ngrounding responses in external documents. Traditional RAG systems rely on\nOptical Character Recognition (OCR) to first process scanned documents into\ntext. However, even state-of-the-art OCRs can introduce errors, especially in\ndegraded or complex documents. Recent vision-language approaches, such as\nColPali, propose direct visual embedding of documents, eliminating the need for\nOCR. This study presents a systematic comparison between a vision-based RAG\nsystem (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2\n(90B) and Nougat OCR across varying document qualities. Beyond conventional\nretrieval accuracy metrics, we introduce a semantic answer evaluation benchmark\nto assess end-to-end question-answering performance. Our findings indicate that\nwhile vision-based RAG performs well on documents it has been fine-tuned on,\nOCR-based RAG is better able to generalize to unseen documents of varying\nquality. We highlight the key trade-offs between computational efficiency and\nsemantic accuracy, offering practical guidance for RAG practitioners in\nselecting between OCR-dependent and vision-based document retrieval systems in\nproduction environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become a popular technique for\nenhancing the reliability and utility of Large Language Models (LLMs) by\ngrounding responses in external documents. Traditional RAG systems rely on\nOptical Character Recognition (OCR) to first process scanned documents into\ntext. However, even state-of-the-art OCRs can introduce errors, especially in\ndegraded or complex documents. Recent vision-language approaches, such as\nColPali, propose direct visual embedding of documents, eliminating the need for\nOCR. This study presents a systematic comparison between a vision-based RAG\nsystem (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2\n(90B) and Nougat OCR across varying document qualities. Beyond conventional\nretrieval accuracy metrics, we introduce a semantic answer evaluation benchmark\nto assess end-to-end question-answering performance. Our findings indicate that\nwhile vision-based RAG performs well on documents it has been fine-tuned on,\nOCR-based RAG is better able to generalize to unseen documents of varying\nquality. We highlight the key trade-offs between computational efficiency and\nsemantic accuracy, offering practical guidance for RAG practitioners in\nselecting between OCR-dependent and vision-based document retrieval systems in\nproduction environments."
                },
                "authors": [
                    {
                        "name": "Alexander Most"
                    },
                    {
                        "name": "Joseph Winjum"
                    },
                    {
                        "name": "Ayan Biswas"
                    },
                    {
                        "name": "Shawn Jones"
                    },
                    {
                        "name": "Nishath Rajiv Ranasinghe"
                    },
                    {
                        "name": "Dan O'Malley"
                    },
                    {
                        "name": "Manish Bhattarai"
                    }
                ],
                "author_detail": {
                    "name": "Manish Bhattarai"
                },
                "author": "Manish Bhattarai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05665v1",
                "updated": "2025-05-08T21:50:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    21,
                    50,
                    43,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T21:50:43Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    21,
                    50,
                    43,
                    3,
                    128,
                    0
                ],
                "title": "Adaptive Stress Testing Black-Box LLM Planners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Stress Testing Black-Box LLM Planners"
                },
                "summary": "Large language models (LLMs) have recently demonstrated success in\ngeneralizing across decision-making tasks including planning, control and\nprediction, but their tendency to hallucinate unsafe and undesired outputs\nposes risks. We argue that detecting such failures is necessary, especially in\nsafety-critical scenarios. Existing black-box methods often detect\nhallucinations by identifying inconsistencies across multiple samples. Many of\nthese approaches typically introduce prompt perturbations like randomizing\ndetail order or generating adversarial inputs, with the intuition that a\nconfident model should produce stable outputs. We first perform a manual case\nstudy showing that other forms of perturbations (e.g., adding noise, removing\nsensor details) cause LLMs to hallucinate in a driving environment. We then\npropose a novel method for efficiently searching the space of prompt\nperturbations using Adaptive Stress Testing (AST) with Monte-Carlo Tree Search\n(MCTS). Our AST formulation enables discovery of scenarios and prompts that\ncause language models to act with high uncertainty. By generating MCTS prompt\nperturbation trees across diverse scenarios, we show that offline analyses can\nbe used at runtime to automatically generate prompts that influence model\nuncertainty, and to inform real-time trust assessments of an LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated success in\ngeneralizing across decision-making tasks including planning, control and\nprediction, but their tendency to hallucinate unsafe and undesired outputs\nposes risks. We argue that detecting such failures is necessary, especially in\nsafety-critical scenarios. Existing black-box methods often detect\nhallucinations by identifying inconsistencies across multiple samples. Many of\nthese approaches typically introduce prompt perturbations like randomizing\ndetail order or generating adversarial inputs, with the intuition that a\nconfident model should produce stable outputs. We first perform a manual case\nstudy showing that other forms of perturbations (e.g., adding noise, removing\nsensor details) cause LLMs to hallucinate in a driving environment. We then\npropose a novel method for efficiently searching the space of prompt\nperturbations using Adaptive Stress Testing (AST) with Monte-Carlo Tree Search\n(MCTS). Our AST formulation enables discovery of scenarios and prompts that\ncause language models to act with high uncertainty. By generating MCTS prompt\nperturbation trees across diverse scenarios, we show that offline analyses can\nbe used at runtime to automatically generate prompts that influence model\nuncertainty, and to inform real-time trust assessments of an LLM."
                },
                "authors": [
                    {
                        "name": "Neeloy Chakraborty"
                    },
                    {
                        "name": "John Pohovey"
                    },
                    {
                        "name": "Melkior Ornik"
                    },
                    {
                        "name": "Katherine Driggs-Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Katherine Driggs-Campbell"
                },
                "author": "Katherine Driggs-Campbell",
                "arxiv_comment": "26 pages, 16 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05660v1",
                "updated": "2025-05-08T21:35:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    21,
                    35,
                    59,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T21:35:59Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    21,
                    35,
                    59,
                    3,
                    128,
                    0
                ],
                "title": "Not Like Us, Hunty: Measuring Perceptions and Behavioral Effects of\n  Minoritized Anthropomorphic Cues in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not Like Us, Hunty: Measuring Perceptions and Behavioral Effects of\n  Minoritized Anthropomorphic Cues in LLMs"
                },
                "summary": "As large language models (LLMs) increasingly adapt and personalize to diverse\nsets of users, there is an increased risk of systems appropriating sociolects,\ni.e., language styles or dialects that are associated with specific minoritized\nlived experiences (e.g., African American English, Queer slang). In this work,\nwe examine whether sociolect usage by an LLM agent affects user reliance on its\noutputs and user perception (satisfaction, frustration, trust, and social\npresence). We designed and conducted user studies where 498 African American\nEnglish (AAE) speakers and 487 Queer slang speakers performed a set of\nquestion-answering tasks with LLM-based suggestions in either standard American\nEnglish (SAE) or their self-identified sociolect. Our findings showed that\nsociolect usage by LLMs influenced both reliance and perceptions, though in\nsome surprising ways. Results suggest that both AAE and Queer slang speakers\nrelied more on the SAE agent, and had more positive perceptions of the SAE\nagent. Yet, only Queer slang speakers felt more social presence from the Queer\nslang agent over the SAE one, whereas only AAE speakers preferred and trusted\nthe SAE agent over the AAE one. These findings emphasize the need to test for\nbehavioral outcomes rather than simply assume that personalization would lead\nto a better and safer reliance outcome. They also highlight the nuanced\ndynamics of minoritized language in machine interactions, underscoring the need\nfor LLMs to be carefully designed to respect cultural and linguistic boundaries\nwhile fostering genuine user engagement and trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly adapt and personalize to diverse\nsets of users, there is an increased risk of systems appropriating sociolects,\ni.e., language styles or dialects that are associated with specific minoritized\nlived experiences (e.g., African American English, Queer slang). In this work,\nwe examine whether sociolect usage by an LLM agent affects user reliance on its\noutputs and user perception (satisfaction, frustration, trust, and social\npresence). We designed and conducted user studies where 498 African American\nEnglish (AAE) speakers and 487 Queer slang speakers performed a set of\nquestion-answering tasks with LLM-based suggestions in either standard American\nEnglish (SAE) or their self-identified sociolect. Our findings showed that\nsociolect usage by LLMs influenced both reliance and perceptions, though in\nsome surprising ways. Results suggest that both AAE and Queer slang speakers\nrelied more on the SAE agent, and had more positive perceptions of the SAE\nagent. Yet, only Queer slang speakers felt more social presence from the Queer\nslang agent over the SAE one, whereas only AAE speakers preferred and trusted\nthe SAE agent over the AAE one. These findings emphasize the need to test for\nbehavioral outcomes rather than simply assume that personalization would lead\nto a better and safer reliance outcome. They also highlight the nuanced\ndynamics of minoritized language in machine interactions, underscoring the need\nfor LLMs to be carefully designed to respect cultural and linguistic boundaries\nwhile fostering genuine user engagement and trust."
                },
                "authors": [
                    {
                        "name": "Jeffrey Basoah"
                    },
                    {
                        "name": "Daniel Chechelnitsky"
                    },
                    {
                        "name": "Tao Long"
                    },
                    {
                        "name": "Katharina Reinecke"
                    },
                    {
                        "name": "Chrysoula Zerva"
                    },
                    {
                        "name": "Kaitlyn Zhou"
                    },
                    {
                        "name": "Mark Díaz"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_doi": "10.1145/3715275.3732045",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715275.3732045",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.05660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to FAccT 2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11711v2",
                "updated": "2025-05-08T20:14:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    20,
                    14,
                    32,
                    3,
                    128,
                    0
                ],
                "published": "2025-03-12T19:06:25Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    19,
                    6,
                    25,
                    2,
                    71,
                    0
                ],
                "title": "Privacy-Preserved Automated Scoring using Federated Learning for\n  Educational Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserved Automated Scoring using Federated Learning for\n  Educational Research"
                },
                "summary": "Data privacy remains a critical concern in educational research, requiring\nstrict adherence to ethical standards and regulatory protocols. While\ntraditional approaches rely on anonymization and centralized data collection,\nthey often expose raw student data to security vulnerabilities and impose\nsubstantial logistical overhead. In this study, we propose a federated learning\n(FL) framework for automated scoring of educational assessments that eliminates\nthe need to share sensitive data across institutions. Our approach leverages\nparameter-efficient fine-tuning of large language models (LLMs) with Low-Rank\nAdaptation (LoRA), enabling each client (school) to train locally while sharing\nonly optimized model updates. To address data heterogeneity, we implement an\nadaptive weighted aggregation strategy that considers both client performance\nand data volume. We benchmark our model against two state-of-the-art FL methods\nand a centralized learning baseline using NGSS-aligned multi-label science\nassessment data from nine middle schools. Results show that our model achieves\nthe highest accuracy (94.5%) among FL approaches, and performs within 0.5-1.0\npercentage points of the centralized model on these metrics. Additionally, it\nachieves comparable rubric-level scoring accuracy, with only a 1.3% difference\nin rubric match and a lower score deviation (MAE), highlighting its\neffectiveness in preserving both prediction quality and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data privacy remains a critical concern in educational research, requiring\nstrict adherence to ethical standards and regulatory protocols. While\ntraditional approaches rely on anonymization and centralized data collection,\nthey often expose raw student data to security vulnerabilities and impose\nsubstantial logistical overhead. In this study, we propose a federated learning\n(FL) framework for automated scoring of educational assessments that eliminates\nthe need to share sensitive data across institutions. Our approach leverages\nparameter-efficient fine-tuning of large language models (LLMs) with Low-Rank\nAdaptation (LoRA), enabling each client (school) to train locally while sharing\nonly optimized model updates. To address data heterogeneity, we implement an\nadaptive weighted aggregation strategy that considers both client performance\nand data volume. We benchmark our model against two state-of-the-art FL methods\nand a centralized learning baseline using NGSS-aligned multi-label science\nassessment data from nine middle schools. Results show that our model achieves\nthe highest accuracy (94.5%) among FL approaches, and performs within 0.5-1.0\npercentage points of the centralized model on these metrics. Additionally, it\nachieves comparable rubric-level scoring accuracy, with only a 1.3% difference\nin rubric match and a lower score deviation (MAE), highlighting its\neffectiveness in preserving both prediction quality and interpretability."
                },
                "authors": [
                    {
                        "name": "Ehsan Latif"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Zhai"
                },
                "author": "Xiaoming Zhai",
                "arxiv_comment": "Accepted to AIED25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05622v1",
                "updated": "2025-05-08T20:01:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    20,
                    1,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T20:01:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    20,
                    1,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical\n  Semantic Planning and Global Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical\n  Semantic Planning and Global Memory"
                },
                "summary": "Aerial vision-and-language navigation (VLN), requiring drones to interpret\nnatural language instructions and navigate complex urban environments, emerges\nas a critical embodied AI challenge that bridges human-robot interaction, 3D\nspatial reasoning, and real-world deployment. Although existing ground VLN\nagents achieved notable results in indoor and outdoor settings, they struggle\nin aerial VLN due to the absence of predefined navigation graphs and the\nexponentially expanding action space in long-horizon exploration. In this work,\nwe propose \\textbf{CityNavAgent}, a large language model (LLM)-empowered agent\nthat significantly reduces the navigation complexity for urban aerial VLN.\nSpecifically, we design a hierarchical semantic planning module (HSPM) that\ndecomposes the long-horizon task into sub-goals with different semantic levels.\nThe agent reaches the target progressively by achieving sub-goals with\ndifferent capacities of the LLM. Additionally, a global memory module storing\nhistorical trajectories into a topological graph is developed to simplify\nnavigation for visited targets. Extensive benchmark experiments show that our\nmethod achieves state-of-the-art performance with significant improvement.\nFurther experiments demonstrate the effectiveness of different modules of\nCityNavAgent for aerial VLN in continuous city environments. The code is\navailable at \\href{https://github.com/VinceOuti/CityNavAgent}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial vision-and-language navigation (VLN), requiring drones to interpret\nnatural language instructions and navigate complex urban environments, emerges\nas a critical embodied AI challenge that bridges human-robot interaction, 3D\nspatial reasoning, and real-world deployment. Although existing ground VLN\nagents achieved notable results in indoor and outdoor settings, they struggle\nin aerial VLN due to the absence of predefined navigation graphs and the\nexponentially expanding action space in long-horizon exploration. In this work,\nwe propose \\textbf{CityNavAgent}, a large language model (LLM)-empowered agent\nthat significantly reduces the navigation complexity for urban aerial VLN.\nSpecifically, we design a hierarchical semantic planning module (HSPM) that\ndecomposes the long-horizon task into sub-goals with different semantic levels.\nThe agent reaches the target progressively by achieving sub-goals with\ndifferent capacities of the LLM. Additionally, a global memory module storing\nhistorical trajectories into a topological graph is developed to simplify\nnavigation for visited targets. Extensive benchmark experiments show that our\nmethod achieves state-of-the-art performance with significant improvement.\nFurther experiments demonstrate the effectiveness of different modules of\nCityNavAgent for aerial VLN in continuous city environments. The code is\navailable at \\href{https://github.com/VinceOuti/CityNavAgent}{link}."
                },
                "authors": [
                    {
                        "name": "Weichen Zhang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Shiquan Yu"
                    },
                    {
                        "name": "Ruiying Peng"
                    },
                    {
                        "name": "Baining Zhao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Jinqiang Cui"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05619v1",
                "updated": "2025-05-08T19:58:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    58,
                    41,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T19:58:41Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    58,
                    41,
                    3,
                    128,
                    0
                ],
                "title": "LiteLMGuard: Seamless and Lightweight On-Device Prompt Filtering for\n  Safeguarding Small Language Models against Quantization-induced Risks and\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteLMGuard: Seamless and Lightweight On-Device Prompt Filtering for\n  Safeguarding Small Language Models against Quantization-induced Risks and\n  Vulnerabilities"
                },
                "summary": "The growing adoption of Large Language Models (LLMs) has influenced the\ndevelopment of their lighter counterparts-Small Language Models (SLMs)-to\nenable on-device deployment across smartphones and edge devices. These SLMs\noffer enhanced privacy, reduced latency, server-free functionality, and\nimproved user experience. However, due to resource constraints of on-device\nenvironment, SLMs undergo size optimization through compression techniques like\nquantization, which can inadvertently introduce fairness, ethical and privacy\nrisks. Critically, quantized SLMs may respond to harmful queries directly,\nwithout requiring adversarial manipulation, raising significant safety and\ntrust concerns.\n  To address this, we propose LiteLMGuard (LLMG), an on-device prompt guard\nthat provides real-time, prompt-level defense for quantized SLMs. Additionally,\nour prompt guard is designed to be model-agnostic such that it can be\nseamlessly integrated with any SLM, operating independently of underlying\narchitectures. Our LLMG formalizes prompt filtering as a deep learning\n(DL)-based prompt answerability classification task, leveraging semantic\nunderstanding to determine whether a query should be answered by any SLM. Using\nour curated dataset, Answerable-or-Not, we trained and fine-tuned several DL\nmodels and selected ELECTRA as the candidate, with 97.75% answerability\nclassification accuracy.\n  Our safety effectiveness evaluations demonstrate that LLMG defends against\nover 87% of harmful prompts, including both direct instruction and jailbreak\nattack strategies. We further showcase its ability to mitigate the Open\nKnowledge Attacks, where compromised SLMs provide unsafe responses without\nadversarial prompting. In terms of prompt filtering effectiveness, LLMG\nachieves near state-of-the-art filtering accuracy of 94%, with an average\nlatency of 135 ms, incurring negligible overhead for users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of Large Language Models (LLMs) has influenced the\ndevelopment of their lighter counterparts-Small Language Models (SLMs)-to\nenable on-device deployment across smartphones and edge devices. These SLMs\noffer enhanced privacy, reduced latency, server-free functionality, and\nimproved user experience. However, due to resource constraints of on-device\nenvironment, SLMs undergo size optimization through compression techniques like\nquantization, which can inadvertently introduce fairness, ethical and privacy\nrisks. Critically, quantized SLMs may respond to harmful queries directly,\nwithout requiring adversarial manipulation, raising significant safety and\ntrust concerns.\n  To address this, we propose LiteLMGuard (LLMG), an on-device prompt guard\nthat provides real-time, prompt-level defense for quantized SLMs. Additionally,\nour prompt guard is designed to be model-agnostic such that it can be\nseamlessly integrated with any SLM, operating independently of underlying\narchitectures. Our LLMG formalizes prompt filtering as a deep learning\n(DL)-based prompt answerability classification task, leveraging semantic\nunderstanding to determine whether a query should be answered by any SLM. Using\nour curated dataset, Answerable-or-Not, we trained and fine-tuned several DL\nmodels and selected ELECTRA as the candidate, with 97.75% answerability\nclassification accuracy.\n  Our safety effectiveness evaluations demonstrate that LLMG defends against\nover 87% of harmful prompts, including both direct instruction and jailbreak\nattack strategies. We further showcase its ability to mitigate the Open\nKnowledge Attacks, where compromised SLMs provide unsafe responses without\nadversarial prompting. In terms of prompt filtering effectiveness, LLMG\nachieves near state-of-the-art filtering accuracy of 94%, with an average\nlatency of 135 ms, incurring negligible overhead for users."
                },
                "authors": [
                    {
                        "name": "Kalyan Nakka"
                    },
                    {
                        "name": "Jimmy Dani"
                    },
                    {
                        "name": "Ausmit Mondal"
                    },
                    {
                        "name": "Nitesh Saxena"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh Saxena"
                },
                "author": "Nitesh Saxena",
                "arxiv_comment": "14 pages, 18 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05616v1",
                "updated": "2025-05-08T19:53:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    53,
                    53,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T19:53:53Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    53,
                    53,
                    3,
                    128,
                    0
                ],
                "title": "Leveraging Large Language Models for enzymatic reaction prediction and\n  characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for enzymatic reaction prediction and\n  characterization"
                },
                "summary": "Predicting enzymatic reactions is crucial for applications in biocatalysis,\nmetabolic engineering, and drug discovery, yet it remains a complex and\nresource-intensive task. Large Language Models (LLMs) have recently\ndemonstrated remarkable success in various scientific domains, e.g., through\ntheir ability to generalize knowledge, reason over complex structures, and\nleverage in-context learning strategies. In this study, we systematically\nevaluate the capability of LLMs, particularly the Llama-3.1 family (8B and\n70B), across three core biochemical tasks: Enzyme Commission number prediction,\nforward synthesis, and retrosynthesis. We compare single-task and multitask\nlearning strategies, employing parameter-efficient fine-tuning via LoRA\nadapters. Additionally, we assess performance across different data regimes to\nexplore their adaptability in low-data settings. Our results demonstrate that\nfine-tuned LLMs capture biochemical knowledge, with multitask learning\nenhancing forward- and retrosynthesis predictions by leveraging shared\nenzymatic information. We also identify key limitations, for example challenges\nin hierarchical EC classification schemes, highlighting areas for further\nimprovement in LLM-driven biochemical modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting enzymatic reactions is crucial for applications in biocatalysis,\nmetabolic engineering, and drug discovery, yet it remains a complex and\nresource-intensive task. Large Language Models (LLMs) have recently\ndemonstrated remarkable success in various scientific domains, e.g., through\ntheir ability to generalize knowledge, reason over complex structures, and\nleverage in-context learning strategies. In this study, we systematically\nevaluate the capability of LLMs, particularly the Llama-3.1 family (8B and\n70B), across three core biochemical tasks: Enzyme Commission number prediction,\nforward synthesis, and retrosynthesis. We compare single-task and multitask\nlearning strategies, employing parameter-efficient fine-tuning via LoRA\nadapters. Additionally, we assess performance across different data regimes to\nexplore their adaptability in low-data settings. Our results demonstrate that\nfine-tuned LLMs capture biochemical knowledge, with multitask learning\nenhancing forward- and retrosynthesis predictions by leveraging shared\nenzymatic information. We also identify key limitations, for example challenges\nin hierarchical EC classification schemes, highlighting areas for further\nimprovement in LLM-driven biochemical modeling."
                },
                "authors": [
                    {
                        "name": "Lorenzo Di Fruscia"
                    },
                    {
                        "name": "Jana Marie Weber"
                    }
                ],
                "author_detail": {
                    "name": "Jana Marie Weber"
                },
                "author": "Jana Marie Weber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02819v2",
                "updated": "2025-05-08T19:52:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    52,
                    34,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-05T17:47:42Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    42,
                    0,
                    125,
                    0
                ],
                "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations"
                },
                "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository."
                },
                "authors": [
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Valentin Malykh"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    },
                    {
                        "name": "Nikos Komodakis"
                    },
                    {
                        "name": "Sergey Zagoruyko"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Zagoruyko"
                },
                "author": "Sergey Zagoruyko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09186v2",
                "updated": "2025-05-08T19:38:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    38,
                    13,
                    3,
                    128,
                    0
                ],
                "published": "2024-10-11T18:39:25Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    18,
                    39,
                    25,
                    4,
                    285,
                    0
                ],
                "title": "Learning Algorithms Made Simple",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Algorithms Made Simple"
                },
                "summary": "In this paper, we discuss learning algorithms and their importance in\ndifferent types of applications which includes training to identify important\npatterns and features in a straightforward, easy-to-understand manner. We will\nreview the main concepts of artificial intelligence (AI), machine learning\n(ML), deep learning (DL), and hybrid models. Some important subsets of Machine\nLearning algorithms such as supervised, unsupervised, and reinforcement\nlearning are also discussed in this paper. These techniques can be used for\nsome important tasks like prediction, classification, and segmentation.\nConvolutional Neural Networks (CNNs) are used for image and video processing\nand many more applications. We dive into the architecture of CNNs and how to\nintegrate CNNs with ML algorithms to build hybrid models. This paper explores\nthe vulnerability of learning algorithms to noise, leading to\nmisclassification. We further discuss the integration of learning algorithms\nwith Large Language Models (LLM) to generate coherent responses applicable to\nmany domains such as healthcare, marketing, and finance by learning important\npatterns from large volumes of data. Furthermore, we discuss the next\ngeneration of learning algorithms and how we may have an unified Adaptive and\nDynamic Network to perform important tasks. Overall, this article provides\nbrief overview of learning algorithms, exploring their current state,\napplications and future direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we discuss learning algorithms and their importance in\ndifferent types of applications which includes training to identify important\npatterns and features in a straightforward, easy-to-understand manner. We will\nreview the main concepts of artificial intelligence (AI), machine learning\n(ML), deep learning (DL), and hybrid models. Some important subsets of Machine\nLearning algorithms such as supervised, unsupervised, and reinforcement\nlearning are also discussed in this paper. These techniques can be used for\nsome important tasks like prediction, classification, and segmentation.\nConvolutional Neural Networks (CNNs) are used for image and video processing\nand many more applications. We dive into the architecture of CNNs and how to\nintegrate CNNs with ML algorithms to build hybrid models. This paper explores\nthe vulnerability of learning algorithms to noise, leading to\nmisclassification. We further discuss the integration of learning algorithms\nwith Large Language Models (LLM) to generate coherent responses applicable to\nmany domains such as healthcare, marketing, and finance by learning important\npatterns from large volumes of data. Furthermore, we discuss the next\ngeneration of learning algorithms and how we may have an unified Adaptive and\nDynamic Network to perform important tasks. Overall, this article provides\nbrief overview of learning algorithms, exploring their current state,\napplications and future direction."
                },
                "authors": [
                    {
                        "name": "Noorbakhsh Amiri Golilarz"
                    },
                    {
                        "name": "Elias Hossain"
                    },
                    {
                        "name": "Abdoljalil Addeh"
                    },
                    {
                        "name": "Keyan Alexander Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Keyan Alexander Rahimi"
                },
                "author": "Keyan Alexander Rahimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04378v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04378v3",
                "updated": "2025-05-08T19:16:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    16,
                    29,
                    3,
                    128,
                    0
                ],
                "published": "2024-12-05T17:54:27Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    54,
                    27,
                    3,
                    340,
                    0
                ],
                "title": "VladVA: Discriminative Fine-tuning of LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VladVA: Discriminative Fine-tuning of LVLMs"
                },
                "summary": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the\nde facto approach for discriminative vision-language representation learning.\nHowever, these models have limited language understanding, often exhibiting a\n\"bag of words\" behavior. At the same time, Large Vision-Language Models\n(LVLMs), which combine vision encoders with LLMs, have been shown to be capable\nof detailed vision-language reasoning, yet their autoregressive nature renders\nthem less suitable for discriminative tasks.\n  In this work, we propose to combine \"the best of both worlds\": a new training\napproach for discriminative fine-tuning of LVLMs that results in strong\ndiscriminative and compositional capabilities. Essentially, our approach\nconverts a generative LVLM into a discriminative one, unlocking its capability\nfor powerful image-text discrimination combined with enhanced language\nunderstanding.\n  Our contributions include (1) a carefully designed training/optimization\nframework that utilizes image-text pairs of variable length and granularity for\ntraining the model with both contrastive and next-token prediction losses. This\nis accompanied by ablation studies that justify the necessity of our\nframework's components; (2) a parameter-efficient adaptation method using a\ncombination of soft prompting and LoRA adapters; (3) significant improvements\nover state-of-the-art CLIP-like models of similar size, including standard\nimage-text retrieval benchmarks and notable gains in compositionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the\nde facto approach for discriminative vision-language representation learning.\nHowever, these models have limited language understanding, often exhibiting a\n\"bag of words\" behavior. At the same time, Large Vision-Language Models\n(LVLMs), which combine vision encoders with LLMs, have been shown to be capable\nof detailed vision-language reasoning, yet their autoregressive nature renders\nthem less suitable for discriminative tasks.\n  In this work, we propose to combine \"the best of both worlds\": a new training\napproach for discriminative fine-tuning of LVLMs that results in strong\ndiscriminative and compositional capabilities. Essentially, our approach\nconverts a generative LVLM into a discriminative one, unlocking its capability\nfor powerful image-text discrimination combined with enhanced language\nunderstanding.\n  Our contributions include (1) a carefully designed training/optimization\nframework that utilizes image-text pairs of variable length and granularity for\ntraining the model with both contrastive and next-token prediction losses. This\nis accompanied by ablation studies that justify the necessity of our\nframework's components; (2) a parameter-efficient adaptation method using a\ncombination of soft prompting and LoRA adapters; (3) significant improvements\nover state-of-the-art CLIP-like models of similar size, including standard\nimage-text retrieval benchmarks and notable gains in compositionality."
                },
                "authors": [
                    {
                        "name": "Yassine Ouali"
                    },
                    {
                        "name": "Adrian Bulat"
                    },
                    {
                        "name": "Alexandros Xenos"
                    },
                    {
                        "name": "Anestis Zaganidis"
                    },
                    {
                        "name": "Ioannis Maniadis Metaxas"
                    },
                    {
                        "name": "Brais Martinez"
                    },
                    {
                        "name": "Georgios Tzimiropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Tzimiropoulos"
                },
                "author": "Georgios Tzimiropoulos",
                "arxiv_comment": "Published at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04378v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04378v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05602v1",
                "updated": "2025-05-08T19:05:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    5,
                    2,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T19:05:02Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    5,
                    2,
                    3,
                    128,
                    0
                ],
                "title": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation\n  Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation\n  Statistics"
                },
                "summary": "As Large Language Models (LLMs) and other AI systems evolve, robustly\nestimating their capabilities from inherently stochastic outputs while\nsystematically quantifying uncertainty in these estimates becomes increasingly\nimportant. Further, advanced AI evaluations often have a nested hierarchical\nstructure, exhibit high levels of complexity, and come with high costs in\ntesting the most advanced AI systems. To address these challenges, we introduce\nHiBayES, a generalizable Hierarchical Bayesian modeling framework for AI\nEvaluation Statistics. HiBayES supports robust inferences in classical\nquestion-answer benchmarks and advanced agentic evaluations, particularly in\nlow-data scenarios (e.g., < 20 data points per evaluation). Built on\nGeneralized Linear Models (GLMs), Bayesian data analysis, and formal model\ncomparison, HiBayES provides principled uncertainty quantification and robust\nparameter estimation. This paper offers a comprehensive introduction to\nHiBayES, including illustrative examples, comparisons to conventional\nstatistical methods, and practical guidance for implementing multilevel\nBayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta\nversion) for out-of-the-box implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) and other AI systems evolve, robustly\nestimating their capabilities from inherently stochastic outputs while\nsystematically quantifying uncertainty in these estimates becomes increasingly\nimportant. Further, advanced AI evaluations often have a nested hierarchical\nstructure, exhibit high levels of complexity, and come with high costs in\ntesting the most advanced AI systems. To address these challenges, we introduce\nHiBayES, a generalizable Hierarchical Bayesian modeling framework for AI\nEvaluation Statistics. HiBayES supports robust inferences in classical\nquestion-answer benchmarks and advanced agentic evaluations, particularly in\nlow-data scenarios (e.g., < 20 data points per evaluation). Built on\nGeneralized Linear Models (GLMs), Bayesian data analysis, and formal model\ncomparison, HiBayES provides principled uncertainty quantification and robust\nparameter estimation. This paper offers a comprehensive introduction to\nHiBayES, including illustrative examples, comparisons to conventional\nstatistical methods, and practical guidance for implementing multilevel\nBayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta\nversion) for out-of-the-box implementation."
                },
                "authors": [
                    {
                        "name": "Lennart Luettgau"
                    },
                    {
                        "name": "Harry Coppock"
                    },
                    {
                        "name": "Magda Dubois"
                    },
                    {
                        "name": "Christopher Summerfield"
                    },
                    {
                        "name": "Cozmin Ududec"
                    }
                ],
                "author_detail": {
                    "name": "Cozmin Ududec"
                },
                "author": "Cozmin Ududec",
                "arxiv_comment": "23 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05600v1",
                "updated": "2025-05-08T19:00:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    0,
                    11,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T19:00:11Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    19,
                    0,
                    11,
                    3,
                    128,
                    0
                ],
                "title": "Enhancing Large Language Models with Faster Code Preprocessing for\n  Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models with Faster Code Preprocessing for\n  Vulnerability Detection"
                },
                "summary": "The application of Artificial Intelligence has become a powerful approach to\ndetecting software vulnerabilities. However, effective vulnerability detection\nrelies on accurately capturing the semantic structure of code and its\ncontextual relationships. Given that the same functionality can be implemented\nin various forms, a preprocessing tool that standardizes code representation is\nimportant. This tool must be efficient, adaptable across programming languages,\nand capable of supporting new transformations. To address this challenge, we\nbuild on the existing SCoPE framework and introduce SCoPE2, an enhanced version\nwith improved performance. We compare both versions in terms of processing time\nand memory usage and evaluate their impact on a Large Language Model (LLM) for\nvulnerability detection. Our results show a 97.3\\% reduction in processing time\nwith SCoPE2, along with an improved F1-score for the LLM, solely due to the\nrefined preprocessing approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of Artificial Intelligence has become a powerful approach to\ndetecting software vulnerabilities. However, effective vulnerability detection\nrelies on accurately capturing the semantic structure of code and its\ncontextual relationships. Given that the same functionality can be implemented\nin various forms, a preprocessing tool that standardizes code representation is\nimportant. This tool must be efficient, adaptable across programming languages,\nand capable of supporting new transformations. To address this challenge, we\nbuild on the existing SCoPE framework and introduce SCoPE2, an enhanced version\nwith improved performance. We compare both versions in terms of processing time\nand memory usage and evaluate their impact on a Large Language Model (LLM) for\nvulnerability detection. Our results show a 97.3\\% reduction in processing time\nwith SCoPE2, along with an improved F1-score for the LLM, solely due to the\nrefined preprocessing approach."
                },
                "authors": [
                    {
                        "name": "José Gonçalves"
                    },
                    {
                        "name": "Miguel Silva"
                    },
                    {
                        "name": "Eva Maia"
                    },
                    {
                        "name": "Isabel Praça"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Praça"
                },
                "author": "Isabel Praça",
                "arxiv_comment": "10 pages, 3 tables, DCAI'25: Distributed Computing and Artificial\n  Intelligence 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05587v1",
                "updated": "2025-05-08T18:41:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    18,
                    41,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T18:41:38Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    18,
                    41,
                    38,
                    3,
                    128,
                    0
                ],
                "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steepest Descent Density Control for Compact 3D Gaussian Splatting"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for\nreal-time, high-resolution novel view synthesis. By representing scenes as a\nmixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for\nefficient rendering and reconstruction. To optimize scene coverage and capture\nfine details, 3DGS employs a densification algorithm to generate additional\npoints. However, this process often leads to redundant point clouds, resulting\nin excessive memory usage, slower performance, and substantial storage demands\n- posing significant challenges for deployment on resource-constrained devices.\nTo address this limitation, we propose a theoretical framework that demystifies\nand improves density control in 3DGS. Our analysis reveals that splitting is\ncrucial for escaping saddle points. Through an optimization-theoretic approach,\nwe establish the necessary conditions for densification, determine the minimal\nnumber of offspring Gaussians, identify the optimal parameter update direction,\nand provide an analytical solution for normalizing off-spring opacity. Building\non these insights, we introduce SteepGS, incorporating steepest density\ncontrol, a principled strategy that minimizes loss while maintaining a compact\npoint cloud. SteepGS achieves a ~50% reduction in Gaussian points without\ncompromising rendering quality, significantly enhancing both efficiency and\nscalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for\nreal-time, high-resolution novel view synthesis. By representing scenes as a\nmixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for\nefficient rendering and reconstruction. To optimize scene coverage and capture\nfine details, 3DGS employs a densification algorithm to generate additional\npoints. However, this process often leads to redundant point clouds, resulting\nin excessive memory usage, slower performance, and substantial storage demands\n- posing significant challenges for deployment on resource-constrained devices.\nTo address this limitation, we propose a theoretical framework that demystifies\nand improves density control in 3DGS. Our analysis reveals that splitting is\ncrucial for escaping saddle points. Through an optimization-theoretic approach,\nwe establish the necessary conditions for densification, determine the minimal\nnumber of offspring Gaussians, identify the optimal parameter update direction,\nand provide an analytical solution for normalizing off-spring opacity. Building\non these insights, we introduce SteepGS, incorporating steepest density\ncontrol, a principled strategy that minimizes loss while maintaining a compact\npoint cloud. SteepGS achieves a ~50% reduction in Gaussian points without\ncompromising rendering quality, significantly enhancing both efficiency and\nscalability."
                },
                "authors": [
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Yuehao Wang"
                    },
                    {
                        "name": "Dilin Wang"
                    },
                    {
                        "name": "Sreyas Mohan"
                    },
                    {
                        "name": "Zhiwen Fan"
                    },
                    {
                        "name": "Lemeng Wu"
                    },
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yu-Ying Yeh"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Rakesh Ranjan"
                    }
                ],
                "author_detail": {
                    "name": "Rakesh Ranjan"
                },
                "author": "Rakesh Ranjan",
                "arxiv_comment": "CVPR 2025, Project page: https://vita-group.github.io/SteepGS/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05584v1",
                "updated": "2025-05-08T18:30:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    18,
                    30,
                    22,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T18:30:22Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    18,
                    30,
                    22,
                    3,
                    128,
                    0
                ],
                "title": "PRIMG : Efficient LLM-driven Test Generation Using Mutant Prioritization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRIMG : Efficient LLM-driven Test Generation Using Mutant Prioritization"
                },
                "summary": "Mutation testing is a widely recognized technique for assessing and enhancing\nthe effectiveness of software test suites by introducing deliberate code\nmutations. However, its application often results in overly large test suites,\nas developers generate numerous tests to kill specific mutants, increasing\ncomputational overhead. This paper introduces PRIMG (Prioritization and\nRefinement Integrated Mutation-driven Generation), a novel framework for\nincremental and adaptive test case generation for Solidity smart contracts.\nPRIMG integrates two core components: a mutation prioritization module, which\nemploys a machine learning model trained on mutant subsumption graphs to\npredict the usefulness of surviving mutants, and a test case generation module,\nwhich utilizes Large Language Models (LLMs) to generate and iteratively refine\ntest cases to achieve syntactic and behavioral correctness.\n  We evaluated PRIMG on real-world Solidity projects from Code4Arena to assess\nits effectiveness in improving mutation scores and generating high-quality test\ncases. The experimental results demonstrate that PRIMG significantly reduces\ntest suite size while maintaining high mutation coverage. The prioritization\nmodule consistently outperformed random mutant selection, enabling the\ngeneration of high-impact tests with reduced computational effort. Furthermore,\nthe refining process enhanced the correctness and utility of LLM-generated\ntests, addressing their inherent limitations in handling edge cases and complex\nprogram logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation testing is a widely recognized technique for assessing and enhancing\nthe effectiveness of software test suites by introducing deliberate code\nmutations. However, its application often results in overly large test suites,\nas developers generate numerous tests to kill specific mutants, increasing\ncomputational overhead. This paper introduces PRIMG (Prioritization and\nRefinement Integrated Mutation-driven Generation), a novel framework for\nincremental and adaptive test case generation for Solidity smart contracts.\nPRIMG integrates two core components: a mutation prioritization module, which\nemploys a machine learning model trained on mutant subsumption graphs to\npredict the usefulness of surviving mutants, and a test case generation module,\nwhich utilizes Large Language Models (LLMs) to generate and iteratively refine\ntest cases to achieve syntactic and behavioral correctness.\n  We evaluated PRIMG on real-world Solidity projects from Code4Arena to assess\nits effectiveness in improving mutation scores and generating high-quality test\ncases. The experimental results demonstrate that PRIMG significantly reduces\ntest suite size while maintaining high mutation coverage. The prioritization\nmodule consistently outperformed random mutant selection, enabling the\ngeneration of high-impact tests with reduced computational effort. Furthermore,\nthe refining process enhanced the correctness and utility of LLM-generated\ntests, addressing their inherent limitations in handling edge cases and complex\nprogram logic."
                },
                "authors": [
                    {
                        "name": "Mohamed Salah Bouafif"
                    },
                    {
                        "name": "Mohammad Hamdaqa"
                    },
                    {
                        "name": "Edward Zulkoski"
                    }
                ],
                "author_detail": {
                    "name": "Edward Zulkoski"
                },
                "author": "Edward Zulkoski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03332v2",
                "updated": "2025-05-08T18:27:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    18,
                    27,
                    45,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-06T09:06:18Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    6,
                    18,
                    1,
                    126,
                    0
                ],
                "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning"
                },
                "summary": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks."
                },
                "authors": [
                    {
                        "name": "Evgeny Markhasin"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Markhasin"
                },
                "author": "Evgeny Markhasin",
                "arxiv_comment": "22 pages, 36 pages (references and appendixes)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05583v1",
                "updated": "2025-05-08T18:27:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    18,
                    27,
                    27,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T18:27:27Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    18,
                    27,
                    27,
                    3,
                    128,
                    0
                ],
                "title": "KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot\n  Hierarchical Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot\n  Hierarchical Text Classification"
                },
                "summary": "Hierarchical Text Classification (HTC) involves assigning documents to labels\norganized within a taxonomy. Most previous research on HTC has focused on\nsupervised methods. However, in real-world scenarios, employing supervised HTC\ncan be challenging due to a lack of annotated data. Moreover, HTC often faces\nissues with large label spaces and long-tail distributions. In this work, we\npresent Knowledge Graphs for zero-shot Hierarchical Text Classification\n(KG-HTC), which aims to address these challenges of HTC in applications by\nintegrating knowledge graphs with Large Language Models (LLMs) to provide\nstructured semantic context during classification. Our method retrieves\nrelevant subgraphs from knowledge graphs related to the input text using a\nRetrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to\nunderstand label semantics at various hierarchy levels. We evaluate KG-HTC on\nthree open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental\nresults show that KG-HTC significantly outperforms three baselines in the\nstrict zero-shot setting, particularly achieving substantial improvements at\ndeeper levels of the hierarchy. This evaluation demonstrates the effectiveness\nof incorporating structured knowledge into LLMs to address HTC's challenges in\nlarge label spaces and long-tailed label distributions. Our code is available\nat: https://github.com/QianboZang/KG-HTC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Text Classification (HTC) involves assigning documents to labels\norganized within a taxonomy. Most previous research on HTC has focused on\nsupervised methods. However, in real-world scenarios, employing supervised HTC\ncan be challenging due to a lack of annotated data. Moreover, HTC often faces\nissues with large label spaces and long-tail distributions. In this work, we\npresent Knowledge Graphs for zero-shot Hierarchical Text Classification\n(KG-HTC), which aims to address these challenges of HTC in applications by\nintegrating knowledge graphs with Large Language Models (LLMs) to provide\nstructured semantic context during classification. Our method retrieves\nrelevant subgraphs from knowledge graphs related to the input text using a\nRetrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to\nunderstand label semantics at various hierarchy levels. We evaluate KG-HTC on\nthree open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental\nresults show that KG-HTC significantly outperforms three baselines in the\nstrict zero-shot setting, particularly achieving substantial improvements at\ndeeper levels of the hierarchy. This evaluation demonstrates the effectiveness\nof incorporating structured knowledge into LLMs to address HTC's challenges in\nlarge label spaces and long-tailed label distributions. Our code is available\nat: https://github.com/QianboZang/KG-HTC."
                },
                "authors": [
                    {
                        "name": "Qianbo Zang"
                    },
                    {
                        "name": "Christophe Zgrzendek"
                    },
                    {
                        "name": "Igor Tchappi"
                    },
                    {
                        "name": "Afshin Khadangi"
                    },
                    {
                        "name": "Johannes Sedlmeir"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Sedlmeir"
                },
                "author": "Johannes Sedlmeir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05467v1",
                "updated": "2025-05-08T17:57:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    57,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:57:40Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    57,
                    40,
                    3,
                    128,
                    0
                ],
                "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant"
                },
                "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Bo Feng"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Weifeng Ge"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Ping Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Huang"
                },
                "author": "Ping Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05465v1",
                "updated": "2025-05-08T17:56:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    57,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:56:57Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    57,
                    3,
                    128,
                    0
                ],
                "title": "ComPO: Preference Alignment via Comparison Oracles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComPO: Preference Alignment via Comparison Oracles"
                },
                "summary": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}."
                },
                "authors": [
                    {
                        "name": "Peter Chen"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wotao Yin"
                    },
                    {
                        "name": "Tianyi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Lin"
                },
                "author": "Tianyi Lin",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05464v1",
                "updated": "2025-05-08T17:56:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    23,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:56:23Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    56,
                    23,
                    3,
                    128,
                    0
                ],
                "title": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging"
                },
                "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation."
                },
                "authors": [
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Siyang Gao"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Manling Li"
                    },
                    {
                        "name": "Junxian He"
                    }
                ],
                "author_detail": {
                    "name": "Junxian He"
                },
                "author": "Junxian He",
                "arxiv_comment": "ICML 2025. Our code is publicly available at\n  https://github.com/shiqichen17/VLM_Merging",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05453v1",
                "updated": "2025-05-08T17:44:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    44,
                    45,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T17:44:45Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    44,
                    45,
                    3,
                    128,
                    0
                ],
                "title": "Conversational Process Model Redesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Process Model Redesign"
                },
                "summary": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria."
                },
                "authors": [
                    {
                        "name": "Nataliia Klievtsova"
                    },
                    {
                        "name": "Timotheus Kampik"
                    },
                    {
                        "name": "Juergen Mangler"
                    },
                    {
                        "name": "Stefanie Rinderle-Ma"
                    }
                ],
                "author_detail": {
                    "name": "Stefanie Rinderle-Ma"
                },
                "author": "Stefanie Rinderle-Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]