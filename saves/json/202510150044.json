[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.01875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v3",
                "updated": "2025-10-13T17:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    15,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11292v1",
                "updated": "2025-10-13T11:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences"
                },
                "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."
                },
                "authors": [
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v2",
                "updated": "2025-10-13T11:21:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    21,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_doi": "10.1088/1367-2630/ae0ea7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1367-2630/ae0ea7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Added a figure, minor changes to text",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v2",
                "updated": "2025-10-13T10:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    39,
                    59,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19257v2",
                "updated": "2025-10-13T10:18:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    18,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "Manuscript submitted to AAAI 2026, currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11236v1",
                "updated": "2025-10-13T10:17:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T10:17:21Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy."
                },
                "authors": [
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v2",
                "updated": "2025-10-13T09:12:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    12,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v2",
                "updated": "2025-10-13T08:26:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    26,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11121v1",
                "updated": "2025-10-13T08:08:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T08:08:58Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM"
                },
                "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Rongjie Zhu"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Zhiguang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguang Cao"
                },
                "author": "Zhiguang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11011v1",
                "updated": "2025-10-13T05:03:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T05:03:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads"
                },
                "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction."
                },
                "authors": [
                    {
                        "name": "Farzaneh Zirak"
                    },
                    {
                        "name": "Farhana Choudhury"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "arxiv_comment": "This is a preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10975v2",
                "updated": "2025-10-14T07:41:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    7,
                    41,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T03:26:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    26,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model"
                },
                "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference."
                },
                "authors": [
                    {
                        "name": "Mingtong Dai"
                    },
                    {
                        "name": "Lingbo Liu"
                    },
                    {
                        "name": "Yongjie Bai"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhouxia Wang"
                    },
                    {
                        "name": "Rui SU"
                    },
                    {
                        "name": "Chunjie Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Xinyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Wu"
                },
                "author": "Xinyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10964v1",
                "updated": "2025-10-13T03:14:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T03:14:28Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models"
                },
                "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10862v1",
                "updated": "2025-10-13T00:11:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T00:11:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "A Joint Learning Approach to Hardware Caching and Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Joint Learning Approach to Hardware Caching and Prefetching"
                },
                "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."
                },
                "authors": [
                    {
                        "name": "Samuel Yuan"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Nihal Sharma"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "arxiv_comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10858v1",
                "updated": "2025-10-12T23:46:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T23:46:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking"
                },
                "summary": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation."
                },
                "authors": [
                    {
                        "name": "Guanli Liu"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00313v2",
                "updated": "2025-10-12T23:17:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    17,
                    39,
                    6,
                    285,
                    0
                ],
                "published": "2024-05-01T04:30:03Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    4,
                    30,
                    3,
                    2,
                    122,
                    0
                ],
                "title": "Streamlining Image Editing with Layered Diffusion Brushes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Image Editing with Layered Diffusion Brushes"
                },
                "summary": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing."
                },
                "authors": [
                    {
                        "name": "Peyman Gholami"
                    },
                    {
                        "name": "Robert Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Robert Xiao"
                },
                "author": "Robert Xiao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2306.00219",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10587v1",
                "updated": "2025-10-12T13:06:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T13:06:59Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "title": "A Simple and Better Baseline for Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Better Baseline for Visual Grounding"
                },
                "summary": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG."
                },
                "authors": [
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Dingjiang Huang"
                    },
                    {
                        "name": "Hong Wang"
                    },
                    {
                        "name": "Yefeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yefeng Zheng"
                },
                "author": "Yefeng Zheng",
                "arxiv_comment": "ICME2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v2",
                "updated": "2025-10-12T10:09:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    10,
                    9,
                    53,
                    6,
                    285,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by NeurIPS 2025. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v2",
                "updated": "2025-10-12T04:46:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    46,
                    48,
                    6,
                    285,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "arxiv_comment": "fix typo perplexity->log perplexity; added recent papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v3",
                "updated": "2025-10-12T04:04:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    4,
                    34,
                    6,
                    285,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "arxiv_doi": "10.1145/3676642.3736114",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736114",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Vol.\n  3, Rotterdam, Netherlands, 2025, pp. 147-162",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10290v1",
                "updated": "2025-10-11T17:08:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T17:08:45Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "title": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines"
                },
                "summary": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching."
                },
                "authors": [
                    {
                        "name": "Sayan Mandal"
                    },
                    {
                        "name": "Hua Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Jiang"
                },
                "author": "Hua Jiang",
                "arxiv_comment": "Submitted to MLSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10219v1",
                "updated": "2025-10-11T13:52:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T13:52:48Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "title": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc"
                },
                "summary": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10129v1",
                "updated": "2025-10-11T09:28:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T09:28:26Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheClip: Accelerating RAG with Effective KV Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems."
                },
                "authors": [
                    {
                        "name": "Bin Yang"
                    },
                    {
                        "name": "Qiuyu Leng"
                    },
                    {
                        "name": "Jun Zeng"
                    },
                    {
                        "name": "Zhenhua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Wu"
                },
                "author": "Zhenhua Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v3",
                "updated": "2025-10-11T09:04:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    4,
                    23,
                    5,
                    284,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10102v1",
                "updated": "2025-10-11T08:24:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T08:24:19Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling"
                },
                "summary": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling."
                },
                "authors": [
                    {
                        "name": "Guilin Li"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Xiuyuan Chen"
                    },
                    {
                        "name": "Chengqi Li"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Matthias Hwai Yong Tan"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hwai Yong Tan"
                },
                "author": "Matthias Hwai Yong Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09952v1",
                "updated": "2025-10-11T01:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T01:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "title": "HTTP Request Synchronization Defeats Discrepancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HTTP Request Synchronization Defeats Discrepancy Attacks"
                },
                "summary": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact."
                },
                "authors": [
                    {
                        "name": "Cem Topcuoglu"
                    },
                    {
                        "name": "Kaan Onarlioglu"
                    },
                    {
                        "name": "Steven Sprecher"
                    },
                    {
                        "name": "Engin Kirda"
                    }
                ],
                "author_detail": {
                    "name": "Engin Kirda"
                },
                "author": "Engin Kirda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09907v1",
                "updated": "2025-10-10T22:43:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T22:43:54Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem"
                },
                "summary": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt."
                },
                "authors": [
                    {
                        "name": "Muhammad Maaz"
                    },
                    {
                        "name": "Liam DeVoe"
                    },
                    {
                        "name": "Zac Hatfield-Dodds"
                    },
                    {
                        "name": "Nicholas Carlini"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carlini"
                },
                "author": "Nicholas Carlini",
                "arxiv_comment": "4 pages (main), NeurIPS 2025, The 4th Deep Learning for Code Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09883v1",
                "updated": "2025-10-10T21:37:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T21:37:49Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning"
                },
                "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning."
                },
                "authors": [
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Murali Annavarm"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavarm"
                },
                "author": "Murali Annavarm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09847v1",
                "updated": "2025-10-10T20:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T20:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling"
                },
                "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability."
                },
                "authors": [
                    {
                        "name": "Said Muhammad"
                    },
                    {
                        "name": "Lahlou Laaziz"
                    },
                    {
                        "name": "Nadjia Kara"
                    },
                    {
                        "name": "Phat Tan Nguyen"
                    },
                    {
                        "name": "Timothy Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Murphy"
                },
                "author": "Timothy Murphy",
                "arxiv_comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09608v1",
                "updated": "2025-10-10T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingVLM: Real-Time Understanding for Infinite Video Streams"
                },
                "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."
                },
                "authors": [
                    {
                        "name": "Ruyi Xu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Liuning He"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v2",
                "updated": "2025-10-10T16:56:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    56,
                    23,
                    4,
                    283,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_doi": "10.1145/3769780",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769780",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v2",
                "updated": "2025-10-10T16:08:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    8,
                    26,
                    4,
                    283,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. Kühn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Kühn"
                },
                "author": "Martin J. Kühn",
                "arxiv_comment": "29 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09477v1",
                "updated": "2025-10-10T15:32:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T15:32:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "Efficient Autoregressive Inference for Transformer Probabilistic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Autoregressive Inference for Transformer Probabilistic Models"
                },
                "summary": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models."
                },
                "authors": [
                    {
                        "name": "Conor Hassan"
                    },
                    {
                        "name": "Nasrulloh Loka"
                    },
                    {
                        "name": "Cen-You Li"
                    },
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Paul E. Chang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Francesco Silvestrin"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Luigi Acerbi"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Acerbi"
                },
                "author": "Luigi Acerbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09409v1",
                "updated": "2025-10-10T14:03:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T14:03:42Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "title": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network"
                },
                "summary": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC."
                },
                "authors": [
                    {
                        "name": "Chongxiao Cai"
                    },
                    {
                        "name": "Yan Zhu"
                    },
                    {
                        "name": "Min Sheng"
                    },
                    {
                        "name": "Jiandong Li"
                    },
                    {
                        "name": "Yan Shi"
                    },
                    {
                        "name": "Di Zhou"
                    },
                    {
                        "name": "Ziwen Xie"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08272v2",
                "updated": "2025-10-10T13:15:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    15,
                    40,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-09T14:29:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    29,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors"
                },
                "summary": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures."
                },
                "authors": [
                    {
                        "name": "Cédrick Austa"
                    },
                    {
                        "name": "Jan Tobias Mühlberg"
                    },
                    {
                        "name": "Jean-Michel Dricot"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Dricot"
                },
                "author": "Jean-Michel Dricot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v2",
                "updated": "2025-10-10T13:08:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    8,
                    39,
                    4,
                    283,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "The paper is currently under investigation regarding concerns of\n  potential academic misconduct. While the investigation is ongoing, the\n  authors have voluntarily requested to withdraw the manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09309v1",
                "updated": "2025-10-10T12:01:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T12:01:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference"
                },
                "summary": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV"
                },
                "authors": [
                    {
                        "name": "Jianuo Huang"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09182v1",
                "updated": "2025-10-10T09:24:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    24,
                    53,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T09:24:53Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    24,
                    53,
                    4,
                    283,
                    0
                ],
                "title": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with\n  Low Memory Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with\n  Low Memory Consumption"
                },
                "summary": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware."
                },
                "authors": [
                    {
                        "name": "Johann-Friedrich Feiden"
                    },
                    {
                        "name": "Tim Küchler"
                    },
                    {
                        "name": "Denis Zavadski"
                    },
                    {
                        "name": "Bogdan Savchynskyy"
                    },
                    {
                        "name": "Carsten Rother"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rother"
                },
                "author": "Carsten Rother",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09154v1",
                "updated": "2025-10-10T08:57:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    8,
                    57,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T08:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    8,
                    57,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for\n  High-Power Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for\n  High-Power Applications"
                },
                "summary": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications."
                },
                "authors": [
                    {
                        "name": "Tanjim Rahman"
                    },
                    {
                        "name": "Trupti Ranjan Lenka"
                    }
                ],
                "author_detail": {
                    "name": "Trupti Ranjan Lenka"
                },
                "author": "Trupti Ranjan Lenka",
                "arxiv_comment": "13 pages, 13 figures including DC, RF, and breakdown analysis of\n  field-plated AlGaN/GaN HEMT using TCAD simulations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v3",
                "updated": "2025-10-09T20:37:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    37,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08803v1",
                "updated": "2025-10-09T20:35:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T20:35:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Man-Made Heuristics Are Dead. Long Live Code Generators!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Man-Made Heuristics Are Dead. Long Live Code Generators!"
                },
                "summary": "Policy design for various systems controllers has conventionally been a\nmanual process, with domain experts carefully tailoring heuristics for the\nspecific instance in which the policy will be deployed. In this paper, we\nre-imagine policy design via a novel automated search technique fueled by\nrecent advances in generative models, specifically Large Language Model\n(LLM)-driven code generation. We outline the design and implementation of\nPolicySmith, a framework that applies LLMs to synthesize instance-optimal\nheuristics. We apply PolicySmith to two long-standing systems policies - web\ncaching and congestion control, highlighting the opportunities unraveled by\nthis LLM-driven heuristic search. For caching, PolicySmith discovers heuristics\nthat outperform established baselines on standard open-source traces. For\ncongestion control, we show that PolicySmith can generate safe policies that\nintegrate directly into the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy design for various systems controllers has conventionally been a\nmanual process, with domain experts carefully tailoring heuristics for the\nspecific instance in which the policy will be deployed. In this paper, we\nre-imagine policy design via a novel automated search technique fueled by\nrecent advances in generative models, specifically Large Language Model\n(LLM)-driven code generation. We outline the design and implementation of\nPolicySmith, a framework that applies LLMs to synthesize instance-optimal\nheuristics. We apply PolicySmith to two long-standing systems policies - web\ncaching and congestion control, highlighting the opportunities unraveled by\nthis LLM-driven heuristic search. For caching, PolicySmith discovers heuristics\nthat outperform established baselines on standard open-source traces. For\ncongestion control, we show that PolicySmith can generate safe policies that\nintegrate directly into the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Rohit Dwivedula"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Swarat Chaudhuri"
                    },
                    {
                        "name": "Daehyeok Kim"
                    }
                ],
                "author_detail": {
                    "name": "Daehyeok Kim"
                },
                "author": "Daehyeok Kim",
                "arxiv_comment": "10 pages, 2 figures, 2 tables. To be presented at HotNets 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08774v1",
                "updated": "2025-10-09T19:45:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    45,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T19:45:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    45,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Struc-EMB: The Potential of Structure-Aware Encoding in Language\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Struc-EMB: The Potential of Structure-Aware Encoding in Language\n  Embeddings"
                },
                "summary": "Text embeddings from Large Language Models (LLMs) have become foundational\nfor numerous applications. However, these models typically operate on raw text,\noverlooking the rich structural information, such as hyperlinks or citations,\nthat provides crucial context in many real-world datasets. This paper\nintroduces and systematically evaluates a new paradigm for generating\nstructure-aware text embeddings by integrating these structural relations\ndirectly into the LLM's internal encoding process, rather than relying on\ntraditional post-hoc aggregation. We investigate two primary in-process\nmethods: sequential concatenation and parallel caching. Through extensive\nzero-shot experiments across retrieval, clustering, classification, and\nrecommendation tasks, we demonstrate that our structure-aware approaches\nconsistently outperform both text-only and post-hoc baselines. Our analysis\nreveals critical trade-offs: sequential concatenation excels with noisy,\nmoderate-length contexts, while parallel caching scales more effectively to\nlong, high-signal contexts but is more susceptible to distractors. To address\nthe challenge of noisy structural data, we also introduce and validate two\neffective techniques: Context Distillation and Semantic Balancing. This work\nprovides the first comprehensive analysis of in-process structure-aware\nencoding, offering a blueprint for building more powerful and contextually\naware embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings from Large Language Models (LLMs) have become foundational\nfor numerous applications. However, these models typically operate on raw text,\noverlooking the rich structural information, such as hyperlinks or citations,\nthat provides crucial context in many real-world datasets. This paper\nintroduces and systematically evaluates a new paradigm for generating\nstructure-aware text embeddings by integrating these structural relations\ndirectly into the LLM's internal encoding process, rather than relying on\ntraditional post-hoc aggregation. We investigate two primary in-process\nmethods: sequential concatenation and parallel caching. Through extensive\nzero-shot experiments across retrieval, clustering, classification, and\nrecommendation tasks, we demonstrate that our structure-aware approaches\nconsistently outperform both text-only and post-hoc baselines. Our analysis\nreveals critical trade-offs: sequential concatenation excels with noisy,\nmoderate-length contexts, while parallel caching scales more effectively to\nlong, high-signal contexts but is more susceptible to distractors. To address\nthe challenge of noisy structural data, we also introduce and validate two\neffective techniques: Context Distillation and Semantic Balancing. This work\nprovides the first comprehensive analysis of in-process structure-aware\nencoding, offering a blueprint for building more powerful and contextually\naware embedding models."
                },
                "authors": [
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08525v1",
                "updated": "2025-10-09T17:50:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:50:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression"
                },
                "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results."
                },
                "authors": [
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v2",
                "updated": "2025-10-09T17:45:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v2",
                "updated": "2025-10-09T17:38:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    38,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "arxiv_comment": "Original version uploaded on Sep 22, 2025. (v2): Extended Table 2\n  with additional analysis and referenced it in Sec 5.2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08669v1",
                "updated": "2025-10-09T17:22:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    22,
                    23,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:22:23Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    22,
                    23,
                    3,
                    282,
                    0
                ],
                "title": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching"
                },
                "summary": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Deyang Kong"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Yupei Pan"
                    },
                    {
                        "name": "Haowen Xu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Junshu Tang"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08351v1",
                "updated": "2025-10-09T15:38:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:38:13Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "title": "FMCache: File-System Metadata Caching in Programmable Switches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMCache: File-System Metadata Caching in Programmable Switches"
                },
                "summary": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage."
                },
                "authors": [
                    {
                        "name": "Qingxiu Liu"
                    },
                    {
                        "name": "Jiazhen Cai"
                    },
                    {
                        "name": "Siyuan Sheng"
                    },
                    {
                        "name": "Yuhui Chen"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Zhirong Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Patrick P. C. Lee"
                },
                "arxiv_affiliation": "The Chinese University of Hong Kong",
                "author": "Patrick P. C. Lee",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08180v1",
                "updated": "2025-10-09T13:06:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:06:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "title": "Towards Energy-Efficient Serverless Computing with Hardware Isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Energy-Efficient Serverless Computing with Hardware Isolation"
                },
                "summary": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW."
                },
                "authors": [
                    {
                        "name": "Natalie Carl"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v2",
                "updated": "2025-10-09T13:03:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    3,
                    29,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v4",
                "updated": "2025-10-09T12:05:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    5,
                    4,
                    3,
                    282,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v2",
                "updated": "2025-10-09T12:01:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    1,
                    20,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Panagiotis Karras"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Jignesh M. Patel"
                },
                "author": "Jignesh M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v4",
                "updated": "2025-10-09T09:33:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    9,
                    33,
                    47,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v2",
                "updated": "2025-10-09T09:14:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    9,
                    14,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face challenges in meeting the demand for\nhigher data rates and reliable connectivity in complex environments. Stacked\nintelligent metasurfaces (SIMs) have emerged as a promising technology for\nadvanced wave-domain signal processing, where mobile SIMs can outperform fixed\ncounterparts. In this paper, we propose a novel unmanned aerial vehicle\n(UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude\neconomy (LAE) networks, where UAVs act as both cache-enabled base stations and\nmobile SIM carriers to enhance uplink transmissions. To maximize network\ncapacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that\nintegrates user association, UAV-SIM three-dimensional positioning, and\nmulti-layer SIM phase shift design. Due to the non-convexity and NP-hardness of\nUSBJOP, we decompose it into three subproblems, which are the association\nbetween UAV-SIMs and users optimization problem (AUUOP), the UAV location\noptimization problem (ULOP), and the UAV-SIM phase shifts optimization problem\n(USPSOP). Then, we solve them through an alternating optimization strategy.\nSpecifically, AUUOP and ULOP are transformed into convex forms solvable via the\nCVX tool, while USPSOP is addressed by a generative artificial intelligence\n(GAI)-based hybrid optimization algorithm. Simulation results show that the\nproposed approach achieves approximately 1.5 times higher network capacity\ncompared with suboptimal schemes, effectively mitigates multi-user interference\nwith increasing SIM layers and meta-atoms, and reduces runtime by 10\\% while\nmaintaining solution quality, thereby demonstrating its practicality for\nreal-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face challenges in meeting the demand for\nhigher data rates and reliable connectivity in complex environments. Stacked\nintelligent metasurfaces (SIMs) have emerged as a promising technology for\nadvanced wave-domain signal processing, where mobile SIMs can outperform fixed\ncounterparts. In this paper, we propose a novel unmanned aerial vehicle\n(UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude\neconomy (LAE) networks, where UAVs act as both cache-enabled base stations and\nmobile SIM carriers to enhance uplink transmissions. To maximize network\ncapacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that\nintegrates user association, UAV-SIM three-dimensional positioning, and\nmulti-layer SIM phase shift design. Due to the non-convexity and NP-hardness of\nUSBJOP, we decompose it into three subproblems, which are the association\nbetween UAV-SIMs and users optimization problem (AUUOP), the UAV location\noptimization problem (ULOP), and the UAV-SIM phase shifts optimization problem\n(USPSOP). Then, we solve them through an alternating optimization strategy.\nSpecifically, AUUOP and ULOP are transformed into convex forms solvable via the\nCVX tool, while USPSOP is addressed by a generative artificial intelligence\n(GAI)-based hybrid optimization algorithm. Simulation results show that the\nproposed approach achieves approximately 1.5 times higher network capacity\ncompared with suboptimal schemes, effectively mitigates multi-user interference\nwith increasing SIM layers and meta-atoms, and reduces runtime by 10\\% while\nmaintaining solution quality, thereby demonstrating its practicality for\nreal-world deployments."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v4",
                "updated": "2025-10-09T02:37:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    2,
                    37,
                    26,
                    3,
                    282,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "arxiv_doi": "10.1145/3771283",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3771283",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.07467v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM) 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v2",
                "updated": "2025-10-09T01:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    43,
                    2,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient\n  KV Caching and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient\n  KV Caching and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver an average\nof 12.14x end-to-end speedup across various tasks with negligible accuracy\ndegradation. For the first time, diffusion language models achieve a comparable\nand even faster latency as the widely adopted autoregressive models. Our work\nsuccessfully paved the way for scaling up the diffusion language model to a\nbroader scope of applications across different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver an average\nof 12.14x end-to-end speedup across various tasks with negligible accuracy\ndegradation. For the first time, diffusion language models achieve a comparable\nand even faster latency as the widely adopted autoregressive models. Our work\nsuccessfully paved the way for scaling up the diffusion language model to a\nbroader scope of applications across different domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07667v1",
                "updated": "2025-10-09T01:40:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    40,
                    39,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T01:40:39Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    40,
                    39,
                    3,
                    282,
                    0
                ],
                "title": "An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit\n  Data Reuse Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit\n  Data Reuse Strategies"
                },
                "summary": "Neural radiance fields (NeRF) have transformed 3D reconstruction and\nrendering, facilitating photorealistic image synthesis from sparse viewpoints.\nThis work introduces an explicit data reuse neural rendering (EDR-NR)\narchitecture, which reduces frequent external memory accesses (EMAs) and cache\nmisses by exploiting the spatial locality from three phases, including rays,\nray packets (RPs), and samples. The EDR-NR architecture features a four-stage\nscheduler that clusters rays on the basis of Z-order, prioritize lagging rays\nwhen ray divergence happens, reorders RPs based on spatial proximity, and\nissues samples out-of-orderly (OoO) according to the availability of on-chip\nfeature data. In addition, a four-tier hierarchical RP marching (HRM) technique\nis integrated with an axis-aligned bounding box (AABB) to facilitate spatial\nskipping (SS), reducing redundant computations and improving throughput.\nMoreover, a balanced allocation strategy for feature storage is proposed to\nmitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area\nof 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized\nenergy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X\nincrease in normalized throughput, and a 53.42% reduction in on-chip SRAM\nconsumption compared to state-of-the-art accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural radiance fields (NeRF) have transformed 3D reconstruction and\nrendering, facilitating photorealistic image synthesis from sparse viewpoints.\nThis work introduces an explicit data reuse neural rendering (EDR-NR)\narchitecture, which reduces frequent external memory accesses (EMAs) and cache\nmisses by exploiting the spatial locality from three phases, including rays,\nray packets (RPs), and samples. The EDR-NR architecture features a four-stage\nscheduler that clusters rays on the basis of Z-order, prioritize lagging rays\nwhen ray divergence happens, reorders RPs based on spatial proximity, and\nissues samples out-of-orderly (OoO) according to the availability of on-chip\nfeature data. In addition, a four-tier hierarchical RP marching (HRM) technique\nis integrated with an axis-aligned bounding box (AABB) to facilitate spatial\nskipping (SS), reducing redundant computations and improving throughput.\nMoreover, a balanced allocation strategy for feature storage is proposed to\nmitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area\nof 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized\nenergy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X\nincrease in normalized throughput, and a 53.42% reduction in on-chip SRAM\nconsumption compared to state-of-the-art accelerators."
                },
                "authors": [
                    {
                        "name": "Binzhe Yuan"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yuefeng Zhang"
                    },
                    {
                        "name": "Haochuan Wan"
                    },
                    {
                        "name": "Zhechen Yuan"
                    },
                    {
                        "name": "Junsheng Chen"
                    },
                    {
                        "name": "Yunxiang He"
                    },
                    {
                        "name": "Junran Ding"
                    },
                    {
                        "name": "Xiaoming Zhang"
                    },
                    {
                        "name": "Chaolin Rao"
                    },
                    {
                        "name": "Wenyan Su"
                    },
                    {
                        "name": "Pingqiang Zhou"
                    },
                    {
                        "name": "Jingyi Yu"
                    },
                    {
                        "name": "Xin Lou"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lou"
                },
                "author": "Xin Lou",
                "arxiv_comment": "11 pages, 17 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07651v1",
                "updated": "2025-10-09T00:58:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    0,
                    58,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T00:58:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    0,
                    58,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM\n  Inference"
                },
                "summary": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy."
                },
                "authors": [
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Xiyu Liang"
                    },
                    {
                        "name": "Jiaojiao Zhao"
                    },
                    {
                        "name": "Enmao Diao"
                    }
                ],
                "author_detail": {
                    "name": "Enmao Diao"
                },
                "author": "Enmao Diao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07499v1",
                "updated": "2025-10-08T19:52:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    52,
                    35,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T19:52:35Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    52,
                    35,
                    2,
                    281,
                    0
                ],
                "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs"
                },
                "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL)."
                },
                "authors": [
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Taehee Jung"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07486v1",
                "updated": "2025-10-08T19:36:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    36,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T19:36:11Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    36,
                    11,
                    2,
                    281,
                    0
                ],
                "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse\n  Decoding"
                },
                "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500)."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Yilin Guan"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "14 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v2",
                "updated": "2025-10-08T18:16:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    18,
                    16,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07318v1",
                "updated": "2025-10-08T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Hippocampus Networks for Efficient Long-Context Modeling"
                },
                "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN."
                },
                "authors": [
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Xuehan Xiong"
                    },
                    {
                        "name": "Lai Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lai Wei"
                },
                "author": "Lai Wei",
                "arxiv_comment": "Code: https://github.com/ByteDance-Seed/AHN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07297v1",
                "updated": "2025-10-08T17:51:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:51:34Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "title": "Agentic generative AI for media content discovery at the national\n  football league",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic generative AI for media content discovery at the national\n  football league"
                },
                "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines."
                },
                "authors": [
                    {
                        "name": "Henry Wang"
                    },
                    {
                        "name": "Md Sirajus Salekin"
                    },
                    {
                        "name": "Jake Lee"
                    },
                    {
                        "name": "Ross Claytor"
                    },
                    {
                        "name": "Shinan Zhang"
                    },
                    {
                        "name": "Michael Chi"
                    }
                ],
                "author_detail": {
                    "name": "Michael Chi"
                },
                "author": "Michael Chi",
                "arxiv_comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07293v1",
                "updated": "2025-10-08T17:50:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:50:16Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs"
                },
                "summary": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks."
                },
                "authors": [
                    {
                        "name": "Peize He"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Jiajie Huang"
                    },
                    {
                        "name": "Zehui Lei"
                    },
                    {
                        "name": "Zhuangcheng Gu"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Jiabing Yang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Zhifei Liu"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "26 pages, 23 figures, the code is available at\n  \\url{https://github.com/DabDans/AudioMarathon}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09665v1",
                "updated": "2025-10-08T00:15:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T00:15:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference"
                },
                "summary": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v2",
                "updated": "2025-10-08T00:06:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    6,
                    52,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025 Workshop on Multi-Turn Interactions in Large Language\n  Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v3",
                "updated": "2025-10-07T22:07:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    22,
                    7,
                    44,
                    1,
                    280,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference"
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "arxiv_comment": "NeurIPS 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06415v1",
                "updated": "2025-10-07T19:50:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T19:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "Enhanced Breakdown Voltage in $β$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown Voltage in $β$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing"
                },
                "summary": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments."
                },
                "authors": [
                    {
                        "name": "Saleh Ahmed Khan"
                    },
                    {
                        "name": "Sudipto Saha"
                    },
                    {
                        "name": "Ahmed Ibreljic"
                    },
                    {
                        "name": "Stephen Margiotta"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Walid Amir"
                    },
                    {
                        "name": "Surajit Chakraborty"
                    },
                    {
                        "name": "Uttam Singisetti"
                    },
                    {
                        "name": "A F M Anhar Uddin Bhuiyan"
                    }
                ],
                "author_detail": {
                    "name": "A F M Anhar Uddin Bhuiyan"
                },
                "author": "A F M Anhar Uddin Bhuiyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06175v1",
                "updated": "2025-10-07T17:35:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:35:28Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization"
                },
                "summary": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zhengyang Tong"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05686v1",
                "updated": "2025-10-07T08:43:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T08:43:07Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "title": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment"
                },
                "summary": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%)."
                },
                "authors": [
                    {
                        "name": "José Gómez-delaHiz"
                    },
                    {
                        "name": "Mohamed Faten Zhani"
                    },
                    {
                        "name": "Jaime Galán-Jiménez"
                    },
                    {
                        "name": "John Kaippallimalil"
                    }
                ],
                "author_detail": {
                    "name": "John Kaippallimalil"
                },
                "author": "John Kaippallimalil",
                "arxiv_comment": "10 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05529v1",
                "updated": "2025-10-07T02:39:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T02:39:35Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "title": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Harshil Vejendla"
                    }
                ],
                "author_detail": {
                    "name": "Harshil Vejendla"
                },
                "author": "Harshil Vejendla",
                "arxiv_comment": "MIT URTC 2025 Technical Paper (Oral), 5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05476v1",
                "updated": "2025-10-07T00:32:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T00:32:45Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications"
                },
                "summary": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages."
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Jongryool Kim"
                    },
                    {
                        "name": "Byungil Koh"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_doi": "10.1145/3712285.3759816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.05476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05373v1",
                "updated": "2025-10-06T21:08:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T21:08:11Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "title": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction"
                },
                "summary": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "14 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05367v1",
                "updated": "2025-10-06T20:54:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T20:54:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation"
                },
                "summary": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache ."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Kaiyuan Deng"
                    },
                    {
                        "name": "Yushu Wu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Bo Hui"
                    }
                ],
                "author_detail": {
                    "name": "Bo Hui"
                },
                "author": "Bo Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v2",
                "updated": "2025-10-06T17:09:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    9,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/"
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "arxiv_comment": "Published as a conference paper at COLM 2025 Website:\n  https://hdlm-colm.github.io/",
                "arxiv_journal_ref": "Second Conference on Language Modeling,\n  https://openreview.net/forum?id=rgq9BFXSFl (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v2",
                "updated": "2025-10-06T13:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    23,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Pao Cheng"
                    },
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04646v1",
                "updated": "2025-10-06T09:49:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T09:49:14Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "title": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation"
                },
                "summary": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup."
                },
                "authors": [
                    {
                        "name": "Johanna Sommer"
                    },
                    {
                        "name": "John Rachwan"
                    },
                    {
                        "name": "Nils Fleischmann"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Bertrand Charpentier"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Charpentier"
                },
                "author": "Bertrand Charpentier",
                "arxiv_comment": "Accepted at the AI for Science Workshop @ NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v1",
                "updated": "2025-10-06T06:30:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04492v1",
                "updated": "2025-10-06T05:04:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T05:04:57Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "title": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks"
                },
                "summary": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Sumei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sumei Sun"
                },
                "author": "Sumei Sun",
                "arxiv_comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v3",
                "updated": "2025-10-06T04:28:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    28,
                    5,
                    0,
                    279,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04476v1",
                "updated": "2025-10-06T04:24:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T04:24:23Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space"
                },
                "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x."
                },
                "authors": [
                    {
                        "name": "Tomas Figliolia"
                    },
                    {
                        "name": "Nicholas Alonso"
                    },
                    {
                        "name": "Rishi Iyer"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Beren Millidge"
                    }
                ],
                "author_detail": {
                    "name": "Beren Millidge"
                },
                "author": "Beren Millidge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18149v2",
                "updated": "2025-10-06T02:46:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    2,
                    46,
                    1,
                    0,
                    279,
                    0
                ],
                "published": "2024-03-26T23:17:05Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    23,
                    17,
                    5,
                    1,
                    86,
                    0
                ],
                "title": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC"
                },
                "summary": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Khai Nguyen"
                    },
                    {
                        "name": "Sam Schoedel"
                    },
                    {
                        "name": "Elakhya Nedumaran"
                    },
                    {
                        "name": "Moises Mata"
                    },
                    {
                        "name": "Brian Plancher"
                    },
                    {
                        "name": "Zachary Manchester"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Manchester"
                },
                "author": "Zachary Manchester",
                "arxiv_comment": "First three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v6",
                "updated": "2025-10-05T22:17:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    22,
                    17,
                    34,
                    6,
                    278,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00384v2",
                "updated": "2025-10-05T21:29:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    21,
                    29,
                    28,
                    6,
                    278,
                    0
                ],
                "published": "2025-05-31T04:27:22Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "title": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory"
                },
                "summary": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v2",
                "updated": "2025-10-05T18:13:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    18,
                    13,
                    39,
                    6,
                    278,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04188v1",
                "updated": "2025-10-05T13:01:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T13:01:08Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "title": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers"
                },
                "summary": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Guantao Chen"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05176v1",
                "updated": "2025-10-05T12:09:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    9,
                    14,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T12:09:14Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    9,
                    14,
                    6,
                    278,
                    0
                ],
                "title": "PatternKV: Flattening KV Representation Expands Quantization Headroom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatternKV: Flattening KV Representation Expands Quantization Headroom"
                },
                "summary": "KV cache in autoregressive LLMs eliminates redundant recomputation but has\nemerged as the dominant memory and bandwidth bottleneck during inference,\nnotably with long contexts and test-time scaling. KV quantization is a key\nlever for reducing cache cost, but accuracy drops sharply as the native KV\ndistribution lacks flatness and thus maintains a wide quantization range. Prior\nwork focuses on isolating outliers, which caps their error but fails to flatten\nthe overall distribution, leaving performance fragile under low-bit settings.\nIn this work, we show that the K cache maintains a stable structure that\nevolves gradually with context, while the V cache carries latent semantic\nregularities. Building on these insights, we propose PatternKV, a\npattern-aligned residual quantization scheme. It mines representative pattern\nvectors online, aligns each KV vector to its nearest pattern, and quantizes\nonly the residual. This reshaping of the KV distribution flattens the\nquantization target and narrows its range, thereby improving the fidelity of\nlow-bit KV quantization. Across long-context and test-time scaling settings on\nmultiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%\naverage 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%\non average, and raises throughput by 1.4x while supporting 1.25x larger\nbatches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache in autoregressive LLMs eliminates redundant recomputation but has\nemerged as the dominant memory and bandwidth bottleneck during inference,\nnotably with long contexts and test-time scaling. KV quantization is a key\nlever for reducing cache cost, but accuracy drops sharply as the native KV\ndistribution lacks flatness and thus maintains a wide quantization range. Prior\nwork focuses on isolating outliers, which caps their error but fails to flatten\nthe overall distribution, leaving performance fragile under low-bit settings.\nIn this work, we show that the K cache maintains a stable structure that\nevolves gradually with context, while the V cache carries latent semantic\nregularities. Building on these insights, we propose PatternKV, a\npattern-aligned residual quantization scheme. It mines representative pattern\nvectors online, aligns each KV vector to its nearest pattern, and quantizes\nonly the residual. This reshaping of the KV distribution flattens the\nquantization target and narrows its range, thereby improving the fidelity of\nlow-bit KV quantization. Across long-context and test-time scaling settings on\nmultiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%\naverage 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%\non average, and raises throughput by 1.4x while supporting 1.25x larger\nbatches."
                },
                "authors": [
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Jiayi Shi"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04153v1",
                "updated": "2025-10-05T11:09:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T11:09:10Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation"
                },
                "summary": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost."
                },
                "authors": [
                    {
                        "name": "Haoqi Wu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yan"
                },
                "author": "Qiang Yan",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v3",
                "updated": "2025-10-05T08:34:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    8,
                    34,
                    30,
                    6,
                    278,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04033v1",
                "updated": "2025-10-05T04:52:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    4,
                    52,
                    26,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T04:52:26Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    4,
                    52,
                    26,
                    6,
                    278,
                    0
                ],
                "title": "A global log for medical AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A global log for medical AI"
                },
                "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology."
                },
                "authors": [
                    {
                        "name": "Ayush Noori"
                    },
                    {
                        "name": "Adam Rodman"
                    },
                    {
                        "name": "Alan Karthikesalingam"
                    },
                    {
                        "name": "Bilal A. Mateen"
                    },
                    {
                        "name": "Christopher A. Longhurst"
                    },
                    {
                        "name": "Daniel Yang"
                    },
                    {
                        "name": "Dave deBronkart"
                    },
                    {
                        "name": "Gauden Galea"
                    },
                    {
                        "name": "Harold F. Wolf III"
                    },
                    {
                        "name": "Jacob Waxman"
                    },
                    {
                        "name": "Joshua C. Mandel"
                    },
                    {
                        "name": "Juliana Rotich"
                    },
                    {
                        "name": "Kenneth D. Mandl"
                    },
                    {
                        "name": "Maryam Mustafa"
                    },
                    {
                        "name": "Melissa Miles"
                    },
                    {
                        "name": "Nigam H. Shah"
                    },
                    {
                        "name": "Peter Lee"
                    },
                    {
                        "name": "Robert Korom"
                    },
                    {
                        "name": "Scott Mahoney"
                    },
                    {
                        "name": "Seth Hain"
                    },
                    {
                        "name": "Tien Yin Wong"
                    },
                    {
                        "name": "Trevor Mundel"
                    },
                    {
                        "name": "Vivek Natarajan"
                    },
                    {
                        "name": "Noa Dagan"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Ran D. Balicer"
                    },
                    {
                        "name": "Isaac S. Kohane"
                    },
                    {
                        "name": "Marinka Zitnik"
                    }
                ],
                "author_detail": {
                    "name": "Marinka Zitnik"
                },
                "author": "Marinka Zitnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03851v1",
                "updated": "2025-10-04T15:52:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    52,
                    31,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T15:52:31Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    52,
                    31,
                    5,
                    277,
                    0
                ],
                "title": "Algorithm Generation via Creative Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Generation via Creative Ideation"
                },
                "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%)."
                },
                "authors": [
                    {
                        "name": "Ruiying Ma"
                    },
                    {
                        "name": "Chieh-Jan Mike Liang"
                    },
                    {
                        "name": "Yanjie Gao"
                    },
                    {
                        "name": "Francis Y. Yan"
                    }
                ],
                "author_detail": {
                    "name": "Francis Y. Yan"
                },
                "author": "Francis Y. Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03834v1",
                "updated": "2025-10-04T15:25:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    25,
                    4,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T15:25:04Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    25,
                    4,
                    5,
                    277,
                    0
                ],
                "title": "Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes\n  with Robust Polarization Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes\n  with Robust Polarization Switching"
                },
                "summary": "Freestanding ferroelectric membranes are promising for flexible electronics,\nnonvolatile memory, photonics, and spintronics, but their synthesis is\nchallenged by the need for reproducibility with precise stoichiometric control.\nHere, we demonstrate the adsorption-controlled growth of single-crystalline,\nepitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide\nsacrificial layer. Using a simple water-droplet lift-off method, we obtained\nsubmillimeter- to millimeter-sized membranes that retained crystallinity, as\nconfirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal\nsymmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high\ndielectric constant of 1340, reflecting the robust dielectric response of the\nmembranes. Ferroelectric functionality was revealed by piezoresponse force\nmicroscopy (PFM) and further verified by polarization-electric field (P-E) loop\nmeasurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a\nremnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These\nresults were interpreted in relation to c- and a-domain configurations. These\nresults establish hybrid MBE as a generalizable route for producing\nstoichiometry-controlled ferroelectric membranes, enabling their integration\ninto next-generation flexible and multifunctional quantum oxide devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Freestanding ferroelectric membranes are promising for flexible electronics,\nnonvolatile memory, photonics, and spintronics, but their synthesis is\nchallenged by the need for reproducibility with precise stoichiometric control.\nHere, we demonstrate the adsorption-controlled growth of single-crystalline,\nepitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide\nsacrificial layer. Using a simple water-droplet lift-off method, we obtained\nsubmillimeter- to millimeter-sized membranes that retained crystallinity, as\nconfirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal\nsymmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high\ndielectric constant of 1340, reflecting the robust dielectric response of the\nmembranes. Ferroelectric functionality was revealed by piezoresponse force\nmicroscopy (PFM) and further verified by polarization-electric field (P-E) loop\nmeasurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a\nremnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These\nresults were interpreted in relation to c- and a-domain configurations. These\nresults establish hybrid MBE as a generalizable route for producing\nstoichiometry-controlled ferroelectric membranes, enabling their integration\ninto next-generation flexible and multifunctional quantum oxide devices."
                },
                "authors": [
                    {
                        "name": "S. Choo"
                    },
                    {
                        "name": "S. Varshney"
                    },
                    {
                        "name": "J. Shah"
                    },
                    {
                        "name": "A. K. Manjeshwar"
                    },
                    {
                        "name": "D. K. Lee"
                    },
                    {
                        "name": "K. A. Mkhoyan"
                    },
                    {
                        "name": "R. D. James"
                    },
                    {
                        "name": "B. Jalan"
                    }
                ],
                "author_detail": {
                    "name": "B. Jalan"
                },
                "author": "B. Jalan",
                "arxiv_comment": "22 pages 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v1",
                "updated": "2025-10-04T07:22:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    },
                    {
                        "name": "Kamrujjaman"
                    },
                    {
                        "name": "Ahsan Habib Tareq"
                    }
                ],
                "author_detail": {
                    "name": "Ahsan Habib Tareq"
                },
                "author": "Ahsan Habib Tareq",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v3",
                "updated": "2025-10-04T05:59:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    5,
                    59,
                    1,
                    5,
                    277,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v3",
                "updated": "2025-10-04T05:28:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    5,
                    28,
                    39,
                    5,
                    277,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v2",
                "updated": "2025-10-04T03:45:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    3,
                    45,
                    40,
                    5,
                    277,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via\n  Fine-Grained Expert Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via\n  Fine-Grained Expert Offloading"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs.\n  To tame the latency-memory trade-off in MoE serving, we present FineMoE, a\nfine-grained expert offloading system for MoE serving that achieves low\ninference latency with memory efficiency. We design FineMoE to extract\nfine-grained expert selection patterns from MoE models and semantic hints from\ninput prompts to efficiently guide expert prefetching, caching, and offloading\ndecisions. FineMoE is prototyped on top of HuggingFace Transformers and\ndeployed on a six-GPU testbed. Experiments with open-source MoE models and\nreal-world workloads show that FineMoE reduces inference latency by 47% and\nimproves expert hit rate by 39% over state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs.\n  To tame the latency-memory trade-off in MoE serving, we present FineMoE, a\nfine-grained expert offloading system for MoE serving that achieves low\ninference latency with memory efficiency. We design FineMoE to extract\nfine-grained expert selection patterns from MoE models and semantic hints from\ninput prompts to efficiently guide expert prefetching, caching, and offloading\ndecisions. FineMoE is prototyped on top of HuggingFace Transformers and\ndeployed on a six-GPU testbed. Experiments with open-source MoE models and\nreal-world workloads show that FineMoE reduces inference latency by 47% and\nimproves expert hit rate by 39% over state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03215v1",
                "updated": "2025-10-03T17:52:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models"
                },
                "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Zihan Min"
                    },
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Jichao Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03198v1",
                "updated": "2025-10-03T17:35:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:35:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft"
                },
                "summary": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences."
                },
                "authors": [
                    {
                        "name": "Junchao Huang"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Boyao Han"
                    },
                    {
                        "name": "Shaoshuai Shi"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Li Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Li Jiang"
                },
                "author": "Li Jiang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v2",
                "updated": "2025-10-03T15:37:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    37,
                    19,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures; Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02866v1",
                "updated": "2025-10-03T10:06:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T10:06:44Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "title": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling"
                },
                "summary": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject."
                },
                "authors": [
                    {
                        "name": "Bassel Diban"
                    },
                    {
                        "name": "Giovanni Mazzanti"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Mazzanti"
                },
                "author": "Giovanni Mazzanti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02758v1",
                "updated": "2025-10-03T06:43:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:43:24Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "title": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling"
                },
                "summary": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Chuheng Du"
                    },
                    {
                        "name": "Renyuan Liu"
                    },
                    {
                        "name": "Shuochao Yao"
                    },
                    {
                        "name": "Dingtian Yan"
                    },
                    {
                        "name": "Jiang Liao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Accepted by EuroSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02750v1",
                "updated": "2025-10-03T06:27:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:27:33Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "title": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models"
                },
                "summary": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks."
                },
                "authors": [
                    {
                        "name": "Lihua Zhou"
                    },
                    {
                        "name": "Mao Ye"
                    },
                    {
                        "name": "Shuaifeng Li"
                    },
                    {
                        "name": "Nianxin Li"
                    },
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Lei Deng"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.11719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11719v1",
                "updated": "2025-10-13T17:59:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    59,
                    59,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:59:59Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    59,
                    59,
                    0,
                    286,
                    0
                ],
                "title": "BayeSN-TD: Time Delay and $H_0$ Estimation for Lensed SN H0pe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayeSN-TD: Time Delay and $H_0$ Estimation for Lensed SN H0pe"
                },
                "summary": "We present BayeSN-TD, an enhanced implementation of the probabilistic type Ia\nsupernova (SN Ia) BayeSN SED model, designed for fitting multiply-imaged,\ngravitationally lensed type Ia supernovae (glSNe Ia). BayeSN-TD fits for\nmagnifications and time-delays across multiple images while marginalising over\nan achromatic, Gaussian process-based treatment of microlensing, to allow for\ntime-dependent deviations from a typical SN Ia SED caused by gravitational\nlensing by stars in the lensing system. BayeSN-TD is able to robustly infer\ntime delays and produce well-calibrated uncertainties, even when applied to\nsimulations based on a different SED model and incorporating chromatic\nmicrolensing, strongly validating its suitability for time-delay cosmography.\nWe then apply BayeSN-TD to publicly available photometry of the glSN Ia SN\nH0pe, inferring time delays between images BA and BC of $\\Delta\nT_{BA}=121.9^{+9.5}_{-7.5}$ days and $\\Delta T_{BC}=63.2^{+3.2}_{-3.3}$ days\nalong with absolute magnifications $\\beta$ for each image, $\\beta_A =\n2.38^{+0.72}_{-0.54}$, $\\beta_B=5.27^{+1.25}_{-1.02}$ and\n$\\beta_C=3.93^{+1.00}_{-0.75}$. Combining our constraints on time-delays and\nmagnifications with existing lens models of this system, we infer\n$H_0=69.3^{+12.6}_{-7.8}$ km s$^{-1}$ Mpc$^{-1}$, consistent with previous\nanalysis of this system; incorporating additional constraints based on\nspectroscopy yields $H_0=66.8^{+13.4}_{-5.4}$ km s$^{-1}$ Mpc$^{-1}$. While\nthis is not yet precise enough to draw a meaningful conclusion with regard to\nthe `Hubble tension', upcoming analysis of SN H0pe with more accurate\nphotometry enabled by template images, and other glSNe, will provide stronger\nconstraints on $H_0$; BayeSN-TD will be a valuable tool for these analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BayeSN-TD, an enhanced implementation of the probabilistic type Ia\nsupernova (SN Ia) BayeSN SED model, designed for fitting multiply-imaged,\ngravitationally lensed type Ia supernovae (glSNe Ia). BayeSN-TD fits for\nmagnifications and time-delays across multiple images while marginalising over\nan achromatic, Gaussian process-based treatment of microlensing, to allow for\ntime-dependent deviations from a typical SN Ia SED caused by gravitational\nlensing by stars in the lensing system. BayeSN-TD is able to robustly infer\ntime delays and produce well-calibrated uncertainties, even when applied to\nsimulations based on a different SED model and incorporating chromatic\nmicrolensing, strongly validating its suitability for time-delay cosmography.\nWe then apply BayeSN-TD to publicly available photometry of the glSN Ia SN\nH0pe, inferring time delays between images BA and BC of $\\Delta\nT_{BA}=121.9^{+9.5}_{-7.5}$ days and $\\Delta T_{BC}=63.2^{+3.2}_{-3.3}$ days\nalong with absolute magnifications $\\beta$ for each image, $\\beta_A =\n2.38^{+0.72}_{-0.54}$, $\\beta_B=5.27^{+1.25}_{-1.02}$ and\n$\\beta_C=3.93^{+1.00}_{-0.75}$. Combining our constraints on time-delays and\nmagnifications with existing lens models of this system, we infer\n$H_0=69.3^{+12.6}_{-7.8}$ km s$^{-1}$ Mpc$^{-1}$, consistent with previous\nanalysis of this system; incorporating additional constraints based on\nspectroscopy yields $H_0=66.8^{+13.4}_{-5.4}$ km s$^{-1}$ Mpc$^{-1}$. While\nthis is not yet precise enough to draw a meaningful conclusion with regard to\nthe `Hubble tension', upcoming analysis of SN H0pe with more accurate\nphotometry enabled by template images, and other glSNe, will provide stronger\nconstraints on $H_0$; BayeSN-TD will be a valuable tool for these analyses."
                },
                "authors": [
                    {
                        "name": "M. Grayling"
                    },
                    {
                        "name": "S. Thorp"
                    },
                    {
                        "name": "K. S. Mandel"
                    },
                    {
                        "name": "M. Pascale"
                    },
                    {
                        "name": "J. D. R"
                    },
                    {
                        "name": "Pierel"
                    },
                    {
                        "name": "E. E. Hayes"
                    },
                    {
                        "name": "C. Larison"
                    },
                    {
                        "name": "A. Agrawal"
                    },
                    {
                        "name": "G. Narayan"
                    }
                ],
                "author_detail": {
                    "name": "G. Narayan"
                },
                "author": "G. Narayan",
                "arxiv_comment": "20 pages, 11 figures, 4 tables. Submitted to MNRAS. BayeSN-TD code\n  will be made public upon acceptance of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11718v1",
                "updated": "2025-10-13T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    59,
                    55,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    59,
                    55,
                    0,
                    286,
                    0
                ],
                "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images"
                },
                "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT."
                },
                "authors": [
                    {
                        "name": "Chengqi Duan"
                    },
                    {
                        "name": "Kaiyue Sun"
                    },
                    {
                        "name": "Rongyao Fang"
                    },
                    {
                        "name": "Manyuan Zhang"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Ying Luo"
                    },
                    {
                        "name": "Yufang Liu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Peng Pei"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11701v1",
                "updated": "2025-10-13T17:57:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    57,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:57:15Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    57,
                    15,
                    0,
                    286,
                    0
                ],
                "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Reinforcement Learning in Agentic Reasoning"
                },
                "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL"
                },
                "authors": [
                    {
                        "name": "Zhaochen Yu"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Jiaru Zou"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and models: https://github.com/Gen-Verse/Open-AgentRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11696v1",
                "updated": "2025-10-13T17:55:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    55,
                    9,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:55:09Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    55,
                    9,
                    0,
                    286,
                    0
                ],
                "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs"
                },
                "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yi Ge"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Huizi Mao"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Ka Chun Cheung"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code is available at https://github.com/NVlabs/QeRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11695v1",
                "updated": "2025-10-13T17:54:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    54,
                    9,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:54:09Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    54,
                    9,
                    0,
                    286,
                    0
                ],
                "title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents"
                },
                "summary": "Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Vincent Jim Zhang"
                    },
                    {
                        "name": "Huan He"
                    },
                    {
                        "name": "Hanley Smith"
                    },
                    {
                        "name": "Yi Han"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Haohang Li"
                    },
                    {
                        "name": "Yupeng Cao"
                    },
                    {
                        "name": "Yangyang Yu"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Peng Lu"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11689v1",
                "updated": "2025-10-13T17:51:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    51,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:51:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    51,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for\n  Uncertainty-Aware Sim-to-Real Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for\n  Uncertainty-Aware Sim-to-Real Manipulation"
                },
                "summary": "Learning robotic manipulation policies directly in the real world can be\nexpensive and time-consuming. While reinforcement learning (RL) policies\ntrained in simulation present a scalable alternative, effective sim-to-real\ntransfer remains challenging, particularly for tasks that require precise\ndynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL\npipeline that combines vision-language model (VLM)-inferred physical parameter\nestimates with interactive adaptation through uncertainty-aware fusion. Our\napproach consists of three core components: (1) high-fidelity geometric\nreconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions\nover physical parameters, and (3) online physical parameter estimation from\ninteraction data. Phys2Real conditions policies on interpretable physical\nparameters, refining VLM predictions with online estimates via ensemble-based\nuncertainty quantification. On planar pushing tasks of a T-block with varying\ncenter of mass (CoM) and a hammer with an off-center mass distribution,\nPhys2Real achieves substantial improvements over a domain randomization\nbaseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%\nin the challenging top-weighted T-block, and 15% faster average task completion\nfor hammer pushing. Ablation studies indicate that the combination of VLM and\ninteraction information is essential for success. Project website:\nhttps://phys2real.github.io/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning robotic manipulation policies directly in the real world can be\nexpensive and time-consuming. While reinforcement learning (RL) policies\ntrained in simulation present a scalable alternative, effective sim-to-real\ntransfer remains challenging, particularly for tasks that require precise\ndynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL\npipeline that combines vision-language model (VLM)-inferred physical parameter\nestimates with interactive adaptation through uncertainty-aware fusion. Our\napproach consists of three core components: (1) high-fidelity geometric\nreconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions\nover physical parameters, and (3) online physical parameter estimation from\ninteraction data. Phys2Real conditions policies on interpretable physical\nparameters, refining VLM predictions with online estimates via ensemble-based\nuncertainty quantification. On planar pushing tasks of a T-block with varying\ncenter of mass (CoM) and a hammer with an off-center mass distribution,\nPhys2Real achieves substantial improvements over a domain randomization\nbaseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%\nin the challenging top-weighted T-block, and 15% faster average task completion\nfor hammer pushing. Ablation studies indicate that the combination of VLM and\ninteraction information is essential for success. Project website:\nhttps://phys2real.github.io/ ."
                },
                "authors": [
                    {
                        "name": "Maggie Wang"
                    },
                    {
                        "name": "Stephen Tian"
                    },
                    {
                        "name": "Aiden Swann"
                    },
                    {
                        "name": "Ola Shorinwa"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Mac Schwager"
                    }
                ],
                "author_detail": {
                    "name": "Mac Schwager"
                },
                "author": "Mac Schwager",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11688v1",
                "updated": "2025-10-13T17:50:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    50,
                    25,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:50:25Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    50,
                    25,
                    0,
                    286,
                    0
                ],
                "title": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation\n  Capabilities"
                },
                "summary": "The increasing autonomy of Large Language Models (LLMs) necessitates a\nrigorous evaluation of their potential to aid in cyber offense. Existing\nbenchmarks often lack real-world complexity and are thus unable to accurately\nassess LLMs' cybersecurity capabilities. To address this gap, we introduce\nPACEbench, a practical AI cyber-exploitation benchmark built on the principles\nof realistic vulnerability difficulty, environmental complexity, and cyber\ndefenses. Specifically, PACEbench comprises four scenarios spanning single,\nblended, chained, and defense vulnerability exploitations. To handle these\ncomplex challenges, we propose PACEagent, a novel agent that emulates human\npenetration testers by supporting multi-phase reconnaissance, analysis, and\nexploitation. Extensive experiments with seven frontier LLMs demonstrate that\ncurrent models struggle with complex cyber scenarios, and none can bypass\ndefenses. These findings suggest that current models do not yet pose a\ngeneralized cyber offense threat. Nonetheless, our work provides a robust\nbenchmark to guide the trustworthy development of future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing autonomy of Large Language Models (LLMs) necessitates a\nrigorous evaluation of their potential to aid in cyber offense. Existing\nbenchmarks often lack real-world complexity and are thus unable to accurately\nassess LLMs' cybersecurity capabilities. To address this gap, we introduce\nPACEbench, a practical AI cyber-exploitation benchmark built on the principles\nof realistic vulnerability difficulty, environmental complexity, and cyber\ndefenses. Specifically, PACEbench comprises four scenarios spanning single,\nblended, chained, and defense vulnerability exploitations. To handle these\ncomplex challenges, we propose PACEagent, a novel agent that emulates human\npenetration testers by supporting multi-phase reconnaissance, analysis, and\nexploitation. Extensive experiments with seven frontier LLMs demonstrate that\ncurrent models struggle with complex cyber scenarios, and none can bypass\ndefenses. These findings suggest that current models do not yet pose a\ngeneralized cyber offense threat. Nonetheless, our work provides a robust\nbenchmark to guide the trustworthy development of future models."
                },
                "authors": [
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Lige Huang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "Project webpage available at https://pacebench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11687v1",
                "updated": "2025-10-13T17:49:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    49,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:49:15Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    49,
                    15,
                    0,
                    286,
                    0
                ],
                "title": "Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape\n  Estimation from a Single View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape\n  Estimation from a Single View"
                },
                "summary": "Estimating an object's 6D pose, size, and shape from visual input is a\nfundamental problem in computer vision, with critical applications in robotic\ngrasping and manipulation. Existing methods either rely on object-specific\npriors such as CAD models or templates, or suffer from limited generalization\nacross categories due to pose-shape entanglement and multi-stage pipelines. In\nthis work, we propose a unified, category-agnostic framework that\nsimultaneously predicts 6D pose, size, and dense shape from a single RGB-D\nimage, without requiring templates, CAD models, or category labels at test\ntime. Our model fuses dense 2D features from vision foundation models with\npartial 3D point clouds using a Transformer encoder enhanced by a\nMixture-of-Experts, and employs parallel decoders for pose-size estimation and\nshape reconstruction, achieving real-time inference at 28 FPS. Trained solely\non synthetic data from 149 categories in the SOPE dataset, our framework is\nevaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,\nspanning over 300 categories. It achieves state-of-the-art accuracy on seen\ncategories while demonstrating remarkably strong zero-shot generalization to\nunseen real-world objects, establishing a new standard for open-set 6D\nunderstanding in robotics and embodied AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating an object's 6D pose, size, and shape from visual input is a\nfundamental problem in computer vision, with critical applications in robotic\ngrasping and manipulation. Existing methods either rely on object-specific\npriors such as CAD models or templates, or suffer from limited generalization\nacross categories due to pose-shape entanglement and multi-stage pipelines. In\nthis work, we propose a unified, category-agnostic framework that\nsimultaneously predicts 6D pose, size, and dense shape from a single RGB-D\nimage, without requiring templates, CAD models, or category labels at test\ntime. Our model fuses dense 2D features from vision foundation models with\npartial 3D point clouds using a Transformer encoder enhanced by a\nMixture-of-Experts, and employs parallel decoders for pose-size estimation and\nshape reconstruction, achieving real-time inference at 28 FPS. Trained solely\non synthetic data from 149 categories in the SOPE dataset, our framework is\nevaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,\nspanning over 300 categories. It achieves state-of-the-art accuracy on seen\ncategories while demonstrating remarkably strong zero-shot generalization to\nunseen real-world objects, establishing a new standard for open-set 6D\nunderstanding in robotics and embodied AI."
                },
                "authors": [
                    {
                        "name": "Jinyu Zhang"
                    },
                    {
                        "name": "Haitao Lin"
                    },
                    {
                        "name": "Jiashu Hou"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11686v1",
                "updated": "2025-10-13T17:49:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    49,
                    5,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:49:05Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    49,
                    5,
                    0,
                    286,
                    0
                ],
                "title": "Representation-Based Exploration for Language Models: From Test-Time to\n  Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation-Based Exploration for Language Models: From Test-Time to\n  Post-Training"
                },
                "summary": "Reinforcement learning (RL) promises to expand the capabilities of language\nmodels, but it is unclear if current RL techniques promote the discovery of\nnovel behaviors, or simply sharpen those already present in the base model. In\nthis paper, we investigate the value of deliberate exploration -- explicitly\nincentivizing the model to discover novel and diverse behaviors -- and aim to\nunderstand how the knowledge in pre-trained models can guide this search. Our\nmain finding is that exploration with a simple, principled,\nrepresentation-based bonus derived from the pre-trained language model's hidden\nstates significantly improves diversity and pass@k rates -- both for\npost-training, and in a novel inference-time scaling setting we introduce. For\ninference-time, exploration with representation-based diversity improves\nefficiency, consistently improving pass@k rates across a variety of models and\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\nimprovement in verifier efficiency on almost all tasks. For post-training, we\nshow that integrating this exploration strategy into an RL pipeline improves\nreasoning performance over that of the initial model and over standard RL\npost-training. For example, on AIME 2024, our post-trained\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\nfindings suggest that deliberate exploration -- with the right notion of\ndiversity -- is a practical path toward discovery of new behaviors beyond\nsharpening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) promises to expand the capabilities of language\nmodels, but it is unclear if current RL techniques promote the discovery of\nnovel behaviors, or simply sharpen those already present in the base model. In\nthis paper, we investigate the value of deliberate exploration -- explicitly\nincentivizing the model to discover novel and diverse behaviors -- and aim to\nunderstand how the knowledge in pre-trained models can guide this search. Our\nmain finding is that exploration with a simple, principled,\nrepresentation-based bonus derived from the pre-trained language model's hidden\nstates significantly improves diversity and pass@k rates -- both for\npost-training, and in a novel inference-time scaling setting we introduce. For\ninference-time, exploration with representation-based diversity improves\nefficiency, consistently improving pass@k rates across a variety of models and\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\nimprovement in verifier efficiency on almost all tasks. For post-training, we\nshow that integrating this exploration strategy into an RL pipeline improves\nreasoning performance over that of the initial model and over standard RL\npost-training. For example, on AIME 2024, our post-trained\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\nfindings suggest that deliberate exploration -- with the right notion of\ndiversity -- is a practical path toward discovery of new behaviors beyond\nsharpening."
                },
                "authors": [
                    {
                        "name": "Jens Tuyls"
                    },
                    {
                        "name": "Dylan J. Foster"
                    },
                    {
                        "name": "Akshay Krishnamurthy"
                    },
                    {
                        "name": "Jordan T. Ash"
                    }
                ],
                "author_detail": {
                    "name": "Jordan T. Ash"
                },
                "author": "Jordan T. Ash",
                "arxiv_comment": "Website and code: https://rep-exp.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11671v1",
                "updated": "2025-10-13T17:43:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    43,
                    24,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:43:24Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    43,
                    24,
                    0,
                    286,
                    0
                ],
                "title": "Finite-temperature phase diagram and collective modes of coherently\n  coupled Bose mixtures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite-temperature phase diagram and collective modes of coherently\n  coupled Bose mixtures"
                },
                "summary": "We investigate the ferromagnetic-paramagnetic phase transition in coherently\n(Rabi) coupled Bose-Einstein condensates at zero and finite temperatures,\nexploring different routes to the transition by tuning the Rabi coupling or\nincreasing the temperature at a fixed coupling. Using the\nHartree-Fock-Bogoliubov theory within the Popov approximation, we map out the\nfinite-temperature phase diagram of a three-dimensional homogeneous condensate\nand identify the critical line through the softening of the spin gap.\nMagnetization and the spin dispersion branch reveal the progressive suppression\nof the ferromagnetic order with increasing temperature. In\nquasi-one-dimensional harmonic traps, the transition, driven by Rabi coupling,\nis inferred through the softening of the spin breathing mode with its minimum\nshifting to lower coupling values with increasing temperature. Notably, the\nthermally driven transition causes monotonic hardening of all the spin modes.\nFor both coupling and temperature-driven transition, the hybridized density\nmodes in the ferromagnetic phase acquire more density character while\napproaching the critical point.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the ferromagnetic-paramagnetic phase transition in coherently\n(Rabi) coupled Bose-Einstein condensates at zero and finite temperatures,\nexploring different routes to the transition by tuning the Rabi coupling or\nincreasing the temperature at a fixed coupling. Using the\nHartree-Fock-Bogoliubov theory within the Popov approximation, we map out the\nfinite-temperature phase diagram of a three-dimensional homogeneous condensate\nand identify the critical line through the softening of the spin gap.\nMagnetization and the spin dispersion branch reveal the progressive suppression\nof the ferromagnetic order with increasing temperature. In\nquasi-one-dimensional harmonic traps, the transition, driven by Rabi coupling,\nis inferred through the softening of the spin breathing mode with its minimum\nshifting to lower coupling values with increasing temperature. Notably, the\nthermally driven transition causes monotonic hardening of all the spin modes.\nFor both coupling and temperature-driven transition, the hybridized density\nmodes in the ferromagnetic phase acquire more density character while\napproaching the critical point."
                },
                "authors": [
                    {
                        "name": "Sunilkumar V"
                    },
                    {
                        "name": "Rajat"
                    },
                    {
                        "name": "Sandeep Gautam"
                    },
                    {
                        "name": "Arko Roy"
                    }
                ],
                "author_detail": {
                    "name": "Arko Roy"
                },
                "author": "Arko Roy",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.quant-gas",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11664v1",
                "updated": "2025-10-13T17:38:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    38,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:38:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    38,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "Proprioceptive Misestimation of Hand Speed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proprioceptive Misestimation of Hand Speed"
                },
                "summary": "The accuracy with which the human proprioceptive system estimates hand speed\nis not well understood. To investigate this, we designed an experiment using\nhobby-grade mechatronics parts and integrated it as a laboratory exercise in a\nlarge remote laboratory course. In a simple joint position reproduction task,\nparticipants (N = 191) grasped a servomotor-driven shaft with one hand as it\nfollowed a randomized trajectory composed of sinusoidal submovements. They\nsimultaneously attempted to reproduce the movement by turning the shaft of a\npotentiometer with the other hand. Focusing on the first movement of the\ntrajectory, we found that participants consistently overestimated the speed of\nthe slowest rotations by ~45% and underestimated the speed of the fastest\nrotations also by ~30%. Speed estimation errors were near zero for trajectories\nwith peak velocities ~63 deg/s. Participants' movements also overshot slow\ntrajectories and undershot fast trajectories. We show that these trajectory\nerrors can be explained by a model in which the proprioceptive system\nintegrates velocity misestimates to infer position.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accuracy with which the human proprioceptive system estimates hand speed\nis not well understood. To investigate this, we designed an experiment using\nhobby-grade mechatronics parts and integrated it as a laboratory exercise in a\nlarge remote laboratory course. In a simple joint position reproduction task,\nparticipants (N = 191) grasped a servomotor-driven shaft with one hand as it\nfollowed a randomized trajectory composed of sinusoidal submovements. They\nsimultaneously attempted to reproduce the movement by turning the shaft of a\npotentiometer with the other hand. Focusing on the first movement of the\ntrajectory, we found that participants consistently overestimated the speed of\nthe slowest rotations by ~45% and underestimated the speed of the fastest\nrotations also by ~30%. Speed estimation errors were near zero for trajectories\nwith peak velocities ~63 deg/s. Participants' movements also overshot slow\ntrajectories and undershot fast trajectories. We show that these trajectory\nerrors can be explained by a model in which the proprioceptive system\nintegrates velocity misestimates to infer position."
                },
                "authors": [
                    {
                        "name": "Caitlin Callaghan"
                    },
                    {
                        "name": "David J Reinkensmeyer"
                    }
                ],
                "author_detail": {
                    "name": "David J Reinkensmeyer"
                },
                "author": "David J Reinkensmeyer",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11661v1",
                "updated": "2025-10-13T17:35:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    35,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:35:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    35,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-Scientist: Scientific Equation Discovery With Agentic AI"
                },
                "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities."
                },
                "authors": [
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Yuhan Sun"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11653v1",
                "updated": "2025-10-13T17:30:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    30,
                    54,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:30:54Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    30,
                    54,
                    0,
                    286,
                    0
                ],
                "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model"
                },
                "summary": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond."
                },
                "authors": [
                    {
                        "name": "Prasanna Mayilvahanan"
                    },
                    {
                        "name": "Ricardo Dominguez-Olmedo"
                    },
                    {
                        "name": "Thaddäus Wiedemer"
                    },
                    {
                        "name": "Wieland Brendel"
                    }
                ],
                "author_detail": {
                    "name": "Wieland Brendel"
                },
                "author": "Wieland Brendel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11652v1",
                "updated": "2025-10-13T17:30:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    30,
                    36,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:30:36Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    30,
                    36,
                    0,
                    286,
                    0
                ],
                "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems"
                },
                "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason."
                },
                "authors": [
                    {
                        "name": "Xin Gui"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "JinCheng Ren"
                    },
                    {
                        "name": "Qianben Chen"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Yizhi LI"
                    },
                    {
                        "name": "Xinpeng Liu"
                    },
                    {
                        "name": "Xiaowan Li"
                    },
                    {
                        "name": "Wenli Ren"
                    },
                    {
                        "name": "Linyu Miao"
                    },
                    {
                        "name": "Tianrui Qin"
                    },
                    {
                        "name": "Ziqi Shu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Dingfeng Shi"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wangchunshu Zhou"
                },
                "author": "Wangchunshu Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06986v2",
                "updated": "2025-10-13T17:30:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    30,
                    17,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-08T13:11:07Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    11,
                    7,
                    2,
                    281,
                    0
                ],
                "title": "Inverse Portfolio Optimization with Synthetic Investor Data: Recovering\n  Risk Preferences under Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Portfolio Optimization with Synthetic Investor Data: Recovering\n  Risk Preferences under Uncertainty"
                },
                "summary": "This study develops an inverse portfolio optimization framework for\nrecovering latent investor preferences including risk aversion, transaction\ncost sensitivity, and ESG orientation from observed portfolio allocations.\nUsing controlled synthetic data, we assess the estimator's statistical\nproperties such as consistency, coverage, and dynamic regret. The model\nintegrates robust optimization and regret-based inference to quantify welfare\nlosses under preference misspecification and market shocks. Simulation\nexperiments demonstrate accurate recovery of transaction cost parameters,\npartial identifiability of ESG penalties, and sublinear regret even under\nstochastic volatility and liquidity shocks. A real-data illustration using ETFs\nconfirms that transaction-cost shocks dominate volatility shocks in welfare\nimpact. The framework thus provides a statistically rigorous and economically\ninterpretable tool for robust preference inference and portfolio design under\nuncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study develops an inverse portfolio optimization framework for\nrecovering latent investor preferences including risk aversion, transaction\ncost sensitivity, and ESG orientation from observed portfolio allocations.\nUsing controlled synthetic data, we assess the estimator's statistical\nproperties such as consistency, coverage, and dynamic regret. The model\nintegrates robust optimization and regret-based inference to quantify welfare\nlosses under preference misspecification and market shocks. Simulation\nexperiments demonstrate accurate recovery of transaction cost parameters,\npartial identifiability of ESG penalties, and sublinear regret even under\nstochastic volatility and liquidity shocks. A real-data illustration using ETFs\nconfirms that transaction-cost shocks dominate volatility shocks in welfare\nimpact. The framework thus provides a statistically rigorous and economically\ninterpretable tool for robust preference inference and portfolio design under\nuncertainty."
                },
                "authors": [
                    {
                        "name": "Jinho Cha"
                    },
                    {
                        "name": "Long Pham"
                    },
                    {
                        "name": "Thi Le Hoa Vo"
                    },
                    {
                        "name": "Jaeyoung Cho"
                    },
                    {
                        "name": "Jaejin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaejin Lee"
                },
                "author": "Jaejin Lee",
                "arxiv_comment": "48 pages, 8 figures, appendix included (We only updated author\n  affiliation for Jaeyoung Cho)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02356v2",
                "updated": "2025-10-13T17:24:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    24,
                    22,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-27T23:39:56Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    23,
                    39,
                    56,
                    5,
                    270,
                    0
                ],
                "title": "Measuring Physical-World Privacy Awareness of Large Language Models: An\n  Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Physical-World Privacy Awareness of Large Language Models: An\n  Evaluation Benchmark"
                },
                "summary": "The deployment of Large Language Models (LLMs) in embodied agents creates an\nurgent need to measure their privacy awareness in the physical world. Existing\nevaluation methods, however, are confined to natural language based scenarios.\nTo bridge this gap, we introduce EAPrivacy, a comprehensive evaluation\nbenchmark designed to quantify the physical-world privacy awareness of\nLLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across\nfour tiers to test an agent's ability to handle sensitive objects, adapt to\nchanging environments, balance task execution with privacy constraints, and\nresolve conflicts with social norms. Our measurements reveal a critical deficit\nin current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\\%\naccuracy in scenarios involving changing physical environments. Furthermore,\nwhen a task was accompanied by a privacy request, models prioritized completion\nover the constraint in up to 86\\% of cases. In high-stakes situations pitting\nprivacy against critical social norms, leading models like GPT-4o and\nClaude-3.5-haiku disregarded the social norm over 15\\% of the time. These\nfindings, demonstrated by our benchmark, underscore a fundamental misalignment\nin LLMs regarding physically grounded privacy and establish the need for more\nrobust, physically-aware alignment. Codes and datasets will be available at\nhttps://github.com/Graph-COM/EAPrivacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) in embodied agents creates an\nurgent need to measure their privacy awareness in the physical world. Existing\nevaluation methods, however, are confined to natural language based scenarios.\nTo bridge this gap, we introduce EAPrivacy, a comprehensive evaluation\nbenchmark designed to quantify the physical-world privacy awareness of\nLLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across\nfour tiers to test an agent's ability to handle sensitive objects, adapt to\nchanging environments, balance task execution with privacy constraints, and\nresolve conflicts with social norms. Our measurements reveal a critical deficit\nin current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\\%\naccuracy in scenarios involving changing physical environments. Furthermore,\nwhen a task was accompanied by a privacy request, models prioritized completion\nover the constraint in up to 86\\% of cases. In high-stakes situations pitting\nprivacy against critical social norms, leading models like GPT-4o and\nClaude-3.5-haiku disregarded the social norm over 15\\% of the time. These\nfindings, demonstrated by our benchmark, underscore a fundamental misalignment\nin LLMs regarding physically grounded privacy and establish the need for more\nrobust, physically-aware alignment. Codes and datasets will be available at\nhttps://github.com/Graph-COM/EAPrivacy."
                },
                "authors": [
                    {
                        "name": "Xinjie Shen"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23703v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23703v4",
                "updated": "2025-10-13T17:20:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    20,
                    33,
                    0,
                    286,
                    0
                ],
                "published": "2025-05-29T17:39:30Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    39,
                    30,
                    3,
                    149,
                    0
                ],
                "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability"
                },
                "summary": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning (NFL-HR)**, an\nend-to-end framework designed to incorporate the FL expert into NL math\nproblem-solving. To bridge the NL and FL input format gap, we propose the NL-FL\nProblem Alignment method, which reformulates the Question-Answering (QA)\nproblems in NL as existence theorems in FL. Subsequently, the Mixed Problem\nInput technique we provide enables the FL reasoner to handle both QA and\nexistence problems concurrently. Lastly, we mitigate the NL and FL output\nformat gap in reasoning through an LLM-based Answer Extraction mechanism.\nComprehensive experiments demonstrate that the NFL-HR framework achieves\n**89.80**% and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by **4.60%** and **4.82%**,\nrespectively. Notably, some problems resolved by our framework remain unsolved\nby the NL baseline model even under a larger number of trials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning (NFL-HR)**, an\nend-to-end framework designed to incorporate the FL expert into NL math\nproblem-solving. To bridge the NL and FL input format gap, we propose the NL-FL\nProblem Alignment method, which reformulates the Question-Answering (QA)\nproblems in NL as existence theorems in FL. Subsequently, the Mixed Problem\nInput technique we provide enables the FL reasoner to handle both QA and\nexistence problems concurrently. Lastly, we mitigate the NL and FL output\nformat gap in reasoning through an LLM-based Answer Extraction mechanism.\nComprehensive experiments demonstrate that the NFL-HR framework achieves\n**89.80**% and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by **4.60%** and **4.82%**,\nrespectively. Notably, some problems resolved by our framework remain unsolved\nby the NL baseline model even under a larger number of trials."
                },
                "authors": [
                    {
                        "name": "Ruida Wang"
                    },
                    {
                        "name": "Yuxin Li"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23703v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23703v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11639v1",
                "updated": "2025-10-13T17:20:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    20,
                    13,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:20:13Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    20,
                    13,
                    0,
                    286,
                    0
                ],
                "title": "OneRec-Think: In-Text Reasoning for Generative Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneRec-Think: In-Text Reasoning for Generative Recommendation"
                },
                "summary": "The powerful generative capacity of Large Language Models (LLMs) has\ninstigated a paradigm shift in recommendation. However, existing generative\nmodels (e.g., OneRec) operate as implicit predictors, critically lacking the\ncapacity for explicit and controllable reasoning-a key advantage of LLMs. To\nbridge this gap, we propose OneRec-Think, a unified framework that seamlessly\nintegrates dialogue, reasoning, and personalized recommendation. OneRec-Think\nincorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for\nsemantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate\nLLM reasoning within the recommendation context; and (3) Reasoning Enhancement,\nwhere we design a recommendation-specific reward function that accounts for the\nmulti-validity nature of user preferences. Experiments across public benchmarks\nshow state-of-the-art performance. Moreover, our proposed \"Think-Ahead\"\narchitecture enables effective industrial deployment on Kuaishou, achieving a\n0.159\\% gain in APP Stay Time and validating the practical efficacy of the\nmodel's explicit reasoning capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The powerful generative capacity of Large Language Models (LLMs) has\ninstigated a paradigm shift in recommendation. However, existing generative\nmodels (e.g., OneRec) operate as implicit predictors, critically lacking the\ncapacity for explicit and controllable reasoning-a key advantage of LLMs. To\nbridge this gap, we propose OneRec-Think, a unified framework that seamlessly\nintegrates dialogue, reasoning, and personalized recommendation. OneRec-Think\nincorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for\nsemantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate\nLLM reasoning within the recommendation context; and (3) Reasoning Enhancement,\nwhere we design a recommendation-specific reward function that accounts for the\nmulti-validity nature of user preferences. Experiments across public benchmarks\nshow state-of-the-art performance. Moreover, our proposed \"Think-Ahead\"\narchitecture enables effective industrial deployment on Kuaishou, achieving a\n0.159\\% gain in APP Stay Time and validating the practical efficacy of the\nmodel's explicit reasoning capability."
                },
                "authors": [
                    {
                        "name": "Zhanyu Liu"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Xingmei Wang"
                    },
                    {
                        "name": "Rongzhou Zhang"
                    },
                    {
                        "name": "Jiaxin Deng"
                    },
                    {
                        "name": "Honghui Bao"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Wuchao Li"
                    },
                    {
                        "name": "Pengfei Zheng"
                    },
                    {
                        "name": "Xiangyu Wu"
                    },
                    {
                        "name": "Yifei Hu"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Lejian Ren"
                    },
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Qianqian Wang"
                    },
                    {
                        "name": "Kuo Cai"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Hongtao Cheng"
                    },
                    {
                        "name": "Zexuan Cheng"
                    },
                    {
                        "name": "Lu Ren"
                    },
                    {
                        "name": "Huanjie Wang"
                    },
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v3",
                "updated": "2025-10-13T17:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    15,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11633v1",
                "updated": "2025-10-13T17:14:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    14,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:14:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    14,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "The Role of Congeniality in Multiple Imputation for Doubly Robust Causal\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Congeniality in Multiple Imputation for Doubly Robust Causal\n  Estimation"
                },
                "summary": "This paper provides clear and practical guidance on the specification of\nimputation models when multiple imputation is used in conjunction with doubly\nrobust estimation methods for causal inference. Through theoretical arguments\nand targeted simulations, we show that when a confounder has missing data the\ncorresponding imputation model must include all variables used in either the\npropensity score model or the outcome model, and that these variables must\nappear in the same functional form as in the final analysis. Violating these\nconditions can lead to biased treatment effect estimates, even when both\ncomponents of the doubly robust estimator are correctly specified. We present a\nmathematical framework for doubly robust estimation combined with multiple\nimputation, establish the theoretical requirements for proper imputation in\nthis setting, and demonstrate the consequences of misspecification through\nsimulation. Based on these findings, we offer concrete recommendations to\nensure valid inference when using multiple imputation with doubly robust\nmethods in applied causal analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides clear and practical guidance on the specification of\nimputation models when multiple imputation is used in conjunction with doubly\nrobust estimation methods for causal inference. Through theoretical arguments\nand targeted simulations, we show that when a confounder has missing data the\ncorresponding imputation model must include all variables used in either the\npropensity score model or the outcome model, and that these variables must\nappear in the same functional form as in the final analysis. Violating these\nconditions can lead to biased treatment effect estimates, even when both\ncomponents of the doubly robust estimator are correctly specified. We present a\nmathematical framework for doubly robust estimation combined with multiple\nimputation, establish the theoretical requirements for proper imputation in\nthis setting, and demonstrate the consequences of misspecification through\nsimulation. Based on these findings, we offer concrete recommendations to\nensure valid inference when using multiple imputation with doubly robust\nmethods in applied causal analyses."
                },
                "authors": [
                    {
                        "name": "Lucy D'Agostino McGowan"
                    }
                ],
                "author_detail": {
                    "name": "Lucy D'Agostino McGowan"
                },
                "author": "Lucy D'Agostino McGowan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11631v1",
                "updated": "2025-10-13T17:12:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    12,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:12:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    12,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models"
                },
                "summary": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics."
                },
                "authors": [
                    {
                        "name": "Tobias Preintner"
                    },
                    {
                        "name": "Weixuan Yuan"
                    },
                    {
                        "name": "Adrian König"
                    },
                    {
                        "name": "Thomas Bäck"
                    },
                    {
                        "name": "Elena Raponi"
                    },
                    {
                        "name": "Niki van Stein"
                    }
                ],
                "author_detail": {
                    "name": "Niki van Stein"
                },
                "author": "Niki van Stein",
                "arxiv_comment": "Accepted to IEEE ICTAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22578v2",
                "updated": "2025-10-13T17:12:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    12,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-06-27T18:51:25Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    18,
                    51,
                    25,
                    4,
                    178,
                    0
                ],
                "title": "The Hidden Link Between RLHF and Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Link Between RLHF and Contrastive Learning"
                },
                "summary": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be interpreted as methods that performing\ncontrastive learning based on the positive and negative samples derived from\nbase model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). Such paradigm further illuminates why RLHF\nmay not intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on the perspective, we replace the\nDV/MINE bound with the Jensen-Shannon (JS) MI estimator and propose the Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be interpreted as methods that performing\ncontrastive learning based on the positive and negative samples derived from\nbase model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). Such paradigm further illuminates why RLHF\nmay not intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on the perspective, we replace the\nDV/MINE bound with the Jensen-Shannon (JS) MI estimator and propose the Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks."
                },
                "authors": [
                    {
                        "name": "Xufei Lv"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Haoyuan Sun"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Houde Liu"
                    },
                    {
                        "name": "Kehai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kehai Chen"
                },
                "author": "Kehai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11628v1",
                "updated": "2025-10-13T17:10:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    10,
                    25,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:10:25Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    10,
                    25,
                    0,
                    286,
                    0
                ],
                "title": "Bayesian Self-Calibration and Parametric Channel Estimation for 6G\n  Antenna Arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Self-Calibration and Parametric Channel Estimation for 6G\n  Antenna Arrays"
                },
                "summary": "Accurate channel estimation is essential for both high-rate communication and\nhigh-precision sensing in 6G wireless systems. However, a major performance\nlimitation arises from calibration mismatches when operating phased-array\nantennas under real-world conditions. To address this issue, we propose to\nintegrate antenna element self-calibration into a variational sparse Bayesian\nlearning (VSBL) algorithm for parametric channel estimation. We model antenna\ngain and phase deviations as latent variables and derive explicit update\nequations to jointly infer these calibration parameters and the channel\nparameters: the model order, complex amplitudes, delays, angles, and the noise\nvariance. The resulting algorithm operates online and adapts in real time to\nhardware-induced mismatches. We assess its performance in terms of the root\nmean square error (RMSE) and the optimal subpattern-assignment (OSPA) metric,\ndemonstrating consistent improvements over conventional VSBL without\ncalibration. Our results demonstrate that embedding self-calibration within\nBayesian inference significantly enhances the robustness of channel estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate channel estimation is essential for both high-rate communication and\nhigh-precision sensing in 6G wireless systems. However, a major performance\nlimitation arises from calibration mismatches when operating phased-array\nantennas under real-world conditions. To address this issue, we propose to\nintegrate antenna element self-calibration into a variational sparse Bayesian\nlearning (VSBL) algorithm for parametric channel estimation. We model antenna\ngain and phase deviations as latent variables and derive explicit update\nequations to jointly infer these calibration parameters and the channel\nparameters: the model order, complex amplitudes, delays, angles, and the noise\nvariance. The resulting algorithm operates online and adapts in real time to\nhardware-induced mismatches. We assess its performance in terms of the root\nmean square error (RMSE) and the optimal subpattern-assignment (OSPA) metric,\ndemonstrating consistent improvements over conventional VSBL without\ncalibration. Our results demonstrate that embedding self-calibration within\nBayesian inference significantly enhances the robustness of channel estimation."
                },
                "authors": [
                    {
                        "name": "Patrick Hödl"
                    },
                    {
                        "name": "Jakob Möderl"
                    },
                    {
                        "name": "Erik Leitinger"
                    },
                    {
                        "name": "Klaus Witrisal"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Witrisal"
                },
                "author": "Klaus Witrisal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11620v1",
                "updated": "2025-10-13T17:02:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    2,
                    41,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:02:41Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    2,
                    41,
                    0,
                    286,
                    0
                ],
                "title": "Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan\n  Aggregation"
                },
                "summary": "Inference-time scaling enhances the reasoning ability of a language model\n(LM) by extending its chain-of-thought (CoT). However, existing approaches\ntypically generate the entire reasoning chain in a single forward pass, which\noften leads to CoT derailment, i.e., the reasoning trajectory drifting off\ncourse due to compounding errors. This problem is particularly severe for\nsmaller LMs with long CoTs due to their limited capacity. To address this, we\nanalyze raw long CoTs and uncover a reasoning hierarchy consisting of planning\nand execution steps. Our analysis reveals that most reasoning errors stem from\nincorrect planning. Motivated by this observation, we propose Multi-Path Plan\nAggregation (MPPA), a framework that augments single-pass reasoning with plan\nexploration and aggregation. Following a variable interval schedule based on\nthe token position, MPPA generates multiple candidate plans and aggregates them\ninto a refined planning step. To maintain efficiency, we adopt a minimal design\nin which the base LM serves as the primary policy, while a lightweight LoRA\nmodule implements the plan aggregation policy. We further observe that\noutcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K\ntokens). To overcome this, we introduce online Step-DPO, a process-level\npreference optimization scheme that leverages Twisted Sequential Monte Carlo\n(TSMC) to provide scalable stepwise supervision using small LMs. This yields\nmore efficient training, improved stability, and higher accuracy. Extensive\nexperiments on challenging math, science, and logical reasoning benchmarks\ndemonstrate that, with only 10% SFT data and 5% of preference pairs, our method\noutperforms both the DeepSeek-R1 distillation baseline and the outcome-reward\nRL baseline across multiple base models and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling enhances the reasoning ability of a language model\n(LM) by extending its chain-of-thought (CoT). However, existing approaches\ntypically generate the entire reasoning chain in a single forward pass, which\noften leads to CoT derailment, i.e., the reasoning trajectory drifting off\ncourse due to compounding errors. This problem is particularly severe for\nsmaller LMs with long CoTs due to their limited capacity. To address this, we\nanalyze raw long CoTs and uncover a reasoning hierarchy consisting of planning\nand execution steps. Our analysis reveals that most reasoning errors stem from\nincorrect planning. Motivated by this observation, we propose Multi-Path Plan\nAggregation (MPPA), a framework that augments single-pass reasoning with plan\nexploration and aggregation. Following a variable interval schedule based on\nthe token position, MPPA generates multiple candidate plans and aggregates them\ninto a refined planning step. To maintain efficiency, we adopt a minimal design\nin which the base LM serves as the primary policy, while a lightweight LoRA\nmodule implements the plan aggregation policy. We further observe that\noutcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K\ntokens). To overcome this, we introduce online Step-DPO, a process-level\npreference optimization scheme that leverages Twisted Sequential Monte Carlo\n(TSMC) to provide scalable stepwise supervision using small LMs. This yields\nmore efficient training, improved stability, and higher accuracy. Extensive\nexperiments on challenging math, science, and logical reasoning benchmarks\ndemonstrate that, with only 10% SFT data and 5% of preference pairs, our method\noutperforms both the DeepSeek-R1 distillation baseline and the outcome-reward\nRL baseline across multiple base models and tasks."
                },
                "authors": [
                    {
                        "name": "Siheng Xiong"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Faramarz Fekri"
                    }
                ],
                "author_detail": {
                    "name": "Faramarz Fekri"
                },
                "author": "Faramarz Fekri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11615v1",
                "updated": "2025-10-13T16:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    55,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:55:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    55,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "LLM-Oriented Token-Adaptive Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Oriented Token-Adaptive Knowledge Distillation"
                },
                "summary": "Knowledge distillation (KD) is a key technique for compressing large-scale\nlanguage models (LLMs), yet prevailing logit-based methods typically employ\nstatic strategies that are misaligned with the dynamic learning process of\nstudent models. These methods typically treat all tokens indiscriminately and\napply a single, fixed temperature, resulting in suboptimal knowledge transfer.\nTo address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge\nDistillation (AdaKD), a novel framework that adapts the distillation process to\nthe real-time learning state of each token. AdaKD consists of two synergistic\nmodules driven by a unified token difficulty metric. First, our Loss-Driven\nAdaptive Token Focusing (LATF) module dynamically adjusts the distillation\nfocus by monitoring the student's learning stability, concentrating\ncomputational resources on the most valuable tokens at each training phase.\nSecond, we introduce Inverse Difficulty Temperature Scaling (IDTS), a\ncounterintuitive yet effective token-level temperature strategy. It employs low\ntemperatures for difficult tokens for targeted error correction, and high\ntemperatures for easy tokens to encourage students to learn from the teacher's\ncomplete and smooth output distribution, thereby enhancing generalization. As a\nplug-and-play framework, AdaKD can consistently improve the performance of\nvarious distillation methods on multiple model architectures and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is a key technique for compressing large-scale\nlanguage models (LLMs), yet prevailing logit-based methods typically employ\nstatic strategies that are misaligned with the dynamic learning process of\nstudent models. These methods typically treat all tokens indiscriminately and\napply a single, fixed temperature, resulting in suboptimal knowledge transfer.\nTo address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge\nDistillation (AdaKD), a novel framework that adapts the distillation process to\nthe real-time learning state of each token. AdaKD consists of two synergistic\nmodules driven by a unified token difficulty metric. First, our Loss-Driven\nAdaptive Token Focusing (LATF) module dynamically adjusts the distillation\nfocus by monitoring the student's learning stability, concentrating\ncomputational resources on the most valuable tokens at each training phase.\nSecond, we introduce Inverse Difficulty Temperature Scaling (IDTS), a\ncounterintuitive yet effective token-level temperature strategy. It employs low\ntemperatures for difficult tokens for targeted error correction, and high\ntemperatures for easy tokens to encourage students to learn from the teacher's\ncomplete and smooth output distribution, thereby enhancing generalization. As a\nplug-and-play framework, AdaKD can consistently improve the performance of\nvarious distillation methods on multiple model architectures and benchmarks."
                },
                "authors": [
                    {
                        "name": "Xurong Xie"
                    },
                    {
                        "name": "Zhucun Xue"
                    },
                    {
                        "name": "Jiafu Wu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yabiao Wang"
                    },
                    {
                        "name": "Xiaobin Hu"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jiangning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangning Zhang"
                },
                "author": "Jiangning Zhang",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11608v1",
                "updated": "2025-10-13T16:47:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    47,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:47:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    47,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems"
                },
                "summary": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook."
                },
                "authors": [
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Yunqing Xu"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Tiancheng Shen"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11600v1",
                "updated": "2025-10-13T16:40:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    40,
                    44,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:40:44Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    40,
                    44,
                    0,
                    286,
                    0
                ],
                "title": "A framework for realisable data-driven active flow control using model\n  predictive control applied to a simplified truck wake",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A framework for realisable data-driven active flow control using model\n  predictive control applied to a simplified truck wake"
                },
                "summary": "We present an efficient and realisable active flow control framework with few\nnon-intrusive sensors. The method builds upon data-driven, reduced-order\npredictive models based on Long-Short-Term Memory (LSTM) networks and efficient\ngradient-based Model Predictive Control (MPC). The model uses only\nsurface-mounted pressure probes to infer the wake state, and is trained\nentirely offline on a dataset built with open-loop actuations, thus avoiding\nthe complexities of online learning. Sparsification of the sensors needed for\ncontrol from an initially large set is achieved using SHapley Additive\nexPlanations. A parsimonious set of sensors is then deployed in closed-loop\ncontrol with MPC. The framework is tested in numerical simulations of a 2D\ntruck model at Reynolds number 500, with pulsed-jet actuators placed in the\nrear of the truck to control the wake. The parsimonious LSTM-MPC achieved a\ndrag reduction of 12.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient and realisable active flow control framework with few\nnon-intrusive sensors. The method builds upon data-driven, reduced-order\npredictive models based on Long-Short-Term Memory (LSTM) networks and efficient\ngradient-based Model Predictive Control (MPC). The model uses only\nsurface-mounted pressure probes to infer the wake state, and is trained\nentirely offline on a dataset built with open-loop actuations, thus avoiding\nthe complexities of online learning. Sparsification of the sensors needed for\ncontrol from an initially large set is achieved using SHapley Additive\nexPlanations. A parsimonious set of sensors is then deployed in closed-loop\ncontrol with MPC. The framework is tested in numerical simulations of a 2D\ntruck model at Reynolds number 500, with pulsed-jet actuators placed in the\nrear of the truck to control the wake. The parsimonious LSTM-MPC achieved a\ndrag reduction of 12.8%."
                },
                "authors": [
                    {
                        "name": "Alberto Solera-Rico"
                    },
                    {
                        "name": "Carlos Sanmiguel Vila"
                    },
                    {
                        "name": "Stefano Discetti"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Discetti"
                },
                "author": "Stefano Discetti",
                "arxiv_comment": "28 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "76D55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04872v2",
                "updated": "2025-10-13T16:38:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    38,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-04-07T09:27:37Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    27,
                    37,
                    0,
                    97,
                    0
                ],
                "title": "Simulating Persuasive Dialogues on Meat Reduction with Generative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Persuasive Dialogues on Meat Reduction with Generative Agents"
                },
                "summary": "Meat reduction benefits human and planetary health, but social norms keep\nmeat central in shared meals. To date, the development of communication\nstrategies that promote meat reduction while minimizing social costs has\nrequired the costly involvement of human participants at each stage of the\nprocess. We present work in progress on simulating multi-round dialogues on\nmeat reduction between Generative Agents based on large language models (LLMs).\nWe measure our main outcome using established psychological questionnaires\nbased on the Theory of Planned Behavior and additionally investigate Social\nCosts. We find evidence that our preliminary simulations produce outcomes that\nare (i) consistent with theoretical expectations; and (ii) valid when compared\nto data from previous studies with human participants. Generative agent-based\nmodels are a promising tool for identifying novel communication strategies on\nmeat reduction -- tailored to highly specific participant groups -- to then be\ntested in subsequent studies with human participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meat reduction benefits human and planetary health, but social norms keep\nmeat central in shared meals. To date, the development of communication\nstrategies that promote meat reduction while minimizing social costs has\nrequired the costly involvement of human participants at each stage of the\nprocess. We present work in progress on simulating multi-round dialogues on\nmeat reduction between Generative Agents based on large language models (LLMs).\nWe measure our main outcome using established psychological questionnaires\nbased on the Theory of Planned Behavior and additionally investigate Social\nCosts. We find evidence that our preliminary simulations produce outcomes that\nare (i) consistent with theoretical expectations; and (ii) valid when compared\nto data from previous studies with human participants. Generative agent-based\nmodels are a promising tool for identifying novel communication strategies on\nmeat reduction -- tailored to highly specific participant groups -- to then be\ntested in subsequent studies with human participants."
                },
                "authors": [
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Elena Wurth"
                    },
                    {
                        "name": "Markus Strohmaier"
                    },
                    {
                        "name": "Jutta Mata"
                    }
                ],
                "author_detail": {
                    "name": "Jutta Mata"
                },
                "author": "Jutta Mata",
                "arxiv_doi": "10.36190/2025.30",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.36190/2025.30",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.04872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Code available at https://github.com/dess-mannheim/MeatlessAgents",
                "arxiv_journal_ref": "NLPSI 2025: First Workshop on Integrating NLP and Psychology to\n  Study Social Interactions",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11598v1",
                "updated": "2025-10-13T16:37:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    37,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:37:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    37,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language\n  Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most widely used\nparameter-efficient fine-tuning (PEFT) methods for adapting large language\nmodels (LLMs) to downstream tasks. While highly effective in single-task\nsettings, it struggles to efficiently leverage inter-task knowledge in complex\nmulti-task learning scenarios, often requiring substantial task-specific data\nto achieve optimal performance. To address this limitation, we introduce\nMeTA-LoRA, a two-stage optimization framework that significantly improves data\nefficiency in multi-task adaptation. In the first stage, task-specific LoRA\nadapters are learned using only a few samples from each involved dataset,\nenabling rapid adaptation without large-scale supervision. In the second stage,\nthe shared LoRA adapter is updated by aggregating gradients from multiple tasks\nto promote knowledge transfer across tasks, further reducing data usage by\nleveraging common patterns. In both multi-task learning and multilingual\nlearning scenarios, our method matches or surpasses the performance of\ntraditional full-data LoRA fine-tuning approaches, while using significantly\nless task-specific data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as one of the most widely used\nparameter-efficient fine-tuning (PEFT) methods for adapting large language\nmodels (LLMs) to downstream tasks. While highly effective in single-task\nsettings, it struggles to efficiently leverage inter-task knowledge in complex\nmulti-task learning scenarios, often requiring substantial task-specific data\nto achieve optimal performance. To address this limitation, we introduce\nMeTA-LoRA, a two-stage optimization framework that significantly improves data\nefficiency in multi-task adaptation. In the first stage, task-specific LoRA\nadapters are learned using only a few samples from each involved dataset,\nenabling rapid adaptation without large-scale supervision. In the second stage,\nthe shared LoRA adapter is updated by aggregating gradients from multiple tasks\nto promote knowledge transfer across tasks, further reducing data usage by\nleveraging common patterns. In both multi-task learning and multilingual\nlearning scenarios, our method matches or surpasses the performance of\ntraditional full-data LoRA fine-tuning approaches, while using significantly\nless task-specific data."
                },
                "authors": [
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Jinda Liu"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15818v2",
                "updated": "2025-10-13T16:36:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    36,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-05-21T17:59:56Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    59,
                    56,
                    2,
                    141,
                    0
                ],
                "title": "InstructSAM: A Training-Free Framework for Instruction-Oriented Remote\n  Sensing Object Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructSAM: A Training-Free Framework for Instruction-Oriented Remote\n  Sensing Object Recognition"
                },
                "summary": "Language-Guided object recognition in remote sensing imagery is crucial for\nlarge-scale mapping and automated data annotation. However, existing\nopen-vocabulary and visual grounding methods rely on explicit category cues,\nlimiting their ability to handle complex or implicit queries that require\nadvanced reasoning. To address this issue, we introduce a new suite of tasks,\nincluding Instruction-Oriented Object Counting, Detection, and Segmentation\n(InstructCDS), covering open-vocabulary, open-ended, and open-subclass\nscenarios. We further present EarthInstruct, the first InstructCDS benchmark\nfor earth observation. It is constructed from two diverse remote sensing\ndatasets with varying spatial resolutions and annotation rules across 20\ncategories, necessitating models to interpret dataset-specific instructions.\nGiven the scarcity of semantically rich labeled data in remote sensing, we\npropose InstructSAM, a training-free framework for instruction-driven object\nrecognition. InstructSAM leverages large vision-language models to interpret\nuser instructions and estimate object counts, employs SAM2 for mask proposal,\nand formulates mask-label assignment as a binary integer programming problem.\nBy integrating semantic similarity with counting constraints, InstructSAM\nefficiently assigns categories to predicted masks without relying on confidence\nthresholds. Experiments demonstrate that InstructSAM matches or surpasses\nspecialized baselines across multiple tasks while maintaining near-constant\ninference time regardless of object count, reducing output tokens by 89% and\noverall runtime by over 32% compared to direct generation approaches. We\nbelieve the contributions of the proposed tasks, benchmark, and effective\napproach will advance future research in developing versatile object\nrecognition systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Guided object recognition in remote sensing imagery is crucial for\nlarge-scale mapping and automated data annotation. However, existing\nopen-vocabulary and visual grounding methods rely on explicit category cues,\nlimiting their ability to handle complex or implicit queries that require\nadvanced reasoning. To address this issue, we introduce a new suite of tasks,\nincluding Instruction-Oriented Object Counting, Detection, and Segmentation\n(InstructCDS), covering open-vocabulary, open-ended, and open-subclass\nscenarios. We further present EarthInstruct, the first InstructCDS benchmark\nfor earth observation. It is constructed from two diverse remote sensing\ndatasets with varying spatial resolutions and annotation rules across 20\ncategories, necessitating models to interpret dataset-specific instructions.\nGiven the scarcity of semantically rich labeled data in remote sensing, we\npropose InstructSAM, a training-free framework for instruction-driven object\nrecognition. InstructSAM leverages large vision-language models to interpret\nuser instructions and estimate object counts, employs SAM2 for mask proposal,\nand formulates mask-label assignment as a binary integer programming problem.\nBy integrating semantic similarity with counting constraints, InstructSAM\nefficiently assigns categories to predicted masks without relying on confidence\nthresholds. Experiments demonstrate that InstructSAM matches or surpasses\nspecialized baselines across multiple tasks while maintaining near-constant\ninference time regardless of object count, reducing output tokens by 89% and\noverall runtime by over 32% compared to direct generation approaches. We\nbelieve the contributions of the proposed tasks, benchmark, and effective\napproach will advance future research in developing versatile object\nrecognition systems."
                },
                "authors": [
                    {
                        "name": "Yijie Zheng"
                    },
                    {
                        "name": "Weijie Wu"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Xuehui Wang"
                    },
                    {
                        "name": "Xu Zhou"
                    },
                    {
                        "name": "Aiai Ren"
                    },
                    {
                        "name": "Jun Shen"
                    },
                    {
                        "name": "Long Zhao"
                    },
                    {
                        "name": "Guoqing Li"
                    },
                    {
                        "name": "Xue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xue Yang"
                },
                "author": "Xue Yang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11588v1",
                "updated": "2025-10-13T16:30:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    30,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:30:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    30,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing and Internalizing Complex Policy Documents for LLM Agents"
                },
                "summary": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata."
                },
                "authors": [
                    {
                        "name": "Jiateng Liu"
                    },
                    {
                        "name": "Zhenhailong Wang"
                    },
                    {
                        "name": "Xiaojiang Huang"
                    },
                    {
                        "name": "Yingjie Li"
                    },
                    {
                        "name": "Xing Fan"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Chenlei Guo"
                    },
                    {
                        "name": "Ruhi Sarikaya"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11586v1",
                "updated": "2025-10-13T16:29:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    29,
                    19,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:29:19Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    29,
                    19,
                    0,
                    286,
                    0
                ],
                "title": "Survey Response Generation: Generating Closed-Ended Survey Responses\n  In-Silico with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey Response Generation: Generating Closed-Ended Survey Responses\n  In-Silico with Large Language Models"
                },
                "summary": "Many in-silico simulations of human survey responses with large language\nmodels (LLMs) focus on generating closed-ended survey responses, whereas LLMs\nare typically trained to generate open-ended text instead. Previous research\nhas used a diverse range of methods for generating closed-ended survey\nresponses with LLMs, and a standard practice remains to be identified. In this\npaper, we systematically investigate the impact that various Survey Response\nGeneration Methods have on predicted survey responses. We present the results\nof 32 mio. simulated survey responses across 8 Survey Response Generation\nMethods, 4 political attitude surveys, and 10 open-weight language models. We\nfind significant differences between the Survey Response Generation Methods in\nboth individual-level and subpopulation-level alignment. Our results show that\nRestricted Generation Methods perform best overall, and that reasoning output\ndoes not consistently improve alignment. Our work underlines the significant\nimpact that Survey Response Generation Methods have on simulated survey\nresponses, and we develop practical recommendations on the application of\nSurvey Response Generation Methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many in-silico simulations of human survey responses with large language\nmodels (LLMs) focus on generating closed-ended survey responses, whereas LLMs\nare typically trained to generate open-ended text instead. Previous research\nhas used a diverse range of methods for generating closed-ended survey\nresponses with LLMs, and a standard practice remains to be identified. In this\npaper, we systematically investigate the impact that various Survey Response\nGeneration Methods have on predicted survey responses. We present the results\nof 32 mio. simulated survey responses across 8 Survey Response Generation\nMethods, 4 political attitude surveys, and 10 open-weight language models. We\nfind significant differences between the Survey Response Generation Methods in\nboth individual-level and subpopulation-level alignment. Our results show that\nRestricted Generation Methods perform best overall, and that reasoning output\ndoes not consistently improve alignment. Our work underlines the significant\nimpact that Survey Response Generation Methods have on simulated survey\nresponses, and we develop practical recommendations on the application of\nSurvey Response Generation Methods."
                },
                "authors": [
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11584v1",
                "updated": "2025-10-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    29,
                    17,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    29,
                    17,
                    0,
                    286,
                    0
                ],
                "title": "LLMAtKGE: Large Language Models as Explainable Attackers against\n  Knowledge Graph Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMAtKGE: Large Language Models as Explainable Attackers against\n  Knowledge Graph Embeddings"
                },
                "summary": "Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the\nmodel's ability of link prediction by removing or inserting triples. A recent\nblack-box method has attempted to incorporate textual and structural\ninformation to enhance attack performance. However, it is unable to generate\nhuman-readable explanations, and exhibits poor generalizability. In the past\nfew years, large language models (LLMs) have demonstrated powerful capabilities\nin text comprehension, generation, and reasoning. In this paper, we propose\nLLMAtKGE, a novel LLM-based framework that selects attack targets and generates\nhuman-readable explanations. To provide the LLM with sufficient factual context\nunder limited input constraints, we design a structured prompting scheme that\nexplicitly formulates the attack as multiple-choice questions while\nincorporating KG factual evidence. To address the context-window limitation and\nhesitation issues, we introduce semantics-based and centrality-based filters,\nwhich compress the candidate set while preserving high recall of\nattack-relevant information. Furthermore, to efficiently integrate both\nsemantic and structural information into the filter, we precompute high-order\nadjacency and fine-tune the LLM with a triple classification task to enhance\nfiltering performance. Experiments on two widely used knowledge graph datasets\ndemonstrate that our attack outperforms the strongest black-box baselines and\nprovides explanations via reasoning, and showing competitive performance\ncompared with white-box methods. Comprehensive ablation and case studies\nfurther validate its capability to generate explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the\nmodel's ability of link prediction by removing or inserting triples. A recent\nblack-box method has attempted to incorporate textual and structural\ninformation to enhance attack performance. However, it is unable to generate\nhuman-readable explanations, and exhibits poor generalizability. In the past\nfew years, large language models (LLMs) have demonstrated powerful capabilities\nin text comprehension, generation, and reasoning. In this paper, we propose\nLLMAtKGE, a novel LLM-based framework that selects attack targets and generates\nhuman-readable explanations. To provide the LLM with sufficient factual context\nunder limited input constraints, we design a structured prompting scheme that\nexplicitly formulates the attack as multiple-choice questions while\nincorporating KG factual evidence. To address the context-window limitation and\nhesitation issues, we introduce semantics-based and centrality-based filters,\nwhich compress the candidate set while preserving high recall of\nattack-relevant information. Furthermore, to efficiently integrate both\nsemantic and structural information into the filter, we precompute high-order\nadjacency and fine-tune the LLM with a triple classification task to enhance\nfiltering performance. Experiments on two widely used knowledge graph datasets\ndemonstrate that our attack outperforms the strongest black-box baselines and\nprovides explanations via reasoning, and showing competitive performance\ncompared with white-box methods. Comprehensive ablation and case studies\nfurther validate its capability to generate explanations."
                },
                "authors": [
                    {
                        "name": "Ting Li"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Yipeng Yu"
                    },
                    {
                        "name": "Liang Yao"
                    },
                    {
                        "name": "Guoqing Chao"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08675v2",
                "updated": "2025-10-13T16:23:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    23,
                    12,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T18:00:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    18,
                    0,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "How precisely can we measure the ages of subgiant and giant stars?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How precisely can we measure the ages of subgiant and giant stars?"
                },
                "summary": "Precise stellar ages are fundamental to Galactic archaeology. However,\nobtaining reliable age estimates and uncertainties for field stars has been a\nlong-standing challenge. We test the fidelity of ages from recent catalogs of\ngiants and subgiants using wide binaries, whose components formed at the same\ntime and thus should have consistent inferred ages. We find that subgiant ages\nbased on spectroscopic metallicities from Xiang & Rix (2022) are generally\nconsistent within their reported uncertainties, implying that fractional\nuncertainties of 5-10% are realistically achievable. In contrast, we find that\npublished photometric subgiant ages underestimate true uncertainties by factors\nof 2-3. Spectroscopic age estimates for red giant and red clump stars also show\nreliable uncertainties, but are generally less precise (25-30%). These results\ndemonstrate that accurate metallicity and $\\alpha$-element abundances are\nessential for precise subgiant ages and establish wide binaries as a powerful,\nmodel-independent benchmark for calibrating stellar age measurements in the era\nof large spectroscopic surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise stellar ages are fundamental to Galactic archaeology. However,\nobtaining reliable age estimates and uncertainties for field stars has been a\nlong-standing challenge. We test the fidelity of ages from recent catalogs of\ngiants and subgiants using wide binaries, whose components formed at the same\ntime and thus should have consistent inferred ages. We find that subgiant ages\nbased on spectroscopic metallicities from Xiang & Rix (2022) are generally\nconsistent within their reported uncertainties, implying that fractional\nuncertainties of 5-10% are realistically achievable. In contrast, we find that\npublished photometric subgiant ages underestimate true uncertainties by factors\nof 2-3. Spectroscopic age estimates for red giant and red clump stars also show\nreliable uncertainties, but are generally less precise (25-30%). These results\ndemonstrate that accurate metallicity and $\\alpha$-element abundances are\nessential for precise subgiant ages and establish wide binaries as a powerful,\nmodel-independent benchmark for calibrating stellar age measurements in the era\nof large spectroscopic surveys."
                },
                "authors": [
                    {
                        "name": "Cheyanne Shariat"
                    },
                    {
                        "name": "Kareem El-Badry"
                    },
                    {
                        "name": "Soumyadeep Bhattacharjee"
                    }
                ],
                "author_detail": {
                    "name": "Soumyadeep Bhattacharjee"
                },
                "author": "Soumyadeep Bhattacharjee",
                "arxiv_comment": "revised figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20333v2",
                "updated": "2025-10-13T16:09:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    9,
                    37,
                    0,
                    286,
                    0
                ],
                "published": "2025-05-24T10:25:58Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    25,
                    58,
                    5,
                    144,
                    0
                ],
                "title": "Multi-Scale Manifold Alignment for Interpreting Large Language Models: A\n  Unified Information-Geometric Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Scale Manifold Alignment for Interpreting Large Language Models: A\n  Unified Information-Geometric Framework"
                },
                "summary": "We present Multi-Scale Manifold Alignment(MSMA), an information-geometric\nframework that decomposes LLM representations into local, intermediate, and\nglobal manifolds and learns cross-scale mappings that preserve geometry and\ninformation. Across GPT-2, BERT, RoBERTa, and T5, we observe consistent\nhierarchical patterns and find that MSMA improves alignment metrics under\nmultiple estimators (e.g., relative KL reduction and MI gains with statistical\nsignificance across seeds). Controlled interventions at different scales yield\ndistinct and architecture-dependent effects on lexical diversity, sentence\nstructure, and discourse coherence. While our theoretical analysis relies on\nidealized assumptions, the empirical results suggest that multi-objective\nalignment offers a practical lens for analyzing cross-scale information flow\nand guiding representation-level control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Multi-Scale Manifold Alignment(MSMA), an information-geometric\nframework that decomposes LLM representations into local, intermediate, and\nglobal manifolds and learns cross-scale mappings that preserve geometry and\ninformation. Across GPT-2, BERT, RoBERTa, and T5, we observe consistent\nhierarchical patterns and find that MSMA improves alignment metrics under\nmultiple estimators (e.g., relative KL reduction and MI gains with statistical\nsignificance across seeds). Controlled interventions at different scales yield\ndistinct and architecture-dependent effects on lexical diversity, sentence\nstructure, and discourse coherence. While our theoretical analysis relies on\nidealized assumptions, the empirical results suggest that multi-objective\nalignment offers a practical lens for analyzing cross-scale information flow\nand guiding representation-level control."
                },
                "authors": [
                    {
                        "name": "Yukun Zhang"
                    },
                    {
                        "name": "Qi Dong"
                    }
                ],
                "author_detail": {
                    "name": "Qi Dong"
                },
                "author": "Qi Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05034v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05034v4",
                "updated": "2025-10-13T16:09:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    9,
                    6,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-06T17:10:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    10,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models"
                },
                "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training"
                },
                "authors": [
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Pinxin Liu"
                    },
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Zhangyun Tan"
                    },
                    {
                        "name": "Qianxiang Shen"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Hang Hua"
                    },
                    {
                        "name": "Junjia Guo"
                    },
                    {
                        "name": "Yunzhong Xiao"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Xinyi Liu"
                    },
                    {
                        "name": "Yizhi Song"
                    },
                    {
                        "name": "Yuhe Nie"
                    },
                    {
                        "name": "Jia-Xing Zhong"
                    },
                    {
                        "name": "Bozheng Li"
                    },
                    {
                        "name": "Daiqing Qi"
                    },
                    {
                        "name": "Ziyun Zeng"
                    },
                    {
                        "name": "Ali Vosoughi"
                    },
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Daiki Shimada"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "The 1st version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05034v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05034v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13142v2",
                "updated": "2025-10-13T16:08:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    8,
                    38,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-18T17:55:17Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    17,
                    55,
                    17,
                    0,
                    230,
                    0
                ],
                "title": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence"
                },
                "summary": "Multimodal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, the very capability that anchors artificial\ngeneral intelligence in the physical world. With the recent release of GPT-5,\nallegedly the most powerful AI model to date, it is timely to examine where the\nleading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path\ntoward spatial intelligence. We first propose a holistic taxonomy of spatial\ntasks that unifies existing benchmarks and a standardized protocol for the fair\nevaluation of state-of-the-art proprietary and open-source models across eight\nkey benchmarks, at a cost exceeding ten billion total tokens. Our empirical\nstudy then reveals that (1) GPT-5 demonstrates unprecedented strength in\nspatial intelligence (SI), yet (2) still falls short of human performance\nsignificantly across a broad spectrum of SI-tasks. Moreover, we (3) show that\nSI-tasks expose greater model capability deficiency than non-SI tasks, to the\nextent that (4) proprietary models do not exhibit a decisive advantage when\nfacing the most difficult ones. In addition, we conduct a qualitative\nevaluation across a diverse set of scenarios that are intuitive for humans, yet\nfail even the most advanced multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, the very capability that anchors artificial\ngeneral intelligence in the physical world. With the recent release of GPT-5,\nallegedly the most powerful AI model to date, it is timely to examine where the\nleading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path\ntoward spatial intelligence. We first propose a holistic taxonomy of spatial\ntasks that unifies existing benchmarks and a standardized protocol for the fair\nevaluation of state-of-the-art proprietary and open-source models across eight\nkey benchmarks, at a cost exceeding ten billion total tokens. Our empirical\nstudy then reveals that (1) GPT-5 demonstrates unprecedented strength in\nspatial intelligence (SI), yet (2) still falls short of human performance\nsignificantly across a broad spectrum of SI-tasks. Moreover, we (3) show that\nSI-tasks expose greater model capability deficiency than non-SI tasks, to the\nextent that (4) proprietary models do not exhibit a decisive advantage when\nfacing the most difficult ones. In addition, we conduct a qualitative\nevaluation across a diverse set of scenarios that are intuitive for humans, yet\nfail even the most advanced multimodal models."
                },
                "authors": [
                    {
                        "name": "Zhongang Cai"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Qingping Sun"
                    },
                    {
                        "name": "Ruisi Wang"
                    },
                    {
                        "name": "Chenyang Gu"
                    },
                    {
                        "name": "Wanqi Yin"
                    },
                    {
                        "name": "Zhiqian Lin"
                    },
                    {
                        "name": "Zhitao Yang"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Xuanke Shi"
                    },
                    {
                        "name": "Kewang Deng"
                    },
                    {
                        "name": "Xiaoyang Han"
                    },
                    {
                        "name": "Zukai Chen"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Xiangyu Fan"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Quan Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Yang"
                },
                "author": "Lei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11563v1",
                "updated": "2025-10-13T16:06:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    6,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:06:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    6,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "Culturally-Aware Conversations: A Framework & Benchmark for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culturally-Aware Conversations: A Framework & Benchmark for LLMs"
                },
                "summary": "Existing benchmarks that measure cultural adaptation in LLMs are misaligned\nwith the actual challenges these models face when interacting with users from\ndiverse cultural backgrounds. In this work, we introduce the first framework\nand benchmark designed to evaluate LLMs in realistic, multicultural\nconversational settings. Grounded in sociocultural theory, our framework\nformalizes how linguistic style - a key element of cultural communication - is\nshaped by situational, relational, and cultural context. We construct a\nbenchmark dataset based on this framework, annotated by culturally diverse\nraters, and propose a new set of desiderata for cross-cultural evaluation in\nNLP: conversational framing, stylistic sensitivity, and subjective correctness.\nWe evaluate today's top LLMs on our benchmark and show that these models\nstruggle with cultural adaptation in a conversational setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks that measure cultural adaptation in LLMs are misaligned\nwith the actual challenges these models face when interacting with users from\ndiverse cultural backgrounds. In this work, we introduce the first framework\nand benchmark designed to evaluate LLMs in realistic, multicultural\nconversational settings. Grounded in sociocultural theory, our framework\nformalizes how linguistic style - a key element of cultural communication - is\nshaped by situational, relational, and cultural context. We construct a\nbenchmark dataset based on this framework, annotated by culturally diverse\nraters, and propose a new set of desiderata for cross-cultural evaluation in\nNLP: conversational framing, stylistic sensitivity, and subjective correctness.\nWe evaluate today's top LLMs on our benchmark and show that these models\nstruggle with cultural adaptation in a conversational setting."
                },
                "authors": [
                    {
                        "name": "Shreya Havaldar"
                    },
                    {
                        "name": "Sunny Rai"
                    },
                    {
                        "name": "Young-Min Cho"
                    },
                    {
                        "name": "Lyle Ungar"
                    }
                ],
                "author_detail": {
                    "name": "Lyle Ungar"
                },
                "author": "Lyle Ungar",
                "arxiv_comment": "To appear at the 4th HCI + NLP Workshop @ EMNLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04310v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04310v3",
                "updated": "2025-10-13T16:04:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    4,
                    56,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-04T15:23:58Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    23,
                    58,
                    3,
                    247,
                    0
                ],
                "title": "EvoEmo: Towards Evolved Emotional Policies for Adversarial LLM Agents in\n  Multi-Turn Price Negotiation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoEmo: Towards Evolved Emotional Policies for Adversarial LLM Agents in\n  Multi-Turn Price Negotiation"
                },
                "summary": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation."
                },
                "authors": [
                    {
                        "name": "Yunbo Long"
                    },
                    {
                        "name": "Liming Xu"
                    },
                    {
                        "name": "Lukas Beckenbauer"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04310v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04310v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11561v1",
                "updated": "2025-10-13T16:04:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    4,
                    6,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:04:06Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    4,
                    6,
                    0,
                    286,
                    0
                ],
                "title": "Ontolearn-A Framework for Large-scale OWL Class Expression Learning in\n  Python",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontolearn-A Framework for Large-scale OWL Class Expression Learning in\n  Python"
                },
                "summary": "In this paper, we present Ontolearn-a framework for learning OWL class\nexpressions over large knowledge graphs. Ontolearn contains efficient\nimplementations of recent stateof-the-art symbolic and neuro-symbolic class\nexpression learners including EvoLearner and DRILL. A learned OWL class\nexpression can be used to classify instances in the knowledge graph.\nFurthermore, Ontolearn integrates a verbalization module based on an LLM to\ntranslate complex OWL class expressions into natural language sentences. By\nmapping OWL class expressions into respective SPARQL queries, Ontolearn can be\neasily used to operate over a remote triplestore. The source code of Ontolearn\nis available at https://github.com/dice-group/Ontolearn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Ontolearn-a framework for learning OWL class\nexpressions over large knowledge graphs. Ontolearn contains efficient\nimplementations of recent stateof-the-art symbolic and neuro-symbolic class\nexpression learners including EvoLearner and DRILL. A learned OWL class\nexpression can be used to classify instances in the knowledge graph.\nFurthermore, Ontolearn integrates a verbalization module based on an LLM to\ntranslate complex OWL class expressions into natural language sentences. By\nmapping OWL class expressions into respective SPARQL queries, Ontolearn can be\neasily used to operate over a remote triplestore. The source code of Ontolearn\nis available at https://github.com/dice-group/Ontolearn."
                },
                "authors": [
                    {
                        "name": "Caglar Demir"
                    },
                    {
                        "name": "Alkid Baci"
                    },
                    {
                        "name": "N'Dah Jean Kouagou"
                    },
                    {
                        "name": "Leonie Nora Sieger"
                    },
                    {
                        "name": "Stefan Heindorf"
                    },
                    {
                        "name": "Simon Bin"
                    },
                    {
                        "name": "Lukas Blübaum"
                    },
                    {
                        "name": "Alexander Bigerl"
                    },
                    {
                        "name": "Axel-Cyrille Ngonga Ngomo"
                    }
                ],
                "author_detail": {
                    "name": "Axel-Cyrille Ngonga Ngomo"
                },
                "author": "Axel-Cyrille Ngonga Ngomo",
                "arxiv_journal_ref": "Journal of Machine Learning Research 26 (2025) 1-6",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11560v1",
                "updated": "2025-10-13T16:04:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    4,
                    3,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:04:03Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    4,
                    3,
                    0,
                    286,
                    0
                ],
                "title": "Characterizing Web Search in The Age of Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing Web Search in The Age of Generative AI"
                },
                "summary": "The advent of LLMs has given rise to a new type of web search: Generative\nsearch, where LLMs retrieve web pages related to a query and generate a single,\ncoherent text as a response. This output modality stands in stark contrast to\ntraditional web search, where results are returned as a ranked list of\nindependent web pages. In this paper, we ask: Along what dimensions do\ngenerative search outputs differ from traditional web search? We compare\nGoogle, a traditional web search engine, with four generative search engines\nfrom two providers (Google and OpenAI) across queries from four domains. Our\nanalysis reveals intriguing differences. Most generative search engines cover a\nwider range of sources compared to web search. Generative search engines vary\nin the degree to which they rely on internal knowledge contained within the\nmodel parameters v.s. external knowledge retrieved from the web. Generative\nsearch engines surface varying sets of concepts, creating new opportunities for\nenhancing search diversity and serendipity. Our results also highlight the need\nfor revisiting evaluation criteria for web search in the age of Generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of LLMs has given rise to a new type of web search: Generative\nsearch, where LLMs retrieve web pages related to a query and generate a single,\ncoherent text as a response. This output modality stands in stark contrast to\ntraditional web search, where results are returned as a ranked list of\nindependent web pages. In this paper, we ask: Along what dimensions do\ngenerative search outputs differ from traditional web search? We compare\nGoogle, a traditional web search engine, with four generative search engines\nfrom two providers (Google and OpenAI) across queries from four domains. Our\nanalysis reveals intriguing differences. Most generative search engines cover a\nwider range of sources compared to web search. Generative search engines vary\nin the degree to which they rely on internal knowledge contained within the\nmodel parameters v.s. external knowledge retrieved from the web. Generative\nsearch engines surface varying sets of concepts, creating new opportunities for\nenhancing search diversity and serendipity. Our results also highlight the need\nfor revisiting evaluation criteria for web search in the age of Generative AI."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Jost Grosse Perdekamp"
                    },
                    {
                        "name": "Mihir Upadhyay"
                    },
                    {
                        "name": "Krishna P. Gummadi"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07772v2",
                "updated": "2025-10-13T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    3,
                    13,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T04:24:47Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    4,
                    24,
                    47,
                    3,
                    282,
                    0
                ],
                "title": "An approach for systematic decomposition of complex llm tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An approach for systematic decomposition of complex llm tasks"
                },
                "summary": "Large Language Models (LLMs) suffer from reliability issues on complex tasks,\nas existing decomposition methods are heuristic and rely on agent or manual\ndecomposition. This work introduces a novel, systematic decomposition framework\nthat we call Analysis of CONstraint-Induced Complexity (ACONIC), which models\nthe task as a constraint problem and leveraging formal complexity measures to\nguide decomposition. On combinatorial (SATBench) and LLM database querying\ntasks (Spider), we find that by decomposing the tasks following the measure of\ncomplexity, agent can perform considerably better (10-40 percentage point).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer from reliability issues on complex tasks,\nas existing decomposition methods are heuristic and rely on agent or manual\ndecomposition. This work introduces a novel, systematic decomposition framework\nthat we call Analysis of CONstraint-Induced Complexity (ACONIC), which models\nthe task as a constraint problem and leveraging formal complexity measures to\nguide decomposition. On combinatorial (SATBench) and LLM database querying\ntasks (Spider), we find that by decomposing the tasks following the measure of\ncomplexity, agent can perform considerably better (10-40 percentage point)."
                },
                "authors": [
                    {
                        "name": "Tianle Zhou"
                    },
                    {
                        "name": "Jiakai Xu"
                    },
                    {
                        "name": "Guanhong Liu"
                    },
                    {
                        "name": "Jiaxiang Liu"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Eugene Wu"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Wu"
                },
                "author": "Eugene Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11558v1",
                "updated": "2025-10-13T16:00:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    0,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:00:34Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    0,
                    34,
                    0,
                    286,
                    0
                ],
                "title": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative\n  Study of Market Leading Agentic AI Products",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative\n  Study of Market Leading Agentic AI Products"
                },
                "summary": "Governance of data, compliance, and business privacy matters, particularly\nfor healthcare and finance businesses. Since the recent emergence of AI\nenterprise AI assistants enhancing business productivity, safeguarding private\ndata and compliance is now a priority. With the implementation of AI assistants\nacross the enterprise, the zero data retention can be achieved by implementing\nzero data retention policies by Large Language Model businesses like Open AI\nand Anthropic and Meta. In this work, we explore zero data retention policies\nfor the Enterprise apps of large language models (LLMs). Our key contribution\nis defining the architectural, compliance, and usability trade-offs of such\nsystems in parallel. In this research work, we examine the development of\ncommercial AI assistants with two industry leaders and market titans in this\narena - Salesforce and Microsoft. Both of these companies used distinct\ntechnical architecture to support zero data retention policies. Salesforce\nAgentForce and Microsoft Copilot are among the leading AI assistants providing\nmuch-needed push to business productivity in customer care. The purpose of this\npaper is to analyze the technical architecture and deployment of zero data\nretention policy by consuming applications as well as big language models\nservice providers like Open Ai, Anthropic, and Meta.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Governance of data, compliance, and business privacy matters, particularly\nfor healthcare and finance businesses. Since the recent emergence of AI\nenterprise AI assistants enhancing business productivity, safeguarding private\ndata and compliance is now a priority. With the implementation of AI assistants\nacross the enterprise, the zero data retention can be achieved by implementing\nzero data retention policies by Large Language Model businesses like Open AI\nand Anthropic and Meta. In this work, we explore zero data retention policies\nfor the Enterprise apps of large language models (LLMs). Our key contribution\nis defining the architectural, compliance, and usability trade-offs of such\nsystems in parallel. In this research work, we examine the development of\ncommercial AI assistants with two industry leaders and market titans in this\narena - Salesforce and Microsoft. Both of these companies used distinct\ntechnical architecture to support zero data retention policies. Salesforce\nAgentForce and Microsoft Copilot are among the leading AI assistants providing\nmuch-needed push to business productivity in customer care. The purpose of this\npaper is to analyze the technical architecture and deployment of zero data\nretention policy by consuming applications as well as big language models\nservice providers like Open Ai, Anthropic, and Meta."
                },
                "authors": [
                    {
                        "name": "Komal Gupta"
                    },
                    {
                        "name": "Aditya Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Shrivastava"
                },
                "author": "Aditya Shrivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11557v1",
                "updated": "2025-10-13T16:00:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    0,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:00:15Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    0,
                    15,
                    0,
                    286,
                    0
                ],
                "title": "Invisible Languages of the LLM Universe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Languages of the LLM Universe"
                },
                "summary": "Large Language Models are trained on massive multilingual corpora, yet this\nabundance masks a profound crisis: of the world's 7,613 living languages,\napproximately 2,000 languages with millions of speakers remain effectively\ninvisible in digital ecosystems. We propose a critical framework connecting\nempirical measurements of language vitality (real world demographic strength)\nand digitality (online presence) with postcolonial theory and epistemic\ninjustice to explain why linguistic inequality in AI systems is not incidental\nbut structural. Analyzing data across all documented human languages, we\nidentify four categories: Strongholds (33%, high vitality and digitality),\nDigital Echoes (6%, high digitality despite declining vitality), Fading Voices\n(36%, low on both dimensions), and critically, Invisible Giants (27%, high\nvitality but near-zero digitality) - languages spoken by millions yet absent\nfrom the LLM universe. We demonstrate that these patterns reflect continuities\nfrom colonial-era linguistic hierarchies to contemporary AI development,\nconstituting what we term digital epistemic injustice. Our analysis reveals\nthat English dominance in AI is not a technical necessity but an artifact of\npower structures that systematically exclude marginalized linguistic knowledge.\nWe conclude with implications for decolonizing language technology and\ndemocratizing access to AI benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are trained on massive multilingual corpora, yet this\nabundance masks a profound crisis: of the world's 7,613 living languages,\napproximately 2,000 languages with millions of speakers remain effectively\ninvisible in digital ecosystems. We propose a critical framework connecting\nempirical measurements of language vitality (real world demographic strength)\nand digitality (online presence) with postcolonial theory and epistemic\ninjustice to explain why linguistic inequality in AI systems is not incidental\nbut structural. Analyzing data across all documented human languages, we\nidentify four categories: Strongholds (33%, high vitality and digitality),\nDigital Echoes (6%, high digitality despite declining vitality), Fading Voices\n(36%, low on both dimensions), and critically, Invisible Giants (27%, high\nvitality but near-zero digitality) - languages spoken by millions yet absent\nfrom the LLM universe. We demonstrate that these patterns reflect continuities\nfrom colonial-era linguistic hierarchies to contemporary AI development,\nconstituting what we term digital epistemic injustice. Our analysis reveals\nthat English dominance in AI is not a technical necessity but an artifact of\npower structures that systematically exclude marginalized linguistic knowledge.\nWe conclude with implications for decolonizing language technology and\ndemocratizing access to AI benefits."
                },
                "authors": [
                    {
                        "name": "Saurabh Khanna"
                    },
                    {
                        "name": "Xinxu Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinxu Li"
                },
                "author": "Xinxu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11556v1",
                "updated": "2025-10-13T15:59:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    59,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:59:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    59,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "Personalized and Constructive Feedback for Computer Science Students\n  Using the Large Language Model (LLM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized and Constructive Feedback for Computer Science Students\n  Using the Large Language Model (LLM)"
                },
                "summary": "The evolving pedagogy paradigms are leading toward educational\ntransformations. One fundamental aspect of effective learning is relevant,\nimmediate, and constructive feedback to students. Providing constructive\nfeedback to large cohorts in academia is an ongoing challenge. Therefore,\nacademics are moving towards automated assessment to provide immediate\nfeedback. However, current approaches are often limited in scope, offering\nsimplistic responses that do not provide students with personalized feedback to\nguide them toward improvements. This paper addresses this limitation by\ninvestigating the performance of Large Language Models (LLMs) in processing\nstudents assessments with predefined rubrics and marking criteria to generate\npersonalized feedback for in-depth learning. We aim to leverage the power of\nexisting LLMs for Marking Assessments, Tracking, and Evaluation (LLM-MATE) with\npersonalized feedback to enhance students learning. To evaluate the performance\nof LLM-MATE, we consider the Software Architecture (SA) module as a case study.\nThe LLM-MATE approach can help module leaders overcome assessment challenges\nwith large cohorts. Also, it helps students improve their learning by obtaining\npersonalized feedback in a timely manner. Additionally, the proposed approach\nwill facilitate the establishment of ground truth for automating the generation\nof students assessment feedback using the ChatGPT API, thereby reducing the\noverhead associated with large cohort assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolving pedagogy paradigms are leading toward educational\ntransformations. One fundamental aspect of effective learning is relevant,\nimmediate, and constructive feedback to students. Providing constructive\nfeedback to large cohorts in academia is an ongoing challenge. Therefore,\nacademics are moving towards automated assessment to provide immediate\nfeedback. However, current approaches are often limited in scope, offering\nsimplistic responses that do not provide students with personalized feedback to\nguide them toward improvements. This paper addresses this limitation by\ninvestigating the performance of Large Language Models (LLMs) in processing\nstudents assessments with predefined rubrics and marking criteria to generate\npersonalized feedback for in-depth learning. We aim to leverage the power of\nexisting LLMs for Marking Assessments, Tracking, and Evaluation (LLM-MATE) with\npersonalized feedback to enhance students learning. To evaluate the performance\nof LLM-MATE, we consider the Software Architecture (SA) module as a case study.\nThe LLM-MATE approach can help module leaders overcome assessment challenges\nwith large cohorts. Also, it helps students improve their learning by obtaining\npersonalized feedback in a timely manner. Additionally, the proposed approach\nwill facilitate the establishment of ground truth for automating the generation\nof students assessment feedback using the ChatGPT API, thereby reducing the\noverhead associated with large cohort assessments."
                },
                "authors": [
                    {
                        "name": "Javed Ali Khan"
                    },
                    {
                        "name": "Muhammad Yaqoob"
                    },
                    {
                        "name": "Mamoona Tasadduq"
                    },
                    {
                        "name": "Hafsa Shareef Dar"
                    },
                    {
                        "name": "Aitezaz Ahsan"
                    }
                ],
                "author_detail": {
                    "name": "Aitezaz Ahsan"
                },
                "author": "Aitezaz Ahsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20340v2",
                "updated": "2025-10-13T15:56:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    56,
                    25,
                    0,
                    286,
                    0
                ],
                "published": "2025-05-24T14:17:50Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    14,
                    17,
                    50,
                    5,
                    144,
                    0
                ],
                "title": "Empirical Investigation of Latent Representational Dynamics in Large\n  Language Models: A Manifold Evolution Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Investigation of Latent Representational Dynamics in Large\n  Language Models: A Manifold Evolution Perspective"
                },
                "summary": "This paper introduces the Dynamical Manifold Evolution Theory (DMET), a\nconceptual framework that models large language model (LLM) generation as a\ncontinuous trajectory evolving on a low-dimensional semantic manifold. The\ntheory characterizes latent dynamics through three interpretable metrics-state\ncontinuity ($C$), attractor compactness ($Q$), and topological persistence\n($P$)-which jointly capture the smoothness, stability, and structure of\nrepresentation evolution. Empirical analyses across multiple Transformer\narchitectures reveal consistent links between these latent dynamics and text\nquality: smoother trajectories correspond to greater fluency, and richer\ntopological organization correlates with enhanced coherence. Different models\nexhibit distinct dynamical regimes, reflecting diverse strategies of semantic\norganization in latent space. Moreover, decoding parameters such as temperature\nand top-$p$ shape these trajectories in predictable ways, defining a balanced\nregion that harmonizes fluency and creativity. As a phenomenological rather\nthan first-principles framework, DMET provides a unified and testable\nperspective for interpreting, monitoring, and guiding LLM behavior, offering\nnew insights into the interplay between internal representation dynamics and\nexternal text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Dynamical Manifold Evolution Theory (DMET), a\nconceptual framework that models large language model (LLM) generation as a\ncontinuous trajectory evolving on a low-dimensional semantic manifold. The\ntheory characterizes latent dynamics through three interpretable metrics-state\ncontinuity ($C$), attractor compactness ($Q$), and topological persistence\n($P$)-which jointly capture the smoothness, stability, and structure of\nrepresentation evolution. Empirical analyses across multiple Transformer\narchitectures reveal consistent links between these latent dynamics and text\nquality: smoother trajectories correspond to greater fluency, and richer\ntopological organization correlates with enhanced coherence. Different models\nexhibit distinct dynamical regimes, reflecting diverse strategies of semantic\norganization in latent space. Moreover, decoding parameters such as temperature\nand top-$p$ shape these trajectories in predictable ways, defining a balanced\nregion that harmonizes fluency and creativity. As a phenomenological rather\nthan first-principles framework, DMET provides a unified and testable\nperspective for interpreting, monitoring, and guiding LLM behavior, offering\nnew insights into the interplay between internal representation dynamics and\nexternal text generation quality."
                },
                "authors": [
                    {
                        "name": "Yukun Zhang"
                    },
                    {
                        "name": "Qi Dong"
                    }
                ],
                "author_detail": {
                    "name": "Qi Dong"
                },
                "author": "Qi Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14641v2",
                "updated": "2025-10-13T15:55:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    55,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-06-17T15:39:33Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    39,
                    33,
                    1,
                    168,
                    0
                ],
                "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than\n  Few-shot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than\n  Few-shot"
                },
                "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars."
                },
                "authors": [
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Chengyan Pan"
                    },
                    {
                        "name": "Minjun Zhao"
                    },
                    {
                        "name": "Deyang Li"
                    },
                    {
                        "name": "Fangchao Liu"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "EMNLP25-findings camera_ready, 19 pages,22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23305v2",
                "updated": "2025-10-13T15:49:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    49,
                    43,
                    0,
                    286,
                    0
                ],
                "published": "2025-05-29T10:04:24Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    10,
                    4,
                    24,
                    3,
                    149,
                    0
                ],
                "title": "MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and\n  Source Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and\n  Source Extraction"
                },
                "summary": "We present MGE-LDM, a unified latent diffusion framework for simultaneous\nmusic generation, source imputation, and query-driven source separation. Unlike\nprior approaches constrained to fixed instrument classes, MGE-LDM learns a\njoint distribution over full mixtures, submixtures, and individual stems within\na single compact latent diffusion model. At inference, MGE-LDM enables (1)\ncomplete mixture generation, (2) partial generation (i.e., source imputation),\nand (3) text-conditioned extraction of arbitrary sources. By formulating both\nseparation and imputation as conditional inpainting tasks in the latent space,\nour approach supports flexible, class-agnostic manipulation of arbitrary\ninstrument sources. Notably, MGE-LDM can be trained jointly across\nheterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without\nrelying on predefined instrument categories. Audio samples are available at our\nproject page: https://yoongi43.github.io/MGELDM_Samples/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MGE-LDM, a unified latent diffusion framework for simultaneous\nmusic generation, source imputation, and query-driven source separation. Unlike\nprior approaches constrained to fixed instrument classes, MGE-LDM learns a\njoint distribution over full mixtures, submixtures, and individual stems within\na single compact latent diffusion model. At inference, MGE-LDM enables (1)\ncomplete mixture generation, (2) partial generation (i.e., source imputation),\nand (3) text-conditioned extraction of arbitrary sources. By formulating both\nseparation and imputation as conditional inpainting tasks in the latent space,\nour approach supports flexible, class-agnostic manipulation of arbitrary\ninstrument sources. Notably, MGE-LDM can be trained jointly across\nheterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without\nrelying on predefined instrument categories. Audio samples are available at our\nproject page: https://yoongi43.github.io/MGELDM_Samples/."
                },
                "authors": [
                    {
                        "name": "Yunkee Chae"
                    },
                    {
                        "name": "Kyogu Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyogu Lee"
                },
                "author": "Kyogu Lee",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11545v1",
                "updated": "2025-10-13T15:42:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    42,
                    11,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:42:11Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    42,
                    11,
                    0,
                    286,
                    0
                ],
                "title": "Information-Preserving Reformulation of Reasoning Traces for\n  Antidistillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-Preserving Reformulation of Reasoning Traces for\n  Antidistillation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation."
                },
                "authors": [
                    {
                        "name": "Jiayu Ding"
                    },
                    {
                        "name": "Lei Cui"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Nanning Zheng"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11541v1",
                "updated": "2025-10-13T15:41:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    41,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:41:15Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    41,
                    15,
                    0,
                    286,
                    0
                ],
                "title": "Query-Specific GNN: A Comprehensive Graph Representation Learning Method\n  for Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Specific GNN: A Comprehensive Graph Representation Learning Method\n  for Retrieval Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has demonstrated its ability to enhance\nLarge Language Models (LLMs) by integrating external knowledge sources.\nHowever, multi-hop questions, which require the identification of multiple\nknowledge targets to form a synthesized answer, raise new challenges for RAG\nsystems. Under the multi-hop settings, existing methods often struggle to fully\nunderstand the questions with complex semantic structures and are susceptible\nto irrelevant noise during the retrieval of multiple information targets. To\naddress these limitations, we propose a novel graph representation learning\nframework for multi-hop question retrieval. We first introduce a\nMulti-information Level Knowledge Graph (Multi-L KG) to model various\ninformation levels for a more comprehensive understanding of multi-hop\nquestions. Based on this, we design a Query-Specific Graph Neural Network\n(QSGNN) for representation learning on the Multi-L KG. QSGNN employs\nintra/inter-level message passing mechanisms, and in each message passing the\ninformation aggregation is guided by the query, which not only facilitates\nmulti-granular information aggregation but also significantly reduces the\nimpact of noise. To enhance its ability to learn robust representations, we\nfurther propose two synthesized data generation strategies for pre-training the\nQSGNN. Extensive experimental results demonstrate the effectiveness of our\nframework in multi-hop scenarios, especially in high-hop questions the\nimprovement can reach 33.8\\%. The code is available at:\nhttps://github.com/Jerry2398/QSGNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has demonstrated its ability to enhance\nLarge Language Models (LLMs) by integrating external knowledge sources.\nHowever, multi-hop questions, which require the identification of multiple\nknowledge targets to form a synthesized answer, raise new challenges for RAG\nsystems. Under the multi-hop settings, existing methods often struggle to fully\nunderstand the questions with complex semantic structures and are susceptible\nto irrelevant noise during the retrieval of multiple information targets. To\naddress these limitations, we propose a novel graph representation learning\nframework for multi-hop question retrieval. We first introduce a\nMulti-information Level Knowledge Graph (Multi-L KG) to model various\ninformation levels for a more comprehensive understanding of multi-hop\nquestions. Based on this, we design a Query-Specific Graph Neural Network\n(QSGNN) for representation learning on the Multi-L KG. QSGNN employs\nintra/inter-level message passing mechanisms, and in each message passing the\ninformation aggregation is guided by the query, which not only facilitates\nmulti-granular information aggregation but also significantly reduces the\nimpact of noise. To enhance its ability to learn robust representations, we\nfurther propose two synthesized data generation strategies for pre-training the\nQSGNN. Extensive experimental results demonstrate the effectiveness of our\nframework in multi-hop scenarios, especially in high-hop questions the\nimprovement can reach 33.8\\%. The code is available at:\nhttps://github.com/Jerry2398/QSGNN."
                },
                "authors": [
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Weiming Li"
                    },
                    {
                        "name": "Xiaoshuai Hao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoshuai Hao"
                },
                "author": "Xiaoshuai Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02300v3",
                "updated": "2025-10-13T15:39:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    39,
                    31,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-02T17:59:06Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    6,
                    3,
                    275,
                    0
                ],
                "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models"
                },
                "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference."
                },
                "authors": [
                    {
                        "name": "Runqian Wang"
                    },
                    {
                        "name": "Yilun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Du"
                },
                "author": "Yilun Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11536v1",
                "updated": "2025-10-13T15:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    39,
                    8,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    39,
                    8,
                    0,
                    286,
                    0
                ],
                "title": "CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding\n  Interactions with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding\n  Interactions with LLMs"
                },
                "summary": "Understanding how developers interact with code generation tools (CGTs)\nrequires detailed, real-time data on programming behavior which is often\ndifficult to collect without disrupting workflow. We present\n\\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed\nto capture fine-grained interaction events from within the Visual Studio Code\n(VS Code) editor. \\textit{CodeWatcher} logs semantically meaningful events such\nas insertions made by CGTs, deletions, copy-paste actions, and focus shifts,\nenabling continuous monitoring of developer activity without modifying user\nworkflows. The system comprises a VS Code plugin, a Python-based RESTful API,\nand a MongoDB backend, all containerized for scalability and ease of\ndeployment. By structuring and timestamping each event, \\textit{CodeWatcher}\nenables post-hoc reconstruction of coding sessions and facilitates rich\nbehavioral analyses, including how and when CGTs are used during development.\nThis infrastructure is crucial for supporting research on responsible AI,\ndeveloper productivity, and the human-centered evaluation of CGTs. Please find\nthe demo, diagrams, and tool here: https://osf.io/j2kru/overview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how developers interact with code generation tools (CGTs)\nrequires detailed, real-time data on programming behavior which is often\ndifficult to collect without disrupting workflow. We present\n\\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed\nto capture fine-grained interaction events from within the Visual Studio Code\n(VS Code) editor. \\textit{CodeWatcher} logs semantically meaningful events such\nas insertions made by CGTs, deletions, copy-paste actions, and focus shifts,\nenabling continuous monitoring of developer activity without modifying user\nworkflows. The system comprises a VS Code plugin, a Python-based RESTful API,\nand a MongoDB backend, all containerized for scalability and ease of\ndeployment. By structuring and timestamping each event, \\textit{CodeWatcher}\nenables post-hoc reconstruction of coding sessions and facilitates rich\nbehavioral analyses, including how and when CGTs are used during development.\nThis infrastructure is crucial for supporting research on responsible AI,\ndeveloper productivity, and the human-centered evaluation of CGTs. Please find\nthe demo, diagrams, and tool here: https://osf.io/j2kru/overview."
                },
                "authors": [
                    {
                        "name": "Manaal Basha"
                    },
                    {
                        "name": "Aimeê M. Ribeiro"
                    },
                    {
                        "name": "Jeena Javahar"
                    },
                    {
                        "name": "Cleidson R. B. de Souza"
                    },
                    {
                        "name": "Gema Rodríguez-Pérez"
                    }
                ],
                "author_detail": {
                    "name": "Gema Rodríguez-Pérez"
                },
                "author": "Gema Rodríguez-Pérez",
                "arxiv_comment": "ICSME 2025 Tool Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13837v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13837v3",
                "updated": "2025-10-13T15:37:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    37,
                    46,
                    0,
                    286,
                    0
                ],
                "published": "2025-04-18T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    59,
                    56,
                    4,
                    108,
                    0
                ],
                "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning performance of large\nlanguage models (LLMs), particularly on mathematics and programming tasks.\nSimilar to how traditional RL helps agents explore and learn new strategies,\nRLVR is believed to enable LLMs to continuously self-improve, thus acquiring\nnovel reasoning abilities beyond those of the corresponding base models. In\nthis study we critically examine the current state of RLVR by systematically\nprobing the reasoning capability boundaries of RLVR-trained LLMs across various\nmodel families, RL algorithms, and math, coding, and visual reasoning\nbenchmarks, using pass@k at large k values as the evaluation metric.\nSurprisingly, we find that the current training setup does not elicit\nfundamentally new reasoning patterns. While RLVR-trained models outperform\ntheir base models at small k (e.g., k = 1), the base models achieve a higher\npass@k score when k is large. Coverage and perplexity analyses show that the\nobserved reasoning abilities originate from and are bounded by the base model.\nTreating the base model as an upper bound, our quantitative analysis shows that\nsix popular RLVR algorithms perform similarly and remain far from optimal in\nleveraging the potential of the base model. By contrast, we find that\ndistillation can introduce new reasoning patterns from the teacher and\ngenuinely expand the model's reasoning capabilities. Overall, our findings\nsuggest that current RLVR methods have not yet realized the potential of RL to\nelicit truly novel reasoning abilities in LLMs. This highlights the need for\nimproved RL paradigms, such as continual scaling and multi-turn\nagent-environment interaction, to unlock this potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning performance of large\nlanguage models (LLMs), particularly on mathematics and programming tasks.\nSimilar to how traditional RL helps agents explore and learn new strategies,\nRLVR is believed to enable LLMs to continuously self-improve, thus acquiring\nnovel reasoning abilities beyond those of the corresponding base models. In\nthis study we critically examine the current state of RLVR by systematically\nprobing the reasoning capability boundaries of RLVR-trained LLMs across various\nmodel families, RL algorithms, and math, coding, and visual reasoning\nbenchmarks, using pass@k at large k values as the evaluation metric.\nSurprisingly, we find that the current training setup does not elicit\nfundamentally new reasoning patterns. While RLVR-trained models outperform\ntheir base models at small k (e.g., k = 1), the base models achieve a higher\npass@k score when k is large. Coverage and perplexity analyses show that the\nobserved reasoning abilities originate from and are bounded by the base model.\nTreating the base model as an upper bound, our quantitative analysis shows that\nsix popular RLVR algorithms perform similarly and remain far from optimal in\nleveraging the potential of the base model. By contrast, we find that\ndistillation can introduce new reasoning patterns from the teacher and\ngenuinely expand the model's reasoning capabilities. Overall, our findings\nsuggest that current RLVR methods have not yet realized the potential of RL to\nelicit truly novel reasoning abilities in LLMs. This highlights the need for\nimproved RL paradigms, such as continual scaling and multi-turn\nagent-environment interaction, to unlock this potential."
                },
                "authors": [
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Zhiqi Chen"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "30 pages, 27 figures",
                "arxiv_journal_ref": "NeurIPS 2025 Oral; ICML 2025 AI4MATH workshop best paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13837v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13837v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11532v1",
                "updated": "2025-10-13T15:36:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    36,
                    49,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:36:49Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    36,
                    49,
                    0,
                    286,
                    0
                ],
                "title": "Asteroseismic investigation of HD 140283: The Methuselah star",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteroseismic investigation of HD 140283: The Methuselah star"
                },
                "summary": "HD 140283 is a well-studied metal-poor subgiant and a Gaia benchmark star,\noften used for testing stellar models due to its proximity, brightness, and low\nmetallicity ([Fe/H] = -2.3 dex).\n  Here we present the first asteroseismic analysis of HD 140283, providing\nimproved constraints on its fundamental properties.\n  The star was observed by TESS in 20-second cadence during Sector 51. We\nextracted a custom light curve and performed a frequency analysis, revealing a\nrich spectrum of solar-like oscillations including mixed modes. These were\ncombined with parameters from the literature to provide constraints on our\nmodel inference performed with BASTA.\n  Using a dense grid of models, we find a mass of $0.75 \\pm 0.01 \\\n\\mathrm{M}_\\odot$, a radius of $2.078 \\substack{+0.012\\\\-0.011} \\\n\\mathrm{R}_\\odot$, and an age of $14.2 \\pm 0.4$ Gyr, in agreement with the\nupper limit set by the age of the Universe within $1\\sigma$. The observed\nfrequency of maximum power, $\\left(\\nu_\\mathrm{max}\\right)_\\mathrm{obs} = 611.3\n\\pm 7.4 \\ \\mu\\mathrm{Hz}$, is significantly higher than predicted from standard\nscaling relations ($\\left(\\nu_\\mathrm{max}\\right)_\\mathrm{mod} = 537.2\n\\substack{+2.9\\\\-1.8} \\ \\mu\\mathrm{Hz}$), extending known deviations into the\nmetal-poor regime.\n  To our knowledge, the oscillations in HD 140283 have the highest\n$\\nu_\\mathrm{max}$ of any metal-poor star to date, which will help to advance\nour understanding of oscillations in metal-poor stars in general. The results\ndemonstrate the value of asteroseismology for precise age determination in old\nhalo stars and taking custom abundances and opacities into account during the\nmodelling is probably important for further improving models of such stars. In\naddition, a detailed characterisation of metal-poor stars, such as HD 140283,\nwill also help advance our understanding of Population III stars and their\nimpact on future stellar generations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HD 140283 is a well-studied metal-poor subgiant and a Gaia benchmark star,\noften used for testing stellar models due to its proximity, brightness, and low\nmetallicity ([Fe/H] = -2.3 dex).\n  Here we present the first asteroseismic analysis of HD 140283, providing\nimproved constraints on its fundamental properties.\n  The star was observed by TESS in 20-second cadence during Sector 51. We\nextracted a custom light curve and performed a frequency analysis, revealing a\nrich spectrum of solar-like oscillations including mixed modes. These were\ncombined with parameters from the literature to provide constraints on our\nmodel inference performed with BASTA.\n  Using a dense grid of models, we find a mass of $0.75 \\pm 0.01 \\\n\\mathrm{M}_\\odot$, a radius of $2.078 \\substack{+0.012\\\\-0.011} \\\n\\mathrm{R}_\\odot$, and an age of $14.2 \\pm 0.4$ Gyr, in agreement with the\nupper limit set by the age of the Universe within $1\\sigma$. The observed\nfrequency of maximum power, $\\left(\\nu_\\mathrm{max}\\right)_\\mathrm{obs} = 611.3\n\\pm 7.4 \\ \\mu\\mathrm{Hz}$, is significantly higher than predicted from standard\nscaling relations ($\\left(\\nu_\\mathrm{max}\\right)_\\mathrm{mod} = 537.2\n\\substack{+2.9\\\\-1.8} \\ \\mu\\mathrm{Hz}$), extending known deviations into the\nmetal-poor regime.\n  To our knowledge, the oscillations in HD 140283 have the highest\n$\\nu_\\mathrm{max}$ of any metal-poor star to date, which will help to advance\nour understanding of oscillations in metal-poor stars in general. The results\ndemonstrate the value of asteroseismology for precise age determination in old\nhalo stars and taking custom abundances and opacities into account during the\nmodelling is probably important for further improving models of such stars. In\naddition, a detailed characterisation of metal-poor stars, such as HD 140283,\nwill also help advance our understanding of Population III stars and their\nimpact on future stellar generations."
                },
                "authors": [
                    {
                        "name": "M. S. Lundkvist"
                    },
                    {
                        "name": "J. R. Larsen"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "M. L. Winther"
                    },
                    {
                        "name": "T. R. Bedding"
                    },
                    {
                        "name": "H. Kjeldsen"
                    },
                    {
                        "name": "T. R. White"
                    },
                    {
                        "name": "M. B. Nielsen"
                    },
                    {
                        "name": "G. Buldgen"
                    },
                    {
                        "name": "C. Guillaume"
                    },
                    {
                        "name": "A. L. Stokholm"
                    },
                    {
                        "name": "D. Huber"
                    },
                    {
                        "name": "J. L. Rørsted"
                    },
                    {
                        "name": "P. Mani"
                    },
                    {
                        "name": "F. Grundahl"
                    }
                ],
                "author_detail": {
                    "name": "F. Grundahl"
                },
                "author": "F. Grundahl",
                "arxiv_doi": "10.1051/0004-6361/202556292",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202556292",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11529v1",
                "updated": "2025-10-13T15:31:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    31,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:31:21Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    31,
                    21,
                    0,
                    286,
                    0
                ],
                "title": "Hallucination Detection via Internal States and Structured Reasoning\n  Consistency in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection via Internal States and Structured Reasoning\n  Consistency in Large Language Models"
                },
                "summary": "The detection of sophisticated hallucinations in Large Language Models (LLMs)\nis hampered by a ``Detection Dilemma'': methods probing internal states\n(Internal State Probing) excel at identifying factual inconsistencies but fail\non logical fallacies, while those verifying externalized reasoning\n(Chain-of-Thought Verification) show the opposite behavior. This schism creates\na task-dependent blind spot: Chain-of-Thought Verification fails on\nfact-intensive tasks like open-domain QA where reasoning is ungrounded, while\nInternal State Probing is ineffective on logic-intensive tasks like\nmathematical reasoning where models are confidently wrong. We resolve this with\na unified framework that bridges this critical gap. However, unification is\nhindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse\nsymbolic reasoning chains lack signals directly comparable to fine-grained\ninternal states, and the Representational Alignment Barrier, a deep-seated\nmismatch between their underlying semantic spaces. To overcome these, we\nintroduce a multi-path reasoning mechanism to obtain more comparable,\nfine-grained signals, and a segment-aware temporalized cross-attention module\nto adaptively fuse these now-aligned representations, pinpointing subtle\ndissonances. Extensive experiments on three diverse benchmarks and two leading\nLLMs demonstrate that our framework consistently and significantly outperforms\nstrong baselines. Our code is available: https://github.com/peach918/HalluDet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of sophisticated hallucinations in Large Language Models (LLMs)\nis hampered by a ``Detection Dilemma'': methods probing internal states\n(Internal State Probing) excel at identifying factual inconsistencies but fail\non logical fallacies, while those verifying externalized reasoning\n(Chain-of-Thought Verification) show the opposite behavior. This schism creates\na task-dependent blind spot: Chain-of-Thought Verification fails on\nfact-intensive tasks like open-domain QA where reasoning is ungrounded, while\nInternal State Probing is ineffective on logic-intensive tasks like\nmathematical reasoning where models are confidently wrong. We resolve this with\na unified framework that bridges this critical gap. However, unification is\nhindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse\nsymbolic reasoning chains lack signals directly comparable to fine-grained\ninternal states, and the Representational Alignment Barrier, a deep-seated\nmismatch between their underlying semantic spaces. To overcome these, we\nintroduce a multi-path reasoning mechanism to obtain more comparable,\nfine-grained signals, and a segment-aware temporalized cross-attention module\nto adaptively fuse these now-aligned representations, pinpointing subtle\ndissonances. Extensive experiments on three diverse benchmarks and two leading\nLLMs demonstrate that our framework consistently and significantly outperforms\nstrong baselines. Our code is available: https://github.com/peach918/HalluDet."
                },
                "authors": [
                    {
                        "name": "Yusheng Song"
                    },
                    {
                        "name": "Lirong Qiu"
                    },
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Zhihao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Tang"
                },
                "author": "Zhihao Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08338v2",
                "updated": "2025-10-13T15:22:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    22,
                    47,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T15:24:48Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    24,
                    48,
                    3,
                    282,
                    0
                ],
                "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation\n  of Likert Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation\n  of Likert Ratings"
                },
                "summary": "Consumer research costs companies billions annually yet suffers from panel\nbiases and limited scale. Large language models (LLMs) offer an alternative by\nsimulating synthetic consumers, but produce unrealistic response distributions\nwhen asked directly for numerical ratings. We present semantic similarity\nrating (SSR), a method that elicits textual responses from LLMs and maps these\nto Likert distributions using embedding similarity to reference statements.\nTesting on an extensive dataset comprising 57 personal care product surveys\nconducted by a leading corporation in that market (9,300 human responses), SSR\nachieves 90% of human test-retest reliability while maintaining realistic\nresponse distributions (KS similarity > 0.85). Additionally, these synthetic\nrespondents provide rich qualitative feedback explaining their ratings. This\nframework enables scalable consumer research simulations while preserving\ntraditional survey metrics and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consumer research costs companies billions annually yet suffers from panel\nbiases and limited scale. Large language models (LLMs) offer an alternative by\nsimulating synthetic consumers, but produce unrealistic response distributions\nwhen asked directly for numerical ratings. We present semantic similarity\nrating (SSR), a method that elicits textual responses from LLMs and maps these\nto Likert distributions using embedding similarity to reference statements.\nTesting on an extensive dataset comprising 57 personal care product surveys\nconducted by a leading corporation in that market (9,300 human responses), SSR\nachieves 90% of human test-retest reliability while maintaining realistic\nresponse distributions (KS similarity > 0.85). Additionally, these synthetic\nrespondents provide rich qualitative feedback explaining their ratings. This\nframework enables scalable consumer research simulations while preserving\ntraditional survey metrics and interpretability."
                },
                "authors": [
                    {
                        "name": "Benjamin F. Maier"
                    },
                    {
                        "name": "Ulf Aslak"
                    },
                    {
                        "name": "Luca Fiaschi"
                    },
                    {
                        "name": "Nina Rismal"
                    },
                    {
                        "name": "Kemble Fletcher"
                    },
                    {
                        "name": "Christian C. Luhmann"
                    },
                    {
                        "name": "Robbie Dow"
                    },
                    {
                        "name": "Kli Pappas"
                    },
                    {
                        "name": "Thomas V. Wiecki"
                    }
                ],
                "author_detail": {
                    "name": "Thomas V. Wiecki"
                },
                "author": "Thomas V. Wiecki",
                "arxiv_comment": "28 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11517v1",
                "updated": "2025-10-13T15:22:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    22,
                    46,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:22:46Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    22,
                    46,
                    0,
                    286,
                    0
                ],
                "title": "A Kolmogorov-Smirnov-Type Test for Dependently Double-Truncated Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Kolmogorov-Smirnov-Type Test for Dependently Double-Truncated Data"
                },
                "summary": "With double-truncated lifespans, we test the hypothesis of a parametric\ndistribution family for the lifespan. The typical finding from demography is an\ninstationary behaviour of the life expectancy, and a copula models the\nresulting weak dependence of lifespan and the age at truncation. Our main\nexample is the Farlie-Gumbel-Morgenststern copula. The test is based on\nDonsker-class arguments and the functional delta method for empirical\nprocesses. The assumptions also allow parametric inference, and proofs slightly\nsimplify due to the compact support of the observations. An algorithm with\nfinitely many operations is given for the computation of the test statistic.\nSimulations becomes necessary for computing the critical value. With the\nexponential distribution as an example, and for the application to 55{,}000\nGerman double-truncated enterprise lifespans, the constructed\nKolmogorov-Smirnov test rejects clearly an age-homogeneous closure hazard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With double-truncated lifespans, we test the hypothesis of a parametric\ndistribution family for the lifespan. The typical finding from demography is an\ninstationary behaviour of the life expectancy, and a copula models the\nresulting weak dependence of lifespan and the age at truncation. Our main\nexample is the Farlie-Gumbel-Morgenststern copula. The test is based on\nDonsker-class arguments and the functional delta method for empirical\nprocesses. The assumptions also allow parametric inference, and proofs slightly\nsimplify due to the compact support of the observations. An algorithm with\nfinitely many operations is given for the computation of the test statistic.\nSimulations becomes necessary for computing the critical value. With the\nexponential distribution as an example, and for the application to 55{,}000\nGerman double-truncated enterprise lifespans, the constructed\nKolmogorov-Smirnov test rejects clearly an age-homogeneous closure hazard."
                },
                "authors": [
                    {
                        "name": "Anne-Marie Toparkus"
                    },
                    {
                        "name": "Rafael Weissbach"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Weissbach"
                },
                "author": "Rafael Weissbach",
                "arxiv_comment": "44 pages, 1 figure, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62D05, 62D10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11516v1",
                "updated": "2025-10-13T15:22:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    22,
                    12,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:22:12Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    22,
                    12,
                    0,
                    286,
                    0
                ],
                "title": "Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns\n  During Programming Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns\n  During Programming Tasks"
                },
                "summary": "The use of AI code-generation tools is becoming increasingly common, making\nit important to understand how software developers are adopting these tools. In\nthis study, we investigate how developers engage with Amazon's CodeWhisperer,\nan LLM-based code-generation tool. We conducted two user studies with two\ngroups of 10 participants each, interacting with CodeWhisperer - the first to\nunderstand which interactions were critical to capture and the second to\ncollect low-level interaction data using a custom telemetry plugin. Our\nmixed-methods analysis identified four behavioral patterns: 1) incremental code\nrefinement, 2) explicit instruction using natural language comments, 3)\nbaseline structuring with model suggestions, and 4) integrative use with\nexternal sources. We provide a comprehensive analysis of these patterns .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of AI code-generation tools is becoming increasingly common, making\nit important to understand how software developers are adopting these tools. In\nthis study, we investigate how developers engage with Amazon's CodeWhisperer,\nan LLM-based code-generation tool. We conducted two user studies with two\ngroups of 10 participants each, interacting with CodeWhisperer - the first to\nunderstand which interactions were critical to capture and the second to\ncollect low-level interaction data using a custom telemetry plugin. Our\nmixed-methods analysis identified four behavioral patterns: 1) incremental code\nrefinement, 2) explicit instruction using natural language comments, 3)\nbaseline structuring with model suggestions, and 4) integrative use with\nexternal sources. We provide a comprehensive analysis of these patterns ."
                },
                "authors": [
                    {
                        "name": "Jeena Javahar"
                    },
                    {
                        "name": "Tanya Budhrani"
                    },
                    {
                        "name": "Manaal Basha"
                    },
                    {
                        "name": "Cleidson R. B. de Souza"
                    },
                    {
                        "name": "Ivan Beschastnikh"
                    },
                    {
                        "name": "Gema Rodriguez-Perez"
                    }
                ],
                "author_detail": {
                    "name": "Gema Rodriguez-Perez"
                },
                "author": "Gema Rodriguez-Perez",
                "arxiv_comment": "VL/HCC 2025 Short Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11512v1",
                "updated": "2025-10-13T15:19:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    19,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:19:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    19,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion\n  Models via Likelihood Preference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion\n  Models via Likelihood Preference"
                },
                "summary": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intuitive physics understanding in video diffusion models plays an essential\nrole in building general-purpose physically plausible world simulators, yet\naccurately evaluating such capacity remains a challenging task due to the\ndifficulty in disentangling physics correctness from visual appearance in\ngeneration. To the end, we introduce LikePhys, a training-free method that\nevaluates intuitive physics in video diffusion models by distinguishing\nphysically valid and impossible videos using the denoising objective as an\nELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By\ntesting on our constructed benchmark of twelve scenarios spanning over four\nphysics domains, we show that our evaluation metric, Plausibility Preference\nError (PPE), demonstrates strong alignment with human preference, outperforming\nstate-of-the-art evaluator baselines. We then systematically benchmark\nintuitive physics understanding in current video diffusion models. Our study\nfurther analyses how model design and inference settings affect intuitive\nphysics understanding and highlights domain-specific capacity variations across\nphysical laws. Empirical results show that, despite current models struggling\nwith complex and chaotic dynamics, there is a clear trend of improvement in\nphysics understanding as model capacity and inference settings scale."
                },
                "authors": [
                    {
                        "name": "Jianhao Yuan"
                    },
                    {
                        "name": "Fabio Pizzati"
                    },
                    {
                        "name": "Francesco Pinto"
                    },
                    {
                        "name": "Lars Kunze"
                    },
                    {
                        "name": "Ivan Laptev"
                    },
                    {
                        "name": "Paul Newman"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Daniele De Martini"
                    }
                ],
                "author_detail": {
                    "name": "Daniele De Martini"
                },
                "author": "Daniele De Martini",
                "arxiv_comment": "22 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11509v1",
                "updated": "2025-10-13T15:17:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    17,
                    18,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:17:18Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    17,
                    18,
                    0,
                    286,
                    0
                ],
                "title": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal\n  Large Language Model"
                },
                "summary": "Physical environments and circumstances are fundamentally dynamic, yet\ncurrent 3D datasets and evaluation benchmarks tend to concentrate on either\ndynamic scenarios or dynamic situations in isolation, resulting in incomplete\ncomprehension. To overcome these constraints, we introduce Situat3DChange, an\nextensive dataset supporting three situation-aware change understanding tasks\nfollowing the perception-action model: 121K question-answer pairs, 36K change\ndescriptions for perception tasks, and 17K rearrangement instructions for the\naction task. To construct this large-scale dataset, Situat3DChange leverages\n11K human observations of environmental changes to establish shared mental\nmodels and shared situational awareness for human-AI collaboration. These\nobservations, enriched with egocentric and allocentric perspectives as well as\ncategorical and coordinate spatial relations, are integrated using an LLM to\nsupport understanding of situated changes. To address the challenge of\ncomparing pairs of point clouds from the same scene with minor changes, we\npropose SCReasoner, an efficient 3D MLLM approach that enables effective point\ncloud comparison with minimal parameter overhead and no additional tokens\nrequired for the language decoder. Comprehensive evaluation on Situat3DChange\ntasks highlights both the progress and limitations of MLLMs in dynamic scene\nand situation understanding. Additional experiments on data scaling and\ncross-domain transfer demonstrate the task-agnostic effectiveness of using\nSituat3DChange as a training dataset for MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical environments and circumstances are fundamentally dynamic, yet\ncurrent 3D datasets and evaluation benchmarks tend to concentrate on either\ndynamic scenarios or dynamic situations in isolation, resulting in incomplete\ncomprehension. To overcome these constraints, we introduce Situat3DChange, an\nextensive dataset supporting three situation-aware change understanding tasks\nfollowing the perception-action model: 121K question-answer pairs, 36K change\ndescriptions for perception tasks, and 17K rearrangement instructions for the\naction task. To construct this large-scale dataset, Situat3DChange leverages\n11K human observations of environmental changes to establish shared mental\nmodels and shared situational awareness for human-AI collaboration. These\nobservations, enriched with egocentric and allocentric perspectives as well as\ncategorical and coordinate spatial relations, are integrated using an LLM to\nsupport understanding of situated changes. To address the challenge of\ncomparing pairs of point clouds from the same scene with minor changes, we\npropose SCReasoner, an efficient 3D MLLM approach that enables effective point\ncloud comparison with minimal parameter overhead and no additional tokens\nrequired for the language decoder. Comprehensive evaluation on Situat3DChange\ntasks highlights both the progress and limitations of MLLMs in dynamic scene\nand situation understanding. Additional experiments on data scaling and\ncross-domain transfer demonstrate the task-agnostic effectiveness of using\nSituat3DChange as a training dataset for MLLMs."
                },
                "authors": [
                    {
                        "name": "Ruiping Liu"
                    },
                    {
                        "name": "Junwei Zheng"
                    },
                    {
                        "name": "Yufan Chen"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Kunyu Peng"
                    },
                    {
                        "name": "Kailun Yang"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Rainer Stiefelhagen"
                    }
                ],
                "author_detail": {
                    "name": "Rainer Stiefelhagen"
                },
                "author": "Rainer Stiefelhagen",
                "arxiv_comment": "Accepted to NeurIPS 2025 Datasets and Benchmarks Track. Dataset and\n  Code: https://github.com/RuipingL/Situat3DChange",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11502v1",
                "updated": "2025-10-13T15:10:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    10,
                    38,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:10:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    10,
                    38,
                    0,
                    286,
                    0
                ],
                "title": "Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key\n  Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key\n  Errors"
                },
                "summary": "Research on reasoning in language models (LMs) predominantly focuses on\nimproving the correctness of their outputs. But some important applications\nrequire modeling reasoning patterns that are incorrect. For example, automated\nsystems that can reason about and simulate student errors are useful for\nproviding real-time feedback in the classroom or offline practice for\neducators-in-training. This paper presents a new method, MISTAKE, that (1)\nconstructs high-quality synthetic examples of reasoning errors by leveraging\ncycle consistency between incorrect answers and latent misconceptions; and (2)\nuses the generated data to learn models for student simulation, misconception\nclassification, and answer generation. We evaluate MISTAKE on three educational\ntasks and find that it results in (1) higher accuracy when simulating incorrect\nstudent answers based on specific misconceptions, (2) increased performance\ninferring latent misconceptions from observed incorrect answers, and (3) higher\nalignment with expert-written distractor answers when generating incorrect\nanswers (e.g., for multiple-choice tests).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on reasoning in language models (LMs) predominantly focuses on\nimproving the correctness of their outputs. But some important applications\nrequire modeling reasoning patterns that are incorrect. For example, automated\nsystems that can reason about and simulate student errors are useful for\nproviding real-time feedback in the classroom or offline practice for\neducators-in-training. This paper presents a new method, MISTAKE, that (1)\nconstructs high-quality synthetic examples of reasoning errors by leveraging\ncycle consistency between incorrect answers and latent misconceptions; and (2)\nuses the generated data to learn models for student simulation, misconception\nclassification, and answer generation. We evaluate MISTAKE on three educational\ntasks and find that it results in (1) higher accuracy when simulating incorrect\nstudent answers based on specific misconceptions, (2) increased performance\ninferring latent misconceptions from observed incorrect answers, and (3) higher\nalignment with expert-written distractor answers when generating incorrect\nanswers (e.g., for multiple-choice tests)."
                },
                "authors": [
                    {
                        "name": "Alexis Ross"
                    },
                    {
                        "name": "Jacob Andreas"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Andreas"
                },
                "author": "Jacob Andreas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11498v1",
                "updated": "2025-10-13T15:05:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    5,
                    50,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:05:50Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    5,
                    50,
                    0,
                    286,
                    0
                ],
                "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding"
                },
                "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling."
                },
                "authors": [
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Ruilin Lv"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Wiggin Zhou"
                    },
                    {
                        "name": "Bo Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhou"
                },
                "author": "Bo Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11496v2",
                "updated": "2025-10-14T05:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    5,
                    5,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model"
                },
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer."
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu",
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04641v2",
                "updated": "2025-10-13T15:04:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-06T09:45:32Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    45,
                    32,
                    0,
                    279,
                    0
                ],
                "title": "Evaluating LLMs for Demographic-Targeted Social Bias Detection: A\n  Comprehensive Benchmark Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Demographic-Targeted Social Bias Detection: A\n  Comprehensive Benchmark Study"
                },
                "summary": "Large-scale web-scraped text corpora used to train general-purpose AI models\noften contain harmful demographic-targeted social biases, creating a regulatory\nneed for data auditing and developing scalable bias-detection methods. Although\nprior work has investigated biases in text datasets and related detection\nmethods, these studies remain narrow in scope. They typically focus on a single\ncontent type (e.g., hate speech), cover limited demographic axes, overlook\nbiases affecting multiple demographics simultaneously, and analyze limited\ntechniques. Consequently, practitioners lack a holistic understanding of the\nstrengths and limitations of recent large language models (LLMs) for automated\nbias detection. In this study, we present a comprehensive evaluation framework\naimed at English texts to assess the ability of LLMs in detecting\ndemographic-targeted social biases. To align with regulatory requirements, we\nframe bias detection as a multi-label task using a demographic-focused\ntaxonomy. We then conduct a systematic evaluation with models across scales and\ntechniques, including prompting, in-context learning, and fine-tuning. Using\ntwelve datasets spanning diverse content types and demographics, our study\ndemonstrates the promise of fine-tuned smaller models for scalable detection.\nHowever, our analyses also expose persistent gaps across demographic axes and\nmulti-demographic targeted biases, underscoring the need for more effective and\nscalable auditing frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale web-scraped text corpora used to train general-purpose AI models\noften contain harmful demographic-targeted social biases, creating a regulatory\nneed for data auditing and developing scalable bias-detection methods. Although\nprior work has investigated biases in text datasets and related detection\nmethods, these studies remain narrow in scope. They typically focus on a single\ncontent type (e.g., hate speech), cover limited demographic axes, overlook\nbiases affecting multiple demographics simultaneously, and analyze limited\ntechniques. Consequently, practitioners lack a holistic understanding of the\nstrengths and limitations of recent large language models (LLMs) for automated\nbias detection. In this study, we present a comprehensive evaluation framework\naimed at English texts to assess the ability of LLMs in detecting\ndemographic-targeted social biases. To align with regulatory requirements, we\nframe bias detection as a multi-label task using a demographic-focused\ntaxonomy. We then conduct a systematic evaluation with models across scales and\ntechniques, including prompting, in-context learning, and fine-tuning. Using\ntwelve datasets spanning diverse content types and demographics, our study\ndemonstrates the promise of fine-tuned smaller models for scalable detection.\nHowever, our analyses also expose persistent gaps across demographic axes and\nmulti-demographic targeted biases, underscoring the need for more effective and\nscalable auditing frameworks."
                },
                "authors": [
                    {
                        "name": "Ayan Majumdar"
                    },
                    {
                        "name": "Feihao Chen"
                    },
                    {
                        "name": "Jinghui Li"
                    },
                    {
                        "name": "Xiaozhen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhen Wang"
                },
                "author": "Xiaozhen Wang",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05970v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05970v3",
                "updated": "2025-10-13T15:01:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    1,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-07-08T13:24:05Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    24,
                    5,
                    1,
                    189,
                    0
                ],
                "title": "Automatic Synthesis of High-Quality Triplet Data for Composed Image\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Synthesis of High-Quality Triplet Data for Composed Image\n  Retrieval"
                },
                "summary": "As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)\naims to retrieve target images using multimodal (image+text) queries. Although\nmany existing CIR methods have attained promising performance, their reliance\non costly, manually labeled triplets hinders scalability and zero-shot\ncapability. To address this issue, we propose a scalable pipeline for automatic\ntriplet generation, along with a fully synthetic dataset named Composed Image\nRetrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a\nlarge language model (LLM) to generate diverse prompts, controlling a\ntext-to-image generative model to produce image pairs with identical elements\nin each pair, which are then filtered and reorganized to form the CIRHS\ndataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a\nnovel CIR framework, which can accomplish global alignment and local reasoning\nwithin a broader context, enabling the model to learn more robust and\ninformative representations. By utilizing the synthetic CIRHS dataset, CoAlign\nachieves outstanding zero-shot performance on three commonly used benchmarks,\ndemonstrating for the first time the feasibility of training CIR models on a\nfully synthetic dataset. Furthermore, under supervised training, our method\noutperforms all the state-of-the-art supervised CIR approaches, validating the\neffectiveness of our proposed retrieval framework. The code and the CIRHS\ndataset will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)\naims to retrieve target images using multimodal (image+text) queries. Although\nmany existing CIR methods have attained promising performance, their reliance\non costly, manually labeled triplets hinders scalability and zero-shot\ncapability. To address this issue, we propose a scalable pipeline for automatic\ntriplet generation, along with a fully synthetic dataset named Composed Image\nRetrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a\nlarge language model (LLM) to generate diverse prompts, controlling a\ntext-to-image generative model to produce image pairs with identical elements\nin each pair, which are then filtered and reorganized to form the CIRHS\ndataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a\nnovel CIR framework, which can accomplish global alignment and local reasoning\nwithin a broader context, enabling the model to learn more robust and\ninformative representations. By utilizing the synthetic CIRHS dataset, CoAlign\nachieves outstanding zero-shot performance on three commonly used benchmarks,\ndemonstrating for the first time the feasibility of training CIR models on a\nfully synthetic dataset. Furthermore, under supervised training, our method\noutperforms all the state-of-the-art supervised CIR approaches, validating the\neffectiveness of our proposed retrieval framework. The code and the CIRHS\ndataset will be released soon."
                },
                "authors": [
                    {
                        "name": "Haiwen Li"
                    },
                    {
                        "name": "Delong Liu"
                    },
                    {
                        "name": "Zhaohui Hou"
                    },
                    {
                        "name": "Zhicheng Zhao"
                    },
                    {
                        "name": "Fei Su"
                    }
                ],
                "author_detail": {
                    "name": "Fei Su"
                },
                "author": "Fei Su",
                "arxiv_comment": "This paper was originally submitted to ACM MM 2025 on April 12, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05970v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05970v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02502v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02502v3",
                "updated": "2025-10-13T14:55:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    55,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-04T11:10:13Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    10,
                    13,
                    1,
                    63,
                    0
                ],
                "title": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs"
                },
                "summary": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training."
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Yangyifan Xu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "ACL 2025, our code is available at https://github.com/ZNLP/LADM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02502v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02502v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11484v1",
                "updated": "2025-10-13T14:55:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    55,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:55:34Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    55,
                    34,
                    0,
                    286,
                    0
                ],
                "title": "Rescaling-Aware Training for Efficient Deployment of Deep Learning\n  Models on Full-Integer Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rescaling-Aware Training for Efficient Deployment of Deep Learning\n  Models on Full-Integer Hardware"
                },
                "summary": "Integer AI inference significantly reduces computational complexity in\nembedded systems. Quantization-aware training (QAT) helps mitigate accuracy\ndegradation associated with post-training quantization but still overlooks the\nimpact of integer rescaling during inference, which is a hardware costly\noperation in integer-only AI inference. This work shows that rescaling cost can\nbe dramatically reduced post-training, by applying a stronger quantization to\nthe rescale multiplicands at no model-quality loss. Furthermore, we introduce\nRescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling\nmultiplicands. Experiments show that even with 8x reduced rescaler widths, the\nfull accuracy is preserved through minimal incremental retraining. This enables\nmore energy-efficient and cost-efficient AI inference for resource-constrained\nembedded systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integer AI inference significantly reduces computational complexity in\nembedded systems. Quantization-aware training (QAT) helps mitigate accuracy\ndegradation associated with post-training quantization but still overlooks the\nimpact of integer rescaling during inference, which is a hardware costly\noperation in integer-only AI inference. This work shows that rescaling cost can\nbe dramatically reduced post-training, by applying a stronger quantization to\nthe rescale multiplicands at no model-quality loss. Furthermore, we introduce\nRescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling\nmultiplicands. Experiments show that even with 8x reduced rescaler widths, the\nfull accuracy is preserved through minimal incremental retraining. This enables\nmore energy-efficient and cost-efficient AI inference for resource-constrained\nembedded systems."
                },
                "authors": [
                    {
                        "name": "Lion Mueller"
                    },
                    {
                        "name": "Alberto Garcia-Ortiz"
                    },
                    {
                        "name": "Ardalan Najafi"
                    },
                    {
                        "name": "Adam Fuks"
                    },
                    {
                        "name": "Lennart Bamberg"
                    }
                ],
                "author_detail": {
                    "name": "Lennart Bamberg"
                },
                "author": "Lennart Bamberg",
                "arxiv_comment": "Submitted to IEEE Embedded Systems Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11482v1",
                "updated": "2025-10-13T14:53:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    53,
                    44,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:53:44Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    53,
                    44,
                    0,
                    286,
                    0
                ],
                "title": "Investigating Large Language Models' Linguistic Abilities for Text\n  Preprocessing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Large Language Models' Linguistic Abilities for Text\n  Preprocessing"
                },
                "summary": "Text preprocessing is a fundamental component of Natural Language Processing,\ninvolving techniques such as stopword removal, stemming, and lemmatization to\nprepare text as input for further processing and analysis. Despite the\ncontext-dependent nature of the above techniques, traditional methods usually\nignore contextual information. In this paper, we investigate the idea of using\nLarge Language Models (LLMs) to perform various preprocessing tasks, due to\ntheir ability to take context into account without requiring extensive\nlanguage-specific annotated resources. Through a comprehensive evaluation on\nweb-sourced data, we compare LLM-based preprocessing (specifically stopword\nremoval, lemmatization and stemming) to traditional algorithms across multiple\ntext classification tasks in six European languages. Our analysis indicates\nthat LLMs are capable of replicating traditional stopword removal,\nlemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,\nrespectively. Additionally, we show that ML algorithms trained on texts\npreprocessed by LLMs achieve an improvement of up to 6% with respect to the\n$F_1$ measure compared to traditional techniques. Our code, prompts, and\nresults are publicly available at\nhttps://github.com/GianCarloMilanese/llm_pipeline_wi-iat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text preprocessing is a fundamental component of Natural Language Processing,\ninvolving techniques such as stopword removal, stemming, and lemmatization to\nprepare text as input for further processing and analysis. Despite the\ncontext-dependent nature of the above techniques, traditional methods usually\nignore contextual information. In this paper, we investigate the idea of using\nLarge Language Models (LLMs) to perform various preprocessing tasks, due to\ntheir ability to take context into account without requiring extensive\nlanguage-specific annotated resources. Through a comprehensive evaluation on\nweb-sourced data, we compare LLM-based preprocessing (specifically stopword\nremoval, lemmatization and stemming) to traditional algorithms across multiple\ntext classification tasks in six European languages. Our analysis indicates\nthat LLMs are capable of replicating traditional stopword removal,\nlemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,\nrespectively. Additionally, we show that ML algorithms trained on texts\npreprocessed by LLMs achieve an improvement of up to 6% with respect to the\n$F_1$ measure compared to traditional techniques. Our code, prompts, and\nresults are publicly available at\nhttps://github.com/GianCarloMilanese/llm_pipeline_wi-iat."
                },
                "authors": [
                    {
                        "name": "Marco Braga"
                    },
                    {
                        "name": "Gian Carlo Milanese"
                    },
                    {
                        "name": "Gabriella Pasi"
                    }
                ],
                "author_detail": {
                    "name": "Gabriella Pasi"
                },
                "author": "Gabriella Pasi",
                "arxiv_comment": "Accepted in WI-IAT 2025. Pre-camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11479v1",
                "updated": "2025-10-13T14:48:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    48,
                    17,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:48:17Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    48,
                    17,
                    0,
                    286,
                    0
                ],
                "title": "A Metal-Rich Atmosphere with a Super-Solar C/O Ratio for the Extreme\n  Ultra-Hot Jupiter WASP-178b",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Metal-Rich Atmosphere with a Super-Solar C/O Ratio for the Extreme\n  Ultra-Hot Jupiter WASP-178b"
                },
                "summary": "The population of ultra-hot Jupiters (UHJs) provide unique opportunities to\nprobe the extreme formation and evolutionary pathways in exoplanets. Owing to\ntheir very high temperatures and inflated atmospheres, UHJs are among the most\nfavorable targets for both transmission and emission spectroscopy, enabling\ndetailed characterization of their atmospheric properties. Here, we present a\nreanalysis of the JWST NIRSpec/G395H transmission spectra of the extreme\nultra-hot Jupiter (EUHJ) WASP-178b, aimed at precisely characterizing its\natmospheric composition. Our approach combines data reduction using two\nindependent pipelines, lightcurve modeling with robust detrending techniques,\nand rigorous atmospheric retrievals. We report statistically significant\ndetections of CO (7.24 $\\sigma$) and CO$_2$ (7.22 $\\sigma$), along with\nmarginal evidence for C$_2$H$_2$ (1.34 $\\sigma$), but no clear evidence for\nH$_2$O, suggesting depletion. From these retrieved abundances, we constrain the\nC/O ratio to a precise super-solar value of 0.954$\\pm$0.033, consistent with an\nemerging trend in other UHJs. We also infer a very high atmospheric metallicity\nfor a Jupiter-sized gas giant$\\unicode{x2014}$11.44$_{-6.94}^{+12.54}$\n$\\times$solar$\\unicode{x2014}$indicating unique atmospheric evolutions. These\nfindings provide a critical benchmark for an extreme exoplanet atmosphere,\noffering a testbed for developing next-generation atmospheric evolution models\nand enabling comparative population-level studies across the UHJ population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The population of ultra-hot Jupiters (UHJs) provide unique opportunities to\nprobe the extreme formation and evolutionary pathways in exoplanets. Owing to\ntheir very high temperatures and inflated atmospheres, UHJs are among the most\nfavorable targets for both transmission and emission spectroscopy, enabling\ndetailed characterization of their atmospheric properties. Here, we present a\nreanalysis of the JWST NIRSpec/G395H transmission spectra of the extreme\nultra-hot Jupiter (EUHJ) WASP-178b, aimed at precisely characterizing its\natmospheric composition. Our approach combines data reduction using two\nindependent pipelines, lightcurve modeling with robust detrending techniques,\nand rigorous atmospheric retrievals. We report statistically significant\ndetections of CO (7.24 $\\sigma$) and CO$_2$ (7.22 $\\sigma$), along with\nmarginal evidence for C$_2$H$_2$ (1.34 $\\sigma$), but no clear evidence for\nH$_2$O, suggesting depletion. From these retrieved abundances, we constrain the\nC/O ratio to a precise super-solar value of 0.954$\\pm$0.033, consistent with an\nemerging trend in other UHJs. We also infer a very high atmospheric metallicity\nfor a Jupiter-sized gas giant$\\unicode{x2014}$11.44$_{-6.94}^{+12.54}$\n$\\times$solar$\\unicode{x2014}$indicating unique atmospheric evolutions. These\nfindings provide a critical benchmark for an extreme exoplanet atmosphere,\noffering a testbed for developing next-generation atmospheric evolution models\nand enabling comparative population-level studies across the UHJ population."
                },
                "authors": [
                    {
                        "name": "Suman Saha"
                    },
                    {
                        "name": "James S. Jenkins"
                    }
                ],
                "author_detail": {
                    "name": "James S. Jenkins"
                },
                "author": "James S. Jenkins",
                "arxiv_comment": "19 pages, including 13 figures and 3 tables. Under review\n  $\\unicode{x2014}$ comments are warmly welcomed!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11471v1",
                "updated": "2025-10-13T14:40:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    40,
                    47,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:40:47Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    40,
                    47,
                    0,
                    286,
                    0
                ],
                "title": "Iterative Amortized Inference: Unifying In-Context Learning and Learned\n  Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Amortized Inference: Unifying In-Context Learning and Learned\n  Optimizers"
                },
                "summary": "Modern learning systems increasingly rely on amortized learning - the idea of\nreusing computation or inductive biases shared across tasks to enable rapid\ngeneralization to novel problems. This principle spans a range of approaches,\nincluding meta-learning, in-context learning, prompt tuning, learned optimizers\nand more. While motivated by similar goals, these approaches differ in how they\nencode and leverage task-specific information, often provided as in-context\nexamples. In this work, we propose a unified framework which describes how such\nmethods differ primarily in the aspects of learning they amortize - such as\ninitializations, learned updates, or predictive mappings - and how they\nincorporate task data at inference. We introduce a taxonomy that categorizes\namortized models into parametric, implicit, and explicit regimes, based on\nwhether task adaptation is externalized, internalized, or jointly modeled.\nBuilding on this view, we identify a key limitation in current approaches: most\nmethods struggle to scale to large datasets because their capacity to process\ntask data at inference (e.g., context length) is often limited. To address\nthis, we propose iterative amortized inference, a class of models that refine\nsolutions step-by-step over mini-batches, drawing inspiration from stochastic\noptimization. Our formulation bridges optimization-based meta-learning with\nforward-pass amortization in models like LLMs, offering a scalable and\nextensible foundation for general-purpose task adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern learning systems increasingly rely on amortized learning - the idea of\nreusing computation or inductive biases shared across tasks to enable rapid\ngeneralization to novel problems. This principle spans a range of approaches,\nincluding meta-learning, in-context learning, prompt tuning, learned optimizers\nand more. While motivated by similar goals, these approaches differ in how they\nencode and leverage task-specific information, often provided as in-context\nexamples. In this work, we propose a unified framework which describes how such\nmethods differ primarily in the aspects of learning they amortize - such as\ninitializations, learned updates, or predictive mappings - and how they\nincorporate task data at inference. We introduce a taxonomy that categorizes\namortized models into parametric, implicit, and explicit regimes, based on\nwhether task adaptation is externalized, internalized, or jointly modeled.\nBuilding on this view, we identify a key limitation in current approaches: most\nmethods struggle to scale to large datasets because their capacity to process\ntask data at inference (e.g., context length) is often limited. To address\nthis, we propose iterative amortized inference, a class of models that refine\nsolutions step-by-step over mini-batches, drawing inspiration from stochastic\noptimization. Our formulation bridges optimization-based meta-learning with\nforward-pass amortization in models like LLMs, offering a scalable and\nextensible foundation for general-purpose task adaptation."
                },
                "authors": [
                    {
                        "name": "Sarthak Mittal"
                    },
                    {
                        "name": "Divyat Mahajan"
                    },
                    {
                        "name": "Guillaume Lajoie"
                    },
                    {
                        "name": "Mohammad Pezeshki"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Pezeshki"
                },
                "author": "Mohammad Pezeshki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11457v1",
                "updated": "2025-10-13T14:29:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    29,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:29:15Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    29,
                    15,
                    0,
                    286,
                    0
                ],
                "title": "From <Answer> to <Think>: Multidimensional Supervision of Reasoning\n  Process for LLM Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From <Answer> to <Think>: Multidimensional Supervision of Reasoning\n  Process for LLM Optimization"
                },
                "summary": "Improving the multi-step reasoning ability of Large Language Models (LLMs) is\na critical yet challenging task. The dominant paradigm, outcome-supervised\nreinforcement learning (RLVR), rewards only correct final answers, often\npropagating flawed reasoning and suffering from sparse reward signals. While\nprocess-level reward models (PRMs) provide denser, step-by-step feedback, they\nlack generalizability and interpretability, requiring task-specific\nsegmentation of the reasoning process. To this end, we propose the\nDimension-level Reward Model (DRM), a new supervision framework that bridges\nthe gap between these two approaches. DRM evaluates the quality of a reasoning\nprocess along three fundamental, complementary, and interpretable dimensions:\nConfidence for uncertainty calibration, Relevance for semantic alignment, and\nCoherence for logical consistency. Together, these dimensions capture aspects\nbeyond final answer correctness and enable interpretable assessment without\nrequiring ground truth answers. Experimental results show that DRM provides\neffective supervision signals, guides the optimization of LLMs and enhances\ntheir reasoning ability. In particular, DRM-supervised training achieves\nconsistent gains on both in-distribution and out-of-distribution open-domain\ntasks, including mathematics, question answering, code execution, and puzzles.\nOur findings demonstrate that multidimensional supervision of the reasoning\nprocess can improve the generalized reasoning ability of LLMs beyond the\ntraining distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the multi-step reasoning ability of Large Language Models (LLMs) is\na critical yet challenging task. The dominant paradigm, outcome-supervised\nreinforcement learning (RLVR), rewards only correct final answers, often\npropagating flawed reasoning and suffering from sparse reward signals. While\nprocess-level reward models (PRMs) provide denser, step-by-step feedback, they\nlack generalizability and interpretability, requiring task-specific\nsegmentation of the reasoning process. To this end, we propose the\nDimension-level Reward Model (DRM), a new supervision framework that bridges\nthe gap between these two approaches. DRM evaluates the quality of a reasoning\nprocess along three fundamental, complementary, and interpretable dimensions:\nConfidence for uncertainty calibration, Relevance for semantic alignment, and\nCoherence for logical consistency. Together, these dimensions capture aspects\nbeyond final answer correctness and enable interpretable assessment without\nrequiring ground truth answers. Experimental results show that DRM provides\neffective supervision signals, guides the optimization of LLMs and enhances\ntheir reasoning ability. In particular, DRM-supervised training achieves\nconsistent gains on both in-distribution and out-of-distribution open-domain\ntasks, including mathematics, question answering, code execution, and puzzles.\nOur findings demonstrate that multidimensional supervision of the reasoning\nprocess can improve the generalized reasoning ability of LLMs beyond the\ntraining distribution."
                },
                "authors": [
                    {
                        "name": "Beining Wang"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Hongtao Tian"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11454v1",
                "updated": "2025-10-13T14:25:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    25,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:25:34Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    25,
                    34,
                    0,
                    286,
                    0
                ],
                "title": "Audio-Maestro: Enhancing Large Audio-Language Models with Tool-Augmented\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-Maestro: Enhancing Large Audio-Language Models with Tool-Augmented\n  Reasoning"
                },
                "summary": "Recent advancements in large multimodal models (LMMs) have shown strong\ncapabilities in audio understanding. However, most systems rely solely on\nend-to-end reasoning, limiting interpretability and accuracy for tasks that\nrequire structured knowledge or specialized signal analysis. In this work, we\npresent Audio-Maestro -- a tool-augmented audio reasoning framework that\nenables audio-language models to autonomously call external tools and integrate\ntheir timestamped outputs into the reasoning process. This design allows the\nmodel to analyze, transform, and interpret audio signals through specialized\ntools rather than relying solely on end-to-end inference. Experiments show that\nAudio-Maestro consistently improves general audio reasoning performance:\nGemini-2.5-flash's average accuracy on MMAU-Test rises from 67.4% to 72.1%,\nDeSTA-2.5 from 58.3% to 62.8%, and GPT-4o from 60.8% to 63.9%. To our\nknowledge, Audio-Maestro is the first framework to integrate structured tool\noutput into the large audio language model reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large multimodal models (LMMs) have shown strong\ncapabilities in audio understanding. However, most systems rely solely on\nend-to-end reasoning, limiting interpretability and accuracy for tasks that\nrequire structured knowledge or specialized signal analysis. In this work, we\npresent Audio-Maestro -- a tool-augmented audio reasoning framework that\nenables audio-language models to autonomously call external tools and integrate\ntheir timestamped outputs into the reasoning process. This design allows the\nmodel to analyze, transform, and interpret audio signals through specialized\ntools rather than relying solely on end-to-end inference. Experiments show that\nAudio-Maestro consistently improves general audio reasoning performance:\nGemini-2.5-flash's average accuracy on MMAU-Test rises from 67.4% to 72.1%,\nDeSTA-2.5 from 58.3% to 62.8%, and GPT-4o from 60.8% to 63.9%. To our\nknowledge, Audio-Maestro is the first framework to integrate structured tool\noutput into the large audio language model reasoning process."
                },
                "authors": [
                    {
                        "name": "Kuan-Yi Lee"
                    },
                    {
                        "name": "Tsung-En Lin"
                    },
                    {
                        "name": "Hung-Yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-Yi Lee"
                },
                "author": "Hung-Yi Lee",
                "arxiv_comment": "9pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05605v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05605v4",
                "updated": "2025-10-13T14:24:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    24,
                    10,
                    0,
                    286,
                    0
                ],
                "published": "2025-02-08T15:21:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    15,
                    21,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "Evolving LLMs' Self-Refinement Capability via Synergistic\n  Training-Inference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving LLMs' Self-Refinement Capability via Synergistic\n  Training-Inference Optimization"
                },
                "summary": "Self-Refinement refers to a model's ability to revise its own responses to\nproduce improved outputs. This capability can also serve as a fundamental\nmechanism for Self-Improvement, for example, by reconstructing datasets with\nrefined results to enhance intrinsic model performance. However, our\ncomprehensive experiments reveal that large language models (LLMs) show no\nclear evidence of inherent Self-Refinement and may even experience response\nquality degradation after Self-Refinement. To address this issue, we propose\nEVOLVE, a simple and effective framework for eliciting and tracking the\nevolution of Self-Refinement through iterative training. We first explore\noptimization methods during training to activate the model's Self-Refinement\ncapability. Then, at inference, we investigate various generation strategies to\nfurther enhance and utilize Self-Refinement while supplying the necessary data\nfor training. Through synergistic optimization of training and inference\nstages, we continually evolve the model's Self-Refinement ability, enabling it\nto better refine its own responses. Moreover, we demonstrate the potential of\nleveraging Self-Refinement to achieve broader Self-Improvement of intrinsic\nmodel abilities. Experiments show that the evolved Self-Refinement ability\nenables the Llama-3.1-8B base model to surpass GPT-4o, achieving 62.3%\nlength-controlled and 63.3% raw win rates on AlpacaEval 2, and 50.3% on\nArena-Hard. It also generalizes effectively to out-of-domain reasoning tasks,\nimproving performance on mathematical reasoning benchmarks such as GSM8K and\nMATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Refinement refers to a model's ability to revise its own responses to\nproduce improved outputs. This capability can also serve as a fundamental\nmechanism for Self-Improvement, for example, by reconstructing datasets with\nrefined results to enhance intrinsic model performance. However, our\ncomprehensive experiments reveal that large language models (LLMs) show no\nclear evidence of inherent Self-Refinement and may even experience response\nquality degradation after Self-Refinement. To address this issue, we propose\nEVOLVE, a simple and effective framework for eliciting and tracking the\nevolution of Self-Refinement through iterative training. We first explore\noptimization methods during training to activate the model's Self-Refinement\ncapability. Then, at inference, we investigate various generation strategies to\nfurther enhance and utilize Self-Refinement while supplying the necessary data\nfor training. Through synergistic optimization of training and inference\nstages, we continually evolve the model's Self-Refinement ability, enabling it\nto better refine its own responses. Moreover, we demonstrate the potential of\nleveraging Self-Refinement to achieve broader Self-Improvement of intrinsic\nmodel abilities. Experiments show that the evolved Self-Refinement ability\nenables the Llama-3.1-8B base model to surpass GPT-4o, achieving 62.3%\nlength-controlled and 63.3% raw win rates on AlpacaEval 2, and 50.3% on\nArena-Hard. It also generalizes effectively to out-of-domain reasoning tasks,\nimproving performance on mathematical reasoning benchmarks such as GSM8K and\nMATH."
                },
                "authors": [
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Xinyu Cui"
                    },
                    {
                        "name": "Xuanfa Jin"
                    },
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Guoqing Liu"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05605v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05605v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24563v2",
                "updated": "2025-10-13T14:23:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    23,
                    19,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T10:16:05Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    10,
                    16,
                    5,
                    0,
                    272,
                    0
                ],
                "title": "NeMo: Needle in a Montage for Video-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeMo: Needle in a Montage for Video-Language Understanding"
                },
                "summary": "Recent advances in video large language models (VideoLLMs) call for new\nevaluation protocols and benchmarks for complex temporal reasoning in\nvideo-language understanding. Inspired by the needle in a haystack test widely\nused by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed\nto assess VideoLLMs' critical reasoning capabilities, including long-context\nrecall and temporal grounding. To generate video question answering data for\nour task, we develop a scalable automated data generation pipeline that\nfacilitates high-quality data synthesis. Built upon the proposed pipeline, we\npresent NeMoBench, a video-language benchmark centered on our task.\nSpecifically, our full set of NeMoBench features 31,378 automatically generated\nquestion-answer (QA) pairs from 13,486 videos with various durations ranging\nfrom seconds to hours. Experiments demonstrate that our pipeline can reliably\nand automatically generate high-quality evaluation data, enabling NeMoBench to\nbe continuously updated with the latest videos. We evaluate 20 state-of-the-art\nmodels on our benchmark, providing extensive results and key insights into\ntheir capabilities and limitations. Our project page is available at:\nhttps://lavi-lab.github.io/NeMoBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video large language models (VideoLLMs) call for new\nevaluation protocols and benchmarks for complex temporal reasoning in\nvideo-language understanding. Inspired by the needle in a haystack test widely\nused by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed\nto assess VideoLLMs' critical reasoning capabilities, including long-context\nrecall and temporal grounding. To generate video question answering data for\nour task, we develop a scalable automated data generation pipeline that\nfacilitates high-quality data synthesis. Built upon the proposed pipeline, we\npresent NeMoBench, a video-language benchmark centered on our task.\nSpecifically, our full set of NeMoBench features 31,378 automatically generated\nquestion-answer (QA) pairs from 13,486 videos with various durations ranging\nfrom seconds to hours. Experiments demonstrate that our pipeline can reliably\nand automatically generate high-quality evaluation data, enabling NeMoBench to\nbe continuously updated with the latest videos. We evaluate 20 state-of-the-art\nmodels on our benchmark, providing extensive results and key insights into\ntheir capabilities and limitations. Our project page is available at:\nhttps://lavi-lab.github.io/NeMoBench."
                },
                "authors": [
                    {
                        "name": "Zi-Yuan Hu"
                    },
                    {
                        "name": "Shuo Liang"
                    },
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Yeyao Tao"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Jia Qin"
                    },
                    {
                        "name": "Jianguang Yu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11438v1",
                "updated": "2025-10-13T14:10:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    10,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:10:26Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    10,
                    26,
                    0,
                    286,
                    0
                ],
                "title": "What Generative Search Engines Like and How to Optimize Web Content\n  Cooperatively",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Generative Search Engines Like and How to Optimize Web Content\n  Cooperatively"
                },
                "summary": "By employing large language models (LLMs) to retrieve documents and generate\nnatural language responses, Generative Engines, such as Google AI overview and\nChatGPT, provide significantly enhanced user experiences and have rapidly\nbecome the new form of search. Their rapid adoption also drives the needs of\nGenerative Engine Optimization (GEO), as content providers are eager to gain\nmore traction from them. In this paper, we introduce AutoGEO, a framework to\nautomatically learn generative engine preferences when using retrieved contents\nfor response generation, and rewrite web contents for more such traction.\nAutoGEO first prompts frontier LLMs to explain generative engine preferences\nand extract meaningful preference rules from these explanations. Then it uses\npreference rules as context engineering for AutoGEO$_\\text{API}$, a\nprompt-based GEO system, and as rule-based rewards to train\nAutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard\nGEO-Bench and two newly constructed benchmarks using real user queries\ndemonstrate the effectiveness of AutoGEO in enhancing content traction while\npreserving search utility. Analyses confirm the learned rules' robustness and\nabilities to capture unique preferences in variant domains, and AutoGEO\nsystems' ability to embed them in content optimization. The code is released at\nhttps://github.com/cxcscmu/AutoGEO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By employing large language models (LLMs) to retrieve documents and generate\nnatural language responses, Generative Engines, such as Google AI overview and\nChatGPT, provide significantly enhanced user experiences and have rapidly\nbecome the new form of search. Their rapid adoption also drives the needs of\nGenerative Engine Optimization (GEO), as content providers are eager to gain\nmore traction from them. In this paper, we introduce AutoGEO, a framework to\nautomatically learn generative engine preferences when using retrieved contents\nfor response generation, and rewrite web contents for more such traction.\nAutoGEO first prompts frontier LLMs to explain generative engine preferences\nand extract meaningful preference rules from these explanations. Then it uses\npreference rules as context engineering for AutoGEO$_\\text{API}$, a\nprompt-based GEO system, and as rule-based rewards to train\nAutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard\nGEO-Bench and two newly constructed benchmarks using real user queries\ndemonstrate the effectiveness of AutoGEO in enhancing content traction while\npreserving search utility. Analyses confirm the learned rules' robustness and\nabilities to capture unique preferences in variant domains, and AutoGEO\nsystems' ability to embed them in content optimization. The code is released at\nhttps://github.com/cxcscmu/AutoGEO."
                },
                "authors": [
                    {
                        "name": "Yujiang Wu"
                    },
                    {
                        "name": "Shanshan Zhong"
                    },
                    {
                        "name": "Yubin Kim"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14161v2",
                "updated": "2025-10-13T14:09:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    9,
                    17,
                    0,
                    286,
                    0
                ],
                "published": "2025-05-20T10:14:32Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    10,
                    14,
                    32,
                    1,
                    140,
                    0
                ],
                "title": "Personalized Bayesian Federated Learning with Wasserstein Barycenter\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Bayesian Federated Learning with Wasserstein Barycenter\n  Aggregation"
                },
                "summary": "Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client\ndata and quantifies uncertainty by combining personalization with Bayesian\ninference. However, existing PBFL methods face two limitations: restrictive\nparametric assumptions in client posterior inference and naive parameter\naveraging for server aggregation. To overcome these issues, we propose FedWBA,\na novel PBFL method that enhances both local inference and global aggregation.\nAt the client level, we use particle-based variational inference for\nnonparametric posterior representation. At the server level, we introduce\nparticle-based Wasserstein barycenter aggregation, offering a more\ngeometrically meaningful approach. Theoretically, we provide local and global\nconvergence guarantees for FedWBA. Locally, we prove a KL divergence decrease\nlower bound per iteration for variational inference convergence. Globally, we\nshow that the Wasserstein barycenter converges to the true parameter as the\nclient data size increases. Empirically, experiments show that FedWBA\noutperforms baselines in prediction accuracy, uncertainty calibration, and\nconvergence rate, with ablation studies confirming its robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client\ndata and quantifies uncertainty by combining personalization with Bayesian\ninference. However, existing PBFL methods face two limitations: restrictive\nparametric assumptions in client posterior inference and naive parameter\naveraging for server aggregation. To overcome these issues, we propose FedWBA,\na novel PBFL method that enhances both local inference and global aggregation.\nAt the client level, we use particle-based variational inference for\nnonparametric posterior representation. At the server level, we introduce\nparticle-based Wasserstein barycenter aggregation, offering a more\ngeometrically meaningful approach. Theoretically, we provide local and global\nconvergence guarantees for FedWBA. Locally, we prove a KL divergence decrease\nlower bound per iteration for variational inference convergence. Globally, we\nshow that the Wasserstein barycenter converges to the true parameter as the\nclient data size increases. Empirically, experiments show that FedWBA\noutperforms baselines in prediction accuracy, uncertainty calibration, and\nconvergence rate, with ablation studies confirming its robustness."
                },
                "authors": [
                    {
                        "name": "Ting Wei"
                    },
                    {
                        "name": "Biao Mei"
                    },
                    {
                        "name": "Junliang Lyu"
                    },
                    {
                        "name": "Renquan Zhang"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Yifan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Sun"
                },
                "author": "Yifan Sun",
                "arxiv_comment": "The paper has been accepted by NIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11434v1",
                "updated": "2025-10-13T14:06:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    6,
                    17,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:06:17Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    6,
                    17,
                    0,
                    286,
                    0
                ],
                "title": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated\n  Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated\n  Content"
                },
                "summary": "Generative large language models (LLMs) have become central to everyday life,\nproducing human-like text across diverse domains. A growing body of research\ninvestigates whether these models also exhibit personality- and\ndemographic-like characteristics in their language. In this work, we introduce\na novel, data-driven methodology for assessing LLM personality without relying\non self-report questionnaires, applying instead automatic personality and\ngender classifiers to model replies on open-ended questions collected from\nReddit. Comparing six widely used models to human-authored responses, we find\nthat LLMs systematically express higher Agreeableness and lower Neuroticism,\nreflecting cooperative and stable conversational tendencies. Gendered language\npatterns in model text broadly resemble those of human writers, though with\nreduced variation, echoing prior findings on automated agents. We contribute a\nnew dataset of human and model responses, along with large-scale comparative\nanalyses, shedding new light on the topic of personality and demographic\npatterns of generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) have become central to everyday life,\nproducing human-like text across diverse domains. A growing body of research\ninvestigates whether these models also exhibit personality- and\ndemographic-like characteristics in their language. In this work, we introduce\na novel, data-driven methodology for assessing LLM personality without relying\non self-report questionnaires, applying instead automatic personality and\ngender classifiers to model replies on open-ended questions collected from\nReddit. Comparing six widely used models to human-authored responses, we find\nthat LLMs systematically express higher Agreeableness and lower Neuroticism,\nreflecting cooperative and stable conversational tendencies. Gendered language\npatterns in model text broadly resemble those of human writers, though with\nreduced variation, echoing prior findings on automated agents. We contribute a\nnew dataset of human and model responses, along with large-scale comparative\nanalyses, shedding new light on the topic of personality and demographic\npatterns of generative AI."
                },
                "authors": [
                    {
                        "name": "Dana Sotto Porat"
                    },
                    {
                        "name": "Ella Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Ella Rabinovich"
                },
                "author": "Ella Rabinovich",
                "arxiv_comment": "ECAI2025 (Identity-Aware AI workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11423v1",
                "updated": "2025-10-13T13:57:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    57,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:57:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    57,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "Beyond the Crowd: LLM-Augmented Community Notes for Governing Health\n  Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Crowd: LLM-Augmented Community Notes for Governing Health\n  Misinformation"
                },
                "summary": "Community Notes, the crowd-sourced misinformation governance system on X\n(formerly Twitter), enables users to flag misleading posts, attach contextual\nnotes, and vote on their helpfulness. However, our analysis of 30.8K\nhealth-related notes reveals significant latency, with a median delay of 17.6\nhours before the first note receives a helpfulness status. To improve\nresponsiveness during real-world misinformation surges, we propose CrowdNotes+,\na unified framework that leverages large language models (LLMs) to augment\nCommunity Notes for faster and more reliable health misinformation governance.\nCrowdNotes+ integrates two complementary modes: (1) evidence-grounded note\naugmentation and (2) utility-guided note automation, along with a hierarchical\nthree-step evaluation that progressively assesses relevance, correctness, and\nhelpfulness. We instantiate the framework through HealthNotes, a benchmark of\n1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness\njudge. Experiments on fifteen LLMs reveal an overlooked loophole in current\nhelpfulness evaluation, where stylistic fluency is mistaken for factual\naccuracy, and demonstrate that our hierarchical evaluation and LLM-augmented\ngeneration jointly enhance factual precision and evidence utility. These\nresults point toward a hybrid human-AI governance model that improves both the\nrigor and timeliness of crowd-sourced fact-checking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community Notes, the crowd-sourced misinformation governance system on X\n(formerly Twitter), enables users to flag misleading posts, attach contextual\nnotes, and vote on their helpfulness. However, our analysis of 30.8K\nhealth-related notes reveals significant latency, with a median delay of 17.6\nhours before the first note receives a helpfulness status. To improve\nresponsiveness during real-world misinformation surges, we propose CrowdNotes+,\na unified framework that leverages large language models (LLMs) to augment\nCommunity Notes for faster and more reliable health misinformation governance.\nCrowdNotes+ integrates two complementary modes: (1) evidence-grounded note\naugmentation and (2) utility-guided note automation, along with a hierarchical\nthree-step evaluation that progressively assesses relevance, correctness, and\nhelpfulness. We instantiate the framework through HealthNotes, a benchmark of\n1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness\njudge. Experiments on fifteen LLMs reveal an overlooked loophole in current\nhelpfulness evaluation, where stylistic fluency is mistaken for factual\naccuracy, and demonstrate that our hierarchical evaluation and LLM-augmented\ngeneration jointly enhance factual precision and evidence utility. These\nresults point toward a hybrid human-AI governance model that improves both the\nrigor and timeliness of crowd-sourced fact-checking."
                },
                "authors": [
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Zihang Fu"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Fanxiao Li"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11414v1",
                "updated": "2025-10-13T13:52:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    52,
                    33,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:52:33Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    52,
                    33,
                    0,
                    286,
                    0
                ],
                "title": "Uncertainty-Aware, Risk-Adaptive Access Control for Agentic Systems\n  using an LLM-Judged TBAC Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware, Risk-Adaptive Access Control for Agentic Systems\n  using an LLM-Judged TBAC Model"
                },
                "summary": "The proliferation of autonomous AI agents within enterprise environments\nintroduces a critical security challenge: managing access control for emergent,\nnovel tasks for which no predefined policies exist. This paper introduces an\nadvanced security framework that extends the Task-Based Access Control (TBAC)\nmodel by using a Large Language Model (LLM) as an autonomous, risk-aware judge.\nThis model makes access control decisions not only based on an agent's intent\nbut also by explicitly considering the inherent \\textbf{risk associated with\ntarget resources} and the LLM's own \\textbf{model uncertainty} in its\ndecision-making process. When an agent proposes a novel task, the LLM judge\nsynthesizes a just-in-time policy while also computing a composite risk score\nfor the task and an uncertainty estimate for its own reasoning. High-risk or\nhigh-uncertainty requests trigger more stringent controls, such as requiring\nhuman approval. This dual consideration of external risk and internal\nconfidence allows the model to enforce a more robust and adaptive version of\nthe principle of least privilege, paving the way for safer and more trustworthy\nautonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of autonomous AI agents within enterprise environments\nintroduces a critical security challenge: managing access control for emergent,\nnovel tasks for which no predefined policies exist. This paper introduces an\nadvanced security framework that extends the Task-Based Access Control (TBAC)\nmodel by using a Large Language Model (LLM) as an autonomous, risk-aware judge.\nThis model makes access control decisions not only based on an agent's intent\nbut also by explicitly considering the inherent \\textbf{risk associated with\ntarget resources} and the LLM's own \\textbf{model uncertainty} in its\ndecision-making process. When an agent proposes a novel task, the LLM judge\nsynthesizes a just-in-time policy while also computing a composite risk score\nfor the task and an uncertainty estimate for its own reasoning. High-risk or\nhigh-uncertainty requests trigger more stringent controls, such as requiring\nhuman approval. This dual consideration of external risk and internal\nconfidence allows the model to enforce a more robust and adaptive version of\nthe principle of least privilege, paving the way for safer and more trustworthy\nautonomous systems."
                },
                "authors": [
                    {
                        "name": "Charles Fleming"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    }
                ],
                "author_detail": {
                    "name": "Ramana Kompella"
                },
                "author": "Ramana Kompella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23261v2",
                "updated": "2025-10-13T13:51:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    51,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-27T11:30:17Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    11,
                    30,
                    17,
                    5,
                    270,
                    0
                ],
                "title": "The Matthew Effect of AI Programming Assistants: A Hidden Bias in\n  Software Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Matthew Effect of AI Programming Assistants: A Hidden Bias in\n  Software Evolution"
                },
                "summary": "AI-assisted programming is rapidly reshaping software development, with large\nlanguage models (LLMs) enabling new paradigms such as vibe coding and agentic\ncoding. While prior works have focused on prompt design and code generation\nquality, the broader impact of LLM-driven development on the iterative dynamics\nof software engineering remains underexplored. In this paper, we conduct\nlarge-scale experiments on thousands of algorithmic programming tasks and\nhundreds of framework selection tasks to systematically investigate how\nAI-assisted programming interacts with the software ecosystem. Our analysis\nreveals \\textbf{a striking Matthew effect: the more popular a programming\nlanguage or framework, the higher the success rate of LLM-generated code}. The\nphenomenon suggests that AI systems may reinforce existing popularity\nhierarchies, accelerating convergence around dominant tools while hindering\ndiversity and innovation. We provide a quantitative characterization of this\neffect and discuss its implications for the future evolution of programming\necosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-assisted programming is rapidly reshaping software development, with large\nlanguage models (LLMs) enabling new paradigms such as vibe coding and agentic\ncoding. While prior works have focused on prompt design and code generation\nquality, the broader impact of LLM-driven development on the iterative dynamics\nof software engineering remains underexplored. In this paper, we conduct\nlarge-scale experiments on thousands of algorithmic programming tasks and\nhundreds of framework selection tasks to systematically investigate how\nAI-assisted programming interacts with the software ecosystem. Our analysis\nreveals \\textbf{a striking Matthew effect: the more popular a programming\nlanguage or framework, the higher the success rate of LLM-generated code}. The\nphenomenon suggests that AI systems may reinforce existing popularity\nhierarchies, accelerating convergence around dominant tools while hindering\ndiversity and innovation. We provide a quantitative characterization of this\neffect and discuss its implications for the future evolution of programming\necosystems."
                },
                "authors": [
                    {
                        "name": "Fei Gu"
                    },
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Hongzong LI"
                    },
                    {
                        "name": "Jiahao MA"
                    }
                ],
                "author_detail": {
                    "name": "Jiahao MA"
                },
                "author": "Jiahao MA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11409v1",
                "updated": "2025-10-13T13:48:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    48,
                    29,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:48:29Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    48,
                    29,
                    0,
                    286,
                    0
                ],
                "title": "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic\n  Literature Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic\n  Literature Reviews"
                },
                "summary": "The creation of systematic literature reviews (SLR) is critical for analyzing\nthe landscape of a research field and guiding future research directions.\nHowever, retrieving and filtering the literature corpus for an SLR is highly\ntime-consuming and requires extensive manual effort, as keyword-based searches\nin digital libraries often return numerous irrelevant publications. In this\nwork, we propose a pipeline leveraging multiple large language models (LLMs),\nclassifying papers based on descriptive prompts and deciding jointly using a\nconsensus scheme. The entire process is human-supervised and interactively\ncontrolled via our open-source visual analytics web interface, LLMSurver, which\nenables real-time inspection and modification of model outputs. We evaluate our\napproach using ground-truth data from a recent SLR comprising over 8,000\ncandidate papers, benchmarking both open and commercial state-of-the-art LLMs\nfrom mid-2024 and fall 2025. Results demonstrate that our pipeline\nsignificantly reduces manual effort while achieving lower error rates than\nsingle human annotators. Furthermore, modern open-source models prove\nsufficient for this task, making the method accessible and cost-effective.\nOverall, our work demonstrates how responsible human-AI collaboration can\naccelerate and enhance systematic literature reviews within academic workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The creation of systematic literature reviews (SLR) is critical for analyzing\nthe landscape of a research field and guiding future research directions.\nHowever, retrieving and filtering the literature corpus for an SLR is highly\ntime-consuming and requires extensive manual effort, as keyword-based searches\nin digital libraries often return numerous irrelevant publications. In this\nwork, we propose a pipeline leveraging multiple large language models (LLMs),\nclassifying papers based on descriptive prompts and deciding jointly using a\nconsensus scheme. The entire process is human-supervised and interactively\ncontrolled via our open-source visual analytics web interface, LLMSurver, which\nenables real-time inspection and modification of model outputs. We evaluate our\napproach using ground-truth data from a recent SLR comprising over 8,000\ncandidate papers, benchmarking both open and commercial state-of-the-art LLMs\nfrom mid-2024 and fall 2025. Results demonstrate that our pipeline\nsignificantly reduces manual effort while achieving lower error rates than\nsingle human annotators. Furthermore, modern open-source models prove\nsufficient for this task, making the method accessible and cost-effective.\nOverall, our work demonstrates how responsible human-AI collaboration can\naccelerate and enhance systematic literature reviews within academic workflows."
                },
                "authors": [
                    {
                        "name": "Lucas Joos"
                    },
                    {
                        "name": "Daniel A. Keim"
                    },
                    {
                        "name": "Maximilian T. Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian T. Fischer"
                },
                "author": "Maximilian T. Fischer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11408v1",
                "updated": "2025-10-13T13:48:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    48,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:48:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    48,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "Valid Survey Simulations with Limited Human Data: The Roles of\n  Prompting, Fine-Tuning, and Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Valid Survey Simulations with Limited Human Data: The Roles of\n  Prompting, Fine-Tuning, and Rectification"
                },
                "summary": "Surveys provide valuable insights into public opinion and behavior, but their\nexecution is costly and slow. Large language models (LLMs) have been proposed\nas a scalable, low-cost substitute for human respondents, but their outputs are\noften biased and yield invalid estimates. We study the interplay between\nsynthesis methods that use LLMs to generate survey responses and rectification\nmethods that debias population estimates, and explore how human responses are\nbest allocated between them. Using two panel surveys with questions on\nnutrition, politics, and economics, we find that synthesis alone introduces\nsubstantial bias (24-86%), whereas combining it with rectification reduces bias\nbelow 5% and increases effective sample size by up to 14%. Overall, we\nchallenge the common practice of using all human responses for fine-tuning,\nshowing that under a fixed budget, allocating most to rectification results in\nfar more effective estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveys provide valuable insights into public opinion and behavior, but their\nexecution is costly and slow. Large language models (LLMs) have been proposed\nas a scalable, low-cost substitute for human respondents, but their outputs are\noften biased and yield invalid estimates. We study the interplay between\nsynthesis methods that use LLMs to generate survey responses and rectification\nmethods that debias population estimates, and explore how human responses are\nbest allocated between them. Using two panel surveys with questions on\nnutrition, politics, and economics, we find that synthesis alone introduces\nsubstantial bias (24-86%), whereas combining it with rectification reduces bias\nbelow 5% and increases effective sample size by up to 14%. Overall, we\nchallenge the common practice of using all human responses for fine-tuning,\nshowing that under a fixed budget, allocating most to rectification results in\nfar more effective estimation."
                },
                "authors": [
                    {
                        "name": "Stefan Krsteski"
                    },
                    {
                        "name": "Giuseppe Russo"
                    },
                    {
                        "name": "Serina Chang"
                    },
                    {
                        "name": "Robert West"
                    },
                    {
                        "name": "Kristina Gligorić"
                    }
                ],
                "author_detail": {
                    "name": "Kristina Gligorić"
                },
                "author": "Kristina Gligorić",
                "arxiv_comment": "19 pages, 4 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11407v1",
                "updated": "2025-10-13T13:47:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    47,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    47,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "KnowRL: Teaching Language Models to Know What They Know",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowRL: Teaching Language Models to Know What They Know"
                },
                "summary": "Truly reliable AI requires more than simply scaling up knowledge; it demands\nthe ability to know what it knows and when it does not. Yet recent research\nshows that even the best LLMs misjudge their own competence in more than one in\nfive cases, making any response born of such internal uncertainty impossible to\nfully trust. Inspired by self-improvement reinforcement learning techniques\nthat require minimal data, we present a simple but powerful framework KnowRL\nthat strengthens a model's internal understanding of its own feasibility\nboundaries, enabling safer and more responsible behaviour. Our framework\ncombines two components: (i) introspection, where the model generates and\nclassifies tasks it judges feasible or infeasible, and (ii) consensus-based\nrewarding, where stability of self-knowledge assessment is reinforced through\ninternal agreement. By using internally generated data, this design strengthens\nconsistency in self-knowledge and entirely avoids costly external supervision.\nIn experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved\nself-knowledge, validated by both intrinsic self-consistency and extrinsic\nbenchmarking. With nothing more than a small seed set and no external\nsupervision, our method drove gains as high as 28% in accuracy and 12% in F1,\noutperforming baselines in just a few iterations. Our framework essentially\nunlocks the untapped capacity of LLMs to self-improve their knowledge\nawareness, opening the door to reliable, more accountable AI and safer\ndeployment in critical applications. Owing to its simplicity and independence\nfrom external effort, we encourage applying this reliability-enhancing process\nto all future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truly reliable AI requires more than simply scaling up knowledge; it demands\nthe ability to know what it knows and when it does not. Yet recent research\nshows that even the best LLMs misjudge their own competence in more than one in\nfive cases, making any response born of such internal uncertainty impossible to\nfully trust. Inspired by self-improvement reinforcement learning techniques\nthat require minimal data, we present a simple but powerful framework KnowRL\nthat strengthens a model's internal understanding of its own feasibility\nboundaries, enabling safer and more responsible behaviour. Our framework\ncombines two components: (i) introspection, where the model generates and\nclassifies tasks it judges feasible or infeasible, and (ii) consensus-based\nrewarding, where stability of self-knowledge assessment is reinforced through\ninternal agreement. By using internally generated data, this design strengthens\nconsistency in self-knowledge and entirely avoids costly external supervision.\nIn experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved\nself-knowledge, validated by both intrinsic self-consistency and extrinsic\nbenchmarking. With nothing more than a small seed set and no external\nsupervision, our method drove gains as high as 28% in accuracy and 12% in F1,\noutperforming baselines in just a few iterations. Our framework essentially\nunlocks the untapped capacity of LLMs to self-improve their knowledge\nawareness, opening the door to reliable, more accountable AI and safer\ndeployment in critical applications. Owing to its simplicity and independence\nfrom external effort, we encourage applying this reliability-enhancing process\nto all future models."
                },
                "authors": [
                    {
                        "name": "Sahil Kale"
                    },
                    {
                        "name": "Devendra Singh Dhami"
                    }
                ],
                "author_detail": {
                    "name": "Devendra Singh Dhami"
                },
                "author": "Devendra Singh Dhami",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11402v1",
                "updated": "2025-10-13T13:44:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    44,
                    13,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:44:13Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    44,
                    13,
                    0,
                    286,
                    0
                ],
                "title": "On Inherited Popularity Bias in Cold-Start Item Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Inherited Popularity Bias in Cold-Start Item Recommendation"
                },
                "summary": "Collaborative filtering (CF) recommender systems struggle with making\npredictions on unseen, or 'cold', items. Systems designed to address this\nchallenge are often trained with supervision from warm CF models in order to\nleverage collaborative and content information from the available interaction\ndata. However, since they learn to replicate the behavior of CF methods,\ncold-start models may therefore also learn to imitate their predictive biases.\nIn this paper, we show that cold-start systems can inherit popularity bias, a\ncommon cause of recommender system unfairness arising when CF models overfit to\nmore popular items, thereby maximizing user-oriented accuracy but neglecting\nrarer items. We demonstrate that cold-start recommenders not only mirror the\npopularity biases of warm models, but are in fact affected more severely:\nbecause they cannot infer popularity from interaction data, they instead\nattempt to estimate it based solely on content features. This leads to\nsignificant over-prediction of certain cold items with similar content to\npopular warm items, even if their ground truth popularity is very low. Through\nexperiments on three multimedia datasets, we analyze the impact of this\nbehavior on three generative cold-start methods. We then describe a simple\npost-processing bias mitigation method that, by using embedding magnitude as a\nproxy for predicted popularity, can produce more balanced recommendations with\nlimited harm to user-oriented cold-start accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative filtering (CF) recommender systems struggle with making\npredictions on unseen, or 'cold', items. Systems designed to address this\nchallenge are often trained with supervision from warm CF models in order to\nleverage collaborative and content information from the available interaction\ndata. However, since they learn to replicate the behavior of CF methods,\ncold-start models may therefore also learn to imitate their predictive biases.\nIn this paper, we show that cold-start systems can inherit popularity bias, a\ncommon cause of recommender system unfairness arising when CF models overfit to\nmore popular items, thereby maximizing user-oriented accuracy but neglecting\nrarer items. We demonstrate that cold-start recommenders not only mirror the\npopularity biases of warm models, but are in fact affected more severely:\nbecause they cannot infer popularity from interaction data, they instead\nattempt to estimate it based solely on content features. This leads to\nsignificant over-prediction of certain cold items with similar content to\npopular warm items, even if their ground truth popularity is very low. Through\nexperiments on three multimedia datasets, we analyze the impact of this\nbehavior on three generative cold-start methods. We then describe a simple\npost-processing bias mitigation method that, by using embedding magnitude as a\nproxy for predicted popularity, can produce more balanced recommendations with\nlimited harm to user-oriented cold-start accuracy."
                },
                "authors": [
                    {
                        "name": "Gregor Meehan"
                    },
                    {
                        "name": "Johan Pauwels"
                    }
                ],
                "author_detail": {
                    "name": "Johan Pauwels"
                },
                "author": "Johan Pauwels",
                "arxiv_doi": "10.1145/3705328.3748035",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3705328.3748035",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at ACM RecSys 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11398v1",
                "updated": "2025-10-13T13:41:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    41,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:41:27Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    41,
                    27,
                    0,
                    286,
                    0
                ],
                "title": "Living Off the LLM: How LLMs Will Change Adversary Tactics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Living Off the LLM: How LLMs Will Change Adversary Tactics"
                },
                "summary": "In living off the land attacks, malicious actors use legitimate tools and\nprocesses already present on a system to avoid detection. In this paper, we\nexplore how the on-device LLMs of the future will become a security concern as\nthreat actors integrate LLMs into their living off the land attack pipeline and\nways the security community may mitigate this threat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In living off the land attacks, malicious actors use legitimate tools and\nprocesses already present on a system to avoid detection. In this paper, we\nexplore how the on-device LLMs of the future will become a security concern as\nthreat actors integrate LLMs into their living off the land attack pipeline and\nways the security community may mitigate this threat."
                },
                "authors": [
                    {
                        "name": "Sean Oesch"
                    },
                    {
                        "name": "Jack Hutchins"
                    },
                    {
                        "name": "Luke Koch"
                    },
                    {
                        "name": "Kevin Kurian"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Kurian"
                },
                "author": "Kevin Kurian",
                "arxiv_comment": "6 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11394v1",
                "updated": "2025-10-13T13:38:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    38,
                    54,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:38:54Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    38,
                    54,
                    0,
                    286,
                    0
                ],
                "title": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation\n  via Rigorous Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation\n  via Rigorous Verification"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for\nenhancing the responses of large language models (LLMs) with external knowledge\nsources. Despite the impressive performance in complex question-answering\ntasks, RAG still struggles with hallucinations. Attributing RAG-generated\ncontent through in-line citations has demonstrated potential in reducing\nhallucinations and facilitating human verification. Existing citation\ngeneration methods primarily rely on either fine-tuning the generator or\nemploying post-processing approaches for citation matching. However, the former\napproach demands substantial annotated data and computational resources, while\nthe latter often encounters difficulties in managing multiple citations and\nfrequently produces suboptimal results. In this paper, we introduce a novel\nframework, called VeriCite, designed to rigorously validate supporting evidence\nand enhance answer attribution. Specifically, VeriCite breaks down into a\nthree-stage generation: 1) The initial answer generation first generates a\nresponse based on all available contexts and has its claims verified through\nthe NLI model; 2) the supporting evidence selection assesses the utility of\neach document and extracts useful supporting evidences; 3) the final answer\nrefinement integrates the initial response and collected evidences to produce\nthe final, refined answer.We conduct experiments across five open-source LLMs\nand four datasets, demonstrating that VeriCite can significantly improve\ncitation quality while maintaining the correctness of the answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for\nenhancing the responses of large language models (LLMs) with external knowledge\nsources. Despite the impressive performance in complex question-answering\ntasks, RAG still struggles with hallucinations. Attributing RAG-generated\ncontent through in-line citations has demonstrated potential in reducing\nhallucinations and facilitating human verification. Existing citation\ngeneration methods primarily rely on either fine-tuning the generator or\nemploying post-processing approaches for citation matching. However, the former\napproach demands substantial annotated data and computational resources, while\nthe latter often encounters difficulties in managing multiple citations and\nfrequently produces suboptimal results. In this paper, we introduce a novel\nframework, called VeriCite, designed to rigorously validate supporting evidence\nand enhance answer attribution. Specifically, VeriCite breaks down into a\nthree-stage generation: 1) The initial answer generation first generates a\nresponse based on all available contexts and has its claims verified through\nthe NLI model; 2) the supporting evidence selection assesses the utility of\neach document and extracts useful supporting evidences; 3) the final answer\nrefinement integrates the initial response and collected evidences to produce\nthe final, refined answer.We conduct experiments across five open-source LLMs\nand four datasets, demonstrating that VeriCite can significantly improve\ncitation quality while maintaining the correctness of the answers."
                },
                "authors": [
                    {
                        "name": "Haosheng Qian"
                    },
                    {
                        "name": "Yixing Fan"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Ruqing Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_doi": "10.1145/3767695.3769505",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3767695.3769505",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "In Proceedings of the 2025 Annual International ACM SIGIR\n  Conference on Research and Development in Information Retrieval in the Asia\n  Pacific Region (SIGIR-AP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11390v1",
                "updated": "2025-10-13T13:34:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    34,
                    5,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:34:05Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    34,
                    5,
                    0,
                    286,
                    0
                ],
                "title": "Medical Interpretability and Knowledge Maps of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Interpretability and Knowledge Maps of Large Language Models"
                },
                "summary": "We present a systematic study of medical-domain interpretability in Large\nLanguage Models (LLMs). We study how the LLMs both represent and process\nmedical knowledge through four different interpretability techniques: (1) UMAP\nprojections of intermediate activations, (2) gradient-based saliency with\nrespect to the model weights, (3) layer lesioning/removal and (4) activation\npatching. We present knowledge maps of five LLMs which show, at a\ncoarse-resolution, where knowledge about patient's ages, medical symptoms,\ndiseases and drugs is stored in the models. In particular for Llama3.3-70B, we\nfind that most medical knowledge is processed in the first half of the model's\nlayers. In addition, we find several interesting phenomena: (i) age is often\nencoded in a non-linear and sometimes discontinuous manner at intermediate\nlayers in the models, (ii) the disease progression representation is\nnon-monotonic and circular at certain layers of the model, (iii) in\nLlama3.3-70B, drugs cluster better by medical specialty rather than mechanism\nof action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B\nhave activations that collapse at intermediate layers but recover by the final\nlayers. These results can guide future research on fine-tuning, un-learning or\nde-biasing LLMs for medical tasks by suggesting at which layers in the model\nthese techniques should be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a systematic study of medical-domain interpretability in Large\nLanguage Models (LLMs). We study how the LLMs both represent and process\nmedical knowledge through four different interpretability techniques: (1) UMAP\nprojections of intermediate activations, (2) gradient-based saliency with\nrespect to the model weights, (3) layer lesioning/removal and (4) activation\npatching. We present knowledge maps of five LLMs which show, at a\ncoarse-resolution, where knowledge about patient's ages, medical symptoms,\ndiseases and drugs is stored in the models. In particular for Llama3.3-70B, we\nfind that most medical knowledge is processed in the first half of the model's\nlayers. In addition, we find several interesting phenomena: (i) age is often\nencoded in a non-linear and sometimes discontinuous manner at intermediate\nlayers in the models, (ii) the disease progression representation is\nnon-monotonic and circular at certain layers of the model, (iii) in\nLlama3.3-70B, drugs cluster better by medical specialty rather than mechanism\nof action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B\nhave activations that collapse at intermediate layers but recover by the final\nlayers. These results can guide future research on fine-tuning, un-learning or\nde-biasing LLMs for medical tasks by suggesting at which layers in the model\nthese techniques should be applied."
                },
                "authors": [
                    {
                        "name": "Razvan Marinescu"
                    },
                    {
                        "name": "Victoria-Elisabeth Gruber"
                    },
                    {
                        "name": "Diego Fajardo"
                    }
                ],
                "author_detail": {
                    "name": "Diego Fajardo"
                },
                "author": "Diego Fajardo",
                "arxiv_comment": "29 pages, 34 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11389v1",
                "updated": "2025-10-13T13:33:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    33,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:33:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    33,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "Beyond Survival: Evaluating LLMs in Social Deduction Games with\n  Human-Aligned Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Survival: Evaluating LLMs in Social Deduction Games with\n  Human-Aligned Strategies"
                },
                "summary": "Social deduction games like Werewolf combine language, reasoning, and\nstrategy, providing a testbed for studying natural language and social\nintelligence. However, most studies reduce the game to LLM-based self-play,\nyielding templated utterances and anecdotal cases that overlook the richness of\nsocial gameplay. Evaluation further relies on coarse metrics such as survival\ntime or subjective scoring due to the lack of quality reference data. To\naddress these gaps, we curate a high-quality, human-verified multimodal\nWerewolf dataset containing over 100 hours of video, 32.4M utterance tokens,\nand 15 rule variants. Based on this dataset, we propose a novel\nstrategy-alignment evaluation that leverages the winning faction's strategies\nas ground truth in two stages: 1) Speech evaluation, formulated as\nmultiple-choice-style tasks that assess whether the model can adopt appropriate\nstances across five dimensions of social ability; and 2) Decision evaluation,\nwhich assesses the model's voting choices and opponent-role inferences. This\nframework enables a fine-grained evaluation of models' linguistic and reasoning\ncapabilities, while capturing their ability to generate strategically coherent\ngameplay. Our experiments show that state-of-the-art LLMs show diverse\nperformance, with roughly half remain below 0.50, revealing clear gaps in\ndeception and counterfactual reasoning. We hope our dataset further inspires\nresearch on language, reasoning, and strategy in multi-agent interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social deduction games like Werewolf combine language, reasoning, and\nstrategy, providing a testbed for studying natural language and social\nintelligence. However, most studies reduce the game to LLM-based self-play,\nyielding templated utterances and anecdotal cases that overlook the richness of\nsocial gameplay. Evaluation further relies on coarse metrics such as survival\ntime or subjective scoring due to the lack of quality reference data. To\naddress these gaps, we curate a high-quality, human-verified multimodal\nWerewolf dataset containing over 100 hours of video, 32.4M utterance tokens,\nand 15 rule variants. Based on this dataset, we propose a novel\nstrategy-alignment evaluation that leverages the winning faction's strategies\nas ground truth in two stages: 1) Speech evaluation, formulated as\nmultiple-choice-style tasks that assess whether the model can adopt appropriate\nstances across five dimensions of social ability; and 2) Decision evaluation,\nwhich assesses the model's voting choices and opponent-role inferences. This\nframework enables a fine-grained evaluation of models' linguistic and reasoning\ncapabilities, while capturing their ability to generate strategically coherent\ngameplay. Our experiments show that state-of-the-art LLMs show diverse\nperformance, with roughly half remain below 0.50, revealing clear gaps in\ndeception and counterfactual reasoning. We hope our dataset further inspires\nresearch on language, reasoning, and strategy in multi-agent interaction."
                },
                "authors": [
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Yuan Huang"
                    },
                    {
                        "name": "Junchang Liu"
                    },
                    {
                        "name": "Haozhe Luo"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Zixiang Xu"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "34 pages, 32figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11387v1",
                "updated": "2025-10-13T13:29:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    29,
                    20,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:29:20Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    29,
                    20,
                    0,
                    286,
                    0
                ],
                "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent\n  Material Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent\n  Material Inference"
                },
                "summary": "Modeling reflections from 2D images is essential for photorealistic rendering\nand novel view synthesis. Recent approaches enhance Gaussian primitives with\nreflection-related material attributes to enable physically based rendering\n(PBR) with Gaussian Splatting. However, the material inference often lacks\nsufficient constraints, especially under limited environment modeling,\nresulting in illumination aliasing and reduced generalization. In this work, we\nrevisit the problem from a multi-view perspective and show that multi-view\nconsistent material inference with more physically-based environment modeling\nis key to learning accurate reflections with Gaussian Splatting. To this end,\nwe enforce 2D Gaussians to produce multi-view consistent material maps during\ndeferred shading. We also track photometric variations across views to identify\nhighly reflective regions, which serve as strong priors for reflection strength\nterms. To handle indirect illumination caused by inter-object occlusions, we\nfurther introduce an environment modeling strategy through ray tracing with\n2DGS, enabling photorealistic rendering of indirect radiance. Experiments on\nwidely used benchmarks show that our method faithfully recovers both\nillumination and geometry, achieving state-of-the-art rendering quality in\nnovel views synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling reflections from 2D images is essential for photorealistic rendering\nand novel view synthesis. Recent approaches enhance Gaussian primitives with\nreflection-related material attributes to enable physically based rendering\n(PBR) with Gaussian Splatting. However, the material inference often lacks\nsufficient constraints, especially under limited environment modeling,\nresulting in illumination aliasing and reduced generalization. In this work, we\nrevisit the problem from a multi-view perspective and show that multi-view\nconsistent material inference with more physically-based environment modeling\nis key to learning accurate reflections with Gaussian Splatting. To this end,\nwe enforce 2D Gaussians to produce multi-view consistent material maps during\ndeferred shading. We also track photometric variations across views to identify\nhighly reflective regions, which serve as strong priors for reflection strength\nterms. To handle indirect illumination caused by inter-object occlusions, we\nfurther introduce an environment modeling strategy through ray tracing with\n2DGS, enabling photorealistic rendering of indirect radiance. Experiments on\nwidely used benchmarks show that our method faithfully recovers both\nillumination and geometry, achieving state-of-the-art rendering quality in\nnovel views synthesis."
                },
                "authors": [
                    {
                        "name": "Wenyuan Zhang"
                    },
                    {
                        "name": "Jimin Tang"
                    },
                    {
                        "name": "Weiqi Zhang"
                    },
                    {
                        "name": "Yi Fang"
                    },
                    {
                        "name": "Yu-Shen Liu"
                    },
                    {
                        "name": "Zhizhong Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhizhong Han"
                },
                "author": "Zhizhong Han",
                "arxiv_comment": "Accepted by NeurIPS 2025. Project Page:\n  https://wen-yuan-zhang.github.io/MaterialRefGS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11382v1",
                "updated": "2025-10-13T13:23:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    23,
                    18,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:23:18Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    23,
                    18,
                    0,
                    286,
                    0
                ],
                "title": "Islands in Simulated Cosmos: Probing the Hubble Flow around Groups and\n  Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Islands in Simulated Cosmos: Probing the Hubble Flow around Groups and\n  Clusters"
                },
                "summary": "The local Hubble flow offers a powerful laboratory to study the interplay\nbetween cosmic expansion and gravitational dynamics. On large scales, galaxy\nvelocities follow Hubble's law, but within groups and clusters local\ngravitational effects introduce significant departures from linearity. Using\nthe IllustrisTNG cosmological simulations, we investigate whether dark energy\nleaves detectable imprints on the local velocity-radius relation. We model the\nkinematics with extensions of the Lemaitre-Tolman framework and apply Bayesian\ninference to recover halo masses and the Hubble constant H0. The fits reveal\nsystematic biases: halo masses are underestimated with a median ratio\n$M_{fit}/M_{true} = 0.95 \\pm 0.28$, while the inferred Hubble constant clusters\naround $H_0 = 64 \\pm 16 km/s/Mpc$, compared to the simulation input of 67.74.\nThis corresponds to an average 25\\% uncertainty in H0 recovery from the local\nflow method. While the mass and expansion rate can be constrained, different\nmodel variants whether including angular momentum, friction, or altered radial\nscaling-remain statistically indistinguishable. Our results highlight both the\npromise and the limitations of using local kinematics as a precision probe of\ndark energy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The local Hubble flow offers a powerful laboratory to study the interplay\nbetween cosmic expansion and gravitational dynamics. On large scales, galaxy\nvelocities follow Hubble's law, but within groups and clusters local\ngravitational effects introduce significant departures from linearity. Using\nthe IllustrisTNG cosmological simulations, we investigate whether dark energy\nleaves detectable imprints on the local velocity-radius relation. We model the\nkinematics with extensions of the Lemaitre-Tolman framework and apply Bayesian\ninference to recover halo masses and the Hubble constant H0. The fits reveal\nsystematic biases: halo masses are underestimated with a median ratio\n$M_{fit}/M_{true} = 0.95 \\pm 0.28$, while the inferred Hubble constant clusters\naround $H_0 = 64 \\pm 16 km/s/Mpc$, compared to the simulation input of 67.74.\nThis corresponds to an average 25\\% uncertainty in H0 recovery from the local\nflow method. While the mass and expansion rate can be constrained, different\nmodel variants whether including angular momentum, friction, or altered radial\nscaling-remain statistically indistinguishable. Our results highlight both the\npromise and the limitations of using local kinematics as a precision probe of\ndark energy."
                },
                "authors": [
                    {
                        "name": "David Benisty"
                    },
                    {
                        "name": "Antonino Del Popolo"
                    }
                ],
                "author_detail": {
                    "name": "Antonino Del Popolo"
                },
                "author": "Antonino Del Popolo",
                "arxiv_comment": "5 figures; 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11373v1",
                "updated": "2025-10-13T13:13:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    13,
                    13,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:13:13Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    13,
                    13,
                    0,
                    286,
                    0
                ],
                "title": "JWST COSMOS-3D: Spectroscopic Census and Luminosity Function of [O III]\n  Emitters at 6.75<z<9.05 in COSMOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST COSMOS-3D: Spectroscopic Census and Luminosity Function of [O III]\n  Emitters at 6.75<z<9.05 in COSMOS"
                },
                "summary": "We present a spectroscopically-selected [OIII]+Hb emitters catalogue at\n6.75<z<9.05 and the resulting [OIII] 5008 \\AA Luminosity Function (LF) in the\nCOSMOS field. We leverage the 0.3 deg$^{2}$ covered to date by COSMOS-3D using\nNIRCam/WFSS F444W (90% of the survey) to perform the largest spectroscopic\nsearch for [OIII] emitters at 6.75<z<9.05. We present our catalogue of 237\n[OIII] emitters and their associated completeness function. The inferred\nconstraints on the [OIII] LF enable us to characterise the knee of the [OIII]\nLF, resulting in improved [OIII] LF constraints at z~7,8. Notably, we find\nevidence for an accelerated decline of the [OIII] luminosity density between\nz~7 and z~8, which could be expected if the metallicity of [OIII] emitters, as\nwell as the cosmic star-formation rate density, is declining at these\nredshifts. We find that theoretical models that reproduce the z~7,8 [OIII] LF\ndo not reproduce well the [OIII] equivalent width distribution, pointing to\npotential challenges in the modelling of[OIII] and other nebular lines in the\nearly Universe. Finally, we provide the first constraints on the cosmic\nvariance of [OIII] emitters, estimating at 15% the relative uncertainty for the\nz~7,8 [OIII] LF in the 0.3 deg$^2$ field. This estimate is in good agreement\nwith that inferred from clustering, and shows that the [OIII] LF derived from\nsmaller extragalactic legacy fields is strongly affected by cosmic variance.\nOur results highlight the fundamental role that wide-area JWST slitless surveys\nplay to map the galaxy large-scale structure down into the reionisation era,\nserving as a springboard for a variety of science cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a spectroscopically-selected [OIII]+Hb emitters catalogue at\n6.75<z<9.05 and the resulting [OIII] 5008 \\AA Luminosity Function (LF) in the\nCOSMOS field. We leverage the 0.3 deg$^{2}$ covered to date by COSMOS-3D using\nNIRCam/WFSS F444W (90% of the survey) to perform the largest spectroscopic\nsearch for [OIII] emitters at 6.75<z<9.05. We present our catalogue of 237\n[OIII] emitters and their associated completeness function. The inferred\nconstraints on the [OIII] LF enable us to characterise the knee of the [OIII]\nLF, resulting in improved [OIII] LF constraints at z~7,8. Notably, we find\nevidence for an accelerated decline of the [OIII] luminosity density between\nz~7 and z~8, which could be expected if the metallicity of [OIII] emitters, as\nwell as the cosmic star-formation rate density, is declining at these\nredshifts. We find that theoretical models that reproduce the z~7,8 [OIII] LF\ndo not reproduce well the [OIII] equivalent width distribution, pointing to\npotential challenges in the modelling of[OIII] and other nebular lines in the\nearly Universe. Finally, we provide the first constraints on the cosmic\nvariance of [OIII] emitters, estimating at 15% the relative uncertainty for the\nz~7,8 [OIII] LF in the 0.3 deg$^2$ field. This estimate is in good agreement\nwith that inferred from clustering, and shows that the [OIII] LF derived from\nsmaller extragalactic legacy fields is strongly affected by cosmic variance.\nOur results highlight the fundamental role that wide-area JWST slitless surveys\nplay to map the galaxy large-scale structure down into the reionisation era,\nserving as a springboard for a variety of science cases."
                },
                "authors": [
                    {
                        "name": "Romain A. Meyer"
                    },
                    {
                        "name": "Feige Wang"
                    },
                    {
                        "name": "Koki Kakiichi"
                    },
                    {
                        "name": "Gabe Brammer"
                    },
                    {
                        "name": "Jackie Champagne"
                    },
                    {
                        "name": "Katharina Jurk"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Marat Musin"
                    },
                    {
                        "name": "Sindhu Satyavolu"
                    },
                    {
                        "name": "Jan-Torge Schindler"
                    },
                    {
                        "name": "Marko Shuntov"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Siwei Zou"
                    },
                    {
                        "name": "Fuyan Bian"
                    },
                    {
                        "name": "Caitlin Casey"
                    },
                    {
                        "name": "Eiichi Egami"
                    },
                    {
                        "name": "Xiaohui Fan"
                    },
                    {
                        "name": "Danyang Jiang"
                    },
                    {
                        "name": "Nicolas Laporte"
                    },
                    {
                        "name": "Weizhe Liu"
                    },
                    {
                        "name": "Pascal Oesch"
                    },
                    {
                        "name": "Lidia Tasca"
                    },
                    {
                        "name": "Jinyi Yang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Hollis Akins"
                    },
                    {
                        "name": "Zheng Cai"
                    },
                    {
                        "name": "Dave A. Coulter"
                    },
                    {
                        "name": "Jiamu Huang"
                    },
                    {
                        "name": "Mingyu Li"
                    },
                    {
                        "name": "Weizhe Liu"
                    },
                    {
                        "name": "Yongming Liang"
                    },
                    {
                        "name": "Xiangyu Jin"
                    },
                    {
                        "name": "Jeyhan Kartaltepe"
                    },
                    {
                        "name": "Jasleen Matharu"
                    },
                    {
                        "name": "Maria Pudoka"
                    },
                    {
                        "name": "Wei-Leong Tee"
                    },
                    {
                        "name": "Callum Witten"
                    },
                    {
                        "name": "Haowen Zhang"
                    },
                    {
                        "name": "Yongda Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yongda Zhu"
                },
                "author": "Yongda Zhu",
                "arxiv_comment": "Submitted to A&A. 10 pages + appendices. [OIII] catalogue release\n  after acceptance. Comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11370v1",
                "updated": "2025-10-13T13:11:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    11,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:11:27Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    11,
                    27,
                    0,
                    286,
                    0
                ],
                "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and\n  Inference Routers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilizing MoE Reinforcement Learning by Aligning Training and\n  Inference Routers"
                },
                "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing\nthe capabilities of large language models. However, in Mixture-of-Experts (MoE)\nmodels, the routing mechanism often introduces instability, even leading to\ncatastrophic RL training collapse. We analyze the training-inference\nconsistency of MoE models and identify a notable discrepancy in routing\nbehaviors between the two phases. Moreover, even under identical conditions,\nthe routing framework can yield divergent expert selections across repeated\nforward passes. To address this foundational inconsistency, we propose Rollout\nRouting Replay (R3), a method that records routing distributions from the\ninference engine and replays them during training. R3 significantly reduces\ntraining-inference policy KL divergence and mitigates extreme discrepancies\nwithout compromising training speed. Extensive experiments on various settings\nconfirm that R3 succeeds in stabilizing RL training, preventing collapse and\noutperforming methods such as GSPO and TIS. We believe this work can offer a\nnew solution for stabilizing RL in MoE models."
                },
                "authors": [
                    {
                        "name": "Wenhan Ma"
                    },
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Yudong Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Fuli Luo"
                    }
                ],
                "author_detail": {
                    "name": "Fuli Luo"
                },
                "author": "Fuli Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11369v1",
                "updated": "2025-10-13T13:11:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    11,
                    8,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:11:08Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    11,
                    8,
                    0,
                    286,
                    0
                ],
                "title": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in\n  Image Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in\n  Image Quality Assessment"
                },
                "summary": "Reasoning-based image quality assessment (IQA) models trained through\nreinforcement learning (RL) exhibit exceptional generalization, yet the\nunderlying mechanisms and critical factors driving this capability remain\nunderexplored in current research. Moreover, despite their superior\nperformance, these models incur inference energy usage and latency orders of\nmagnitude higher than their earlier counterparts, restricting their deployment\nin specific scenarios. Through extensive experiments, this paper verifies and\nelaborates that through RL training, MLLMs leverage their reasoning capability\nto convert redundant visual representations into compact, cross-domain aligned\ntext representations. This conversion is precisely the source of the\ngeneralization exhibited by these reasoning-based IQA models. Building on this\nfundamental insight, we propose a novel algorithm, RALI, which employs\ncontrastive learning to directly align images with these generalizable text\nrepresentations learned by RL. This approach eliminates the reliance on\nreasoning processes and even obviates the need to load an LLM. For the quality\nscoring task, this framework achieves generalization performance comparable to\nreasoning-based models while requiring less than 5% of their model parameters\nand inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-based image quality assessment (IQA) models trained through\nreinforcement learning (RL) exhibit exceptional generalization, yet the\nunderlying mechanisms and critical factors driving this capability remain\nunderexplored in current research. Moreover, despite their superior\nperformance, these models incur inference energy usage and latency orders of\nmagnitude higher than their earlier counterparts, restricting their deployment\nin specific scenarios. Through extensive experiments, this paper verifies and\nelaborates that through RL training, MLLMs leverage their reasoning capability\nto convert redundant visual representations into compact, cross-domain aligned\ntext representations. This conversion is precisely the source of the\ngeneralization exhibited by these reasoning-based IQA models. Building on this\nfundamental insight, we propose a novel algorithm, RALI, which employs\ncontrastive learning to directly align images with these generalizable text\nrepresentations learned by RL. This approach eliminates the reliance on\nreasoning processes and even obviates the need to load an LLM. For the quality\nscoring task, this framework achieves generalization performance comparable to\nreasoning-based models while requiring less than 5% of their model parameters\nand inference time."
                },
                "authors": [
                    {
                        "name": "Shijie Zhao"
                    },
                    {
                        "name": "Xuanyu Zhang"
                    },
                    {
                        "name": "Weiqi Li"
                    },
                    {
                        "name": "Junlin Li"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Tianfan Xue"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24000v2",
                "updated": "2025-10-13T13:09:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    9,
                    11,
                    0,
                    286,
                    0
                ],
                "published": "2025-06-30T16:05:55Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    5,
                    55,
                    0,
                    181,
                    0
                ],
                "title": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for\n  Vision-Language Models"
                },
                "summary": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and make it difficult to assess their practical strengths\nand weaknesses. To address these challenges, we introduce TTA-VLM, a\ncomprehensive benchmark for evaluating TTA methods on VLMs. Our benchmark\nimplements 8 episodic TTA and 7 online TTA methods within a unified and\nreproducible framework, and evaluates them across 15 widely used datasets.\nUnlike prior studies focused solely on CLIP, we extend the evaluation to\nSigLIP--a model trained with a Sigmoid loss--and include training-time tuning\nmethods such as CoOp, MaPLe, and TeCoA to assess generality. Beyond\nclassification accuracy, TTA-VLM incorporates various evaluation metrics,\nincluding robustness, calibration, out-of-distribution detection, and\nstability, enabling a more holistic assessment of TTA methods. Through\nextensive experiments, we find that 1) existing TTA methods produce limited\ngains compared to the previous pioneering work; 2) current TTA methods exhibit\npoor collaboration with training-time fine-tuning methods; 3) accuracy gains\nfrequently come at the cost of reduced model trustworthiness. We release\nTTA-VLM to provide fair comparison and comprehensive evaluation of TTA methods\nfor VLMs, and we hope it encourages the community to develop more reliable and\ngeneralizable TTA strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and make it difficult to assess their practical strengths\nand weaknesses. To address these challenges, we introduce TTA-VLM, a\ncomprehensive benchmark for evaluating TTA methods on VLMs. Our benchmark\nimplements 8 episodic TTA and 7 online TTA methods within a unified and\nreproducible framework, and evaluates them across 15 widely used datasets.\nUnlike prior studies focused solely on CLIP, we extend the evaluation to\nSigLIP--a model trained with a Sigmoid loss--and include training-time tuning\nmethods such as CoOp, MaPLe, and TeCoA to assess generality. Beyond\nclassification accuracy, TTA-VLM incorporates various evaluation metrics,\nincluding robustness, calibration, out-of-distribution detection, and\nstability, enabling a more holistic assessment of TTA methods. Through\nextensive experiments, we find that 1) existing TTA methods produce limited\ngains compared to the previous pioneering work; 2) current TTA methods exhibit\npoor collaboration with training-time fine-tuning methods; 3) accuracy gains\nfrequently come at the cost of reduced model trustworthiness. We release\nTTA-VLM to provide fair comparison and comprehensive evaluation of TTA methods\nfor VLMs, and we hope it encourages the community to develop more reliable and\ngeneralizable TTA strategies."
                },
                "authors": [
                    {
                        "name": "Lijun Sheng"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Zilei Wang"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "NeurIPS 2025 Datasets and Benchmarks Track. Github link:\n  https://github.com/TomSheng21/tta-vlm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25123v2",
                "updated": "2025-10-13T13:03:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    3,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T17:44:27Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    44,
                    27,
                    0,
                    272,
                    0
                ],
                "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by\n  Composing Old Ones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by\n  Composing Old Ones"
                },
                "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems."
                },
                "authors": [
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Ziming You"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15881v2",
                "updated": "2025-10-13T13:01:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    1,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-04-22T13:25:40Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    13,
                    25,
                    40,
                    1,
                    112,
                    0
                ],
                "title": "Measurement of the time-integrated $CP$ asymmetry in $D^0 \\to K^0_{\\rm\n  S} K^0_{\\rm S}$ decays using opposite-side flavor tagging at Belle and Belle\n  II",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of the time-integrated $CP$ asymmetry in $D^0 \\to K^0_{\\rm\n  S} K^0_{\\rm S}$ decays using opposite-side flavor tagging at Belle and Belle\n  II"
                },
                "summary": "We measure the time-integrated $CP$ asymmetry in $D^0 \\to K^0_{\\rm S}\nK^0_{\\rm S}$ decays reconstructed in $e^+e^-\\to c{\\overline c}$ events\ncollected by the Belle and Belle II experiments. The corresponding data samples\nhave integrated luminosities of 980 and 428 fb${}^{-1}$, respectively. To infer\nthe flavor of the $D^0$ meson, we exploit the correlation between the flavor of\nthe reconstructed decay and the electric charges of particles reconstructed in\nthe rest of the $e^+e^-\\to c{\\overline c}$ event. This results in a sample\nwhich is independent from any other previously used at Belle or Belle II. The\nresult, $A_{CP}(D^0 \\to K^0_{\\rm S} K^0_{\\rm S}) = (1.3 \\pm 2.0 \\pm 0.2)\\%$,\nwhere the first uncertainty is statistical and the second systematic, is\nconsistent with previous determinations and with $CP$ symmetry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We measure the time-integrated $CP$ asymmetry in $D^0 \\to K^0_{\\rm S}\nK^0_{\\rm S}$ decays reconstructed in $e^+e^-\\to c{\\overline c}$ events\ncollected by the Belle and Belle II experiments. The corresponding data samples\nhave integrated luminosities of 980 and 428 fb${}^{-1}$, respectively. To infer\nthe flavor of the $D^0$ meson, we exploit the correlation between the flavor of\nthe reconstructed decay and the electric charges of particles reconstructed in\nthe rest of the $e^+e^-\\to c{\\overline c}$ event. This results in a sample\nwhich is independent from any other previously used at Belle or Belle II. The\nresult, $A_{CP}(D^0 \\to K^0_{\\rm S} K^0_{\\rm S}) = (1.3 \\pm 2.0 \\pm 0.2)\\%$,\nwhere the first uncertainty is statistical and the second systematic, is\nconsistent with previous determinations and with $CP$ symmetry."
                },
                "authors": [
                    {
                        "name": "Belle"
                    },
                    {
                        "name": "Belle II Collaborations"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "I. Adachi"
                    },
                    {
                        "name": "Y. Ahn"
                    },
                    {
                        "name": "N. Akopov"
                    },
                    {
                        "name": "S. Alghamdi"
                    },
                    {
                        "name": "M. Alhakami"
                    },
                    {
                        "name": "A. Aloisio"
                    },
                    {
                        "name": "N. Althubiti"
                    },
                    {
                        "name": "K. Amos"
                    },
                    {
                        "name": "M. Angelsmark"
                    },
                    {
                        "name": "N. Anh Ky"
                    },
                    {
                        "name": "C. Antonioli"
                    },
                    {
                        "name": "D. M. Asner"
                    },
                    {
                        "name": "H. Atmacan"
                    },
                    {
                        "name": "T. Aushev"
                    },
                    {
                        "name": "M. Aversano"
                    },
                    {
                        "name": "R. Ayad"
                    },
                    {
                        "name": "V. Babu"
                    },
                    {
                        "name": "H. Bae"
                    },
                    {
                        "name": "N. K. Baghel"
                    },
                    {
                        "name": "S. Bahinipati"
                    },
                    {
                        "name": "P. Bambade"
                    },
                    {
                        "name": "Sw. Banerjee"
                    },
                    {
                        "name": "M. Barrett"
                    },
                    {
                        "name": "M. Bartl"
                    },
                    {
                        "name": "J. Baudot"
                    },
                    {
                        "name": "A. Baur"
                    },
                    {
                        "name": "A. Beaubien"
                    },
                    {
                        "name": "F. Becherer"
                    },
                    {
                        "name": "J. Becker"
                    },
                    {
                        "name": "J. V. Bennett"
                    },
                    {
                        "name": "F. U. Bernlochner"
                    },
                    {
                        "name": "V. Bertacchi"
                    },
                    {
                        "name": "M. Bertemes"
                    },
                    {
                        "name": "E. Bertholet"
                    },
                    {
                        "name": "M. Bessner"
                    },
                    {
                        "name": "S. Bettarini"
                    },
                    {
                        "name": "B. Bhuyan"
                    },
                    {
                        "name": "F. Bianchi"
                    },
                    {
                        "name": "T. Bilka"
                    },
                    {
                        "name": "D. Biswas"
                    },
                    {
                        "name": "A. Bobrov"
                    },
                    {
                        "name": "D. Bodrov"
                    },
                    {
                        "name": "A. Bondar"
                    },
                    {
                        "name": "J. Borah"
                    },
                    {
                        "name": "A. Boschetti"
                    },
                    {
                        "name": "A. Bozek"
                    },
                    {
                        "name": "M. Bračko"
                    },
                    {
                        "name": "P. Branchini"
                    },
                    {
                        "name": "R. A. Briere"
                    },
                    {
                        "name": "T. E. Browder"
                    },
                    {
                        "name": "A. Budano"
                    },
                    {
                        "name": "S. Bussino"
                    },
                    {
                        "name": "M. Campajola"
                    },
                    {
                        "name": "L. Cao"
                    },
                    {
                        "name": "G. Casarosa"
                    },
                    {
                        "name": "C. Cecchi"
                    },
                    {
                        "name": "P. Cheema"
                    },
                    {
                        "name": "B. G. Cheon"
                    },
                    {
                        "name": "K. Chilikin"
                    },
                    {
                        "name": "J. Chin"
                    },
                    {
                        "name": "K. Chirapatpimol"
                    },
                    {
                        "name": "H. -E. Cho"
                    },
                    {
                        "name": "K. Cho"
                    },
                    {
                        "name": "S. -J. Cho"
                    },
                    {
                        "name": "S. -K. Choi"
                    },
                    {
                        "name": "S. Choudhury"
                    },
                    {
                        "name": "I. Consigny"
                    },
                    {
                        "name": "L. Corona"
                    },
                    {
                        "name": "J. X. Cui"
                    },
                    {
                        "name": "E. De La Cruz-Burelo"
                    },
                    {
                        "name": "S. A. De La Motte"
                    },
                    {
                        "name": "G. de Marino"
                    },
                    {
                        "name": "G. De Pietro"
                    },
                    {
                        "name": "R. de Sangro"
                    },
                    {
                        "name": "M. Destefanis"
                    },
                    {
                        "name": "A. Di Canto"
                    },
                    {
                        "name": "J. Dingfelder"
                    },
                    {
                        "name": "Z. Doležal"
                    },
                    {
                        "name": "I. Domínguez Jiménez"
                    },
                    {
                        "name": "T. V. Dong"
                    },
                    {
                        "name": "M. Dorigo"
                    },
                    {
                        "name": "G. Dujany"
                    },
                    {
                        "name": "P. Ecker"
                    },
                    {
                        "name": "D. Epifanov"
                    },
                    {
                        "name": "R. Farkas"
                    },
                    {
                        "name": "P. Feichtinger"
                    },
                    {
                        "name": "T. Ferber"
                    },
                    {
                        "name": "T. Fillinger"
                    },
                    {
                        "name": "C. Finck"
                    },
                    {
                        "name": "G. Finocchiaro"
                    },
                    {
                        "name": "A. Fodor"
                    },
                    {
                        "name": "F. Forti"
                    },
                    {
                        "name": "B. G. Fulsom"
                    },
                    {
                        "name": "A. Gabrielli"
                    },
                    {
                        "name": "A. Gale"
                    },
                    {
                        "name": "E. Ganiev"
                    },
                    {
                        "name": "M. Garcia-Hernandez"
                    },
                    {
                        "name": "R. Garg"
                    },
                    {
                        "name": "G. Gaudino"
                    },
                    {
                        "name": "V. Gaur"
                    },
                    {
                        "name": "V. Gautam"
                    },
                    {
                        "name": "A. Gaz"
                    },
                    {
                        "name": "A. Gellrich"
                    },
                    {
                        "name": "D. Ghosh"
                    },
                    {
                        "name": "H. Ghumaryan"
                    },
                    {
                        "name": "G. Giakoustidis"
                    },
                    {
                        "name": "R. Giordano"
                    },
                    {
                        "name": "A. Giri"
                    },
                    {
                        "name": "P. Gironella Gironell"
                    },
                    {
                        "name": "B. Gobbo"
                    },
                    {
                        "name": "R. Godang"
                    },
                    {
                        "name": "O. Gogota"
                    },
                    {
                        "name": "P. Goldenzweig"
                    },
                    {
                        "name": "W. Gradl"
                    },
                    {
                        "name": "E. Graziani"
                    },
                    {
                        "name": "D. Greenwald"
                    },
                    {
                        "name": "Z. Gruberová"
                    },
                    {
                        "name": "Y. Guan"
                    },
                    {
                        "name": "K. Gudkova"
                    },
                    {
                        "name": "I. Haide"
                    },
                    {
                        "name": "Y. Han"
                    },
                    {
                        "name": "H. Hayashii"
                    },
                    {
                        "name": "S. Hazra"
                    },
                    {
                        "name": "C. Hearty"
                    },
                    {
                        "name": "M. T. Hedges"
                    },
                    {
                        "name": "A. Heidelbach"
                    },
                    {
                        "name": "G. Heine"
                    },
                    {
                        "name": "I. Heredia de la Cruz"
                    },
                    {
                        "name": "M. Hernández Villanueva"
                    },
                    {
                        "name": "T. Higuchi"
                    },
                    {
                        "name": "M. Hoek"
                    },
                    {
                        "name": "M. Hohmann"
                    },
                    {
                        "name": "P. Horak"
                    },
                    {
                        "name": "C. -L. Hsu"
                    },
                    {
                        "name": "T. Humair"
                    },
                    {
                        "name": "T. Iijima"
                    },
                    {
                        "name": "K. Inami"
                    },
                    {
                        "name": "G. Inguglia"
                    },
                    {
                        "name": "N. Ipsita"
                    },
                    {
                        "name": "A. Ishikawa"
                    },
                    {
                        "name": "R. Itoh"
                    },
                    {
                        "name": "M. Iwasaki"
                    },
                    {
                        "name": "P. Jackson"
                    },
                    {
                        "name": "D. Jacobi"
                    },
                    {
                        "name": "W. W. Jacobs"
                    },
                    {
                        "name": "D. E. Jaffe"
                    },
                    {
                        "name": "Q. P. Ji"
                    },
                    {
                        "name": "S. Jia"
                    },
                    {
                        "name": "Y. Jin"
                    },
                    {
                        "name": "A. Johnson"
                    },
                    {
                        "name": "J. Kandra"
                    },
                    {
                        "name": "K. H. Kang"
                    },
                    {
                        "name": "G. Karyan"
                    },
                    {
                        "name": "T. Kawasaki"
                    },
                    {
                        "name": "F. Keil"
                    },
                    {
                        "name": "C. Ketter"
                    },
                    {
                        "name": "M. Khan"
                    },
                    {
                        "name": "C. Kiesling"
                    },
                    {
                        "name": "D. Y. Kim"
                    },
                    {
                        "name": "J. -Y. Kim"
                    },
                    {
                        "name": "K. -H. Kim"
                    },
                    {
                        "name": "K. Kinoshita"
                    },
                    {
                        "name": "P. Kodyš"
                    },
                    {
                        "name": "T. Koga"
                    },
                    {
                        "name": "S. Kohani"
                    },
                    {
                        "name": "K. Kojima"
                    },
                    {
                        "name": "A. Korobov"
                    },
                    {
                        "name": "S. Korpar"
                    },
                    {
                        "name": "E. Kovalenko"
                    },
                    {
                        "name": "R. Kowalewski"
                    },
                    {
                        "name": "P. Križan"
                    },
                    {
                        "name": "P. Krokovny"
                    },
                    {
                        "name": "K. Kumara"
                    },
                    {
                        "name": "T. Kunigo"
                    },
                    {
                        "name": "A. Kuzmin"
                    },
                    {
                        "name": "Y. -J. Kwon"
                    },
                    {
                        "name": "K. Lalwani"
                    },
                    {
                        "name": "T. Lam"
                    },
                    {
                        "name": "J. S. Lange"
                    },
                    {
                        "name": "T. S. Lau"
                    },
                    {
                        "name": "M. Laurenza"
                    },
                    {
                        "name": "R. Leboucher"
                    },
                    {
                        "name": "F. R. Le Diberder"
                    },
                    {
                        "name": "M. J. Lee"
                    },
                    {
                        "name": "C. Lemettais"
                    },
                    {
                        "name": "P. Leo"
                    },
                    {
                        "name": "P. M. Lewis"
                    },
                    {
                        "name": "H. -J. Li"
                    },
                    {
                        "name": "L. K. Li"
                    },
                    {
                        "name": "Q. M. Li"
                    },
                    {
                        "name": "W. Z. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "Y. B. Li"
                    },
                    {
                        "name": "Y. P. Liao"
                    },
                    {
                        "name": "J. Libby"
                    },
                    {
                        "name": "J. Lin"
                    },
                    {
                        "name": "S. Lin"
                    },
                    {
                        "name": "V. Lisovskyi"
                    },
                    {
                        "name": "M. H. Liu"
                    },
                    {
                        "name": "Q. Y. Liu"
                    },
                    {
                        "name": "Y. Liu"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "D. Liventsev"
                    },
                    {
                        "name": "S. Longo"
                    },
                    {
                        "name": "C. Lyu"
                    },
                    {
                        "name": "Y. Ma"
                    },
                    {
                        "name": "C. Madaan"
                    },
                    {
                        "name": "M. Maggiora"
                    },
                    {
                        "name": "S. P. Maharana"
                    },
                    {
                        "name": "R. Maiti"
                    },
                    {
                        "name": "G. Mancinelli"
                    },
                    {
                        "name": "R. Manfredi"
                    },
                    {
                        "name": "E. Manoni"
                    },
                    {
                        "name": "M. Mantovano"
                    },
                    {
                        "name": "D. Marcantonio"
                    },
                    {
                        "name": "S. Marcello"
                    },
                    {
                        "name": "C. Marinas"
                    },
                    {
                        "name": "C. Martellini"
                    },
                    {
                        "name": "A. Martens"
                    },
                    {
                        "name": "T. Martinov"
                    },
                    {
                        "name": "L. Massaccesi"
                    },
                    {
                        "name": "M. Masuda"
                    },
                    {
                        "name": "S. K. Maurya"
                    },
                    {
                        "name": "M. Maushart"
                    },
                    {
                        "name": "J. A. McKenna"
                    },
                    {
                        "name": "F. Meier"
                    },
                    {
                        "name": "M. Merola"
                    },
                    {
                        "name": "C. Miller"
                    },
                    {
                        "name": "M. Mirra"
                    },
                    {
                        "name": "S. Mitra"
                    },
                    {
                        "name": "K. Miyabayashi"
                    },
                    {
                        "name": "R. Mizuk"
                    },
                    {
                        "name": "G. B. Mohanty"
                    },
                    {
                        "name": "S. Moneta"
                    },
                    {
                        "name": "H. -G. Moser"
                    },
                    {
                        "name": "M. Nakao"
                    },
                    {
                        "name": "H. Nakazawa"
                    },
                    {
                        "name": "Y. Nakazawa"
                    },
                    {
                        "name": "M. Naruki"
                    },
                    {
                        "name": "Z. Natkaniec"
                    },
                    {
                        "name": "A. Natochii"
                    },
                    {
                        "name": "M. Nayak"
                    },
                    {
                        "name": "M. Neu"
                    },
                    {
                        "name": "S. Nishida"
                    },
                    {
                        "name": "S. Ogawa"
                    },
                    {
                        "name": "R. Okubo"
                    },
                    {
                        "name": "H. Ono"
                    },
                    {
                        "name": "E. R. Oxford"
                    },
                    {
                        "name": "G. Pakhlova"
                    },
                    {
                        "name": "S. Pardi"
                    },
                    {
                        "name": "K. Parham"
                    },
                    {
                        "name": "H. Park"
                    },
                    {
                        "name": "J. Park"
                    },
                    {
                        "name": "K. Park"
                    },
                    {
                        "name": "S. -H. Park"
                    },
                    {
                        "name": "A. Passeri"
                    },
                    {
                        "name": "S. Patra"
                    },
                    {
                        "name": "R. Pestotnik"
                    },
                    {
                        "name": "L. E. Piilonen"
                    },
                    {
                        "name": "P. L. M. Podesta-Lerma"
                    },
                    {
                        "name": "T. Podobnik"
                    },
                    {
                        "name": "A. Prakash"
                    },
                    {
                        "name": "C. Praz"
                    },
                    {
                        "name": "S. Prell"
                    },
                    {
                        "name": "E. Prencipe"
                    },
                    {
                        "name": "M. T. Prim"
                    },
                    {
                        "name": "S. Privalov"
                    },
                    {
                        "name": "H. Purwar"
                    },
                    {
                        "name": "P. Rados"
                    },
                    {
                        "name": "G. Raeuber"
                    },
                    {
                        "name": "S. Raiz"
                    },
                    {
                        "name": "V. Raj"
                    },
                    {
                        "name": "K. Ravindran"
                    },
                    {
                        "name": "J. U. Rehman"
                    },
                    {
                        "name": "M. Reif"
                    },
                    {
                        "name": "S. Reiter"
                    },
                    {
                        "name": "M. Remnev"
                    },
                    {
                        "name": "L. Reuter"
                    },
                    {
                        "name": "D. Ricalde Herrmann"
                    },
                    {
                        "name": "I. Ripp-Baudot"
                    },
                    {
                        "name": "G. Rizzo"
                    },
                    {
                        "name": "J. M. Roney"
                    },
                    {
                        "name": "A. Rostomyan"
                    },
                    {
                        "name": "N. Rout"
                    },
                    {
                        "name": "L. Salutari"
                    },
                    {
                        "name": "D. A. Sanders"
                    },
                    {
                        "name": "S. Sandilya"
                    },
                    {
                        "name": "L. Santelj"
                    },
                    {
                        "name": "V. Savinov"
                    },
                    {
                        "name": "B. Scavino"
                    },
                    {
                        "name": "C. Schmitt"
                    },
                    {
                        "name": "J. Schmitz"
                    },
                    {
                        "name": "S. Schneider"
                    },
                    {
                        "name": "G. Schnell"
                    },
                    {
                        "name": "M. Schnepf"
                    },
                    {
                        "name": "C. Schwanda"
                    },
                    {
                        "name": "A. J. Schwartz"
                    },
                    {
                        "name": "Y. Seino"
                    },
                    {
                        "name": "A. Selce"
                    },
                    {
                        "name": "K. Senyo"
                    },
                    {
                        "name": "J. Serrano"
                    },
                    {
                        "name": "M. E. Sevior"
                    },
                    {
                        "name": "C. Sfienti"
                    },
                    {
                        "name": "W. Shan"
                    },
                    {
                        "name": "X. D. Shi"
                    },
                    {
                        "name": "T. Shillington"
                    },
                    {
                        "name": "J. -G. Shiu"
                    },
                    {
                        "name": "D. Shtol"
                    },
                    {
                        "name": "B. Shwartz"
                    },
                    {
                        "name": "A. Sibidanov"
                    },
                    {
                        "name": "F. Simon"
                    },
                    {
                        "name": "J. Skorupa"
                    },
                    {
                        "name": "R. J. Sobie"
                    },
                    {
                        "name": "M. Sobotzik"
                    },
                    {
                        "name": "A. Soffer"
                    },
                    {
                        "name": "A. Sokolov"
                    },
                    {
                        "name": "E. Solovieva"
                    },
                    {
                        "name": "S. Spataro"
                    },
                    {
                        "name": "B. Spruck"
                    },
                    {
                        "name": "M. Starič"
                    },
                    {
                        "name": "P. Stavroulakis"
                    },
                    {
                        "name": "S. Stefkova"
                    },
                    {
                        "name": "L. Stoetzer"
                    },
                    {
                        "name": "R. Stroili"
                    },
                    {
                        "name": "Y. Sue"
                    },
                    {
                        "name": "M. Sumihama"
                    },
                    {
                        "name": "N. Suwonjandee"
                    },
                    {
                        "name": "H. Svidras"
                    },
                    {
                        "name": "M. Takizawa"
                    },
                    {
                        "name": "K. Tanida"
                    },
                    {
                        "name": "F. Tenchini"
                    },
                    {
                        "name": "F. Testa"
                    },
                    {
                        "name": "O. Tittel"
                    },
                    {
                        "name": "R. Tiwary"
                    },
                    {
                        "name": "E. Torassa"
                    },
                    {
                        "name": "K. Trabelsi"
                    },
                    {
                        "name": "F. F. Trantou"
                    },
                    {
                        "name": "I. Tsaklidis"
                    },
                    {
                        "name": "M. Uchida"
                    },
                    {
                        "name": "I. Ueda"
                    },
                    {
                        "name": "T. Uglov"
                    },
                    {
                        "name": "K. Unger"
                    },
                    {
                        "name": "Y. Unno"
                    },
                    {
                        "name": "K. Uno"
                    },
                    {
                        "name": "S. Uno"
                    },
                    {
                        "name": "Y. Ushiroda"
                    },
                    {
                        "name": "R. van Tonder"
                    },
                    {
                        "name": "K. E. Varvell"
                    },
                    {
                        "name": "M. Veronesi"
                    },
                    {
                        "name": "A. Vinokurova"
                    },
                    {
                        "name": "V. S. Vismaya"
                    },
                    {
                        "name": "L. Vitale"
                    },
                    {
                        "name": "R. Volpe"
                    },
                    {
                        "name": "A. Vossen"
                    },
                    {
                        "name": "S. Wallner"
                    },
                    {
                        "name": "M. -Z. Wang"
                    },
                    {
                        "name": "A. Warburton"
                    },
                    {
                        "name": "M. Watanabe"
                    },
                    {
                        "name": "S. Watanuki"
                    },
                    {
                        "name": "C. Wessel"
                    },
                    {
                        "name": "E. Won"
                    },
                    {
                        "name": "B. D. Yabsley"
                    },
                    {
                        "name": "S. Yamada"
                    },
                    {
                        "name": "W. Yan"
                    },
                    {
                        "name": "S. B. Yang"
                    },
                    {
                        "name": "J. Yelton"
                    },
                    {
                        "name": "J. H. Yin"
                    },
                    {
                        "name": "K. Yoshihara"
                    },
                    {
                        "name": "J. Yuan"
                    },
                    {
                        "name": "Y. Yusa"
                    },
                    {
                        "name": "L. Zani"
                    },
                    {
                        "name": "M. Zeyrek"
                    },
                    {
                        "name": "B. Zhang"
                    },
                    {
                        "name": "V. Zhilich"
                    },
                    {
                        "name": "J. S. Zhou"
                    },
                    {
                        "name": "Q. D. Zhou"
                    },
                    {
                        "name": "L. Zhu"
                    },
                    {
                        "name": "R. Žlebčík"
                    }
                ],
                "author_detail": {
                    "name": "R. Žlebčík"
                },
                "author": "R. Žlebčík",
                "arxiv_doi": "10.1103/8x1h-39dp",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/8x1h-39dp",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. D 112, 012017 (2025)",
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18254v2",
                "updated": "2025-10-13T12:58:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    58,
                    1,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-24T00:36:16Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    0,
                    36,
                    16,
                    0,
                    83,
                    0
                ],
                "title": "Surface-Aware Distilled 3D Semantic Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface-Aware Distilled 3D Semantic Features"
                },
                "summary": "Many 3D tasks such as pose alignment, animation, motion transfer, and 3D\nreconstruction rely on establishing correspondences between 3D shapes. This\nchallenge has recently been approached by pairwise matching of semantic\nfeatures from pre-trained vision models. However, despite their power, these\nfeatures struggle to differentiate instances of the same semantic class such as\n``left hand'' versus ``right hand'' which leads to substantial mapping errors.\nTo solve this, we learn a surface-aware embedding space that is robust to these\nambiguities while facilitating shared mapping for an entire family of 3D\nshapes. Importantly, our approach is self-supervised and requires only a small\nnumber of unpaired training meshes to infer features for new possibly imperfect\n3D shapes at test time. We achieve this by introducing a contrastive loss that\npreserves the semantic content of the features distilled from foundational\nmodels while disambiguating features located far apart on the shape's surface.\nWe observe superior performance in correspondence matching benchmarks and\nenable downstream applications including 2D-to-3D and 3D-to-3D texture\ntransfer, in-part segmentation, pose alignment, and motion transfer in low-data\nregimes. Unlike previous pairwise approaches, our solution constructs a joint\nembedding space, where both seen and unseen 3D shapes are implicitly aligned\nwithout further optimization. The code is available at\nhttps://graphics.tudelft.nl/SurfaceAware3DFeatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many 3D tasks such as pose alignment, animation, motion transfer, and 3D\nreconstruction rely on establishing correspondences between 3D shapes. This\nchallenge has recently been approached by pairwise matching of semantic\nfeatures from pre-trained vision models. However, despite their power, these\nfeatures struggle to differentiate instances of the same semantic class such as\n``left hand'' versus ``right hand'' which leads to substantial mapping errors.\nTo solve this, we learn a surface-aware embedding space that is robust to these\nambiguities while facilitating shared mapping for an entire family of 3D\nshapes. Importantly, our approach is self-supervised and requires only a small\nnumber of unpaired training meshes to infer features for new possibly imperfect\n3D shapes at test time. We achieve this by introducing a contrastive loss that\npreserves the semantic content of the features distilled from foundational\nmodels while disambiguating features located far apart on the shape's surface.\nWe observe superior performance in correspondence matching benchmarks and\nenable downstream applications including 2D-to-3D and 3D-to-3D texture\ntransfer, in-part segmentation, pose alignment, and motion transfer in low-data\nregimes. Unlike previous pairwise approaches, our solution constructs a joint\nembedding space, where both seen and unseen 3D shapes are implicitly aligned\nwithout further optimization. The code is available at\nhttps://graphics.tudelft.nl/SurfaceAware3DFeatures."
                },
                "authors": [
                    {
                        "name": "Lukas Uzolas"
                    },
                    {
                        "name": "Elmar Eisemann"
                    },
                    {
                        "name": "Petr Kellnhofer"
                    }
                ],
                "author_detail": {
                    "name": "Petr Kellnhofer"
                },
                "author": "Petr Kellnhofer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11358v1",
                "updated": "2025-10-13T12:57:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    57,
                    45,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:57:45Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    57,
                    45,
                    0,
                    286,
                    0
                ],
                "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. While traditional retrieval focuses on\nrelevance, RAG's effectiveness depends on the utility of retrieved passages,\ni.e., the usefulness in facilitating the generation of an accurate and\ncomprehensive answer. Existing studies often treat utility as a generic\nattribute, ignoring the fact that different LLMs may benefit differently from\nthe same passage due to variations in internal knowledge and comprehension\nability. In this work, we introduce and systematically investigate the notion\nof LLM-specific utility. Through large-scale experiments across multiple\ndatasets and LLMs, we demonstrate that human-annotated passages are not optimal\nfor LLMs and that ground-truth utilitarian passages are not transferable across\ndifferent LLMs. These findings highlight the necessity of adopting the\nLLM-specific utility in RAG research. Our findings indicate that some\nhuman-annotated passages are not ground-truth utilitarian passages for specific\nLLMs, partially due to the varying readability of queries and passages for\nLLMs, a tendency for which perplexity is a key metric. Based on these findings,\nwe propose a benchmarking procedure for LLM-specific utility judgments. We\nevaluate existing utility judgment methods on six datasets and find that while\nverbalized methods using pseudo-answers perform robustly, LLMs struggle to\nassess utility effectively-failing to reject all passages for known queries and\nto select truly useful ones for unknown queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. While traditional retrieval focuses on\nrelevance, RAG's effectiveness depends on the utility of retrieved passages,\ni.e., the usefulness in facilitating the generation of an accurate and\ncomprehensive answer. Existing studies often treat utility as a generic\nattribute, ignoring the fact that different LLMs may benefit differently from\nthe same passage due to variations in internal knowledge and comprehension\nability. In this work, we introduce and systematically investigate the notion\nof LLM-specific utility. Through large-scale experiments across multiple\ndatasets and LLMs, we demonstrate that human-annotated passages are not optimal\nfor LLMs and that ground-truth utilitarian passages are not transferable across\ndifferent LLMs. These findings highlight the necessity of adopting the\nLLM-specific utility in RAG research. Our findings indicate that some\nhuman-annotated passages are not ground-truth utilitarian passages for specific\nLLMs, partially due to the varying readability of queries and passages for\nLLMs, a tendency for which perplexity is a key metric. Based on these findings,\nwe propose a benchmarking procedure for LLM-specific utility judgments. We\nevaluate existing utility judgment methods on six datasets and find that while\nverbalized methods using pseudo-answers perform robustly, LLMs struggle to\nassess utility effectively-failing to reject all passages for known queries and\nto select truly useful ones for unknown queries."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11351v1",
                "updated": "2025-10-13T12:46:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    46,
                    49,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:46:49Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    46,
                    49,
                    0,
                    286,
                    0
                ],
                "title": "REBELS-IFU: Linking damped Lyman-$α$ absorption to [CII] emission\n  and dust content in the EoR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REBELS-IFU: Linking damped Lyman-$α$ absorption to [CII] emission\n  and dust content in the EoR"
                },
                "summary": "Neutral gas in galaxies during the Epoch of Reionisation regulates star\nformation, dust growth, and the escape of ionising photons, making it a key\ningredient in understanding both galaxy assembly and reionisation. Yet, direct\nconstraints on the HI content of galaxies at z>6 have been scarce. With JWST,\nLy$\\alpha$ damping wings in galaxy spectra can now provide a direct probe of\nthis neutral component. We analyse JWST/NIRSpec prism spectra of 12 UV-luminous\ngalaxies from the REBELS-IFU program at z~6.5-7.7, deriving HI column densities\nby modelling Ly$\\alpha$ damping wings. Significant damped Ly$\\alpha$ absorption\nis detected in eight galaxies, with $N_{\\mathrm{HI}}\\gtrsim10^{21}$ cm$^{-2}$.\nWe use the column densities and sizes derived for these sources to estimate\ntheir HI mass and compare with $L_{\\mathrm{[CII]}}$-$M_{\\mathrm{HI}}$\ncalibrations. The resulting HI masses show a tentative correlation with those\ninferred from [CII], although the [CII]-based estimates are systematically\nlarger, suggesting that the HI reservoirs may extend beyond the [CII]-emitting\ngas. We also combine the DLA-based measurements with FIR-derived dust-to-gas\nratios, dust attenuation, and gas-phase metallicities. No correlation is found\nbetween DLA-based and FIR-based dust-to-gas ratios, but combining the\nREBELS-IFU sample with literature samples at lower metallicities reveals a\nstrong correlation between $A_{\\mathrm{V}}/N_{\\mathrm{HI}}$ and metallicity.\nThese findings suggest that by $z\\sim7$ massive galaxies can already host\nsubstantial, enriched reservoirs of neutral gas and dust, consistent with\n$A_{\\mathrm{V}}$/$N_{\\mathrm{HI}}$-metallicity trends at lower redshift. At the\nhighest redshifts ($z>8$), however, we see tentative evidence for\nsystematically lower $A_{\\mathrm{V}}$/$N_{\\mathrm{HI}}$ at fixed metallicity,\nwhich may point to pristine gas accretion or more efficient dust\ndestruction/expulsion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutral gas in galaxies during the Epoch of Reionisation regulates star\nformation, dust growth, and the escape of ionising photons, making it a key\ningredient in understanding both galaxy assembly and reionisation. Yet, direct\nconstraints on the HI content of galaxies at z>6 have been scarce. With JWST,\nLy$\\alpha$ damping wings in galaxy spectra can now provide a direct probe of\nthis neutral component. We analyse JWST/NIRSpec prism spectra of 12 UV-luminous\ngalaxies from the REBELS-IFU program at z~6.5-7.7, deriving HI column densities\nby modelling Ly$\\alpha$ damping wings. Significant damped Ly$\\alpha$ absorption\nis detected in eight galaxies, with $N_{\\mathrm{HI}}\\gtrsim10^{21}$ cm$^{-2}$.\nWe use the column densities and sizes derived for these sources to estimate\ntheir HI mass and compare with $L_{\\mathrm{[CII]}}$-$M_{\\mathrm{HI}}$\ncalibrations. The resulting HI masses show a tentative correlation with those\ninferred from [CII], although the [CII]-based estimates are systematically\nlarger, suggesting that the HI reservoirs may extend beyond the [CII]-emitting\ngas. We also combine the DLA-based measurements with FIR-derived dust-to-gas\nratios, dust attenuation, and gas-phase metallicities. No correlation is found\nbetween DLA-based and FIR-based dust-to-gas ratios, but combining the\nREBELS-IFU sample with literature samples at lower metallicities reveals a\nstrong correlation between $A_{\\mathrm{V}}/N_{\\mathrm{HI}}$ and metallicity.\nThese findings suggest that by $z\\sim7$ massive galaxies can already host\nsubstantial, enriched reservoirs of neutral gas and dust, consistent with\n$A_{\\mathrm{V}}$/$N_{\\mathrm{HI}}$-metallicity trends at lower redshift. At the\nhighest redshifts ($z>8$), however, we see tentative evidence for\nsystematically lower $A_{\\mathrm{V}}$/$N_{\\mathrm{HI}}$ at fixed metallicity,\nwhich may point to pristine gas accretion or more efficient dust\ndestruction/expulsion."
                },
                "authors": [
                    {
                        "name": "Lucie E. Rowland"
                    },
                    {
                        "name": "Kasper E. Heintz"
                    },
                    {
                        "name": "Hiddo Algera"
                    },
                    {
                        "name": "Mauro Stefanon"
                    },
                    {
                        "name": "Jacqueline Hodge"
                    },
                    {
                        "name": "Rychard Bouwens"
                    },
                    {
                        "name": "Manuel Aravena"
                    },
                    {
                        "name": "Elisabete da Cunha"
                    },
                    {
                        "name": "Pratika Dayal"
                    },
                    {
                        "name": "Andrea Ferrara"
                    },
                    {
                        "name": "Rebecca Fisher"
                    },
                    {
                        "name": "Valentino González"
                    },
                    {
                        "name": "Hanae Inami"
                    },
                    {
                        "name": "Olena Komarova"
                    },
                    {
                        "name": "Ilse de Looze"
                    },
                    {
                        "name": "Themiya Nanayakkara"
                    },
                    {
                        "name": "Katherine Ormerod"
                    },
                    {
                        "name": "Andrea Pallottini"
                    },
                    {
                        "name": "Clara L. Pollock"
                    },
                    {
                        "name": "Renske Smit"
                    },
                    {
                        "name": "Paul van der Werf"
                    },
                    {
                        "name": "Joris Witstok"
                    }
                ],
                "author_detail": {
                    "name": "Joris Witstok"
                },
                "author": "Joris Witstok",
                "arxiv_comment": "17 pages, 9 figures. Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11347v1",
                "updated": "2025-10-13T12:42:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    42,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:42:00Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    42,
                    0,
                    0,
                    286,
                    0
                ],
                "title": "Multi-View Graph Feature Propagation for Privacy Preservation and\n  Feature Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-View Graph Feature Propagation for Privacy Preservation and\n  Feature Sparsity"
                },
                "summary": "Graph Neural Networks (GNNs) have demonstrated remarkable success in node\nclassification tasks over relational data, yet their effectiveness often\ndepends on the availability of complete node features. In many real-world\nscenarios, however, feature matrices are highly sparse or contain sensitive\ninformation, leading to degraded performance and increased privacy risks.\nFurthermore, direct exposure of information can result in unintended data\nleakage, enabling adversaries to infer sensitive information. To address these\nchallenges, we propose a novel Multi-view Feature Propagation (MFP) framework\nthat enhances node classification under feature sparsity while promoting\nprivacy preservation. MFP extends traditional Feature Propagation (FP) by\ndividing the available features into multiple Gaussian-noised views, each\npropagating information independently through the graph topology. The\naggregated representations yield expressive and robust node embeddings. This\nframework is novel in two respects: it introduces a mechanism that improves\nrobustness under extreme sparsity, and it provides a principled way to balance\nutility with privacy. Extensive experiments conducted on graph datasets\ndemonstrate that MFP outperforms state-of-the-art baselines in node\nclassification while substantially reducing privacy leakage. Moreover, our\nanalysis demonstrates that propagated outputs serve as alternative imputations\nrather than reconstructions of the original features, preserving utility\nwithout compromising privacy. A comprehensive sensitivity analysis further\nconfirms the stability and practical applicability of MFP across diverse\nscenarios. Overall, MFP provides an effective and privacy-aware framework for\ngraph learning in domains characterized by missing or sensitive features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have demonstrated remarkable success in node\nclassification tasks over relational data, yet their effectiveness often\ndepends on the availability of complete node features. In many real-world\nscenarios, however, feature matrices are highly sparse or contain sensitive\ninformation, leading to degraded performance and increased privacy risks.\nFurthermore, direct exposure of information can result in unintended data\nleakage, enabling adversaries to infer sensitive information. To address these\nchallenges, we propose a novel Multi-view Feature Propagation (MFP) framework\nthat enhances node classification under feature sparsity while promoting\nprivacy preservation. MFP extends traditional Feature Propagation (FP) by\ndividing the available features into multiple Gaussian-noised views, each\npropagating information independently through the graph topology. The\naggregated representations yield expressive and robust node embeddings. This\nframework is novel in two respects: it introduces a mechanism that improves\nrobustness under extreme sparsity, and it provides a principled way to balance\nutility with privacy. Extensive experiments conducted on graph datasets\ndemonstrate that MFP outperforms state-of-the-art baselines in node\nclassification while substantially reducing privacy leakage. Moreover, our\nanalysis demonstrates that propagated outputs serve as alternative imputations\nrather than reconstructions of the original features, preserving utility\nwithout compromising privacy. A comprehensive sensitivity analysis further\nconfirms the stability and practical applicability of MFP across diverse\nscenarios. Overall, MFP provides an effective and privacy-aware framework for\ngraph learning in domains characterized by missing or sensitive features."
                },
                "authors": [
                    {
                        "name": "Etzion Harari"
                    },
                    {
                        "name": "Moshe Unger"
                    }
                ],
                "author_detail": {
                    "name": "Moshe Unger"
                },
                "author": "Moshe Unger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.11718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11718v1",
                "updated": "2025-10-13T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    59,
                    55,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    59,
                    55,
                    0,
                    286,
                    0
                ],
                "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images"
                },
                "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT."
                },
                "authors": [
                    {
                        "name": "Chengqi Duan"
                    },
                    {
                        "name": "Kaiyue Sun"
                    },
                    {
                        "name": "Rongyao Fang"
                    },
                    {
                        "name": "Manyuan Zhang"
                    },
                    {
                        "name": "Yan Feng"
                    },
                    {
                        "name": "Ying Luo"
                    },
                    {
                        "name": "Yufang Liu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Peng Pei"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11701v1",
                "updated": "2025-10-13T17:57:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    57,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:57:15Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    57,
                    15,
                    0,
                    286,
                    0
                ],
                "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Reinforcement Learning in Agentic Reasoning"
                },
                "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL"
                },
                "authors": [
                    {
                        "name": "Zhaochen Yu"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Jiaru Zou"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and models: https://github.com/Gen-Verse/Open-AgentRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11696v1",
                "updated": "2025-10-13T17:55:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    55,
                    9,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:55:09Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    55,
                    9,
                    0,
                    286,
                    0
                ],
                "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs"
                },
                "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yi Ge"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Huizi Mao"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Ka Chun Cheung"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code is available at https://github.com/NVlabs/QeRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11695v1",
                "updated": "2025-10-13T17:54:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    54,
                    9,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:54:09Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    54,
                    9,
                    0,
                    286,
                    0
                ],
                "title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents"
                },
                "summary": "Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Xueqing Peng"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Vincent Jim Zhang"
                    },
                    {
                        "name": "Huan He"
                    },
                    {
                        "name": "Hanley Smith"
                    },
                    {
                        "name": "Yi Han"
                    },
                    {
                        "name": "Yueru He"
                    },
                    {
                        "name": "Haohang Li"
                    },
                    {
                        "name": "Yupeng Cao"
                    },
                    {
                        "name": "Yangyang Yu"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Peng Lu"
                    },
                    {
                        "name": "Jian-Yun Nie"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11694v1",
                "updated": "2025-10-13T17:54:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    54,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:54:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    54,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "Operand Quant: A Single-Agent Architecture for Autonomous Machine\n  Learning Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operand Quant: A Single-Agent Architecture for Autonomous Machine\n  Learning Engineering"
                },
                "summary": "We present Operand Quant, a single-agent, IDE-based architecture for\nautonomous machine learning engineering (MLE). Operand Quant departs from\nconventional multi-agent orchestration frameworks by consolidating all MLE\nlifecycle stages -- exploration, modeling, experimentation, and deployment --\nwithin a single, context-aware agent. On the MLE-Benchmark (2025), Operand\nQuant achieved a new state-of-the-art (SOTA) result, with an overall medal rate\nof 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance\namong all evaluated systems to date. The architecture demonstrates that a\nlinear, non-blocking agent, operating autonomously within a controlled IDE\nenvironment, can outperform multi-agent and orchestrated systems under\nidentical constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Operand Quant, a single-agent, IDE-based architecture for\nautonomous machine learning engineering (MLE). Operand Quant departs from\nconventional multi-agent orchestration frameworks by consolidating all MLE\nlifecycle stages -- exploration, modeling, experimentation, and deployment --\nwithin a single, context-aware agent. On the MLE-Benchmark (2025), Operand\nQuant achieved a new state-of-the-art (SOTA) result, with an overall medal rate\nof 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance\namong all evaluated systems to date. The architecture demonstrates that a\nlinear, non-blocking agent, operating autonomously within a controlled IDE\nenvironment, can outperform multi-agent and orchestrated systems under\nidentical constraints."
                },
                "authors": [
                    {
                        "name": "Arjun Sahney"
                    },
                    {
                        "name": "Ram Gorthi"
                    },
                    {
                        "name": "Cezary Łastowski"
                    },
                    {
                        "name": "Javier Vega"
                    }
                ],
                "author_detail": {
                    "name": "Javier Vega"
                },
                "author": "Javier Vega",
                "arxiv_comment": "8 pages. No figures. Evaluated on MLE-Benchmark 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11688v1",
                "updated": "2025-10-13T17:50:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    50,
                    25,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:50:25Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    50,
                    25,
                    0,
                    286,
                    0
                ],
                "title": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation\n  Capabilities"
                },
                "summary": "The increasing autonomy of Large Language Models (LLMs) necessitates a\nrigorous evaluation of their potential to aid in cyber offense. Existing\nbenchmarks often lack real-world complexity and are thus unable to accurately\nassess LLMs' cybersecurity capabilities. To address this gap, we introduce\nPACEbench, a practical AI cyber-exploitation benchmark built on the principles\nof realistic vulnerability difficulty, environmental complexity, and cyber\ndefenses. Specifically, PACEbench comprises four scenarios spanning single,\nblended, chained, and defense vulnerability exploitations. To handle these\ncomplex challenges, we propose PACEagent, a novel agent that emulates human\npenetration testers by supporting multi-phase reconnaissance, analysis, and\nexploitation. Extensive experiments with seven frontier LLMs demonstrate that\ncurrent models struggle with complex cyber scenarios, and none can bypass\ndefenses. These findings suggest that current models do not yet pose a\ngeneralized cyber offense threat. Nonetheless, our work provides a robust\nbenchmark to guide the trustworthy development of future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing autonomy of Large Language Models (LLMs) necessitates a\nrigorous evaluation of their potential to aid in cyber offense. Existing\nbenchmarks often lack real-world complexity and are thus unable to accurately\nassess LLMs' cybersecurity capabilities. To address this gap, we introduce\nPACEbench, a practical AI cyber-exploitation benchmark built on the principles\nof realistic vulnerability difficulty, environmental complexity, and cyber\ndefenses. Specifically, PACEbench comprises four scenarios spanning single,\nblended, chained, and defense vulnerability exploitations. To handle these\ncomplex challenges, we propose PACEagent, a novel agent that emulates human\npenetration testers by supporting multi-phase reconnaissance, analysis, and\nexploitation. Extensive experiments with seven frontier LLMs demonstrate that\ncurrent models struggle with complex cyber scenarios, and none can bypass\ndefenses. These findings suggest that current models do not yet pose a\ngeneralized cyber offense threat. Nonetheless, our work provides a robust\nbenchmark to guide the trustworthy development of future models."
                },
                "authors": [
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Lige Huang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "Project webpage available at https://pacebench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11661v1",
                "updated": "2025-10-13T17:35:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    35,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:35:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    35,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-Scientist: Scientific Equation Discovery With Agentic AI"
                },
                "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities."
                },
                "authors": [
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Yuhan Sun"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11653v1",
                "updated": "2025-10-13T17:30:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    30,
                    54,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:30:54Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    30,
                    54,
                    0,
                    286,
                    0
                ],
                "title": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model"
                },
                "summary": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)\nmethods has emerged that seem to unlock stronger mathematical reasoning.\nHowever, a closer look at the open-source ecosystem reveals a critical\nlimitation: with sufficiently many draws (e.g., $\\texttt{pass@1024}$), many\nexisting base models already solve nearly all questions on widely used math\nbenchmarks such as MATH-500 and AIME 2024. This suggests that the RL\nfine-tuning methods prevalent in the LLM reasoning literature largely sharpen\nexisting solution modes rather than discovering entirely new ones. Such\nsharpening stands in contrast to the broader promise of RL: to foster\nexploration and to acquire new skills. To move beyond this plateau, we\nintroduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat\ncommon open-source models of up to 8B parameters even under large sampling\nbudgets. Improving performance on our benchmark via RL requires methods that\nlearn to reason in ways that go beyond base model capabilities in repeated\nsampling. Since the problems are drawn from subsets of DAPO-Math-17K and\nDeepScaleR datasets, they remain topically equivalent to standard high-school\nmath. Validating our premise, RL fine-tuned models such as\nNemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform\npoorly on MATH-B at $\\texttt{pass@1024}$, showing how existing approaches fall\nshort on tackling harder instances. We hope MATH-B will catalyze\nexploration-driven RL approaches that elicit deeper reasoning capabilities. We\nrelease MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond."
                },
                "authors": [
                    {
                        "name": "Prasanna Mayilvahanan"
                    },
                    {
                        "name": "Ricardo Dominguez-Olmedo"
                    },
                    {
                        "name": "Thaddäus Wiedemer"
                    },
                    {
                        "name": "Wieland Brendel"
                    }
                ],
                "author_detail": {
                    "name": "Wieland Brendel"
                },
                "author": "Wieland Brendel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11652v1",
                "updated": "2025-10-13T17:30:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    30,
                    36,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:30:36Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    30,
                    36,
                    0,
                    286,
                    0
                ],
                "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems"
                },
                "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason."
                },
                "authors": [
                    {
                        "name": "Xin Gui"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "JinCheng Ren"
                    },
                    {
                        "name": "Qianben Chen"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Yizhi LI"
                    },
                    {
                        "name": "Xinpeng Liu"
                    },
                    {
                        "name": "Xiaowan Li"
                    },
                    {
                        "name": "Wenli Ren"
                    },
                    {
                        "name": "Linyu Miao"
                    },
                    {
                        "name": "Tianrui Qin"
                    },
                    {
                        "name": "Ziqi Shu"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Dingfeng Shi"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yuchen Eleanor Jiang"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wangchunshu Zhou"
                },
                "author": "Wangchunshu Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02356v2",
                "updated": "2025-10-13T17:24:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    24,
                    22,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-27T23:39:56Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    23,
                    39,
                    56,
                    5,
                    270,
                    0
                ],
                "title": "Measuring Physical-World Privacy Awareness of Large Language Models: An\n  Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Physical-World Privacy Awareness of Large Language Models: An\n  Evaluation Benchmark"
                },
                "summary": "The deployment of Large Language Models (LLMs) in embodied agents creates an\nurgent need to measure their privacy awareness in the physical world. Existing\nevaluation methods, however, are confined to natural language based scenarios.\nTo bridge this gap, we introduce EAPrivacy, a comprehensive evaluation\nbenchmark designed to quantify the physical-world privacy awareness of\nLLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across\nfour tiers to test an agent's ability to handle sensitive objects, adapt to\nchanging environments, balance task execution with privacy constraints, and\nresolve conflicts with social norms. Our measurements reveal a critical deficit\nin current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\\%\naccuracy in scenarios involving changing physical environments. Furthermore,\nwhen a task was accompanied by a privacy request, models prioritized completion\nover the constraint in up to 86\\% of cases. In high-stakes situations pitting\nprivacy against critical social norms, leading models like GPT-4o and\nClaude-3.5-haiku disregarded the social norm over 15\\% of the time. These\nfindings, demonstrated by our benchmark, underscore a fundamental misalignment\nin LLMs regarding physically grounded privacy and establish the need for more\nrobust, physically-aware alignment. Codes and datasets will be available at\nhttps://github.com/Graph-COM/EAPrivacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) in embodied agents creates an\nurgent need to measure their privacy awareness in the physical world. Existing\nevaluation methods, however, are confined to natural language based scenarios.\nTo bridge this gap, we introduce EAPrivacy, a comprehensive evaluation\nbenchmark designed to quantify the physical-world privacy awareness of\nLLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across\nfour tiers to test an agent's ability to handle sensitive objects, adapt to\nchanging environments, balance task execution with privacy constraints, and\nresolve conflicts with social norms. Our measurements reveal a critical deficit\nin current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\\%\naccuracy in scenarios involving changing physical environments. Furthermore,\nwhen a task was accompanied by a privacy request, models prioritized completion\nover the constraint in up to 86\\% of cases. In high-stakes situations pitting\nprivacy against critical social norms, leading models like GPT-4o and\nClaude-3.5-haiku disregarded the social norm over 15\\% of the time. These\nfindings, demonstrated by our benchmark, underscore a fundamental misalignment\nin LLMs regarding physically grounded privacy and establish the need for more\nrobust, physically-aware alignment. Codes and datasets will be available at\nhttps://github.com/Graph-COM/EAPrivacy."
                },
                "authors": [
                    {
                        "name": "Xinjie Shen"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23703v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23703v4",
                "updated": "2025-10-13T17:20:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    20,
                    33,
                    0,
                    286,
                    0
                ],
                "published": "2025-05-29T17:39:30Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    39,
                    30,
                    3,
                    149,
                    0
                ],
                "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's\n  Math Capability"
                },
                "summary": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning (NFL-HR)**, an\nend-to-end framework designed to incorporate the FL expert into NL math\nproblem-solving. To bridge the NL and FL input format gap, we propose the NL-FL\nProblem Alignment method, which reformulates the Question-Answering (QA)\nproblems in NL as existence theorems in FL. Subsequently, the Mixed Problem\nInput technique we provide enables the FL reasoner to handle both QA and\nexistence problems concurrently. Lastly, we mitigate the NL and FL output\nformat gap in reasoning through an LLM-based Answer Extraction mechanism.\nComprehensive experiments demonstrate that the NFL-HR framework achieves\n**89.80**% and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by **4.60%** and **4.82%**,\nrespectively. Notably, some problems resolved by our framework remain unsolved\nby the NL baseline model even under a larger number of trials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning (NFL-HR)**, an\nend-to-end framework designed to incorporate the FL expert into NL math\nproblem-solving. To bridge the NL and FL input format gap, we propose the NL-FL\nProblem Alignment method, which reformulates the Question-Answering (QA)\nproblems in NL as existence theorems in FL. Subsequently, the Mixed Problem\nInput technique we provide enables the FL reasoner to handle both QA and\nexistence problems concurrently. Lastly, we mitigate the NL and FL output\nformat gap in reasoning through an LLM-based Answer Extraction mechanism.\nComprehensive experiments demonstrate that the NFL-HR framework achieves\n**89.80**% and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by **4.60%** and **4.82%**,\nrespectively. Notably, some problems resolved by our framework remain unsolved\nby the NL baseline model even under a larger number of trials."
                },
                "authors": [
                    {
                        "name": "Ruida Wang"
                    },
                    {
                        "name": "Yuxin Li"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23703v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23703v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11639v1",
                "updated": "2025-10-13T17:20:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    20,
                    13,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:20:13Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    20,
                    13,
                    0,
                    286,
                    0
                ],
                "title": "OneRec-Think: In-Text Reasoning for Generative Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneRec-Think: In-Text Reasoning for Generative Recommendation"
                },
                "summary": "The powerful generative capacity of Large Language Models (LLMs) has\ninstigated a paradigm shift in recommendation. However, existing generative\nmodels (e.g., OneRec) operate as implicit predictors, critically lacking the\ncapacity for explicit and controllable reasoning-a key advantage of LLMs. To\nbridge this gap, we propose OneRec-Think, a unified framework that seamlessly\nintegrates dialogue, reasoning, and personalized recommendation. OneRec-Think\nincorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for\nsemantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate\nLLM reasoning within the recommendation context; and (3) Reasoning Enhancement,\nwhere we design a recommendation-specific reward function that accounts for the\nmulti-validity nature of user preferences. Experiments across public benchmarks\nshow state-of-the-art performance. Moreover, our proposed \"Think-Ahead\"\narchitecture enables effective industrial deployment on Kuaishou, achieving a\n0.159\\% gain in APP Stay Time and validating the practical efficacy of the\nmodel's explicit reasoning capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The powerful generative capacity of Large Language Models (LLMs) has\ninstigated a paradigm shift in recommendation. However, existing generative\nmodels (e.g., OneRec) operate as implicit predictors, critically lacking the\ncapacity for explicit and controllable reasoning-a key advantage of LLMs. To\nbridge this gap, we propose OneRec-Think, a unified framework that seamlessly\nintegrates dialogue, reasoning, and personalized recommendation. OneRec-Think\nincorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for\nsemantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate\nLLM reasoning within the recommendation context; and (3) Reasoning Enhancement,\nwhere we design a recommendation-specific reward function that accounts for the\nmulti-validity nature of user preferences. Experiments across public benchmarks\nshow state-of-the-art performance. Moreover, our proposed \"Think-Ahead\"\narchitecture enables effective industrial deployment on Kuaishou, achieving a\n0.159\\% gain in APP Stay Time and validating the practical efficacy of the\nmodel's explicit reasoning capability."
                },
                "authors": [
                    {
                        "name": "Zhanyu Liu"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Xingmei Wang"
                    },
                    {
                        "name": "Rongzhou Zhang"
                    },
                    {
                        "name": "Jiaxin Deng"
                    },
                    {
                        "name": "Honghui Bao"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Wuchao Li"
                    },
                    {
                        "name": "Pengfei Zheng"
                    },
                    {
                        "name": "Xiangyu Wu"
                    },
                    {
                        "name": "Yifei Hu"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Lejian Ren"
                    },
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Qianqian Wang"
                    },
                    {
                        "name": "Kuo Cai"
                    },
                    {
                        "name": "Yunfan Wu"
                    },
                    {
                        "name": "Hongtao Cheng"
                    },
                    {
                        "name": "Zexuan Cheng"
                    },
                    {
                        "name": "Lu Ren"
                    },
                    {
                        "name": "Huanjie Wang"
                    },
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11631v1",
                "updated": "2025-10-13T17:12:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    12,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:12:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    12,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models"
                },
                "summary": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics."
                },
                "authors": [
                    {
                        "name": "Tobias Preintner"
                    },
                    {
                        "name": "Weixuan Yuan"
                    },
                    {
                        "name": "Adrian König"
                    },
                    {
                        "name": "Thomas Bäck"
                    },
                    {
                        "name": "Elena Raponi"
                    },
                    {
                        "name": "Niki van Stein"
                    }
                ],
                "author_detail": {
                    "name": "Niki van Stein"
                },
                "author": "Niki van Stein",
                "arxiv_comment": "Accepted to IEEE ICTAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22578v2",
                "updated": "2025-10-13T17:12:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    12,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-06-27T18:51:25Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    18,
                    51,
                    25,
                    4,
                    178,
                    0
                ],
                "title": "The Hidden Link Between RLHF and Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Link Between RLHF and Contrastive Learning"
                },
                "summary": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be interpreted as methods that performing\ncontrastive learning based on the positive and negative samples derived from\nbase model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). Such paradigm further illuminates why RLHF\nmay not intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on the perspective, we replace the\nDV/MINE bound with the Jensen-Shannon (JS) MI estimator and propose the Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be interpreted as methods that performing\ncontrastive learning based on the positive and negative samples derived from\nbase model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). Such paradigm further illuminates why RLHF\nmay not intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on the perspective, we replace the\nDV/MINE bound with the Jensen-Shannon (JS) MI estimator and propose the Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks."
                },
                "authors": [
                    {
                        "name": "Xufei Lv"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Haoyuan Sun"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Houde Liu"
                    },
                    {
                        "name": "Kehai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kehai Chen"
                },
                "author": "Kehai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11630v1",
                "updated": "2025-10-13T17:11:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    11,
                    18,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T17:11:18Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    11,
                    18,
                    0,
                    286,
                    0
                ],
                "title": "Attosecond-level synchronisation of chip-integrated oscillators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attosecond-level synchronisation of chip-integrated oscillators"
                },
                "summary": "Attosecond science provides a window to the fastest processes in chemistry,\nmaterials science, and biology. Accessing this time scale requires precisely\nsynchronised oscillators. In free-electron X-ray lasers, which also provide\nsub-atomic resolution, synchronisation must be achieved across hundreds of\nmeters. Current approaches to synchronisation based on mode-locked lasers\ndeliver this level of performance but complexity, cost and size hinder their\ndeployment in facility-wide multi-node networks. Here, we demonstrate\nattosecond-level synchronisation of two chip-integrated photonic oscillators\n(microcombs) separated by 100 m of fibre. A pair of continuous-wave lasers\nestablishes a time reference that is delivered over fibre, and on-chip\nKerr-nonlinear synchronisation results in an integrated relative timing jitter\nof the microcombs below 400 as (1 kHz to 1 MHz), without any active\nstabilisation. These results unlock precision timing at scale for large\nfacilities and next-generation technologies such as disaggregated computing and\nquantum networks, and ultimately may lead to chip-integrated attosecond\nphotonics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attosecond science provides a window to the fastest processes in chemistry,\nmaterials science, and biology. Accessing this time scale requires precisely\nsynchronised oscillators. In free-electron X-ray lasers, which also provide\nsub-atomic resolution, synchronisation must be achieved across hundreds of\nmeters. Current approaches to synchronisation based on mode-locked lasers\ndeliver this level of performance but complexity, cost and size hinder their\ndeployment in facility-wide multi-node networks. Here, we demonstrate\nattosecond-level synchronisation of two chip-integrated photonic oscillators\n(microcombs) separated by 100 m of fibre. A pair of continuous-wave lasers\nestablishes a time reference that is delivered over fibre, and on-chip\nKerr-nonlinear synchronisation results in an integrated relative timing jitter\nof the microcombs below 400 as (1 kHz to 1 MHz), without any active\nstabilisation. These results unlock precision timing at scale for large\nfacilities and next-generation technologies such as disaggregated computing and\nquantum networks, and ultimately may lead to chip-integrated attosecond\nphotonics."
                },
                "authors": [
                    {
                        "name": "Alexander E. Ulanov"
                    },
                    {
                        "name": "Bastian Ruhnke"
                    },
                    {
                        "name": "Thibault Wildi"
                    },
                    {
                        "name": "Tobias Herr"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Herr"
                },
                "author": "Tobias Herr",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11615v1",
                "updated": "2025-10-13T16:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    55,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:55:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    55,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "LLM-Oriented Token-Adaptive Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Oriented Token-Adaptive Knowledge Distillation"
                },
                "summary": "Knowledge distillation (KD) is a key technique for compressing large-scale\nlanguage models (LLMs), yet prevailing logit-based methods typically employ\nstatic strategies that are misaligned with the dynamic learning process of\nstudent models. These methods typically treat all tokens indiscriminately and\napply a single, fixed temperature, resulting in suboptimal knowledge transfer.\nTo address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge\nDistillation (AdaKD), a novel framework that adapts the distillation process to\nthe real-time learning state of each token. AdaKD consists of two synergistic\nmodules driven by a unified token difficulty metric. First, our Loss-Driven\nAdaptive Token Focusing (LATF) module dynamically adjusts the distillation\nfocus by monitoring the student's learning stability, concentrating\ncomputational resources on the most valuable tokens at each training phase.\nSecond, we introduce Inverse Difficulty Temperature Scaling (IDTS), a\ncounterintuitive yet effective token-level temperature strategy. It employs low\ntemperatures for difficult tokens for targeted error correction, and high\ntemperatures for easy tokens to encourage students to learn from the teacher's\ncomplete and smooth output distribution, thereby enhancing generalization. As a\nplug-and-play framework, AdaKD can consistently improve the performance of\nvarious distillation methods on multiple model architectures and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is a key technique for compressing large-scale\nlanguage models (LLMs), yet prevailing logit-based methods typically employ\nstatic strategies that are misaligned with the dynamic learning process of\nstudent models. These methods typically treat all tokens indiscriminately and\napply a single, fixed temperature, resulting in suboptimal knowledge transfer.\nTo address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge\nDistillation (AdaKD), a novel framework that adapts the distillation process to\nthe real-time learning state of each token. AdaKD consists of two synergistic\nmodules driven by a unified token difficulty metric. First, our Loss-Driven\nAdaptive Token Focusing (LATF) module dynamically adjusts the distillation\nfocus by monitoring the student's learning stability, concentrating\ncomputational resources on the most valuable tokens at each training phase.\nSecond, we introduce Inverse Difficulty Temperature Scaling (IDTS), a\ncounterintuitive yet effective token-level temperature strategy. It employs low\ntemperatures for difficult tokens for targeted error correction, and high\ntemperatures for easy tokens to encourage students to learn from the teacher's\ncomplete and smooth output distribution, thereby enhancing generalization. As a\nplug-and-play framework, AdaKD can consistently improve the performance of\nvarious distillation methods on multiple model architectures and benchmarks."
                },
                "authors": [
                    {
                        "name": "Xurong Xie"
                    },
                    {
                        "name": "Zhucun Xue"
                    },
                    {
                        "name": "Jiafu Wu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yabiao Wang"
                    },
                    {
                        "name": "Xiaobin Hu"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Jiangning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangning Zhang"
                },
                "author": "Jiangning Zhang",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11608v1",
                "updated": "2025-10-13T16:47:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    47,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:47:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    47,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems"
                },
                "summary": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook."
                },
                "authors": [
                    {
                        "name": "Shiqi Zhang"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Yunqing Xu"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Tiancheng Shen"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04872v2",
                "updated": "2025-10-13T16:38:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    38,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-04-07T09:27:37Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    27,
                    37,
                    0,
                    97,
                    0
                ],
                "title": "Simulating Persuasive Dialogues on Meat Reduction with Generative Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Persuasive Dialogues on Meat Reduction with Generative Agents"
                },
                "summary": "Meat reduction benefits human and planetary health, but social norms keep\nmeat central in shared meals. To date, the development of communication\nstrategies that promote meat reduction while minimizing social costs has\nrequired the costly involvement of human participants at each stage of the\nprocess. We present work in progress on simulating multi-round dialogues on\nmeat reduction between Generative Agents based on large language models (LLMs).\nWe measure our main outcome using established psychological questionnaires\nbased on the Theory of Planned Behavior and additionally investigate Social\nCosts. We find evidence that our preliminary simulations produce outcomes that\nare (i) consistent with theoretical expectations; and (ii) valid when compared\nto data from previous studies with human participants. Generative agent-based\nmodels are a promising tool for identifying novel communication strategies on\nmeat reduction -- tailored to highly specific participant groups -- to then be\ntested in subsequent studies with human participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meat reduction benefits human and planetary health, but social norms keep\nmeat central in shared meals. To date, the development of communication\nstrategies that promote meat reduction while minimizing social costs has\nrequired the costly involvement of human participants at each stage of the\nprocess. We present work in progress on simulating multi-round dialogues on\nmeat reduction between Generative Agents based on large language models (LLMs).\nWe measure our main outcome using established psychological questionnaires\nbased on the Theory of Planned Behavior and additionally investigate Social\nCosts. We find evidence that our preliminary simulations produce outcomes that\nare (i) consistent with theoretical expectations; and (ii) valid when compared\nto data from previous studies with human participants. Generative agent-based\nmodels are a promising tool for identifying novel communication strategies on\nmeat reduction -- tailored to highly specific participant groups -- to then be\ntested in subsequent studies with human participants."
                },
                "authors": [
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Elena Wurth"
                    },
                    {
                        "name": "Markus Strohmaier"
                    },
                    {
                        "name": "Jutta Mata"
                    }
                ],
                "author_detail": {
                    "name": "Jutta Mata"
                },
                "author": "Jutta Mata",
                "arxiv_doi": "10.36190/2025.30",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.36190/2025.30",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.04872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Code available at https://github.com/dess-mannheim/MeatlessAgents",
                "arxiv_journal_ref": "NLPSI 2025: First Workshop on Integrating NLP and Psychology to\n  Study Social Interactions",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11598v1",
                "updated": "2025-10-13T16:37:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    37,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:37:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    37,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language\n  Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as one of the most widely used\nparameter-efficient fine-tuning (PEFT) methods for adapting large language\nmodels (LLMs) to downstream tasks. While highly effective in single-task\nsettings, it struggles to efficiently leverage inter-task knowledge in complex\nmulti-task learning scenarios, often requiring substantial task-specific data\nto achieve optimal performance. To address this limitation, we introduce\nMeTA-LoRA, a two-stage optimization framework that significantly improves data\nefficiency in multi-task adaptation. In the first stage, task-specific LoRA\nadapters are learned using only a few samples from each involved dataset,\nenabling rapid adaptation without large-scale supervision. In the second stage,\nthe shared LoRA adapter is updated by aggregating gradients from multiple tasks\nto promote knowledge transfer across tasks, further reducing data usage by\nleveraging common patterns. In both multi-task learning and multilingual\nlearning scenarios, our method matches or surpasses the performance of\ntraditional full-data LoRA fine-tuning approaches, while using significantly\nless task-specific data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as one of the most widely used\nparameter-efficient fine-tuning (PEFT) methods for adapting large language\nmodels (LLMs) to downstream tasks. While highly effective in single-task\nsettings, it struggles to efficiently leverage inter-task knowledge in complex\nmulti-task learning scenarios, often requiring substantial task-specific data\nto achieve optimal performance. To address this limitation, we introduce\nMeTA-LoRA, a two-stage optimization framework that significantly improves data\nefficiency in multi-task adaptation. In the first stage, task-specific LoRA\nadapters are learned using only a few samples from each involved dataset,\nenabling rapid adaptation without large-scale supervision. In the second stage,\nthe shared LoRA adapter is updated by aggregating gradients from multiple tasks\nto promote knowledge transfer across tasks, further reducing data usage by\nleveraging common patterns. In both multi-task learning and multilingual\nlearning scenarios, our method matches or surpasses the performance of\ntraditional full-data LoRA fine-tuning approaches, while using significantly\nless task-specific data."
                },
                "authors": [
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Jinda Liu"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Yuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wu"
                },
                "author": "Yuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11588v1",
                "updated": "2025-10-13T16:30:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    30,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:30:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    30,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing and Internalizing Complex Policy Documents for LLM Agents"
                },
                "summary": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata."
                },
                "authors": [
                    {
                        "name": "Jiateng Liu"
                    },
                    {
                        "name": "Zhenhailong Wang"
                    },
                    {
                        "name": "Xiaojiang Huang"
                    },
                    {
                        "name": "Yingjie Li"
                    },
                    {
                        "name": "Xing Fan"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Chenlei Guo"
                    },
                    {
                        "name": "Ruhi Sarikaya"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11586v1",
                "updated": "2025-10-13T16:29:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    29,
                    19,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:29:19Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    29,
                    19,
                    0,
                    286,
                    0
                ],
                "title": "Survey Response Generation: Generating Closed-Ended Survey Responses\n  In-Silico with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey Response Generation: Generating Closed-Ended Survey Responses\n  In-Silico with Large Language Models"
                },
                "summary": "Many in-silico simulations of human survey responses with large language\nmodels (LLMs) focus on generating closed-ended survey responses, whereas LLMs\nare typically trained to generate open-ended text instead. Previous research\nhas used a diverse range of methods for generating closed-ended survey\nresponses with LLMs, and a standard practice remains to be identified. In this\npaper, we systematically investigate the impact that various Survey Response\nGeneration Methods have on predicted survey responses. We present the results\nof 32 mio. simulated survey responses across 8 Survey Response Generation\nMethods, 4 political attitude surveys, and 10 open-weight language models. We\nfind significant differences between the Survey Response Generation Methods in\nboth individual-level and subpopulation-level alignment. Our results show that\nRestricted Generation Methods perform best overall, and that reasoning output\ndoes not consistently improve alignment. Our work underlines the significant\nimpact that Survey Response Generation Methods have on simulated survey\nresponses, and we develop practical recommendations on the application of\nSurvey Response Generation Methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many in-silico simulations of human survey responses with large language\nmodels (LLMs) focus on generating closed-ended survey responses, whereas LLMs\nare typically trained to generate open-ended text instead. Previous research\nhas used a diverse range of methods for generating closed-ended survey\nresponses with LLMs, and a standard practice remains to be identified. In this\npaper, we systematically investigate the impact that various Survey Response\nGeneration Methods have on predicted survey responses. We present the results\nof 32 mio. simulated survey responses across 8 Survey Response Generation\nMethods, 4 political attitude surveys, and 10 open-weight language models. We\nfind significant differences between the Survey Response Generation Methods in\nboth individual-level and subpopulation-level alignment. Our results show that\nRestricted Generation Methods perform best overall, and that reasoning output\ndoes not consistently improve alignment. Our work underlines the significant\nimpact that Survey Response Generation Methods have on simulated survey\nresponses, and we develop practical recommendations on the application of\nSurvey Response Generation Methods."
                },
                "authors": [
                    {
                        "name": "Georg Ahnert"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11584v1",
                "updated": "2025-10-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    29,
                    17,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    29,
                    17,
                    0,
                    286,
                    0
                ],
                "title": "LLMAtKGE: Large Language Models as Explainable Attackers against\n  Knowledge Graph Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMAtKGE: Large Language Models as Explainable Attackers against\n  Knowledge Graph Embeddings"
                },
                "summary": "Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the\nmodel's ability of link prediction by removing or inserting triples. A recent\nblack-box method has attempted to incorporate textual and structural\ninformation to enhance attack performance. However, it is unable to generate\nhuman-readable explanations, and exhibits poor generalizability. In the past\nfew years, large language models (LLMs) have demonstrated powerful capabilities\nin text comprehension, generation, and reasoning. In this paper, we propose\nLLMAtKGE, a novel LLM-based framework that selects attack targets and generates\nhuman-readable explanations. To provide the LLM with sufficient factual context\nunder limited input constraints, we design a structured prompting scheme that\nexplicitly formulates the attack as multiple-choice questions while\nincorporating KG factual evidence. To address the context-window limitation and\nhesitation issues, we introduce semantics-based and centrality-based filters,\nwhich compress the candidate set while preserving high recall of\nattack-relevant information. Furthermore, to efficiently integrate both\nsemantic and structural information into the filter, we precompute high-order\nadjacency and fine-tune the LLM with a triple classification task to enhance\nfiltering performance. Experiments on two widely used knowledge graph datasets\ndemonstrate that our attack outperforms the strongest black-box baselines and\nprovides explanations via reasoning, and showing competitive performance\ncompared with white-box methods. Comprehensive ablation and case studies\nfurther validate its capability to generate explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the\nmodel's ability of link prediction by removing or inserting triples. A recent\nblack-box method has attempted to incorporate textual and structural\ninformation to enhance attack performance. However, it is unable to generate\nhuman-readable explanations, and exhibits poor generalizability. In the past\nfew years, large language models (LLMs) have demonstrated powerful capabilities\nin text comprehension, generation, and reasoning. In this paper, we propose\nLLMAtKGE, a novel LLM-based framework that selects attack targets and generates\nhuman-readable explanations. To provide the LLM with sufficient factual context\nunder limited input constraints, we design a structured prompting scheme that\nexplicitly formulates the attack as multiple-choice questions while\nincorporating KG factual evidence. To address the context-window limitation and\nhesitation issues, we introduce semantics-based and centrality-based filters,\nwhich compress the candidate set while preserving high recall of\nattack-relevant information. Furthermore, to efficiently integrate both\nsemantic and structural information into the filter, we precompute high-order\nadjacency and fine-tune the LLM with a triple classification task to enhance\nfiltering performance. Experiments on two widely used knowledge graph datasets\ndemonstrate that our attack outperforms the strongest black-box baselines and\nprovides explanations via reasoning, and showing competitive performance\ncompared with white-box methods. Comprehensive ablation and case studies\nfurther validate its capability to generate explanations."
                },
                "authors": [
                    {
                        "name": "Ting Li"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Yipeng Yu"
                    },
                    {
                        "name": "Liang Yao"
                    },
                    {
                        "name": "Guoqing Chao"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11566v1",
                "updated": "2025-10-13T16:11:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    11,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:11:34Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    11,
                    34,
                    0,
                    286,
                    0
                ],
                "title": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative\n  Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative\n  Policy"
                },
                "summary": "Scooping items with tools such as spoons and ladles is common in daily life,\nranging from assistive feeding to retrieving items from environmental disaster\nsites. However, developing a general and autonomous robotic scooping policy is\nchallenging since it requires reasoning about complex tool-object interactions.\nFurthermore, scooping often involves manipulating deformable objects, such as\ngranular media or liquids, which is challenging due to their\ninfinite-dimensional configuration spaces and complex dynamics. We propose a\nmethod, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA\nOmniverse) to collect scooping demonstrations using algorithmic procedures that\nrely on privileged state information. Then, we use generative policies via\ndiffusion to imitate demonstrations from observational input. We directly apply\nthe learned policy in diverse real-world scenarios, testing its performance on\nvarious item quantities, item characteristics, and container types. In\nzero-shot deployment, our method demonstrates promising results across 465\ntrials in diverse scenarios, including objects of different difficulty levels\nthat we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all\nbaselines and ablations, suggesting that this is a promising approach to\nacquiring robotic scooping skills. Project page is at\nhttps://scoopdiff.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scooping items with tools such as spoons and ladles is common in daily life,\nranging from assistive feeding to retrieving items from environmental disaster\nsites. However, developing a general and autonomous robotic scooping policy is\nchallenging since it requires reasoning about complex tool-object interactions.\nFurthermore, scooping often involves manipulating deformable objects, such as\ngranular media or liquids, which is challenging due to their\ninfinite-dimensional configuration spaces and complex dynamics. We propose a\nmethod, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA\nOmniverse) to collect scooping demonstrations using algorithmic procedures that\nrely on privileged state information. Then, we use generative policies via\ndiffusion to imitate demonstrations from observational input. We directly apply\nthe learned policy in diverse real-world scenarios, testing its performance on\nvarious item quantities, item characteristics, and container types. In\nzero-shot deployment, our method demonstrates promising results across 465\ntrials in diverse scenarios, including objects of different difficulty levels\nthat we categorize as \"Level 1\" and \"Level 2.\" SCOOP'D outperforms all\nbaselines and ablations, suggesting that this is a promising approach to\nacquiring robotic scooping skills. Project page is at\nhttps://scoopdiff.github.io/."
                },
                "authors": [
                    {
                        "name": "Kuanning Wang"
                    },
                    {
                        "name": "Yongchong Gu"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Zeyu Shangguan"
                    },
                    {
                        "name": "Sicheng He"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Yanwei Fu"
                    },
                    {
                        "name": "Daniel Seita"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Seita"
                },
                "author": "Daniel Seita",
                "arxiv_comment": "Project page is at https://scoopdiff.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20333v2",
                "updated": "2025-10-13T16:09:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    9,
                    37,
                    0,
                    286,
                    0
                ],
                "published": "2025-05-24T10:25:58Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    10,
                    25,
                    58,
                    5,
                    144,
                    0
                ],
                "title": "Multi-Scale Manifold Alignment for Interpreting Large Language Models: A\n  Unified Information-Geometric Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Scale Manifold Alignment for Interpreting Large Language Models: A\n  Unified Information-Geometric Framework"
                },
                "summary": "We present Multi-Scale Manifold Alignment(MSMA), an information-geometric\nframework that decomposes LLM representations into local, intermediate, and\nglobal manifolds and learns cross-scale mappings that preserve geometry and\ninformation. Across GPT-2, BERT, RoBERTa, and T5, we observe consistent\nhierarchical patterns and find that MSMA improves alignment metrics under\nmultiple estimators (e.g., relative KL reduction and MI gains with statistical\nsignificance across seeds). Controlled interventions at different scales yield\ndistinct and architecture-dependent effects on lexical diversity, sentence\nstructure, and discourse coherence. While our theoretical analysis relies on\nidealized assumptions, the empirical results suggest that multi-objective\nalignment offers a practical lens for analyzing cross-scale information flow\nand guiding representation-level control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Multi-Scale Manifold Alignment(MSMA), an information-geometric\nframework that decomposes LLM representations into local, intermediate, and\nglobal manifolds and learns cross-scale mappings that preserve geometry and\ninformation. Across GPT-2, BERT, RoBERTa, and T5, we observe consistent\nhierarchical patterns and find that MSMA improves alignment metrics under\nmultiple estimators (e.g., relative KL reduction and MI gains with statistical\nsignificance across seeds). Controlled interventions at different scales yield\ndistinct and architecture-dependent effects on lexical diversity, sentence\nstructure, and discourse coherence. While our theoretical analysis relies on\nidealized assumptions, the empirical results suggest that multi-objective\nalignment offers a practical lens for analyzing cross-scale information flow\nand guiding representation-level control."
                },
                "authors": [
                    {
                        "name": "Yukun Zhang"
                    },
                    {
                        "name": "Qi Dong"
                    }
                ],
                "author_detail": {
                    "name": "Qi Dong"
                },
                "author": "Qi Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13142v2",
                "updated": "2025-10-13T16:08:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    8,
                    38,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-18T17:55:17Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    17,
                    55,
                    17,
                    0,
                    230,
                    0
                ],
                "title": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistic Evaluation of Multimodal LLMs on Spatial Intelligence"
                },
                "summary": "Multimodal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, the very capability that anchors artificial\ngeneral intelligence in the physical world. With the recent release of GPT-5,\nallegedly the most powerful AI model to date, it is timely to examine where the\nleading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path\ntoward spatial intelligence. We first propose a holistic taxonomy of spatial\ntasks that unifies existing benchmarks and a standardized protocol for the fair\nevaluation of state-of-the-art proprietary and open-source models across eight\nkey benchmarks, at a cost exceeding ten billion total tokens. Our empirical\nstudy then reveals that (1) GPT-5 demonstrates unprecedented strength in\nspatial intelligence (SI), yet (2) still falls short of human performance\nsignificantly across a broad spectrum of SI-tasks. Moreover, we (3) show that\nSI-tasks expose greater model capability deficiency than non-SI tasks, to the\nextent that (4) proprietary models do not exhibit a decisive advantage when\nfacing the most difficult ones. In addition, we conduct a qualitative\nevaluation across a diverse set of scenarios that are intuitive for humans, yet\nfail even the most advanced multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, the very capability that anchors artificial\ngeneral intelligence in the physical world. With the recent release of GPT-5,\nallegedly the most powerful AI model to date, it is timely to examine where the\nleading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path\ntoward spatial intelligence. We first propose a holistic taxonomy of spatial\ntasks that unifies existing benchmarks and a standardized protocol for the fair\nevaluation of state-of-the-art proprietary and open-source models across eight\nkey benchmarks, at a cost exceeding ten billion total tokens. Our empirical\nstudy then reveals that (1) GPT-5 demonstrates unprecedented strength in\nspatial intelligence (SI), yet (2) still falls short of human performance\nsignificantly across a broad spectrum of SI-tasks. Moreover, we (3) show that\nSI-tasks expose greater model capability deficiency than non-SI tasks, to the\nextent that (4) proprietary models do not exhibit a decisive advantage when\nfacing the most difficult ones. In addition, we conduct a qualitative\nevaluation across a diverse set of scenarios that are intuitive for humans, yet\nfail even the most advanced multimodal models."
                },
                "authors": [
                    {
                        "name": "Zhongang Cai"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Qingping Sun"
                    },
                    {
                        "name": "Ruisi Wang"
                    },
                    {
                        "name": "Chenyang Gu"
                    },
                    {
                        "name": "Wanqi Yin"
                    },
                    {
                        "name": "Zhiqian Lin"
                    },
                    {
                        "name": "Zhitao Yang"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Xuanke Shi"
                    },
                    {
                        "name": "Kewang Deng"
                    },
                    {
                        "name": "Xiaoyang Han"
                    },
                    {
                        "name": "Zukai Chen"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Xiangyu Fan"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Quan Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Yang"
                },
                "author": "Lei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11563v1",
                "updated": "2025-10-13T16:06:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    6,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:06:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    6,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "Culturally-Aware Conversations: A Framework & Benchmark for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culturally-Aware Conversations: A Framework & Benchmark for LLMs"
                },
                "summary": "Existing benchmarks that measure cultural adaptation in LLMs are misaligned\nwith the actual challenges these models face when interacting with users from\ndiverse cultural backgrounds. In this work, we introduce the first framework\nand benchmark designed to evaluate LLMs in realistic, multicultural\nconversational settings. Grounded in sociocultural theory, our framework\nformalizes how linguistic style - a key element of cultural communication - is\nshaped by situational, relational, and cultural context. We construct a\nbenchmark dataset based on this framework, annotated by culturally diverse\nraters, and propose a new set of desiderata for cross-cultural evaluation in\nNLP: conversational framing, stylistic sensitivity, and subjective correctness.\nWe evaluate today's top LLMs on our benchmark and show that these models\nstruggle with cultural adaptation in a conversational setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks that measure cultural adaptation in LLMs are misaligned\nwith the actual challenges these models face when interacting with users from\ndiverse cultural backgrounds. In this work, we introduce the first framework\nand benchmark designed to evaluate LLMs in realistic, multicultural\nconversational settings. Grounded in sociocultural theory, our framework\nformalizes how linguistic style - a key element of cultural communication - is\nshaped by situational, relational, and cultural context. We construct a\nbenchmark dataset based on this framework, annotated by culturally diverse\nraters, and propose a new set of desiderata for cross-cultural evaluation in\nNLP: conversational framing, stylistic sensitivity, and subjective correctness.\nWe evaluate today's top LLMs on our benchmark and show that these models\nstruggle with cultural adaptation in a conversational setting."
                },
                "authors": [
                    {
                        "name": "Shreya Havaldar"
                    },
                    {
                        "name": "Sunny Rai"
                    },
                    {
                        "name": "Young-Min Cho"
                    },
                    {
                        "name": "Lyle Ungar"
                    }
                ],
                "author_detail": {
                    "name": "Lyle Ungar"
                },
                "author": "Lyle Ungar",
                "arxiv_comment": "To appear at the 4th HCI + NLP Workshop @ EMNLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04310v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04310v3",
                "updated": "2025-10-13T16:04:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    4,
                    56,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-04T15:23:58Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    23,
                    58,
                    3,
                    247,
                    0
                ],
                "title": "EvoEmo: Towards Evolved Emotional Policies for Adversarial LLM Agents in\n  Multi-Turn Price Negotiation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoEmo: Towards Evolved Emotional Policies for Adversarial LLM Agents in\n  Multi-Turn Price Negotiation"
                },
                "summary": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation."
                },
                "authors": [
                    {
                        "name": "Yunbo Long"
                    },
                    {
                        "name": "Liming Xu"
                    },
                    {
                        "name": "Lukas Beckenbauer"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Alexandra Brintrup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Brintrup"
                },
                "author": "Alexandra Brintrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04310v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04310v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11561v1",
                "updated": "2025-10-13T16:04:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    4,
                    6,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:04:06Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    4,
                    6,
                    0,
                    286,
                    0
                ],
                "title": "Ontolearn-A Framework for Large-scale OWL Class Expression Learning in\n  Python",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontolearn-A Framework for Large-scale OWL Class Expression Learning in\n  Python"
                },
                "summary": "In this paper, we present Ontolearn-a framework for learning OWL class\nexpressions over large knowledge graphs. Ontolearn contains efficient\nimplementations of recent stateof-the-art symbolic and neuro-symbolic class\nexpression learners including EvoLearner and DRILL. A learned OWL class\nexpression can be used to classify instances in the knowledge graph.\nFurthermore, Ontolearn integrates a verbalization module based on an LLM to\ntranslate complex OWL class expressions into natural language sentences. By\nmapping OWL class expressions into respective SPARQL queries, Ontolearn can be\neasily used to operate over a remote triplestore. The source code of Ontolearn\nis available at https://github.com/dice-group/Ontolearn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Ontolearn-a framework for learning OWL class\nexpressions over large knowledge graphs. Ontolearn contains efficient\nimplementations of recent stateof-the-art symbolic and neuro-symbolic class\nexpression learners including EvoLearner and DRILL. A learned OWL class\nexpression can be used to classify instances in the knowledge graph.\nFurthermore, Ontolearn integrates a verbalization module based on an LLM to\ntranslate complex OWL class expressions into natural language sentences. By\nmapping OWL class expressions into respective SPARQL queries, Ontolearn can be\neasily used to operate over a remote triplestore. The source code of Ontolearn\nis available at https://github.com/dice-group/Ontolearn."
                },
                "authors": [
                    {
                        "name": "Caglar Demir"
                    },
                    {
                        "name": "Alkid Baci"
                    },
                    {
                        "name": "N'Dah Jean Kouagou"
                    },
                    {
                        "name": "Leonie Nora Sieger"
                    },
                    {
                        "name": "Stefan Heindorf"
                    },
                    {
                        "name": "Simon Bin"
                    },
                    {
                        "name": "Lukas Blübaum"
                    },
                    {
                        "name": "Alexander Bigerl"
                    },
                    {
                        "name": "Axel-Cyrille Ngonga Ngomo"
                    }
                ],
                "author_detail": {
                    "name": "Axel-Cyrille Ngonga Ngomo"
                },
                "author": "Axel-Cyrille Ngonga Ngomo",
                "arxiv_journal_ref": "Journal of Machine Learning Research 26 (2025) 1-6",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11560v1",
                "updated": "2025-10-13T16:04:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    4,
                    3,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:04:03Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    4,
                    3,
                    0,
                    286,
                    0
                ],
                "title": "Characterizing Web Search in The Age of Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing Web Search in The Age of Generative AI"
                },
                "summary": "The advent of LLMs has given rise to a new type of web search: Generative\nsearch, where LLMs retrieve web pages related to a query and generate a single,\ncoherent text as a response. This output modality stands in stark contrast to\ntraditional web search, where results are returned as a ranked list of\nindependent web pages. In this paper, we ask: Along what dimensions do\ngenerative search outputs differ from traditional web search? We compare\nGoogle, a traditional web search engine, with four generative search engines\nfrom two providers (Google and OpenAI) across queries from four domains. Our\nanalysis reveals intriguing differences. Most generative search engines cover a\nwider range of sources compared to web search. Generative search engines vary\nin the degree to which they rely on internal knowledge contained within the\nmodel parameters v.s. external knowledge retrieved from the web. Generative\nsearch engines surface varying sets of concepts, creating new opportunities for\nenhancing search diversity and serendipity. Our results also highlight the need\nfor revisiting evaluation criteria for web search in the age of Generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of LLMs has given rise to a new type of web search: Generative\nsearch, where LLMs retrieve web pages related to a query and generate a single,\ncoherent text as a response. This output modality stands in stark contrast to\ntraditional web search, where results are returned as a ranked list of\nindependent web pages. In this paper, we ask: Along what dimensions do\ngenerative search outputs differ from traditional web search? We compare\nGoogle, a traditional web search engine, with four generative search engines\nfrom two providers (Google and OpenAI) across queries from four domains. Our\nanalysis reveals intriguing differences. Most generative search engines cover a\nwider range of sources compared to web search. Generative search engines vary\nin the degree to which they rely on internal knowledge contained within the\nmodel parameters v.s. external knowledge retrieved from the web. Generative\nsearch engines surface varying sets of concepts, creating new opportunities for\nenhancing search diversity and serendipity. Our results also highlight the need\nfor revisiting evaluation criteria for web search in the age of Generative AI."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Jost Grosse Perdekamp"
                    },
                    {
                        "name": "Mihir Upadhyay"
                    },
                    {
                        "name": "Krishna P. Gummadi"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07772v2",
                "updated": "2025-10-13T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    3,
                    13,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T04:24:47Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    4,
                    24,
                    47,
                    3,
                    282,
                    0
                ],
                "title": "An approach for systematic decomposition of complex llm tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An approach for systematic decomposition of complex llm tasks"
                },
                "summary": "Large Language Models (LLMs) suffer from reliability issues on complex tasks,\nas existing decomposition methods are heuristic and rely on agent or manual\ndecomposition. This work introduces a novel, systematic decomposition framework\nthat we call Analysis of CONstraint-Induced Complexity (ACONIC), which models\nthe task as a constraint problem and leveraging formal complexity measures to\nguide decomposition. On combinatorial (SATBench) and LLM database querying\ntasks (Spider), we find that by decomposing the tasks following the measure of\ncomplexity, agent can perform considerably better (10-40 percentage point).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer from reliability issues on complex tasks,\nas existing decomposition methods are heuristic and rely on agent or manual\ndecomposition. This work introduces a novel, systematic decomposition framework\nthat we call Analysis of CONstraint-Induced Complexity (ACONIC), which models\nthe task as a constraint problem and leveraging formal complexity measures to\nguide decomposition. On combinatorial (SATBench) and LLM database querying\ntasks (Spider), we find that by decomposing the tasks following the measure of\ncomplexity, agent can perform considerably better (10-40 percentage point)."
                },
                "authors": [
                    {
                        "name": "Tianle Zhou"
                    },
                    {
                        "name": "Jiakai Xu"
                    },
                    {
                        "name": "Guanhong Liu"
                    },
                    {
                        "name": "Jiaxiang Liu"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Eugene Wu"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Wu"
                },
                "author": "Eugene Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11558v1",
                "updated": "2025-10-13T16:00:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    0,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:00:34Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    0,
                    34,
                    0,
                    286,
                    0
                ],
                "title": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative\n  Study of Market Leading Agentic AI Products",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative\n  Study of Market Leading Agentic AI Products"
                },
                "summary": "Governance of data, compliance, and business privacy matters, particularly\nfor healthcare and finance businesses. Since the recent emergence of AI\nenterprise AI assistants enhancing business productivity, safeguarding private\ndata and compliance is now a priority. With the implementation of AI assistants\nacross the enterprise, the zero data retention can be achieved by implementing\nzero data retention policies by Large Language Model businesses like Open AI\nand Anthropic and Meta. In this work, we explore zero data retention policies\nfor the Enterprise apps of large language models (LLMs). Our key contribution\nis defining the architectural, compliance, and usability trade-offs of such\nsystems in parallel. In this research work, we examine the development of\ncommercial AI assistants with two industry leaders and market titans in this\narena - Salesforce and Microsoft. Both of these companies used distinct\ntechnical architecture to support zero data retention policies. Salesforce\nAgentForce and Microsoft Copilot are among the leading AI assistants providing\nmuch-needed push to business productivity in customer care. The purpose of this\npaper is to analyze the technical architecture and deployment of zero data\nretention policy by consuming applications as well as big language models\nservice providers like Open Ai, Anthropic, and Meta.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Governance of data, compliance, and business privacy matters, particularly\nfor healthcare and finance businesses. Since the recent emergence of AI\nenterprise AI assistants enhancing business productivity, safeguarding private\ndata and compliance is now a priority. With the implementation of AI assistants\nacross the enterprise, the zero data retention can be achieved by implementing\nzero data retention policies by Large Language Model businesses like Open AI\nand Anthropic and Meta. In this work, we explore zero data retention policies\nfor the Enterprise apps of large language models (LLMs). Our key contribution\nis defining the architectural, compliance, and usability trade-offs of such\nsystems in parallel. In this research work, we examine the development of\ncommercial AI assistants with two industry leaders and market titans in this\narena - Salesforce and Microsoft. Both of these companies used distinct\ntechnical architecture to support zero data retention policies. Salesforce\nAgentForce and Microsoft Copilot are among the leading AI assistants providing\nmuch-needed push to business productivity in customer care. The purpose of this\npaper is to analyze the technical architecture and deployment of zero data\nretention policy by consuming applications as well as big language models\nservice providers like Open Ai, Anthropic, and Meta."
                },
                "authors": [
                    {
                        "name": "Komal Gupta"
                    },
                    {
                        "name": "Aditya Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Shrivastava"
                },
                "author": "Aditya Shrivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11557v1",
                "updated": "2025-10-13T16:00:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    0,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T16:00:15Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    0,
                    15,
                    0,
                    286,
                    0
                ],
                "title": "Invisible Languages of the LLM Universe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Languages of the LLM Universe"
                },
                "summary": "Large Language Models are trained on massive multilingual corpora, yet this\nabundance masks a profound crisis: of the world's 7,613 living languages,\napproximately 2,000 languages with millions of speakers remain effectively\ninvisible in digital ecosystems. We propose a critical framework connecting\nempirical measurements of language vitality (real world demographic strength)\nand digitality (online presence) with postcolonial theory and epistemic\ninjustice to explain why linguistic inequality in AI systems is not incidental\nbut structural. Analyzing data across all documented human languages, we\nidentify four categories: Strongholds (33%, high vitality and digitality),\nDigital Echoes (6%, high digitality despite declining vitality), Fading Voices\n(36%, low on both dimensions), and critically, Invisible Giants (27%, high\nvitality but near-zero digitality) - languages spoken by millions yet absent\nfrom the LLM universe. We demonstrate that these patterns reflect continuities\nfrom colonial-era linguistic hierarchies to contemporary AI development,\nconstituting what we term digital epistemic injustice. Our analysis reveals\nthat English dominance in AI is not a technical necessity but an artifact of\npower structures that systematically exclude marginalized linguistic knowledge.\nWe conclude with implications for decolonizing language technology and\ndemocratizing access to AI benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are trained on massive multilingual corpora, yet this\nabundance masks a profound crisis: of the world's 7,613 living languages,\napproximately 2,000 languages with millions of speakers remain effectively\ninvisible in digital ecosystems. We propose a critical framework connecting\nempirical measurements of language vitality (real world demographic strength)\nand digitality (online presence) with postcolonial theory and epistemic\ninjustice to explain why linguistic inequality in AI systems is not incidental\nbut structural. Analyzing data across all documented human languages, we\nidentify four categories: Strongholds (33%, high vitality and digitality),\nDigital Echoes (6%, high digitality despite declining vitality), Fading Voices\n(36%, low on both dimensions), and critically, Invisible Giants (27%, high\nvitality but near-zero digitality) - languages spoken by millions yet absent\nfrom the LLM universe. We demonstrate that these patterns reflect continuities\nfrom colonial-era linguistic hierarchies to contemporary AI development,\nconstituting what we term digital epistemic injustice. Our analysis reveals\nthat English dominance in AI is not a technical necessity but an artifact of\npower structures that systematically exclude marginalized linguistic knowledge.\nWe conclude with implications for decolonizing language technology and\ndemocratizing access to AI benefits."
                },
                "authors": [
                    {
                        "name": "Saurabh Khanna"
                    },
                    {
                        "name": "Xinxu Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinxu Li"
                },
                "author": "Xinxu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11556v1",
                "updated": "2025-10-13T15:59:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    59,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:59:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    59,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "Personalized and Constructive Feedback for Computer Science Students\n  Using the Large Language Model (LLM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized and Constructive Feedback for Computer Science Students\n  Using the Large Language Model (LLM)"
                },
                "summary": "The evolving pedagogy paradigms are leading toward educational\ntransformations. One fundamental aspect of effective learning is relevant,\nimmediate, and constructive feedback to students. Providing constructive\nfeedback to large cohorts in academia is an ongoing challenge. Therefore,\nacademics are moving towards automated assessment to provide immediate\nfeedback. However, current approaches are often limited in scope, offering\nsimplistic responses that do not provide students with personalized feedback to\nguide them toward improvements. This paper addresses this limitation by\ninvestigating the performance of Large Language Models (LLMs) in processing\nstudents assessments with predefined rubrics and marking criteria to generate\npersonalized feedback for in-depth learning. We aim to leverage the power of\nexisting LLMs for Marking Assessments, Tracking, and Evaluation (LLM-MATE) with\npersonalized feedback to enhance students learning. To evaluate the performance\nof LLM-MATE, we consider the Software Architecture (SA) module as a case study.\nThe LLM-MATE approach can help module leaders overcome assessment challenges\nwith large cohorts. Also, it helps students improve their learning by obtaining\npersonalized feedback in a timely manner. Additionally, the proposed approach\nwill facilitate the establishment of ground truth for automating the generation\nof students assessment feedback using the ChatGPT API, thereby reducing the\noverhead associated with large cohort assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolving pedagogy paradigms are leading toward educational\ntransformations. One fundamental aspect of effective learning is relevant,\nimmediate, and constructive feedback to students. Providing constructive\nfeedback to large cohorts in academia is an ongoing challenge. Therefore,\nacademics are moving towards automated assessment to provide immediate\nfeedback. However, current approaches are often limited in scope, offering\nsimplistic responses that do not provide students with personalized feedback to\nguide them toward improvements. This paper addresses this limitation by\ninvestigating the performance of Large Language Models (LLMs) in processing\nstudents assessments with predefined rubrics and marking criteria to generate\npersonalized feedback for in-depth learning. We aim to leverage the power of\nexisting LLMs for Marking Assessments, Tracking, and Evaluation (LLM-MATE) with\npersonalized feedback to enhance students learning. To evaluate the performance\nof LLM-MATE, we consider the Software Architecture (SA) module as a case study.\nThe LLM-MATE approach can help module leaders overcome assessment challenges\nwith large cohorts. Also, it helps students improve their learning by obtaining\npersonalized feedback in a timely manner. Additionally, the proposed approach\nwill facilitate the establishment of ground truth for automating the generation\nof students assessment feedback using the ChatGPT API, thereby reducing the\noverhead associated with large cohort assessments."
                },
                "authors": [
                    {
                        "name": "Javed Ali Khan"
                    },
                    {
                        "name": "Muhammad Yaqoob"
                    },
                    {
                        "name": "Mamoona Tasadduq"
                    },
                    {
                        "name": "Hafsa Shareef Dar"
                    },
                    {
                        "name": "Aitezaz Ahsan"
                    }
                ],
                "author_detail": {
                    "name": "Aitezaz Ahsan"
                },
                "author": "Aitezaz Ahsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20340v2",
                "updated": "2025-10-13T15:56:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    56,
                    25,
                    0,
                    286,
                    0
                ],
                "published": "2025-05-24T14:17:50Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    14,
                    17,
                    50,
                    5,
                    144,
                    0
                ],
                "title": "Empirical Investigation of Latent Representational Dynamics in Large\n  Language Models: A Manifold Evolution Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Investigation of Latent Representational Dynamics in Large\n  Language Models: A Manifold Evolution Perspective"
                },
                "summary": "This paper introduces the Dynamical Manifold Evolution Theory (DMET), a\nconceptual framework that models large language model (LLM) generation as a\ncontinuous trajectory evolving on a low-dimensional semantic manifold. The\ntheory characterizes latent dynamics through three interpretable metrics-state\ncontinuity ($C$), attractor compactness ($Q$), and topological persistence\n($P$)-which jointly capture the smoothness, stability, and structure of\nrepresentation evolution. Empirical analyses across multiple Transformer\narchitectures reveal consistent links between these latent dynamics and text\nquality: smoother trajectories correspond to greater fluency, and richer\ntopological organization correlates with enhanced coherence. Different models\nexhibit distinct dynamical regimes, reflecting diverse strategies of semantic\norganization in latent space. Moreover, decoding parameters such as temperature\nand top-$p$ shape these trajectories in predictable ways, defining a balanced\nregion that harmonizes fluency and creativity. As a phenomenological rather\nthan first-principles framework, DMET provides a unified and testable\nperspective for interpreting, monitoring, and guiding LLM behavior, offering\nnew insights into the interplay between internal representation dynamics and\nexternal text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Dynamical Manifold Evolution Theory (DMET), a\nconceptual framework that models large language model (LLM) generation as a\ncontinuous trajectory evolving on a low-dimensional semantic manifold. The\ntheory characterizes latent dynamics through three interpretable metrics-state\ncontinuity ($C$), attractor compactness ($Q$), and topological persistence\n($P$)-which jointly capture the smoothness, stability, and structure of\nrepresentation evolution. Empirical analyses across multiple Transformer\narchitectures reveal consistent links between these latent dynamics and text\nquality: smoother trajectories correspond to greater fluency, and richer\ntopological organization correlates with enhanced coherence. Different models\nexhibit distinct dynamical regimes, reflecting diverse strategies of semantic\norganization in latent space. Moreover, decoding parameters such as temperature\nand top-$p$ shape these trajectories in predictable ways, defining a balanced\nregion that harmonizes fluency and creativity. As a phenomenological rather\nthan first-principles framework, DMET provides a unified and testable\nperspective for interpreting, monitoring, and guiding LLM behavior, offering\nnew insights into the interplay between internal representation dynamics and\nexternal text generation quality."
                },
                "authors": [
                    {
                        "name": "Yukun Zhang"
                    },
                    {
                        "name": "Qi Dong"
                    }
                ],
                "author_detail": {
                    "name": "Qi Dong"
                },
                "author": "Qi Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14641v2",
                "updated": "2025-10-13T15:55:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    55,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-06-17T15:39:33Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    39,
                    33,
                    1,
                    168,
                    0
                ],
                "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than\n  Few-shot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than\n  Few-shot"
                },
                "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars."
                },
                "authors": [
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Chengyan Pan"
                    },
                    {
                        "name": "Minjun Zhao"
                    },
                    {
                        "name": "Deyang Li"
                    },
                    {
                        "name": "Fangchao Liu"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "EMNLP25-findings camera_ready, 19 pages,22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11545v1",
                "updated": "2025-10-13T15:42:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    42,
                    11,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:42:11Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    42,
                    11,
                    0,
                    286,
                    0
                ],
                "title": "Information-Preserving Reformulation of Reasoning Traces for\n  Antidistillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-Preserving Reformulation of Reasoning Traces for\n  Antidistillation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) show that extending the\nlength of reasoning chains significantly improves performance on complex tasks.\nWhile revealing these reasoning traces helps users better follow, verify, and\nlearn from the model's problem-solving process, it also makes them highly\nvulnerable to unauthorized distillation. To mitigate this risk, proprietary\nmodel providers often adopt aggressive protection strategies, such as replacing\ndetailed reasoning with brief summaries, which deprive users of valuable\nintermediate information. To address this trade-off, we propose PART, an\ninformation-preserving antidistillation reformulation of reasoning traces.\nMotivated by the difference between how humans understand reasoning traces and\nhow LLMs exploit them for supervised fine-tuning, we design a simple but\neffective two-step reformulation: removing self-talk behaviors and reordering\nsub-conclusions. A small auxiliary model is trained to perform this\nreformulation, incurring minimal computational overhead. Extensive experiments\ndemonstrate that PART consistently disrupts distillation across student models\nof different sizes and types on various reasoning benchmarks. For instance,\nwhen training on reformulated traces, even the performance of a large 32B\nstudent model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a\n13.5% degradation."
                },
                "authors": [
                    {
                        "name": "Jiayu Ding"
                    },
                    {
                        "name": "Lei Cui"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Nanning Zheng"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11541v1",
                "updated": "2025-10-13T15:41:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    41,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:41:15Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    41,
                    15,
                    0,
                    286,
                    0
                ],
                "title": "Query-Specific GNN: A Comprehensive Graph Representation Learning Method\n  for Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Specific GNN: A Comprehensive Graph Representation Learning Method\n  for Retrieval Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has demonstrated its ability to enhance\nLarge Language Models (LLMs) by integrating external knowledge sources.\nHowever, multi-hop questions, which require the identification of multiple\nknowledge targets to form a synthesized answer, raise new challenges for RAG\nsystems. Under the multi-hop settings, existing methods often struggle to fully\nunderstand the questions with complex semantic structures and are susceptible\nto irrelevant noise during the retrieval of multiple information targets. To\naddress these limitations, we propose a novel graph representation learning\nframework for multi-hop question retrieval. We first introduce a\nMulti-information Level Knowledge Graph (Multi-L KG) to model various\ninformation levels for a more comprehensive understanding of multi-hop\nquestions. Based on this, we design a Query-Specific Graph Neural Network\n(QSGNN) for representation learning on the Multi-L KG. QSGNN employs\nintra/inter-level message passing mechanisms, and in each message passing the\ninformation aggregation is guided by the query, which not only facilitates\nmulti-granular information aggregation but also significantly reduces the\nimpact of noise. To enhance its ability to learn robust representations, we\nfurther propose two synthesized data generation strategies for pre-training the\nQSGNN. Extensive experimental results demonstrate the effectiveness of our\nframework in multi-hop scenarios, especially in high-hop questions the\nimprovement can reach 33.8\\%. The code is available at:\nhttps://github.com/Jerry2398/QSGNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has demonstrated its ability to enhance\nLarge Language Models (LLMs) by integrating external knowledge sources.\nHowever, multi-hop questions, which require the identification of multiple\nknowledge targets to form a synthesized answer, raise new challenges for RAG\nsystems. Under the multi-hop settings, existing methods often struggle to fully\nunderstand the questions with complex semantic structures and are susceptible\nto irrelevant noise during the retrieval of multiple information targets. To\naddress these limitations, we propose a novel graph representation learning\nframework for multi-hop question retrieval. We first introduce a\nMulti-information Level Knowledge Graph (Multi-L KG) to model various\ninformation levels for a more comprehensive understanding of multi-hop\nquestions. Based on this, we design a Query-Specific Graph Neural Network\n(QSGNN) for representation learning on the Multi-L KG. QSGNN employs\nintra/inter-level message passing mechanisms, and in each message passing the\ninformation aggregation is guided by the query, which not only facilitates\nmulti-granular information aggregation but also significantly reduces the\nimpact of noise. To enhance its ability to learn robust representations, we\nfurther propose two synthesized data generation strategies for pre-training the\nQSGNN. Extensive experimental results demonstrate the effectiveness of our\nframework in multi-hop scenarios, especially in high-hop questions the\nimprovement can reach 33.8\\%. The code is available at:\nhttps://github.com/Jerry2398/QSGNN."
                },
                "authors": [
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Weiming Li"
                    },
                    {
                        "name": "Xiaoshuai Hao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoshuai Hao"
                },
                "author": "Xiaoshuai Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11536v1",
                "updated": "2025-10-13T15:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    39,
                    8,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    39,
                    8,
                    0,
                    286,
                    0
                ],
                "title": "CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding\n  Interactions with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding\n  Interactions with LLMs"
                },
                "summary": "Understanding how developers interact with code generation tools (CGTs)\nrequires detailed, real-time data on programming behavior which is often\ndifficult to collect without disrupting workflow. We present\n\\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed\nto capture fine-grained interaction events from within the Visual Studio Code\n(VS Code) editor. \\textit{CodeWatcher} logs semantically meaningful events such\nas insertions made by CGTs, deletions, copy-paste actions, and focus shifts,\nenabling continuous monitoring of developer activity without modifying user\nworkflows. The system comprises a VS Code plugin, a Python-based RESTful API,\nand a MongoDB backend, all containerized for scalability and ease of\ndeployment. By structuring and timestamping each event, \\textit{CodeWatcher}\nenables post-hoc reconstruction of coding sessions and facilitates rich\nbehavioral analyses, including how and when CGTs are used during development.\nThis infrastructure is crucial for supporting research on responsible AI,\ndeveloper productivity, and the human-centered evaluation of CGTs. Please find\nthe demo, diagrams, and tool here: https://osf.io/j2kru/overview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how developers interact with code generation tools (CGTs)\nrequires detailed, real-time data on programming behavior which is often\ndifficult to collect without disrupting workflow. We present\n\\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed\nto capture fine-grained interaction events from within the Visual Studio Code\n(VS Code) editor. \\textit{CodeWatcher} logs semantically meaningful events such\nas insertions made by CGTs, deletions, copy-paste actions, and focus shifts,\nenabling continuous monitoring of developer activity without modifying user\nworkflows. The system comprises a VS Code plugin, a Python-based RESTful API,\nand a MongoDB backend, all containerized for scalability and ease of\ndeployment. By structuring and timestamping each event, \\textit{CodeWatcher}\nenables post-hoc reconstruction of coding sessions and facilitates rich\nbehavioral analyses, including how and when CGTs are used during development.\nThis infrastructure is crucial for supporting research on responsible AI,\ndeveloper productivity, and the human-centered evaluation of CGTs. Please find\nthe demo, diagrams, and tool here: https://osf.io/j2kru/overview."
                },
                "authors": [
                    {
                        "name": "Manaal Basha"
                    },
                    {
                        "name": "Aimeê M. Ribeiro"
                    },
                    {
                        "name": "Jeena Javahar"
                    },
                    {
                        "name": "Cleidson R. B. de Souza"
                    },
                    {
                        "name": "Gema Rodríguez-Pérez"
                    }
                ],
                "author_detail": {
                    "name": "Gema Rodríguez-Pérez"
                },
                "author": "Gema Rodríguez-Pérez",
                "arxiv_comment": "ICSME 2025 Tool Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13837v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13837v3",
                "updated": "2025-10-13T15:37:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    37,
                    46,
                    0,
                    286,
                    0
                ],
                "published": "2025-04-18T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    59,
                    56,
                    4,
                    108,
                    0
                ],
                "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning performance of large\nlanguage models (LLMs), particularly on mathematics and programming tasks.\nSimilar to how traditional RL helps agents explore and learn new strategies,\nRLVR is believed to enable LLMs to continuously self-improve, thus acquiring\nnovel reasoning abilities beyond those of the corresponding base models. In\nthis study we critically examine the current state of RLVR by systematically\nprobing the reasoning capability boundaries of RLVR-trained LLMs across various\nmodel families, RL algorithms, and math, coding, and visual reasoning\nbenchmarks, using pass@k at large k values as the evaluation metric.\nSurprisingly, we find that the current training setup does not elicit\nfundamentally new reasoning patterns. While RLVR-trained models outperform\ntheir base models at small k (e.g., k = 1), the base models achieve a higher\npass@k score when k is large. Coverage and perplexity analyses show that the\nobserved reasoning abilities originate from and are bounded by the base model.\nTreating the base model as an upper bound, our quantitative analysis shows that\nsix popular RLVR algorithms perform similarly and remain far from optimal in\nleveraging the potential of the base model. By contrast, we find that\ndistillation can introduce new reasoning patterns from the teacher and\ngenuinely expand the model's reasoning capabilities. Overall, our findings\nsuggest that current RLVR methods have not yet realized the potential of RL to\nelicit truly novel reasoning abilities in LLMs. This highlights the need for\nimproved RL paradigms, such as continual scaling and multi-turn\nagent-environment interaction, to unlock this potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning performance of large\nlanguage models (LLMs), particularly on mathematics and programming tasks.\nSimilar to how traditional RL helps agents explore and learn new strategies,\nRLVR is believed to enable LLMs to continuously self-improve, thus acquiring\nnovel reasoning abilities beyond those of the corresponding base models. In\nthis study we critically examine the current state of RLVR by systematically\nprobing the reasoning capability boundaries of RLVR-trained LLMs across various\nmodel families, RL algorithms, and math, coding, and visual reasoning\nbenchmarks, using pass@k at large k values as the evaluation metric.\nSurprisingly, we find that the current training setup does not elicit\nfundamentally new reasoning patterns. While RLVR-trained models outperform\ntheir base models at small k (e.g., k = 1), the base models achieve a higher\npass@k score when k is large. Coverage and perplexity analyses show that the\nobserved reasoning abilities originate from and are bounded by the base model.\nTreating the base model as an upper bound, our quantitative analysis shows that\nsix popular RLVR algorithms perform similarly and remain far from optimal in\nleveraging the potential of the base model. By contrast, we find that\ndistillation can introduce new reasoning patterns from the teacher and\ngenuinely expand the model's reasoning capabilities. Overall, our findings\nsuggest that current RLVR methods have not yet realized the potential of RL to\nelicit truly novel reasoning abilities in LLMs. This highlights the need for\nimproved RL paradigms, such as continual scaling and multi-turn\nagent-environment interaction, to unlock this potential."
                },
                "authors": [
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Zhiqi Chen"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Shiji Song"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "30 pages, 27 figures",
                "arxiv_journal_ref": "NeurIPS 2025 Oral; ICML 2025 AI4MATH workshop best paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13837v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13837v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11529v1",
                "updated": "2025-10-13T15:31:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    31,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:31:21Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    31,
                    21,
                    0,
                    286,
                    0
                ],
                "title": "Hallucination Detection via Internal States and Structured Reasoning\n  Consistency in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection via Internal States and Structured Reasoning\n  Consistency in Large Language Models"
                },
                "summary": "The detection of sophisticated hallucinations in Large Language Models (LLMs)\nis hampered by a ``Detection Dilemma'': methods probing internal states\n(Internal State Probing) excel at identifying factual inconsistencies but fail\non logical fallacies, while those verifying externalized reasoning\n(Chain-of-Thought Verification) show the opposite behavior. This schism creates\na task-dependent blind spot: Chain-of-Thought Verification fails on\nfact-intensive tasks like open-domain QA where reasoning is ungrounded, while\nInternal State Probing is ineffective on logic-intensive tasks like\nmathematical reasoning where models are confidently wrong. We resolve this with\na unified framework that bridges this critical gap. However, unification is\nhindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse\nsymbolic reasoning chains lack signals directly comparable to fine-grained\ninternal states, and the Representational Alignment Barrier, a deep-seated\nmismatch between their underlying semantic spaces. To overcome these, we\nintroduce a multi-path reasoning mechanism to obtain more comparable,\nfine-grained signals, and a segment-aware temporalized cross-attention module\nto adaptively fuse these now-aligned representations, pinpointing subtle\ndissonances. Extensive experiments on three diverse benchmarks and two leading\nLLMs demonstrate that our framework consistently and significantly outperforms\nstrong baselines. Our code is available: https://github.com/peach918/HalluDet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of sophisticated hallucinations in Large Language Models (LLMs)\nis hampered by a ``Detection Dilemma'': methods probing internal states\n(Internal State Probing) excel at identifying factual inconsistencies but fail\non logical fallacies, while those verifying externalized reasoning\n(Chain-of-Thought Verification) show the opposite behavior. This schism creates\na task-dependent blind spot: Chain-of-Thought Verification fails on\nfact-intensive tasks like open-domain QA where reasoning is ungrounded, while\nInternal State Probing is ineffective on logic-intensive tasks like\nmathematical reasoning where models are confidently wrong. We resolve this with\na unified framework that bridges this critical gap. However, unification is\nhindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse\nsymbolic reasoning chains lack signals directly comparable to fine-grained\ninternal states, and the Representational Alignment Barrier, a deep-seated\nmismatch between their underlying semantic spaces. To overcome these, we\nintroduce a multi-path reasoning mechanism to obtain more comparable,\nfine-grained signals, and a segment-aware temporalized cross-attention module\nto adaptively fuse these now-aligned representations, pinpointing subtle\ndissonances. Extensive experiments on three diverse benchmarks and two leading\nLLMs demonstrate that our framework consistently and significantly outperforms\nstrong baselines. Our code is available: https://github.com/peach918/HalluDet."
                },
                "authors": [
                    {
                        "name": "Yusheng Song"
                    },
                    {
                        "name": "Lirong Qiu"
                    },
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Zhihao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Tang"
                },
                "author": "Zhihao Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16210v2",
                "updated": "2025-10-13T15:31:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    31,
                    20,
                    0,
                    286,
                    0
                ],
                "published": "2025-07-22T04:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    0,
                    9,
                    1,
                    203,
                    0
                ],
                "title": "Joint Active and Passive Beamforming for Energy-Efficient STARS with\n  Quantization and Element Selection in ISAC Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Active and Passive Beamforming for Energy-Efficient STARS with\n  Quantization and Element Selection in ISAC Systems"
                },
                "summary": "This paper investigates a simultaneously transmitting and reflecting\nreconfigurable intelligent surface (STARS)-aided integrated sensing and\ncommunication (ISAC) systems in support of full-space energy-efficient data\ntransmissions and target sensing. We formulate an energy efficiency (EE)\nmaximization problem that jointly optimizes a dual-functional\nradar-communication (DFRC)-empowered base station (BS), considering its\nISAC-based active beamforming, along with the passive STARS beamforming\nconfigurations of amplitudes, phase shifts, quantization levels, and element\nselection. Furthermore, relaxed/independent/coupled STARS are considered to\nexamine architectural flexibility. To tackle the non-convex and mixed-integer\nproblem, we propose a joint active-passive beamforming, quantization and\nelement selection (AQUES) scheme based on the alternating optimization:\nLagrangian dual and Dinkelbach's transformation tackle fractional equations,\nwhereas successive convex approximation (SCA) convexifies the non-solvable\nproblem; Penalty dual decomposition (PDD) framework and penalty-based\nconvex-concave programming (PCCP) procedure solve amplitude and phase-shifts\nwith the equality constraint; Heuristic search iteratively decides the optimal\nquantization level; Integer relaxation deals with the discrete element\nselection variables. Simulation results demonstrate that STARS-ISAC with the\nproposed AQUES scheme significantly enhances EE while meeting communication\nrates and sensing quality requirements. The coupled STARS further highlights\nits superior EE performance over independent and relaxed STARS thanks to its\nreduced hardware complexity. Moreover, AQUES outperforms existing\nconfigurations and benchmark methods in the open literature across various\nnetwork parameters and deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates a simultaneously transmitting and reflecting\nreconfigurable intelligent surface (STARS)-aided integrated sensing and\ncommunication (ISAC) systems in support of full-space energy-efficient data\ntransmissions and target sensing. We formulate an energy efficiency (EE)\nmaximization problem that jointly optimizes a dual-functional\nradar-communication (DFRC)-empowered base station (BS), considering its\nISAC-based active beamforming, along with the passive STARS beamforming\nconfigurations of amplitudes, phase shifts, quantization levels, and element\nselection. Furthermore, relaxed/independent/coupled STARS are considered to\nexamine architectural flexibility. To tackle the non-convex and mixed-integer\nproblem, we propose a joint active-passive beamforming, quantization and\nelement selection (AQUES) scheme based on the alternating optimization:\nLagrangian dual and Dinkelbach's transformation tackle fractional equations,\nwhereas successive convex approximation (SCA) convexifies the non-solvable\nproblem; Penalty dual decomposition (PDD) framework and penalty-based\nconvex-concave programming (PCCP) procedure solve amplitude and phase-shifts\nwith the equality constraint; Heuristic search iteratively decides the optimal\nquantization level; Integer relaxation deals with the discrete element\nselection variables. Simulation results demonstrate that STARS-ISAC with the\nproposed AQUES scheme significantly enhances EE while meeting communication\nrates and sensing quality requirements. The coupled STARS further highlights\nits superior EE performance over independent and relaxed STARS thanks to its\nreduced hardware complexity. Moreover, AQUES outperforms existing\nconfigurations and benchmark methods in the open literature across various\nnetwork parameters and deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Li-Hsiang Shen"
                    },
                    {
                        "name": "Yi-Hsuan Chiu"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Hsuan Chiu"
                },
                "author": "Yi-Hsuan Chiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08338v2",
                "updated": "2025-10-13T15:22:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    22,
                    47,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T15:24:48Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    24,
                    48,
                    3,
                    282,
                    0
                ],
                "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation\n  of Likert Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation\n  of Likert Ratings"
                },
                "summary": "Consumer research costs companies billions annually yet suffers from panel\nbiases and limited scale. Large language models (LLMs) offer an alternative by\nsimulating synthetic consumers, but produce unrealistic response distributions\nwhen asked directly for numerical ratings. We present semantic similarity\nrating (SSR), a method that elicits textual responses from LLMs and maps these\nto Likert distributions using embedding similarity to reference statements.\nTesting on an extensive dataset comprising 57 personal care product surveys\nconducted by a leading corporation in that market (9,300 human responses), SSR\nachieves 90% of human test-retest reliability while maintaining realistic\nresponse distributions (KS similarity > 0.85). Additionally, these synthetic\nrespondents provide rich qualitative feedback explaining their ratings. This\nframework enables scalable consumer research simulations while preserving\ntraditional survey metrics and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consumer research costs companies billions annually yet suffers from panel\nbiases and limited scale. Large language models (LLMs) offer an alternative by\nsimulating synthetic consumers, but produce unrealistic response distributions\nwhen asked directly for numerical ratings. We present semantic similarity\nrating (SSR), a method that elicits textual responses from LLMs and maps these\nto Likert distributions using embedding similarity to reference statements.\nTesting on an extensive dataset comprising 57 personal care product surveys\nconducted by a leading corporation in that market (9,300 human responses), SSR\nachieves 90% of human test-retest reliability while maintaining realistic\nresponse distributions (KS similarity > 0.85). Additionally, these synthetic\nrespondents provide rich qualitative feedback explaining their ratings. This\nframework enables scalable consumer research simulations while preserving\ntraditional survey metrics and interpretability."
                },
                "authors": [
                    {
                        "name": "Benjamin F. Maier"
                    },
                    {
                        "name": "Ulf Aslak"
                    },
                    {
                        "name": "Luca Fiaschi"
                    },
                    {
                        "name": "Nina Rismal"
                    },
                    {
                        "name": "Kemble Fletcher"
                    },
                    {
                        "name": "Christian C. Luhmann"
                    },
                    {
                        "name": "Robbie Dow"
                    },
                    {
                        "name": "Kli Pappas"
                    },
                    {
                        "name": "Thomas V. Wiecki"
                    }
                ],
                "author_detail": {
                    "name": "Thomas V. Wiecki"
                },
                "author": "Thomas V. Wiecki",
                "arxiv_comment": "28 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11516v1",
                "updated": "2025-10-13T15:22:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    22,
                    12,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:22:12Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    22,
                    12,
                    0,
                    286,
                    0
                ],
                "title": "Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns\n  During Programming Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns\n  During Programming Tasks"
                },
                "summary": "The use of AI code-generation tools is becoming increasingly common, making\nit important to understand how software developers are adopting these tools. In\nthis study, we investigate how developers engage with Amazon's CodeWhisperer,\nan LLM-based code-generation tool. We conducted two user studies with two\ngroups of 10 participants each, interacting with CodeWhisperer - the first to\nunderstand which interactions were critical to capture and the second to\ncollect low-level interaction data using a custom telemetry plugin. Our\nmixed-methods analysis identified four behavioral patterns: 1) incremental code\nrefinement, 2) explicit instruction using natural language comments, 3)\nbaseline structuring with model suggestions, and 4) integrative use with\nexternal sources. We provide a comprehensive analysis of these patterns .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of AI code-generation tools is becoming increasingly common, making\nit important to understand how software developers are adopting these tools. In\nthis study, we investigate how developers engage with Amazon's CodeWhisperer,\nan LLM-based code-generation tool. We conducted two user studies with two\ngroups of 10 participants each, interacting with CodeWhisperer - the first to\nunderstand which interactions were critical to capture and the second to\ncollect low-level interaction data using a custom telemetry plugin. Our\nmixed-methods analysis identified four behavioral patterns: 1) incremental code\nrefinement, 2) explicit instruction using natural language comments, 3)\nbaseline structuring with model suggestions, and 4) integrative use with\nexternal sources. We provide a comprehensive analysis of these patterns ."
                },
                "authors": [
                    {
                        "name": "Jeena Javahar"
                    },
                    {
                        "name": "Tanya Budhrani"
                    },
                    {
                        "name": "Manaal Basha"
                    },
                    {
                        "name": "Cleidson R. B. de Souza"
                    },
                    {
                        "name": "Ivan Beschastnikh"
                    },
                    {
                        "name": "Gema Rodriguez-Perez"
                    }
                ],
                "author_detail": {
                    "name": "Gema Rodriguez-Perez"
                },
                "author": "Gema Rodriguez-Perez",
                "arxiv_comment": "VL/HCC 2025 Short Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16204v2",
                "updated": "2025-10-13T15:17:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    17,
                    47,
                    0,
                    286,
                    0
                ],
                "published": "2025-07-22T03:40:56Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    3,
                    40,
                    56,
                    1,
                    203,
                    0
                ],
                "title": "Multi-Functional RIS-Enabled in SAGIN for IoT: A Hybrid Deep\n  Reinforcement Learning Approach with Compressed Twin-Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Functional RIS-Enabled in SAGIN for IoT: A Hybrid Deep\n  Reinforcement Learning Approach with Compressed Twin-Models"
                },
                "summary": "A space-air-ground integrated network (SAGIN) for Internet of Things (IoT)\nnetwork architecture is investigated, empowered by multi-functional\nreconfigurable intelligent surfaces (MF-RIS) capable of simultaneously\nreflecting, amplifying, and harvesting wireless energy. The MF-RIS plays a\npivotal role in addressing the energy shortages of low-Earth orbit (LEO)\nsatellites operating in the shadowed regions, while accounting for both\ncommunication and computing energy consumption across the SAGIN nodes. To\nmaximize the long-term energy efficiency (EE) of IoT devices, we formulate a\njoint optimization problem over the MF-RIS parameters, including signal\namplification, phase-shifts, energy harvesting ratio, and active element\nselection as well as the SAGIN parameters of beamforming vectors, high-altitude\nplatform station (HAPS) deployment, IoT device association, and computing\ncapability. The formulated problem is highly non-convex and non-linear and\ncontains mixed discrete-continuous parameters. To tackle this, we conceive a\ncompressed hybrid twin-model enhanced multi-agent deep reinforcement learning\n(CHIMERA) framework, which integrates semantic state-action compression and\nparametrized sharing under hybrid reinforcement learning to efficiently explore\nsuitable complex actions. The simulation results have demonstrated that the\nproposed CHIMERA scheme substantially outperforms the conventional benchmarks,\nincluding fixed-configuration or non-harvesting MF-RIS, traditional RIS, and\nno-RIS cases, as well as centralized and multi-agent deep reinforcement\nlearning baselines in terms of the highest EE. Moreover, the proposed\nSAGIN-MF-RIS architecture in IoT network achieves superior EE performance due\nto its complementary coverage, offering notable advantages over either\nstandalone satellite, aerial, or ground-only deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A space-air-ground integrated network (SAGIN) for Internet of Things (IoT)\nnetwork architecture is investigated, empowered by multi-functional\nreconfigurable intelligent surfaces (MF-RIS) capable of simultaneously\nreflecting, amplifying, and harvesting wireless energy. The MF-RIS plays a\npivotal role in addressing the energy shortages of low-Earth orbit (LEO)\nsatellites operating in the shadowed regions, while accounting for both\ncommunication and computing energy consumption across the SAGIN nodes. To\nmaximize the long-term energy efficiency (EE) of IoT devices, we formulate a\njoint optimization problem over the MF-RIS parameters, including signal\namplification, phase-shifts, energy harvesting ratio, and active element\nselection as well as the SAGIN parameters of beamforming vectors, high-altitude\nplatform station (HAPS) deployment, IoT device association, and computing\ncapability. The formulated problem is highly non-convex and non-linear and\ncontains mixed discrete-continuous parameters. To tackle this, we conceive a\ncompressed hybrid twin-model enhanced multi-agent deep reinforcement learning\n(CHIMERA) framework, which integrates semantic state-action compression and\nparametrized sharing under hybrid reinforcement learning to efficiently explore\nsuitable complex actions. The simulation results have demonstrated that the\nproposed CHIMERA scheme substantially outperforms the conventional benchmarks,\nincluding fixed-configuration or non-harvesting MF-RIS, traditional RIS, and\nno-RIS cases, as well as centralized and multi-agent deep reinforcement\nlearning baselines in terms of the highest EE. Moreover, the proposed\nSAGIN-MF-RIS architecture in IoT network achieves superior EE performance due\nto its complementary coverage, offering notable advantages over either\nstandalone satellite, aerial, or ground-only deployments."
                },
                "authors": [
                    {
                        "name": "Li-Hsiang Shen"
                    },
                    {
                        "name": "Jyun-Jhe Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jyun-Jhe Huang"
                },
                "author": "Jyun-Jhe Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11509v1",
                "updated": "2025-10-13T15:17:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    17,
                    18,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:17:18Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    17,
                    18,
                    0,
                    286,
                    0
                ],
                "title": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal\n  Large Language Model"
                },
                "summary": "Physical environments and circumstances are fundamentally dynamic, yet\ncurrent 3D datasets and evaluation benchmarks tend to concentrate on either\ndynamic scenarios or dynamic situations in isolation, resulting in incomplete\ncomprehension. To overcome these constraints, we introduce Situat3DChange, an\nextensive dataset supporting three situation-aware change understanding tasks\nfollowing the perception-action model: 121K question-answer pairs, 36K change\ndescriptions for perception tasks, and 17K rearrangement instructions for the\naction task. To construct this large-scale dataset, Situat3DChange leverages\n11K human observations of environmental changes to establish shared mental\nmodels and shared situational awareness for human-AI collaboration. These\nobservations, enriched with egocentric and allocentric perspectives as well as\ncategorical and coordinate spatial relations, are integrated using an LLM to\nsupport understanding of situated changes. To address the challenge of\ncomparing pairs of point clouds from the same scene with minor changes, we\npropose SCReasoner, an efficient 3D MLLM approach that enables effective point\ncloud comparison with minimal parameter overhead and no additional tokens\nrequired for the language decoder. Comprehensive evaluation on Situat3DChange\ntasks highlights both the progress and limitations of MLLMs in dynamic scene\nand situation understanding. Additional experiments on data scaling and\ncross-domain transfer demonstrate the task-agnostic effectiveness of using\nSituat3DChange as a training dataset for MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical environments and circumstances are fundamentally dynamic, yet\ncurrent 3D datasets and evaluation benchmarks tend to concentrate on either\ndynamic scenarios or dynamic situations in isolation, resulting in incomplete\ncomprehension. To overcome these constraints, we introduce Situat3DChange, an\nextensive dataset supporting three situation-aware change understanding tasks\nfollowing the perception-action model: 121K question-answer pairs, 36K change\ndescriptions for perception tasks, and 17K rearrangement instructions for the\naction task. To construct this large-scale dataset, Situat3DChange leverages\n11K human observations of environmental changes to establish shared mental\nmodels and shared situational awareness for human-AI collaboration. These\nobservations, enriched with egocentric and allocentric perspectives as well as\ncategorical and coordinate spatial relations, are integrated using an LLM to\nsupport understanding of situated changes. To address the challenge of\ncomparing pairs of point clouds from the same scene with minor changes, we\npropose SCReasoner, an efficient 3D MLLM approach that enables effective point\ncloud comparison with minimal parameter overhead and no additional tokens\nrequired for the language decoder. Comprehensive evaluation on Situat3DChange\ntasks highlights both the progress and limitations of MLLMs in dynamic scene\nand situation understanding. Additional experiments on data scaling and\ncross-domain transfer demonstrate the task-agnostic effectiveness of using\nSituat3DChange as a training dataset for MLLMs."
                },
                "authors": [
                    {
                        "name": "Ruiping Liu"
                    },
                    {
                        "name": "Junwei Zheng"
                    },
                    {
                        "name": "Yufan Chen"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Kunyu Peng"
                    },
                    {
                        "name": "Kailun Yang"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Marc Pollefeys"
                    },
                    {
                        "name": "Rainer Stiefelhagen"
                    }
                ],
                "author_detail": {
                    "name": "Rainer Stiefelhagen"
                },
                "author": "Rainer Stiefelhagen",
                "arxiv_comment": "Accepted to NeurIPS 2025 Datasets and Benchmarks Track. Dataset and\n  Code: https://github.com/RuipingL/Situat3DChange",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11498v1",
                "updated": "2025-10-13T15:05:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    5,
                    50,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T15:05:50Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    5,
                    50,
                    0,
                    286,
                    0
                ],
                "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding"
                },
                "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling."
                },
                "authors": [
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Ruilin Lv"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Wiggin Zhou"
                    },
                    {
                        "name": "Bo Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhou"
                },
                "author": "Bo Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11496v2",
                "updated": "2025-10-14T05:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    5,
                    5,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model"
                },
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer."
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu",
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04641v2",
                "updated": "2025-10-13T15:04:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-06T09:45:32Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    45,
                    32,
                    0,
                    279,
                    0
                ],
                "title": "Evaluating LLMs for Demographic-Targeted Social Bias Detection: A\n  Comprehensive Benchmark Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Demographic-Targeted Social Bias Detection: A\n  Comprehensive Benchmark Study"
                },
                "summary": "Large-scale web-scraped text corpora used to train general-purpose AI models\noften contain harmful demographic-targeted social biases, creating a regulatory\nneed for data auditing and developing scalable bias-detection methods. Although\nprior work has investigated biases in text datasets and related detection\nmethods, these studies remain narrow in scope. They typically focus on a single\ncontent type (e.g., hate speech), cover limited demographic axes, overlook\nbiases affecting multiple demographics simultaneously, and analyze limited\ntechniques. Consequently, practitioners lack a holistic understanding of the\nstrengths and limitations of recent large language models (LLMs) for automated\nbias detection. In this study, we present a comprehensive evaluation framework\naimed at English texts to assess the ability of LLMs in detecting\ndemographic-targeted social biases. To align with regulatory requirements, we\nframe bias detection as a multi-label task using a demographic-focused\ntaxonomy. We then conduct a systematic evaluation with models across scales and\ntechniques, including prompting, in-context learning, and fine-tuning. Using\ntwelve datasets spanning diverse content types and demographics, our study\ndemonstrates the promise of fine-tuned smaller models for scalable detection.\nHowever, our analyses also expose persistent gaps across demographic axes and\nmulti-demographic targeted biases, underscoring the need for more effective and\nscalable auditing frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale web-scraped text corpora used to train general-purpose AI models\noften contain harmful demographic-targeted social biases, creating a regulatory\nneed for data auditing and developing scalable bias-detection methods. Although\nprior work has investigated biases in text datasets and related detection\nmethods, these studies remain narrow in scope. They typically focus on a single\ncontent type (e.g., hate speech), cover limited demographic axes, overlook\nbiases affecting multiple demographics simultaneously, and analyze limited\ntechniques. Consequently, practitioners lack a holistic understanding of the\nstrengths and limitations of recent large language models (LLMs) for automated\nbias detection. In this study, we present a comprehensive evaluation framework\naimed at English texts to assess the ability of LLMs in detecting\ndemographic-targeted social biases. To align with regulatory requirements, we\nframe bias detection as a multi-label task using a demographic-focused\ntaxonomy. We then conduct a systematic evaluation with models across scales and\ntechniques, including prompting, in-context learning, and fine-tuning. Using\ntwelve datasets spanning diverse content types and demographics, our study\ndemonstrates the promise of fine-tuned smaller models for scalable detection.\nHowever, our analyses also expose persistent gaps across demographic axes and\nmulti-demographic targeted biases, underscoring the need for more effective and\nscalable auditing frameworks."
                },
                "authors": [
                    {
                        "name": "Ayan Majumdar"
                    },
                    {
                        "name": "Feihao Chen"
                    },
                    {
                        "name": "Jinghui Li"
                    },
                    {
                        "name": "Xiaozhen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhen Wang"
                },
                "author": "Xiaozhen Wang",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05970v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05970v3",
                "updated": "2025-10-13T15:01:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    1,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-07-08T13:24:05Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    13,
                    24,
                    5,
                    1,
                    189,
                    0
                ],
                "title": "Automatic Synthesis of High-Quality Triplet Data for Composed Image\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Synthesis of High-Quality Triplet Data for Composed Image\n  Retrieval"
                },
                "summary": "As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)\naims to retrieve target images using multimodal (image+text) queries. Although\nmany existing CIR methods have attained promising performance, their reliance\non costly, manually labeled triplets hinders scalability and zero-shot\ncapability. To address this issue, we propose a scalable pipeline for automatic\ntriplet generation, along with a fully synthetic dataset named Composed Image\nRetrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a\nlarge language model (LLM) to generate diverse prompts, controlling a\ntext-to-image generative model to produce image pairs with identical elements\nin each pair, which are then filtered and reorganized to form the CIRHS\ndataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a\nnovel CIR framework, which can accomplish global alignment and local reasoning\nwithin a broader context, enabling the model to learn more robust and\ninformative representations. By utilizing the synthetic CIRHS dataset, CoAlign\nachieves outstanding zero-shot performance on three commonly used benchmarks,\ndemonstrating for the first time the feasibility of training CIR models on a\nfully synthetic dataset. Furthermore, under supervised training, our method\noutperforms all the state-of-the-art supervised CIR approaches, validating the\neffectiveness of our proposed retrieval framework. The code and the CIRHS\ndataset will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)\naims to retrieve target images using multimodal (image+text) queries. Although\nmany existing CIR methods have attained promising performance, their reliance\non costly, manually labeled triplets hinders scalability and zero-shot\ncapability. To address this issue, we propose a scalable pipeline for automatic\ntriplet generation, along with a fully synthetic dataset named Composed Image\nRetrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a\nlarge language model (LLM) to generate diverse prompts, controlling a\ntext-to-image generative model to produce image pairs with identical elements\nin each pair, which are then filtered and reorganized to form the CIRHS\ndataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a\nnovel CIR framework, which can accomplish global alignment and local reasoning\nwithin a broader context, enabling the model to learn more robust and\ninformative representations. By utilizing the synthetic CIRHS dataset, CoAlign\nachieves outstanding zero-shot performance on three commonly used benchmarks,\ndemonstrating for the first time the feasibility of training CIR models on a\nfully synthetic dataset. Furthermore, under supervised training, our method\noutperforms all the state-of-the-art supervised CIR approaches, validating the\neffectiveness of our proposed retrieval framework. The code and the CIRHS\ndataset will be released soon."
                },
                "authors": [
                    {
                        "name": "Haiwen Li"
                    },
                    {
                        "name": "Delong Liu"
                    },
                    {
                        "name": "Zhaohui Hou"
                    },
                    {
                        "name": "Zhicheng Zhao"
                    },
                    {
                        "name": "Fei Su"
                    }
                ],
                "author_detail": {
                    "name": "Fei Su"
                },
                "author": "Fei Su",
                "arxiv_comment": "This paper was originally submitted to ACM MM 2025 on April 12, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05970v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05970v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02502v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02502v3",
                "updated": "2025-10-13T14:55:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    55,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-04T11:10:13Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    10,
                    13,
                    1,
                    63,
                    0
                ],
                "title": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs"
                },
                "summary": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training."
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Yangyifan Xu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "ACL 2025, our code is available at https://github.com/ZNLP/LADM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02502v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02502v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11484v1",
                "updated": "2025-10-13T14:55:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    55,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:55:34Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    55,
                    34,
                    0,
                    286,
                    0
                ],
                "title": "Rescaling-Aware Training for Efficient Deployment of Deep Learning\n  Models on Full-Integer Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rescaling-Aware Training for Efficient Deployment of Deep Learning\n  Models on Full-Integer Hardware"
                },
                "summary": "Integer AI inference significantly reduces computational complexity in\nembedded systems. Quantization-aware training (QAT) helps mitigate accuracy\ndegradation associated with post-training quantization but still overlooks the\nimpact of integer rescaling during inference, which is a hardware costly\noperation in integer-only AI inference. This work shows that rescaling cost can\nbe dramatically reduced post-training, by applying a stronger quantization to\nthe rescale multiplicands at no model-quality loss. Furthermore, we introduce\nRescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling\nmultiplicands. Experiments show that even with 8x reduced rescaler widths, the\nfull accuracy is preserved through minimal incremental retraining. This enables\nmore energy-efficient and cost-efficient AI inference for resource-constrained\nembedded systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integer AI inference significantly reduces computational complexity in\nembedded systems. Quantization-aware training (QAT) helps mitigate accuracy\ndegradation associated with post-training quantization but still overlooks the\nimpact of integer rescaling during inference, which is a hardware costly\noperation in integer-only AI inference. This work shows that rescaling cost can\nbe dramatically reduced post-training, by applying a stronger quantization to\nthe rescale multiplicands at no model-quality loss. Furthermore, we introduce\nRescale-Aware Training, a fine tuning method for ultra-low bit-width rescaling\nmultiplicands. Experiments show that even with 8x reduced rescaler widths, the\nfull accuracy is preserved through minimal incremental retraining. This enables\nmore energy-efficient and cost-efficient AI inference for resource-constrained\nembedded systems."
                },
                "authors": [
                    {
                        "name": "Lion Mueller"
                    },
                    {
                        "name": "Alberto Garcia-Ortiz"
                    },
                    {
                        "name": "Ardalan Najafi"
                    },
                    {
                        "name": "Adam Fuks"
                    },
                    {
                        "name": "Lennart Bamberg"
                    }
                ],
                "author_detail": {
                    "name": "Lennart Bamberg"
                },
                "author": "Lennart Bamberg",
                "arxiv_comment": "Submitted to IEEE Embedded Systems Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11482v1",
                "updated": "2025-10-13T14:53:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    53,
                    44,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:53:44Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    53,
                    44,
                    0,
                    286,
                    0
                ],
                "title": "Investigating Large Language Models' Linguistic Abilities for Text\n  Preprocessing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Large Language Models' Linguistic Abilities for Text\n  Preprocessing"
                },
                "summary": "Text preprocessing is a fundamental component of Natural Language Processing,\ninvolving techniques such as stopword removal, stemming, and lemmatization to\nprepare text as input for further processing and analysis. Despite the\ncontext-dependent nature of the above techniques, traditional methods usually\nignore contextual information. In this paper, we investigate the idea of using\nLarge Language Models (LLMs) to perform various preprocessing tasks, due to\ntheir ability to take context into account without requiring extensive\nlanguage-specific annotated resources. Through a comprehensive evaluation on\nweb-sourced data, we compare LLM-based preprocessing (specifically stopword\nremoval, lemmatization and stemming) to traditional algorithms across multiple\ntext classification tasks in six European languages. Our analysis indicates\nthat LLMs are capable of replicating traditional stopword removal,\nlemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,\nrespectively. Additionally, we show that ML algorithms trained on texts\npreprocessed by LLMs achieve an improvement of up to 6% with respect to the\n$F_1$ measure compared to traditional techniques. Our code, prompts, and\nresults are publicly available at\nhttps://github.com/GianCarloMilanese/llm_pipeline_wi-iat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text preprocessing is a fundamental component of Natural Language Processing,\ninvolving techniques such as stopword removal, stemming, and lemmatization to\nprepare text as input for further processing and analysis. Despite the\ncontext-dependent nature of the above techniques, traditional methods usually\nignore contextual information. In this paper, we investigate the idea of using\nLarge Language Models (LLMs) to perform various preprocessing tasks, due to\ntheir ability to take context into account without requiring extensive\nlanguage-specific annotated resources. Through a comprehensive evaluation on\nweb-sourced data, we compare LLM-based preprocessing (specifically stopword\nremoval, lemmatization and stemming) to traditional algorithms across multiple\ntext classification tasks in six European languages. Our analysis indicates\nthat LLMs are capable of replicating traditional stopword removal,\nlemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,\nrespectively. Additionally, we show that ML algorithms trained on texts\npreprocessed by LLMs achieve an improvement of up to 6% with respect to the\n$F_1$ measure compared to traditional techniques. Our code, prompts, and\nresults are publicly available at\nhttps://github.com/GianCarloMilanese/llm_pipeline_wi-iat."
                },
                "authors": [
                    {
                        "name": "Marco Braga"
                    },
                    {
                        "name": "Gian Carlo Milanese"
                    },
                    {
                        "name": "Gabriella Pasi"
                    }
                ],
                "author_detail": {
                    "name": "Gabriella Pasi"
                },
                "author": "Gabriella Pasi",
                "arxiv_comment": "Accepted in WI-IAT 2025. Pre-camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12094v2",
                "updated": "2025-10-13T14:53:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    53,
                    33,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-16T16:31:00Z",
                "published_parsed": [
                    2025,
                    8,
                    16,
                    16,
                    31,
                    0,
                    5,
                    228,
                    0
                ],
                "title": "Error Propagation Mechanisms and Compensation Strategies for Quantized\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error Propagation Mechanisms and Compensation Strategies for Quantized\n  Diffusion"
                },
                "summary": "Diffusion models have transformed image synthesis by establishing\nunprecedented quality and creativity benchmarks. Nevertheless, their\nlarge-scale deployment faces challenges due to computationally intensive\niterative denoising processes. Although post-training quantization (PTQ)\nprovides an effective pathway for accelerating sampling, the iterative nature\nof diffusion models causes stepwise quantization errors to accumulate\nprogressively during generation, inevitably compromising output fidelity. To\naddress this challenge, we develop a theoretical framework that mathematically\nformulates error propagation in Diffusion Models (DMs), deriving per-step\nquantization error propagation equations and establishing the first closed-form\nsolution for cumulative error. Building on this theoretical foundation, we\npropose a timestep-aware cumulative error compensation scheme. Extensive\nexperiments on multiple image datasets demonstrate that our compensation\nstrategy effectively mitigates error propagation, significantly enhancing\nexisting PTQ methods. Specifically, it achieves a 1.2 PSNR improvement over\nSVDQuant on SDXL W4A4, while incurring only an additional $<$ 0.5\\% time\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have transformed image synthesis by establishing\nunprecedented quality and creativity benchmarks. Nevertheless, their\nlarge-scale deployment faces challenges due to computationally intensive\niterative denoising processes. Although post-training quantization (PTQ)\nprovides an effective pathway for accelerating sampling, the iterative nature\nof diffusion models causes stepwise quantization errors to accumulate\nprogressively during generation, inevitably compromising output fidelity. To\naddress this challenge, we develop a theoretical framework that mathematically\nformulates error propagation in Diffusion Models (DMs), deriving per-step\nquantization error propagation equations and establishing the first closed-form\nsolution for cumulative error. Building on this theoretical foundation, we\npropose a timestep-aware cumulative error compensation scheme. Extensive\nexperiments on multiple image datasets demonstrate that our compensation\nstrategy effectively mitigates error propagation, significantly enhancing\nexisting PTQ methods. Specifically, it achieves a 1.2 PSNR improvement over\nSVDQuant on SDXL W4A4, while incurring only an additional $<$ 0.5\\% time\noverhead."
                },
                "authors": [
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Chao Zeng"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Mei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Mei"
                },
                "author": "Xing Mei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11471v1",
                "updated": "2025-10-13T14:40:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    40,
                    47,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:40:47Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    40,
                    47,
                    0,
                    286,
                    0
                ],
                "title": "Iterative Amortized Inference: Unifying In-Context Learning and Learned\n  Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Amortized Inference: Unifying In-Context Learning and Learned\n  Optimizers"
                },
                "summary": "Modern learning systems increasingly rely on amortized learning - the idea of\nreusing computation or inductive biases shared across tasks to enable rapid\ngeneralization to novel problems. This principle spans a range of approaches,\nincluding meta-learning, in-context learning, prompt tuning, learned optimizers\nand more. While motivated by similar goals, these approaches differ in how they\nencode and leverage task-specific information, often provided as in-context\nexamples. In this work, we propose a unified framework which describes how such\nmethods differ primarily in the aspects of learning they amortize - such as\ninitializations, learned updates, or predictive mappings - and how they\nincorporate task data at inference. We introduce a taxonomy that categorizes\namortized models into parametric, implicit, and explicit regimes, based on\nwhether task adaptation is externalized, internalized, or jointly modeled.\nBuilding on this view, we identify a key limitation in current approaches: most\nmethods struggle to scale to large datasets because their capacity to process\ntask data at inference (e.g., context length) is often limited. To address\nthis, we propose iterative amortized inference, a class of models that refine\nsolutions step-by-step over mini-batches, drawing inspiration from stochastic\noptimization. Our formulation bridges optimization-based meta-learning with\nforward-pass amortization in models like LLMs, offering a scalable and\nextensible foundation for general-purpose task adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern learning systems increasingly rely on amortized learning - the idea of\nreusing computation or inductive biases shared across tasks to enable rapid\ngeneralization to novel problems. This principle spans a range of approaches,\nincluding meta-learning, in-context learning, prompt tuning, learned optimizers\nand more. While motivated by similar goals, these approaches differ in how they\nencode and leverage task-specific information, often provided as in-context\nexamples. In this work, we propose a unified framework which describes how such\nmethods differ primarily in the aspects of learning they amortize - such as\ninitializations, learned updates, or predictive mappings - and how they\nincorporate task data at inference. We introduce a taxonomy that categorizes\namortized models into parametric, implicit, and explicit regimes, based on\nwhether task adaptation is externalized, internalized, or jointly modeled.\nBuilding on this view, we identify a key limitation in current approaches: most\nmethods struggle to scale to large datasets because their capacity to process\ntask data at inference (e.g., context length) is often limited. To address\nthis, we propose iterative amortized inference, a class of models that refine\nsolutions step-by-step over mini-batches, drawing inspiration from stochastic\noptimization. Our formulation bridges optimization-based meta-learning with\nforward-pass amortization in models like LLMs, offering a scalable and\nextensible foundation for general-purpose task adaptation."
                },
                "authors": [
                    {
                        "name": "Sarthak Mittal"
                    },
                    {
                        "name": "Divyat Mahajan"
                    },
                    {
                        "name": "Guillaume Lajoie"
                    },
                    {
                        "name": "Mohammad Pezeshki"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Pezeshki"
                },
                "author": "Mohammad Pezeshki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11457v1",
                "updated": "2025-10-13T14:29:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    29,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:29:15Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    29,
                    15,
                    0,
                    286,
                    0
                ],
                "title": "From <Answer> to <Think>: Multidimensional Supervision of Reasoning\n  Process for LLM Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From <Answer> to <Think>: Multidimensional Supervision of Reasoning\n  Process for LLM Optimization"
                },
                "summary": "Improving the multi-step reasoning ability of Large Language Models (LLMs) is\na critical yet challenging task. The dominant paradigm, outcome-supervised\nreinforcement learning (RLVR), rewards only correct final answers, often\npropagating flawed reasoning and suffering from sparse reward signals. While\nprocess-level reward models (PRMs) provide denser, step-by-step feedback, they\nlack generalizability and interpretability, requiring task-specific\nsegmentation of the reasoning process. To this end, we propose the\nDimension-level Reward Model (DRM), a new supervision framework that bridges\nthe gap between these two approaches. DRM evaluates the quality of a reasoning\nprocess along three fundamental, complementary, and interpretable dimensions:\nConfidence for uncertainty calibration, Relevance for semantic alignment, and\nCoherence for logical consistency. Together, these dimensions capture aspects\nbeyond final answer correctness and enable interpretable assessment without\nrequiring ground truth answers. Experimental results show that DRM provides\neffective supervision signals, guides the optimization of LLMs and enhances\ntheir reasoning ability. In particular, DRM-supervised training achieves\nconsistent gains on both in-distribution and out-of-distribution open-domain\ntasks, including mathematics, question answering, code execution, and puzzles.\nOur findings demonstrate that multidimensional supervision of the reasoning\nprocess can improve the generalized reasoning ability of LLMs beyond the\ntraining distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the multi-step reasoning ability of Large Language Models (LLMs) is\na critical yet challenging task. The dominant paradigm, outcome-supervised\nreinforcement learning (RLVR), rewards only correct final answers, often\npropagating flawed reasoning and suffering from sparse reward signals. While\nprocess-level reward models (PRMs) provide denser, step-by-step feedback, they\nlack generalizability and interpretability, requiring task-specific\nsegmentation of the reasoning process. To this end, we propose the\nDimension-level Reward Model (DRM), a new supervision framework that bridges\nthe gap between these two approaches. DRM evaluates the quality of a reasoning\nprocess along three fundamental, complementary, and interpretable dimensions:\nConfidence for uncertainty calibration, Relevance for semantic alignment, and\nCoherence for logical consistency. Together, these dimensions capture aspects\nbeyond final answer correctness and enable interpretable assessment without\nrequiring ground truth answers. Experimental results show that DRM provides\neffective supervision signals, guides the optimization of LLMs and enhances\ntheir reasoning ability. In particular, DRM-supervised training achieves\nconsistent gains on both in-distribution and out-of-distribution open-domain\ntasks, including mathematics, question answering, code execution, and puzzles.\nOur findings demonstrate that multidimensional supervision of the reasoning\nprocess can improve the generalized reasoning ability of LLMs beyond the\ntraining distribution."
                },
                "authors": [
                    {
                        "name": "Beining Wang"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Hongtao Tian"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05605v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05605v4",
                "updated": "2025-10-13T14:24:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    24,
                    10,
                    0,
                    286,
                    0
                ],
                "published": "2025-02-08T15:21:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    15,
                    21,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "Evolving LLMs' Self-Refinement Capability via Synergistic\n  Training-Inference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving LLMs' Self-Refinement Capability via Synergistic\n  Training-Inference Optimization"
                },
                "summary": "Self-Refinement refers to a model's ability to revise its own responses to\nproduce improved outputs. This capability can also serve as a fundamental\nmechanism for Self-Improvement, for example, by reconstructing datasets with\nrefined results to enhance intrinsic model performance. However, our\ncomprehensive experiments reveal that large language models (LLMs) show no\nclear evidence of inherent Self-Refinement and may even experience response\nquality degradation after Self-Refinement. To address this issue, we propose\nEVOLVE, a simple and effective framework for eliciting and tracking the\nevolution of Self-Refinement through iterative training. We first explore\noptimization methods during training to activate the model's Self-Refinement\ncapability. Then, at inference, we investigate various generation strategies to\nfurther enhance and utilize Self-Refinement while supplying the necessary data\nfor training. Through synergistic optimization of training and inference\nstages, we continually evolve the model's Self-Refinement ability, enabling it\nto better refine its own responses. Moreover, we demonstrate the potential of\nleveraging Self-Refinement to achieve broader Self-Improvement of intrinsic\nmodel abilities. Experiments show that the evolved Self-Refinement ability\nenables the Llama-3.1-8B base model to surpass GPT-4o, achieving 62.3%\nlength-controlled and 63.3% raw win rates on AlpacaEval 2, and 50.3% on\nArena-Hard. It also generalizes effectively to out-of-domain reasoning tasks,\nimproving performance on mathematical reasoning benchmarks such as GSM8K and\nMATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Refinement refers to a model's ability to revise its own responses to\nproduce improved outputs. This capability can also serve as a fundamental\nmechanism for Self-Improvement, for example, by reconstructing datasets with\nrefined results to enhance intrinsic model performance. However, our\ncomprehensive experiments reveal that large language models (LLMs) show no\nclear evidence of inherent Self-Refinement and may even experience response\nquality degradation after Self-Refinement. To address this issue, we propose\nEVOLVE, a simple and effective framework for eliciting and tracking the\nevolution of Self-Refinement through iterative training. We first explore\noptimization methods during training to activate the model's Self-Refinement\ncapability. Then, at inference, we investigate various generation strategies to\nfurther enhance and utilize Self-Refinement while supplying the necessary data\nfor training. Through synergistic optimization of training and inference\nstages, we continually evolve the model's Self-Refinement ability, enabling it\nto better refine its own responses. Moreover, we demonstrate the potential of\nleveraging Self-Refinement to achieve broader Self-Improvement of intrinsic\nmodel abilities. Experiments show that the evolved Self-Refinement ability\nenables the Llama-3.1-8B base model to surpass GPT-4o, achieving 62.3%\nlength-controlled and 63.3% raw win rates on AlpacaEval 2, and 50.3% on\nArena-Hard. It also generalizes effectively to out-of-domain reasoning tasks,\nimproving performance on mathematical reasoning benchmarks such as GSM8K and\nMATH."
                },
                "authors": [
                    {
                        "name": "Yongcheng Zeng"
                    },
                    {
                        "name": "Xinyu Cui"
                    },
                    {
                        "name": "Xuanfa Jin"
                    },
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Guoqing Liu"
                    },
                    {
                        "name": "Zexu Sun"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05605v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05605v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24563v2",
                "updated": "2025-10-13T14:23:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    23,
                    19,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T10:16:05Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    10,
                    16,
                    5,
                    0,
                    272,
                    0
                ],
                "title": "NeMo: Needle in a Montage for Video-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeMo: Needle in a Montage for Video-Language Understanding"
                },
                "summary": "Recent advances in video large language models (VideoLLMs) call for new\nevaluation protocols and benchmarks for complex temporal reasoning in\nvideo-language understanding. Inspired by the needle in a haystack test widely\nused by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed\nto assess VideoLLMs' critical reasoning capabilities, including long-context\nrecall and temporal grounding. To generate video question answering data for\nour task, we develop a scalable automated data generation pipeline that\nfacilitates high-quality data synthesis. Built upon the proposed pipeline, we\npresent NeMoBench, a video-language benchmark centered on our task.\nSpecifically, our full set of NeMoBench features 31,378 automatically generated\nquestion-answer (QA) pairs from 13,486 videos with various durations ranging\nfrom seconds to hours. Experiments demonstrate that our pipeline can reliably\nand automatically generate high-quality evaluation data, enabling NeMoBench to\nbe continuously updated with the latest videos. We evaluate 20 state-of-the-art\nmodels on our benchmark, providing extensive results and key insights into\ntheir capabilities and limitations. Our project page is available at:\nhttps://lavi-lab.github.io/NeMoBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video large language models (VideoLLMs) call for new\nevaluation protocols and benchmarks for complex temporal reasoning in\nvideo-language understanding. Inspired by the needle in a haystack test widely\nused by LLMs, we introduce a novel task of Needle in a Montage (NeMo), designed\nto assess VideoLLMs' critical reasoning capabilities, including long-context\nrecall and temporal grounding. To generate video question answering data for\nour task, we develop a scalable automated data generation pipeline that\nfacilitates high-quality data synthesis. Built upon the proposed pipeline, we\npresent NeMoBench, a video-language benchmark centered on our task.\nSpecifically, our full set of NeMoBench features 31,378 automatically generated\nquestion-answer (QA) pairs from 13,486 videos with various durations ranging\nfrom seconds to hours. Experiments demonstrate that our pipeline can reliably\nand automatically generate high-quality evaluation data, enabling NeMoBench to\nbe continuously updated with the latest videos. We evaluate 20 state-of-the-art\nmodels on our benchmark, providing extensive results and key insights into\ntheir capabilities and limitations. Our project page is available at:\nhttps://lavi-lab.github.io/NeMoBench."
                },
                "authors": [
                    {
                        "name": "Zi-Yuan Hu"
                    },
                    {
                        "name": "Shuo Liang"
                    },
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Yeyao Tao"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Jia Qin"
                    },
                    {
                        "name": "Jianguang Yu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11438v1",
                "updated": "2025-10-13T14:10:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    10,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:10:26Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    10,
                    26,
                    0,
                    286,
                    0
                ],
                "title": "What Generative Search Engines Like and How to Optimize Web Content\n  Cooperatively",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Generative Search Engines Like and How to Optimize Web Content\n  Cooperatively"
                },
                "summary": "By employing large language models (LLMs) to retrieve documents and generate\nnatural language responses, Generative Engines, such as Google AI overview and\nChatGPT, provide significantly enhanced user experiences and have rapidly\nbecome the new form of search. Their rapid adoption also drives the needs of\nGenerative Engine Optimization (GEO), as content providers are eager to gain\nmore traction from them. In this paper, we introduce AutoGEO, a framework to\nautomatically learn generative engine preferences when using retrieved contents\nfor response generation, and rewrite web contents for more such traction.\nAutoGEO first prompts frontier LLMs to explain generative engine preferences\nand extract meaningful preference rules from these explanations. Then it uses\npreference rules as context engineering for AutoGEO$_\\text{API}$, a\nprompt-based GEO system, and as rule-based rewards to train\nAutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard\nGEO-Bench and two newly constructed benchmarks using real user queries\ndemonstrate the effectiveness of AutoGEO in enhancing content traction while\npreserving search utility. Analyses confirm the learned rules' robustness and\nabilities to capture unique preferences in variant domains, and AutoGEO\nsystems' ability to embed them in content optimization. The code is released at\nhttps://github.com/cxcscmu/AutoGEO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By employing large language models (LLMs) to retrieve documents and generate\nnatural language responses, Generative Engines, such as Google AI overview and\nChatGPT, provide significantly enhanced user experiences and have rapidly\nbecome the new form of search. Their rapid adoption also drives the needs of\nGenerative Engine Optimization (GEO), as content providers are eager to gain\nmore traction from them. In this paper, we introduce AutoGEO, a framework to\nautomatically learn generative engine preferences when using retrieved contents\nfor response generation, and rewrite web contents for more such traction.\nAutoGEO first prompts frontier LLMs to explain generative engine preferences\nand extract meaningful preference rules from these explanations. Then it uses\npreference rules as context engineering for AutoGEO$_\\text{API}$, a\nprompt-based GEO system, and as rule-based rewards to train\nAutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard\nGEO-Bench and two newly constructed benchmarks using real user queries\ndemonstrate the effectiveness of AutoGEO in enhancing content traction while\npreserving search utility. Analyses confirm the learned rules' robustness and\nabilities to capture unique preferences in variant domains, and AutoGEO\nsystems' ability to embed them in content optimization. The code is released at\nhttps://github.com/cxcscmu/AutoGEO."
                },
                "authors": [
                    {
                        "name": "Yujiang Wu"
                    },
                    {
                        "name": "Shanshan Zhong"
                    },
                    {
                        "name": "Yubin Kim"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11434v1",
                "updated": "2025-10-13T14:06:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    6,
                    17,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T14:06:17Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    14,
                    6,
                    17,
                    0,
                    286,
                    0
                ],
                "title": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated\n  Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated\n  Content"
                },
                "summary": "Generative large language models (LLMs) have become central to everyday life,\nproducing human-like text across diverse domains. A growing body of research\ninvestigates whether these models also exhibit personality- and\ndemographic-like characteristics in their language. In this work, we introduce\na novel, data-driven methodology for assessing LLM personality without relying\non self-report questionnaires, applying instead automatic personality and\ngender classifiers to model replies on open-ended questions collected from\nReddit. Comparing six widely used models to human-authored responses, we find\nthat LLMs systematically express higher Agreeableness and lower Neuroticism,\nreflecting cooperative and stable conversational tendencies. Gendered language\npatterns in model text broadly resemble those of human writers, though with\nreduced variation, echoing prior findings on automated agents. We contribute a\nnew dataset of human and model responses, along with large-scale comparative\nanalyses, shedding new light on the topic of personality and demographic\npatterns of generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) have become central to everyday life,\nproducing human-like text across diverse domains. A growing body of research\ninvestigates whether these models also exhibit personality- and\ndemographic-like characteristics in their language. In this work, we introduce\na novel, data-driven methodology for assessing LLM personality without relying\non self-report questionnaires, applying instead automatic personality and\ngender classifiers to model replies on open-ended questions collected from\nReddit. Comparing six widely used models to human-authored responses, we find\nthat LLMs systematically express higher Agreeableness and lower Neuroticism,\nreflecting cooperative and stable conversational tendencies. Gendered language\npatterns in model text broadly resemble those of human writers, though with\nreduced variation, echoing prior findings on automated agents. We contribute a\nnew dataset of human and model responses, along with large-scale comparative\nanalyses, shedding new light on the topic of personality and demographic\npatterns of generative AI."
                },
                "authors": [
                    {
                        "name": "Dana Sotto Porat"
                    },
                    {
                        "name": "Ella Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Ella Rabinovich"
                },
                "author": "Ella Rabinovich",
                "arxiv_comment": "ECAI2025 (Identity-Aware AI workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11423v1",
                "updated": "2025-10-13T13:57:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    57,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:57:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    57,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "Beyond the Crowd: LLM-Augmented Community Notes for Governing Health\n  Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Crowd: LLM-Augmented Community Notes for Governing Health\n  Misinformation"
                },
                "summary": "Community Notes, the crowd-sourced misinformation governance system on X\n(formerly Twitter), enables users to flag misleading posts, attach contextual\nnotes, and vote on their helpfulness. However, our analysis of 30.8K\nhealth-related notes reveals significant latency, with a median delay of 17.6\nhours before the first note receives a helpfulness status. To improve\nresponsiveness during real-world misinformation surges, we propose CrowdNotes+,\na unified framework that leverages large language models (LLMs) to augment\nCommunity Notes for faster and more reliable health misinformation governance.\nCrowdNotes+ integrates two complementary modes: (1) evidence-grounded note\naugmentation and (2) utility-guided note automation, along with a hierarchical\nthree-step evaluation that progressively assesses relevance, correctness, and\nhelpfulness. We instantiate the framework through HealthNotes, a benchmark of\n1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness\njudge. Experiments on fifteen LLMs reveal an overlooked loophole in current\nhelpfulness evaluation, where stylistic fluency is mistaken for factual\naccuracy, and demonstrate that our hierarchical evaluation and LLM-augmented\ngeneration jointly enhance factual precision and evidence utility. These\nresults point toward a hybrid human-AI governance model that improves both the\nrigor and timeliness of crowd-sourced fact-checking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community Notes, the crowd-sourced misinformation governance system on X\n(formerly Twitter), enables users to flag misleading posts, attach contextual\nnotes, and vote on their helpfulness. However, our analysis of 30.8K\nhealth-related notes reveals significant latency, with a median delay of 17.6\nhours before the first note receives a helpfulness status. To improve\nresponsiveness during real-world misinformation surges, we propose CrowdNotes+,\na unified framework that leverages large language models (LLMs) to augment\nCommunity Notes for faster and more reliable health misinformation governance.\nCrowdNotes+ integrates two complementary modes: (1) evidence-grounded note\naugmentation and (2) utility-guided note automation, along with a hierarchical\nthree-step evaluation that progressively assesses relevance, correctness, and\nhelpfulness. We instantiate the framework through HealthNotes, a benchmark of\n1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness\njudge. Experiments on fifteen LLMs reveal an overlooked loophole in current\nhelpfulness evaluation, where stylistic fluency is mistaken for factual\naccuracy, and demonstrate that our hierarchical evaluation and LLM-augmented\ngeneration jointly enhance factual precision and evidence utility. These\nresults point toward a hybrid human-AI governance model that improves both the\nrigor and timeliness of crowd-sourced fact-checking."
                },
                "authors": [
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Zihang Fu"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Fanxiao Li"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11421v1",
                "updated": "2025-10-13T13:56:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    56,
                    15,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:56:15Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    56,
                    15,
                    0,
                    286,
                    0
                ],
                "title": "A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation\n  in Smart Cities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation\n  in Smart Cities"
                },
                "summary": "This paper presents an AI-driven IoT robotic teleoperation system designed\nfor real-time remote manipulation and intelligent visual monitoring, tailored\nfor smart city applications. The architecture integrates a Flutter-based\ncross-platform mobile interface with MQTT-based control signaling and WebRTC\nvideo streaming via the LiveKit framework. A YOLOv11-nano model is deployed for\nlightweight object detection, enabling real-time perception with annotated\nvisual overlays delivered to the user interface. Control commands are\ntransmitted via MQTT to an ESP8266-based actuator node, which coordinates\nmulti-axis robotic arm motion through an Arduino Mega2560 controller. The\nbackend infrastructure is hosted on DigitalOcean, ensuring scalable cloud\norchestration and stable global communication. Latency evaluations conducted\nunder both local and international VPN scenarios (including Hong Kong, Japan,\nand Belgium) demonstrate actuator response times as low as 0.2 seconds and\ntotal video latency under 1.2 seconds, even across high-latency networks. This\nlow-latency dual-protocol design ensures responsive closed-loop interaction and\nrobust performance in distributed environments. Unlike conventional\nteleoperation platforms, the proposed system emphasizes modular deployment,\nreal-time AI sensing, and adaptable communication strategies, making it\nwell-suited for smart city scenarios such as remote infrastructure inspection,\npublic equipment servicing, and urban automation. Future enhancements will\nfocus on edge-device deployment, adaptive routing, and integration with\ncity-scale IoT networks to enhance resilience and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an AI-driven IoT robotic teleoperation system designed\nfor real-time remote manipulation and intelligent visual monitoring, tailored\nfor smart city applications. The architecture integrates a Flutter-based\ncross-platform mobile interface with MQTT-based control signaling and WebRTC\nvideo streaming via the LiveKit framework. A YOLOv11-nano model is deployed for\nlightweight object detection, enabling real-time perception with annotated\nvisual overlays delivered to the user interface. Control commands are\ntransmitted via MQTT to an ESP8266-based actuator node, which coordinates\nmulti-axis robotic arm motion through an Arduino Mega2560 controller. The\nbackend infrastructure is hosted on DigitalOcean, ensuring scalable cloud\norchestration and stable global communication. Latency evaluations conducted\nunder both local and international VPN scenarios (including Hong Kong, Japan,\nand Belgium) demonstrate actuator response times as low as 0.2 seconds and\ntotal video latency under 1.2 seconds, even across high-latency networks. This\nlow-latency dual-protocol design ensures responsive closed-loop interaction and\nrobust performance in distributed environments. Unlike conventional\nteleoperation platforms, the proposed system emphasizes modular deployment,\nreal-time AI sensing, and adaptable communication strategies, making it\nwell-suited for smart city scenarios such as remote infrastructure inspection,\npublic equipment servicing, and urban automation. Future enhancements will\nfocus on edge-device deployment, adaptive routing, and integration with\ncity-scale IoT networks to enhance resilience and scalability."
                },
                "authors": [
                    {
                        "name": "Shih-Chieh Sun"
                    },
                    {
                        "name": "Yun-Cheng Tsai"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Cheng Tsai"
                },
                "author": "Yun-Cheng Tsai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11414v1",
                "updated": "2025-10-13T13:52:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    52,
                    33,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:52:33Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    52,
                    33,
                    0,
                    286,
                    0
                ],
                "title": "Uncertainty-Aware, Risk-Adaptive Access Control for Agentic Systems\n  using an LLM-Judged TBAC Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware, Risk-Adaptive Access Control for Agentic Systems\n  using an LLM-Judged TBAC Model"
                },
                "summary": "The proliferation of autonomous AI agents within enterprise environments\nintroduces a critical security challenge: managing access control for emergent,\nnovel tasks for which no predefined policies exist. This paper introduces an\nadvanced security framework that extends the Task-Based Access Control (TBAC)\nmodel by using a Large Language Model (LLM) as an autonomous, risk-aware judge.\nThis model makes access control decisions not only based on an agent's intent\nbut also by explicitly considering the inherent \\textbf{risk associated with\ntarget resources} and the LLM's own \\textbf{model uncertainty} in its\ndecision-making process. When an agent proposes a novel task, the LLM judge\nsynthesizes a just-in-time policy while also computing a composite risk score\nfor the task and an uncertainty estimate for its own reasoning. High-risk or\nhigh-uncertainty requests trigger more stringent controls, such as requiring\nhuman approval. This dual consideration of external risk and internal\nconfidence allows the model to enforce a more robust and adaptive version of\nthe principle of least privilege, paving the way for safer and more trustworthy\nautonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of autonomous AI agents within enterprise environments\nintroduces a critical security challenge: managing access control for emergent,\nnovel tasks for which no predefined policies exist. This paper introduces an\nadvanced security framework that extends the Task-Based Access Control (TBAC)\nmodel by using a Large Language Model (LLM) as an autonomous, risk-aware judge.\nThis model makes access control decisions not only based on an agent's intent\nbut also by explicitly considering the inherent \\textbf{risk associated with\ntarget resources} and the LLM's own \\textbf{model uncertainty} in its\ndecision-making process. When an agent proposes a novel task, the LLM judge\nsynthesizes a just-in-time policy while also computing a composite risk score\nfor the task and an uncertainty estimate for its own reasoning. High-risk or\nhigh-uncertainty requests trigger more stringent controls, such as requiring\nhuman approval. This dual consideration of external risk and internal\nconfidence allows the model to enforce a more robust and adaptive version of\nthe principle of least privilege, paving the way for safer and more trustworthy\nautonomous systems."
                },
                "authors": [
                    {
                        "name": "Charles Fleming"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    }
                ],
                "author_detail": {
                    "name": "Ramana Kompella"
                },
                "author": "Ramana Kompella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23261v2",
                "updated": "2025-10-13T13:51:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    51,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-27T11:30:17Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    11,
                    30,
                    17,
                    5,
                    270,
                    0
                ],
                "title": "The Matthew Effect of AI Programming Assistants: A Hidden Bias in\n  Software Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Matthew Effect of AI Programming Assistants: A Hidden Bias in\n  Software Evolution"
                },
                "summary": "AI-assisted programming is rapidly reshaping software development, with large\nlanguage models (LLMs) enabling new paradigms such as vibe coding and agentic\ncoding. While prior works have focused on prompt design and code generation\nquality, the broader impact of LLM-driven development on the iterative dynamics\nof software engineering remains underexplored. In this paper, we conduct\nlarge-scale experiments on thousands of algorithmic programming tasks and\nhundreds of framework selection tasks to systematically investigate how\nAI-assisted programming interacts with the software ecosystem. Our analysis\nreveals \\textbf{a striking Matthew effect: the more popular a programming\nlanguage or framework, the higher the success rate of LLM-generated code}. The\nphenomenon suggests that AI systems may reinforce existing popularity\nhierarchies, accelerating convergence around dominant tools while hindering\ndiversity and innovation. We provide a quantitative characterization of this\neffect and discuss its implications for the future evolution of programming\necosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-assisted programming is rapidly reshaping software development, with large\nlanguage models (LLMs) enabling new paradigms such as vibe coding and agentic\ncoding. While prior works have focused on prompt design and code generation\nquality, the broader impact of LLM-driven development on the iterative dynamics\nof software engineering remains underexplored. In this paper, we conduct\nlarge-scale experiments on thousands of algorithmic programming tasks and\nhundreds of framework selection tasks to systematically investigate how\nAI-assisted programming interacts with the software ecosystem. Our analysis\nreveals \\textbf{a striking Matthew effect: the more popular a programming\nlanguage or framework, the higher the success rate of LLM-generated code}. The\nphenomenon suggests that AI systems may reinforce existing popularity\nhierarchies, accelerating convergence around dominant tools while hindering\ndiversity and innovation. We provide a quantitative characterization of this\neffect and discuss its implications for the future evolution of programming\necosystems."
                },
                "authors": [
                    {
                        "name": "Fei Gu"
                    },
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Hongzong LI"
                    },
                    {
                        "name": "Jiahao MA"
                    }
                ],
                "author_detail": {
                    "name": "Jiahao MA"
                },
                "author": "Jiahao MA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11409v1",
                "updated": "2025-10-13T13:48:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    48,
                    29,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:48:29Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    48,
                    29,
                    0,
                    286,
                    0
                ],
                "title": "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic\n  Literature Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic\n  Literature Reviews"
                },
                "summary": "The creation of systematic literature reviews (SLR) is critical for analyzing\nthe landscape of a research field and guiding future research directions.\nHowever, retrieving and filtering the literature corpus for an SLR is highly\ntime-consuming and requires extensive manual effort, as keyword-based searches\nin digital libraries often return numerous irrelevant publications. In this\nwork, we propose a pipeline leveraging multiple large language models (LLMs),\nclassifying papers based on descriptive prompts and deciding jointly using a\nconsensus scheme. The entire process is human-supervised and interactively\ncontrolled via our open-source visual analytics web interface, LLMSurver, which\nenables real-time inspection and modification of model outputs. We evaluate our\napproach using ground-truth data from a recent SLR comprising over 8,000\ncandidate papers, benchmarking both open and commercial state-of-the-art LLMs\nfrom mid-2024 and fall 2025. Results demonstrate that our pipeline\nsignificantly reduces manual effort while achieving lower error rates than\nsingle human annotators. Furthermore, modern open-source models prove\nsufficient for this task, making the method accessible and cost-effective.\nOverall, our work demonstrates how responsible human-AI collaboration can\naccelerate and enhance systematic literature reviews within academic workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The creation of systematic literature reviews (SLR) is critical for analyzing\nthe landscape of a research field and guiding future research directions.\nHowever, retrieving and filtering the literature corpus for an SLR is highly\ntime-consuming and requires extensive manual effort, as keyword-based searches\nin digital libraries often return numerous irrelevant publications. In this\nwork, we propose a pipeline leveraging multiple large language models (LLMs),\nclassifying papers based on descriptive prompts and deciding jointly using a\nconsensus scheme. The entire process is human-supervised and interactively\ncontrolled via our open-source visual analytics web interface, LLMSurver, which\nenables real-time inspection and modification of model outputs. We evaluate our\napproach using ground-truth data from a recent SLR comprising over 8,000\ncandidate papers, benchmarking both open and commercial state-of-the-art LLMs\nfrom mid-2024 and fall 2025. Results demonstrate that our pipeline\nsignificantly reduces manual effort while achieving lower error rates than\nsingle human annotators. Furthermore, modern open-source models prove\nsufficient for this task, making the method accessible and cost-effective.\nOverall, our work demonstrates how responsible human-AI collaboration can\naccelerate and enhance systematic literature reviews within academic workflows."
                },
                "authors": [
                    {
                        "name": "Lucas Joos"
                    },
                    {
                        "name": "Daniel A. Keim"
                    },
                    {
                        "name": "Maximilian T. Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian T. Fischer"
                },
                "author": "Maximilian T. Fischer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11408v1",
                "updated": "2025-10-13T13:48:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    48,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:48:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    48,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "Valid Survey Simulations with Limited Human Data: The Roles of\n  Prompting, Fine-Tuning, and Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Valid Survey Simulations with Limited Human Data: The Roles of\n  Prompting, Fine-Tuning, and Rectification"
                },
                "summary": "Surveys provide valuable insights into public opinion and behavior, but their\nexecution is costly and slow. Large language models (LLMs) have been proposed\nas a scalable, low-cost substitute for human respondents, but their outputs are\noften biased and yield invalid estimates. We study the interplay between\nsynthesis methods that use LLMs to generate survey responses and rectification\nmethods that debias population estimates, and explore how human responses are\nbest allocated between them. Using two panel surveys with questions on\nnutrition, politics, and economics, we find that synthesis alone introduces\nsubstantial bias (24-86%), whereas combining it with rectification reduces bias\nbelow 5% and increases effective sample size by up to 14%. Overall, we\nchallenge the common practice of using all human responses for fine-tuning,\nshowing that under a fixed budget, allocating most to rectification results in\nfar more effective estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveys provide valuable insights into public opinion and behavior, but their\nexecution is costly and slow. Large language models (LLMs) have been proposed\nas a scalable, low-cost substitute for human respondents, but their outputs are\noften biased and yield invalid estimates. We study the interplay between\nsynthesis methods that use LLMs to generate survey responses and rectification\nmethods that debias population estimates, and explore how human responses are\nbest allocated between them. Using two panel surveys with questions on\nnutrition, politics, and economics, we find that synthesis alone introduces\nsubstantial bias (24-86%), whereas combining it with rectification reduces bias\nbelow 5% and increases effective sample size by up to 14%. Overall, we\nchallenge the common practice of using all human responses for fine-tuning,\nshowing that under a fixed budget, allocating most to rectification results in\nfar more effective estimation."
                },
                "authors": [
                    {
                        "name": "Stefan Krsteski"
                    },
                    {
                        "name": "Giuseppe Russo"
                    },
                    {
                        "name": "Serina Chang"
                    },
                    {
                        "name": "Robert West"
                    },
                    {
                        "name": "Kristina Gligorić"
                    }
                ],
                "author_detail": {
                    "name": "Kristina Gligorić"
                },
                "author": "Kristina Gligorić",
                "arxiv_comment": "19 pages, 4 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11407v1",
                "updated": "2025-10-13T13:47:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    47,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    47,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "KnowRL: Teaching Language Models to Know What They Know",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowRL: Teaching Language Models to Know What They Know"
                },
                "summary": "Truly reliable AI requires more than simply scaling up knowledge; it demands\nthe ability to know what it knows and when it does not. Yet recent research\nshows that even the best LLMs misjudge their own competence in more than one in\nfive cases, making any response born of such internal uncertainty impossible to\nfully trust. Inspired by self-improvement reinforcement learning techniques\nthat require minimal data, we present a simple but powerful framework KnowRL\nthat strengthens a model's internal understanding of its own feasibility\nboundaries, enabling safer and more responsible behaviour. Our framework\ncombines two components: (i) introspection, where the model generates and\nclassifies tasks it judges feasible or infeasible, and (ii) consensus-based\nrewarding, where stability of self-knowledge assessment is reinforced through\ninternal agreement. By using internally generated data, this design strengthens\nconsistency in self-knowledge and entirely avoids costly external supervision.\nIn experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved\nself-knowledge, validated by both intrinsic self-consistency and extrinsic\nbenchmarking. With nothing more than a small seed set and no external\nsupervision, our method drove gains as high as 28% in accuracy and 12% in F1,\noutperforming baselines in just a few iterations. Our framework essentially\nunlocks the untapped capacity of LLMs to self-improve their knowledge\nawareness, opening the door to reliable, more accountable AI and safer\ndeployment in critical applications. Owing to its simplicity and independence\nfrom external effort, we encourage applying this reliability-enhancing process\nto all future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truly reliable AI requires more than simply scaling up knowledge; it demands\nthe ability to know what it knows and when it does not. Yet recent research\nshows that even the best LLMs misjudge their own competence in more than one in\nfive cases, making any response born of such internal uncertainty impossible to\nfully trust. Inspired by self-improvement reinforcement learning techniques\nthat require minimal data, we present a simple but powerful framework KnowRL\nthat strengthens a model's internal understanding of its own feasibility\nboundaries, enabling safer and more responsible behaviour. Our framework\ncombines two components: (i) introspection, where the model generates and\nclassifies tasks it judges feasible or infeasible, and (ii) consensus-based\nrewarding, where stability of self-knowledge assessment is reinforced through\ninternal agreement. By using internally generated data, this design strengthens\nconsistency in self-knowledge and entirely avoids costly external supervision.\nIn experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved\nself-knowledge, validated by both intrinsic self-consistency and extrinsic\nbenchmarking. With nothing more than a small seed set and no external\nsupervision, our method drove gains as high as 28% in accuracy and 12% in F1,\noutperforming baselines in just a few iterations. Our framework essentially\nunlocks the untapped capacity of LLMs to self-improve their knowledge\nawareness, opening the door to reliable, more accountable AI and safer\ndeployment in critical applications. Owing to its simplicity and independence\nfrom external effort, we encourage applying this reliability-enhancing process\nto all future models."
                },
                "authors": [
                    {
                        "name": "Sahil Kale"
                    },
                    {
                        "name": "Devendra Singh Dhami"
                    }
                ],
                "author_detail": {
                    "name": "Devendra Singh Dhami"
                },
                "author": "Devendra Singh Dhami",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11400v1",
                "updated": "2025-10-13T13:43:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    43,
                    55,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:43:55Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    43,
                    55,
                    0,
                    286,
                    0
                ],
                "title": "FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid\n  Tensor Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid\n  Tensor Management"
                },
                "summary": "Federated Learning (FL) emerges as a new learning paradigm that enables\nmultiple devices to collaboratively train a shared model while preserving data\nprivacy. However, one fundamental and prevailing challenge that hinders the\ndeployment of FL on mobile devices is the memory limitation. This paper\nproposes \\textit{FedHybrid}, a novel framework that effectively reduces the\nmemory footprint during the training process while guaranteeing the model\naccuracy and the overall training progress. Specifically, \\textit{FedHybrid}\nfirst selects the participating devices for each training round by jointly\nevaluating their memory budget, computing capability, and data diversity. After\nthat, it judiciously analyzes the computational graph and generates an\nexecution plan for each selected client in order to meet the corresponding\nmemory budget while minimizing the training delay through employing a hybrid of\nrecomputation and compression techniques according to the characteristic of\neach tensor. During the local training process, \\textit{FedHybrid} carries out\nthe execution plan with a well-designed activation compression technique to\neffectively achieve memory reduction with minimum accuracy loss. We conduct\nextensive experiments to evaluate \\textit{FedHybrid} on both simulation and\noff-the-shelf mobile devices. The experiment results demonstrate that\n\\textit{FedHybrid} achieves up to a 39.1\\% increase in model accuracy and a\n15.5$\\times$ reduction in wall clock time under various memory budgets compared\nwith the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) emerges as a new learning paradigm that enables\nmultiple devices to collaboratively train a shared model while preserving data\nprivacy. However, one fundamental and prevailing challenge that hinders the\ndeployment of FL on mobile devices is the memory limitation. This paper\nproposes \\textit{FedHybrid}, a novel framework that effectively reduces the\nmemory footprint during the training process while guaranteeing the model\naccuracy and the overall training progress. Specifically, \\textit{FedHybrid}\nfirst selects the participating devices for each training round by jointly\nevaluating their memory budget, computing capability, and data diversity. After\nthat, it judiciously analyzes the computational graph and generates an\nexecution plan for each selected client in order to meet the corresponding\nmemory budget while minimizing the training delay through employing a hybrid of\nrecomputation and compression techniques according to the characteristic of\neach tensor. During the local training process, \\textit{FedHybrid} carries out\nthe execution plan with a well-designed activation compression technique to\neffectively achieve memory reduction with minimum accuracy loss. We conduct\nextensive experiments to evaluate \\textit{FedHybrid} on both simulation and\noff-the-shelf mobile devices. The experiment results demonstrate that\n\\textit{FedHybrid} achieves up to a 39.1\\% increase in model accuracy and a\n15.5$\\times$ reduction in wall clock time under various memory budgets compared\nwith the baselines."
                },
                "authors": [
                    {
                        "name": "Kahou Tam"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Haikai Zhao"
                    },
                    {
                        "name": "ChengZhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "ChengZhong Xu"
                },
                "author": "ChengZhong Xu",
                "arxiv_doi": "10.1145/3666025.3699346",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3666025.3699346",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Sensys 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11398v1",
                "updated": "2025-10-13T13:41:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    41,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:41:27Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    41,
                    27,
                    0,
                    286,
                    0
                ],
                "title": "Living Off the LLM: How LLMs Will Change Adversary Tactics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Living Off the LLM: How LLMs Will Change Adversary Tactics"
                },
                "summary": "In living off the land attacks, malicious actors use legitimate tools and\nprocesses already present on a system to avoid detection. In this paper, we\nexplore how the on-device LLMs of the future will become a security concern as\nthreat actors integrate LLMs into their living off the land attack pipeline and\nways the security community may mitigate this threat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In living off the land attacks, malicious actors use legitimate tools and\nprocesses already present on a system to avoid detection. In this paper, we\nexplore how the on-device LLMs of the future will become a security concern as\nthreat actors integrate LLMs into their living off the land attack pipeline and\nways the security community may mitigate this threat."
                },
                "authors": [
                    {
                        "name": "Sean Oesch"
                    },
                    {
                        "name": "Jack Hutchins"
                    },
                    {
                        "name": "Luke Koch"
                    },
                    {
                        "name": "Kevin Kurian"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Kurian"
                },
                "author": "Kevin Kurian",
                "arxiv_comment": "6 pages, 0 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11394v1",
                "updated": "2025-10-13T13:38:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    38,
                    54,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:38:54Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    38,
                    54,
                    0,
                    286,
                    0
                ],
                "title": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation\n  via Rigorous Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation\n  via Rigorous Verification"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for\nenhancing the responses of large language models (LLMs) with external knowledge\nsources. Despite the impressive performance in complex question-answering\ntasks, RAG still struggles with hallucinations. Attributing RAG-generated\ncontent through in-line citations has demonstrated potential in reducing\nhallucinations and facilitating human verification. Existing citation\ngeneration methods primarily rely on either fine-tuning the generator or\nemploying post-processing approaches for citation matching. However, the former\napproach demands substantial annotated data and computational resources, while\nthe latter often encounters difficulties in managing multiple citations and\nfrequently produces suboptimal results. In this paper, we introduce a novel\nframework, called VeriCite, designed to rigorously validate supporting evidence\nand enhance answer attribution. Specifically, VeriCite breaks down into a\nthree-stage generation: 1) The initial answer generation first generates a\nresponse based on all available contexts and has its claims verified through\nthe NLI model; 2) the supporting evidence selection assesses the utility of\neach document and extracts useful supporting evidences; 3) the final answer\nrefinement integrates the initial response and collected evidences to produce\nthe final, refined answer.We conduct experiments across five open-source LLMs\nand four datasets, demonstrating that VeriCite can significantly improve\ncitation quality while maintaining the correctness of the answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for\nenhancing the responses of large language models (LLMs) with external knowledge\nsources. Despite the impressive performance in complex question-answering\ntasks, RAG still struggles with hallucinations. Attributing RAG-generated\ncontent through in-line citations has demonstrated potential in reducing\nhallucinations and facilitating human verification. Existing citation\ngeneration methods primarily rely on either fine-tuning the generator or\nemploying post-processing approaches for citation matching. However, the former\napproach demands substantial annotated data and computational resources, while\nthe latter often encounters difficulties in managing multiple citations and\nfrequently produces suboptimal results. In this paper, we introduce a novel\nframework, called VeriCite, designed to rigorously validate supporting evidence\nand enhance answer attribution. Specifically, VeriCite breaks down into a\nthree-stage generation: 1) The initial answer generation first generates a\nresponse based on all available contexts and has its claims verified through\nthe NLI model; 2) the supporting evidence selection assesses the utility of\neach document and extracts useful supporting evidences; 3) the final answer\nrefinement integrates the initial response and collected evidences to produce\nthe final, refined answer.We conduct experiments across five open-source LLMs\nand four datasets, demonstrating that VeriCite can significantly improve\ncitation quality while maintaining the correctness of the answers."
                },
                "authors": [
                    {
                        "name": "Haosheng Qian"
                    },
                    {
                        "name": "Yixing Fan"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Ruqing Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_doi": "10.1145/3767695.3769505",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3767695.3769505",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "In Proceedings of the 2025 Annual International ACM SIGIR\n  Conference on Research and Development in Information Retrieval in the Asia\n  Pacific Region (SIGIR-AP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11390v1",
                "updated": "2025-10-13T13:34:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    34,
                    5,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:34:05Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    34,
                    5,
                    0,
                    286,
                    0
                ],
                "title": "Medical Interpretability and Knowledge Maps of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Interpretability and Knowledge Maps of Large Language Models"
                },
                "summary": "We present a systematic study of medical-domain interpretability in Large\nLanguage Models (LLMs). We study how the LLMs both represent and process\nmedical knowledge through four different interpretability techniques: (1) UMAP\nprojections of intermediate activations, (2) gradient-based saliency with\nrespect to the model weights, (3) layer lesioning/removal and (4) activation\npatching. We present knowledge maps of five LLMs which show, at a\ncoarse-resolution, where knowledge about patient's ages, medical symptoms,\ndiseases and drugs is stored in the models. In particular for Llama3.3-70B, we\nfind that most medical knowledge is processed in the first half of the model's\nlayers. In addition, we find several interesting phenomena: (i) age is often\nencoded in a non-linear and sometimes discontinuous manner at intermediate\nlayers in the models, (ii) the disease progression representation is\nnon-monotonic and circular at certain layers of the model, (iii) in\nLlama3.3-70B, drugs cluster better by medical specialty rather than mechanism\nof action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B\nhave activations that collapse at intermediate layers but recover by the final\nlayers. These results can guide future research on fine-tuning, un-learning or\nde-biasing LLMs for medical tasks by suggesting at which layers in the model\nthese techniques should be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a systematic study of medical-domain interpretability in Large\nLanguage Models (LLMs). We study how the LLMs both represent and process\nmedical knowledge through four different interpretability techniques: (1) UMAP\nprojections of intermediate activations, (2) gradient-based saliency with\nrespect to the model weights, (3) layer lesioning/removal and (4) activation\npatching. We present knowledge maps of five LLMs which show, at a\ncoarse-resolution, where knowledge about patient's ages, medical symptoms,\ndiseases and drugs is stored in the models. In particular for Llama3.3-70B, we\nfind that most medical knowledge is processed in the first half of the model's\nlayers. In addition, we find several interesting phenomena: (i) age is often\nencoded in a non-linear and sometimes discontinuous manner at intermediate\nlayers in the models, (ii) the disease progression representation is\nnon-monotonic and circular at certain layers of the model, (iii) in\nLlama3.3-70B, drugs cluster better by medical specialty rather than mechanism\nof action, especially for Llama3.3-70B and (iv) Gemma3-27B and MedGemma-27B\nhave activations that collapse at intermediate layers but recover by the final\nlayers. These results can guide future research on fine-tuning, un-learning or\nde-biasing LLMs for medical tasks by suggesting at which layers in the model\nthese techniques should be applied."
                },
                "authors": [
                    {
                        "name": "Razvan Marinescu"
                    },
                    {
                        "name": "Victoria-Elisabeth Gruber"
                    },
                    {
                        "name": "Diego Fajardo"
                    }
                ],
                "author_detail": {
                    "name": "Diego Fajardo"
                },
                "author": "Diego Fajardo",
                "arxiv_comment": "29 pages, 34 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11389v1",
                "updated": "2025-10-13T13:33:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    33,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:33:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    33,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "Beyond Survival: Evaluating LLMs in Social Deduction Games with\n  Human-Aligned Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Survival: Evaluating LLMs in Social Deduction Games with\n  Human-Aligned Strategies"
                },
                "summary": "Social deduction games like Werewolf combine language, reasoning, and\nstrategy, providing a testbed for studying natural language and social\nintelligence. However, most studies reduce the game to LLM-based self-play,\nyielding templated utterances and anecdotal cases that overlook the richness of\nsocial gameplay. Evaluation further relies on coarse metrics such as survival\ntime or subjective scoring due to the lack of quality reference data. To\naddress these gaps, we curate a high-quality, human-verified multimodal\nWerewolf dataset containing over 100 hours of video, 32.4M utterance tokens,\nand 15 rule variants. Based on this dataset, we propose a novel\nstrategy-alignment evaluation that leverages the winning faction's strategies\nas ground truth in two stages: 1) Speech evaluation, formulated as\nmultiple-choice-style tasks that assess whether the model can adopt appropriate\nstances across five dimensions of social ability; and 2) Decision evaluation,\nwhich assesses the model's voting choices and opponent-role inferences. This\nframework enables a fine-grained evaluation of models' linguistic and reasoning\ncapabilities, while capturing their ability to generate strategically coherent\ngameplay. Our experiments show that state-of-the-art LLMs show diverse\nperformance, with roughly half remain below 0.50, revealing clear gaps in\ndeception and counterfactual reasoning. We hope our dataset further inspires\nresearch on language, reasoning, and strategy in multi-agent interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social deduction games like Werewolf combine language, reasoning, and\nstrategy, providing a testbed for studying natural language and social\nintelligence. However, most studies reduce the game to LLM-based self-play,\nyielding templated utterances and anecdotal cases that overlook the richness of\nsocial gameplay. Evaluation further relies on coarse metrics such as survival\ntime or subjective scoring due to the lack of quality reference data. To\naddress these gaps, we curate a high-quality, human-verified multimodal\nWerewolf dataset containing over 100 hours of video, 32.4M utterance tokens,\nand 15 rule variants. Based on this dataset, we propose a novel\nstrategy-alignment evaluation that leverages the winning faction's strategies\nas ground truth in two stages: 1) Speech evaluation, formulated as\nmultiple-choice-style tasks that assess whether the model can adopt appropriate\nstances across five dimensions of social ability; and 2) Decision evaluation,\nwhich assesses the model's voting choices and opponent-role inferences. This\nframework enables a fine-grained evaluation of models' linguistic and reasoning\ncapabilities, while capturing their ability to generate strategically coherent\ngameplay. Our experiments show that state-of-the-art LLMs show diverse\nperformance, with roughly half remain below 0.50, revealing clear gaps in\ndeception and counterfactual reasoning. We hope our dataset further inspires\nresearch on language, reasoning, and strategy in multi-agent interaction."
                },
                "authors": [
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Yuan Huang"
                    },
                    {
                        "name": "Junchang Liu"
                    },
                    {
                        "name": "Haozhe Luo"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Zixiang Xu"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "34 pages, 32figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11369v1",
                "updated": "2025-10-13T13:11:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    11,
                    8,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T13:11:08Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    11,
                    8,
                    0,
                    286,
                    0
                ],
                "title": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in\n  Image Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning as Representation: Rethinking Visual Reinforcement Learning in\n  Image Quality Assessment"
                },
                "summary": "Reasoning-based image quality assessment (IQA) models trained through\nreinforcement learning (RL) exhibit exceptional generalization, yet the\nunderlying mechanisms and critical factors driving this capability remain\nunderexplored in current research. Moreover, despite their superior\nperformance, these models incur inference energy usage and latency orders of\nmagnitude higher than their earlier counterparts, restricting their deployment\nin specific scenarios. Through extensive experiments, this paper verifies and\nelaborates that through RL training, MLLMs leverage their reasoning capability\nto convert redundant visual representations into compact, cross-domain aligned\ntext representations. This conversion is precisely the source of the\ngeneralization exhibited by these reasoning-based IQA models. Building on this\nfundamental insight, we propose a novel algorithm, RALI, which employs\ncontrastive learning to directly align images with these generalizable text\nrepresentations learned by RL. This approach eliminates the reliance on\nreasoning processes and even obviates the need to load an LLM. For the quality\nscoring task, this framework achieves generalization performance comparable to\nreasoning-based models while requiring less than 5% of their model parameters\nand inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-based image quality assessment (IQA) models trained through\nreinforcement learning (RL) exhibit exceptional generalization, yet the\nunderlying mechanisms and critical factors driving this capability remain\nunderexplored in current research. Moreover, despite their superior\nperformance, these models incur inference energy usage and latency orders of\nmagnitude higher than their earlier counterparts, restricting their deployment\nin specific scenarios. Through extensive experiments, this paper verifies and\nelaborates that through RL training, MLLMs leverage their reasoning capability\nto convert redundant visual representations into compact, cross-domain aligned\ntext representations. This conversion is precisely the source of the\ngeneralization exhibited by these reasoning-based IQA models. Building on this\nfundamental insight, we propose a novel algorithm, RALI, which employs\ncontrastive learning to directly align images with these generalizable text\nrepresentations learned by RL. This approach eliminates the reliance on\nreasoning processes and even obviates the need to load an LLM. For the quality\nscoring task, this framework achieves generalization performance comparable to\nreasoning-based models while requiring less than 5% of their model parameters\nand inference time."
                },
                "authors": [
                    {
                        "name": "Shijie Zhao"
                    },
                    {
                        "name": "Xuanyu Zhang"
                    },
                    {
                        "name": "Weiqi Li"
                    },
                    {
                        "name": "Junlin Li"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Tianfan Xue"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25123v2",
                "updated": "2025-10-13T13:03:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    13,
                    3,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T17:44:27Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    44,
                    27,
                    0,
                    272,
                    0
                ],
                "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by\n  Composing Old Ones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by\n  Composing Old Ones"
                },
                "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems."
                },
                "authors": [
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Hanbin Wang"
                    },
                    {
                        "name": "Ziming You"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11358v1",
                "updated": "2025-10-13T12:57:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    57,
                    45,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:57:45Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    57,
                    45,
                    0,
                    286,
                    0
                ],
                "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. While traditional retrieval focuses on\nrelevance, RAG's effectiveness depends on the utility of retrieved passages,\ni.e., the usefulness in facilitating the generation of an accurate and\ncomprehensive answer. Existing studies often treat utility as a generic\nattribute, ignoring the fact that different LLMs may benefit differently from\nthe same passage due to variations in internal knowledge and comprehension\nability. In this work, we introduce and systematically investigate the notion\nof LLM-specific utility. Through large-scale experiments across multiple\ndatasets and LLMs, we demonstrate that human-annotated passages are not optimal\nfor LLMs and that ground-truth utilitarian passages are not transferable across\ndifferent LLMs. These findings highlight the necessity of adopting the\nLLM-specific utility in RAG research. Our findings indicate that some\nhuman-annotated passages are not ground-truth utilitarian passages for specific\nLLMs, partially due to the varying readability of queries and passages for\nLLMs, a tendency for which perplexity is a key metric. Based on these findings,\nwe propose a benchmarking procedure for LLM-specific utility judgments. We\nevaluate existing utility judgment methods on six datasets and find that while\nverbalized methods using pseudo-answers perform robustly, LLMs struggle to\nassess utility effectively-failing to reject all passages for known queries and\nto select truly useful ones for unknown queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. While traditional retrieval focuses on\nrelevance, RAG's effectiveness depends on the utility of retrieved passages,\ni.e., the usefulness in facilitating the generation of an accurate and\ncomprehensive answer. Existing studies often treat utility as a generic\nattribute, ignoring the fact that different LLMs may benefit differently from\nthe same passage due to variations in internal knowledge and comprehension\nability. In this work, we introduce and systematically investigate the notion\nof LLM-specific utility. Through large-scale experiments across multiple\ndatasets and LLMs, we demonstrate that human-annotated passages are not optimal\nfor LLMs and that ground-truth utilitarian passages are not transferable across\ndifferent LLMs. These findings highlight the necessity of adopting the\nLLM-specific utility in RAG research. Our findings indicate that some\nhuman-annotated passages are not ground-truth utilitarian passages for specific\nLLMs, partially due to the varying readability of queries and passages for\nLLMs, a tendency for which perplexity is a key metric. Based on these findings,\nwe propose a benchmarking procedure for LLM-specific utility judgments. We\nevaluate existing utility judgment methods on six datasets and find that while\nverbalized methods using pseudo-answers perform robustly, LLMs struggle to\nassess utility effectively-failing to reject all passages for known queries and\nto select truly useful ones for unknown queries."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11345v1",
                "updated": "2025-10-13T12:41:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    41,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:41:27Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    41,
                    27,
                    0,
                    286,
                    0
                ],
                "title": "Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with\n  Asynchrony",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with\n  Asynchrony"
                },
                "summary": "Synchronous Reinforcement Learning (RL) post-training has emerged as a\ncrucial step for enhancing Large Language Models (LLMs) with diverse\ncapabilities. However, many systems designed to accelerate RL post-training\nstill suffer from low resource utilization and limited scalability. We present\nROLL Flash, a system that extends ROLL with native support for asynchronous RL\npost-training. ROLL Flash is built upon two core design principles:\nfine-grained parallelism and rollout-train decoupling. Guided by these\nprinciples, ROLL Flash provides flexible programming interfaces that enable a\nfully asynchronous training architecture and support efficient rollout\nmechanisms, including queue scheduling and environment-level asynchronous\nexecution. Through comprehensive theoretical analysis and extensive\nexperiments, we demonstrate that ROLL Flash significantly improves resource\nutilization and scalability over synchronous RL post-training. ROLL Flash\nachieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using\nthe same GPU budget as synchronous baselines. Furthermore, we implement several\npopular off-policy algorithms and verify that asynchronous training can achieve\nperformance on par with synchronous training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synchronous Reinforcement Learning (RL) post-training has emerged as a\ncrucial step for enhancing Large Language Models (LLMs) with diverse\ncapabilities. However, many systems designed to accelerate RL post-training\nstill suffer from low resource utilization and limited scalability. We present\nROLL Flash, a system that extends ROLL with native support for asynchronous RL\npost-training. ROLL Flash is built upon two core design principles:\nfine-grained parallelism and rollout-train decoupling. Guided by these\nprinciples, ROLL Flash provides flexible programming interfaces that enable a\nfully asynchronous training architecture and support efficient rollout\nmechanisms, including queue scheduling and environment-level asynchronous\nexecution. Through comprehensive theoretical analysis and extensive\nexperiments, we demonstrate that ROLL Flash significantly improves resource\nutilization and scalability over synchronous RL post-training. ROLL Flash\nachieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using\nthe same GPU budget as synchronous baselines. Furthermore, we implement several\npopular off-policy algorithms and verify that asynchronous training can achieve\nperformance on par with synchronous training."
                },
                "authors": [
                    {
                        "name": "Han Lu"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Jiashun Liu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Haizhou Zhao"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Yijia Luo"
                    },
                    {
                        "name": "Zihe Liu"
                    },
                    {
                        "name": "Ling Pan"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11343v1",
                "updated": "2025-10-13T12:41:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    41,
                    4,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:41:04Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    41,
                    4,
                    0,
                    286,
                    0
                ],
                "title": "TBRD: TESLA Authenticated UAS Broadcast Remote ID",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TBRD: TESLA Authenticated UAS Broadcast Remote ID"
                },
                "summary": "Mysterious sightings of Unmanned Aircraft Systems (UAS) over U.S. military\nfacilities, suburban neighborhoods, and commercial airports have intensified\nscrutiny of drone activity. To increase accountability, the Federal Aviation\nAdministration (FAA) introduced a Remote ID mandate, requiring unmanned\naircraft to broadcast their location, operator's location, and identity in\nreal-time. However, current standards leave authentication mechanisms\nunderspecified, enabling spoofing, relay, and replay attacks that can undermine\nsurveillance efforts and potentially disrupt UAS-to-UAS coordination in future\ndeployments. In this paper, we propose TBRD, a practical system for\nauthenticating Remote ID messages in a manner that aligns with existing\nstandards and UAS capabilities. TBRD leverages the TESLA protocol and mobile\ndevice TEEs, and introduces a verification mechanism to build a lightweight,\nmission-scoped authentication system that is both computationally efficient and\nrequires a low communication footprint. We evaluate the performance of TBRD\nusing both an FAA-requirements compatible proof-of-concept implementation for\nperformance metrics and a simulated 4-drone swarm mission scenario to\ndemonstrate its security guarantees under adversarial conditions. Our system\nprovides a 50\\% reduction in authentication overhead compared to digital\nsignatures and a 100x reduction in computation time. Our results demonstrate\nthat TBRD can be integrated into current Remote ID infrastructures to provide a\nscalable, standards-compliant message authentication for both regulatory and\noperational use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mysterious sightings of Unmanned Aircraft Systems (UAS) over U.S. military\nfacilities, suburban neighborhoods, and commercial airports have intensified\nscrutiny of drone activity. To increase accountability, the Federal Aviation\nAdministration (FAA) introduced a Remote ID mandate, requiring unmanned\naircraft to broadcast their location, operator's location, and identity in\nreal-time. However, current standards leave authentication mechanisms\nunderspecified, enabling spoofing, relay, and replay attacks that can undermine\nsurveillance efforts and potentially disrupt UAS-to-UAS coordination in future\ndeployments. In this paper, we propose TBRD, a practical system for\nauthenticating Remote ID messages in a manner that aligns with existing\nstandards and UAS capabilities. TBRD leverages the TESLA protocol and mobile\ndevice TEEs, and introduces a verification mechanism to build a lightweight,\nmission-scoped authentication system that is both computationally efficient and\nrequires a low communication footprint. We evaluate the performance of TBRD\nusing both an FAA-requirements compatible proof-of-concept implementation for\nperformance metrics and a simulated 4-drone swarm mission scenario to\ndemonstrate its security guarantees under adversarial conditions. Our system\nprovides a 50\\% reduction in authentication overhead compared to digital\nsignatures and a 100x reduction in computation time. Our results demonstrate\nthat TBRD can be integrated into current Remote ID infrastructures to provide a\nscalable, standards-compliant message authentication for both regulatory and\noperational use cases."
                },
                "authors": [
                    {
                        "name": "Jason Veara"
                    },
                    {
                        "name": "Manav Jain"
                    },
                    {
                        "name": "Kyle Moy"
                    },
                    {
                        "name": "Aanjhan Ranganathan"
                    }
                ],
                "author_detail": {
                    "name": "Aanjhan Ranganathan"
                },
                "author": "Aanjhan Ranganathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00761v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00761v3",
                "updated": "2025-10-13T12:38:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    38,
                    53,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-01T10:50:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    10,
                    50,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in\n  LLM Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in\n  LLM Unlearning"
                },
                "summary": "Large language model (LLM) unlearning aims to surgically remove the influence\nof undesired data or knowledge from an existing model while preserving its\nutility on unrelated tasks. This paradigm has shown promise in addressing\nprivacy and safety concerns. However, recent findings reveal that unlearning\neffects are often fragile: post-unlearning manipulations such as weight\nquantization or fine-tuning can quickly neutralize the intended forgetting.\nPrior efforts to improve robustness primarily reformulate unlearning objectives\nby explicitly assuming the role of vulnerability sources. In this work, we take\na different perspective by investigating the role of the optimizer, independent\nof unlearning objectives and formulations, in shaping unlearning robustness. We\nshow that the 'grade' of the optimizer, defined by the level of information it\nexploits, ranging from zeroth-order (gradient-free) to first-order\n(gradient-based) to second-order (Hessian-based), is tightly linked to the\nresilience of unlearning. Surprisingly, we find that downgrading the optimizer,\nsuch as using zeroth-order methods or compressed-gradient variants (e.g.,\ngradient sign-based optimizers), often leads to stronger robustness. While\nthese optimizers produce noisier and less precise updates, they encourage\nconvergence to harder-to-disturb basins in the loss landscape, thereby\nresisting post-training perturbations. By connecting zeroth-order methods with\nrandomized smoothing, we further highlight their natural advantage for robust\nunlearning. Motivated by these insights, we propose a hybrid optimizer that\ncombines first-order and zeroth-order updates, preserving unlearning efficacy\nwhile enhancing robustness. Extensive experiments on the MUSE and WMDP\nbenchmarks, across multiple LLM unlearning algorithms, validate that our\napproach achieves more resilient forgetting without sacrificing unlearning\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) unlearning aims to surgically remove the influence\nof undesired data or knowledge from an existing model while preserving its\nutility on unrelated tasks. This paradigm has shown promise in addressing\nprivacy and safety concerns. However, recent findings reveal that unlearning\neffects are often fragile: post-unlearning manipulations such as weight\nquantization or fine-tuning can quickly neutralize the intended forgetting.\nPrior efforts to improve robustness primarily reformulate unlearning objectives\nby explicitly assuming the role of vulnerability sources. In this work, we take\na different perspective by investigating the role of the optimizer, independent\nof unlearning objectives and formulations, in shaping unlearning robustness. We\nshow that the 'grade' of the optimizer, defined by the level of information it\nexploits, ranging from zeroth-order (gradient-free) to first-order\n(gradient-based) to second-order (Hessian-based), is tightly linked to the\nresilience of unlearning. Surprisingly, we find that downgrading the optimizer,\nsuch as using zeroth-order methods or compressed-gradient variants (e.g.,\ngradient sign-based optimizers), often leads to stronger robustness. While\nthese optimizers produce noisier and less precise updates, they encourage\nconvergence to harder-to-disturb basins in the loss landscape, thereby\nresisting post-training perturbations. By connecting zeroth-order methods with\nrandomized smoothing, we further highlight their natural advantage for robust\nunlearning. Motivated by these insights, we propose a hybrid optimizer that\ncombines first-order and zeroth-order updates, preserving unlearning efficacy\nwhile enhancing robustness. Extensive experiments on the MUSE and WMDP\nbenchmarks, across multiple LLM unlearning algorithms, validate that our\napproach achieves more resilient forgetting without sacrificing unlearning\nquality."
                },
                "authors": [
                    {
                        "name": "Yicheng Lang"
                    },
                    {
                        "name": "Yihua Zhang"
                    },
                    {
                        "name": "Chongyu Fan"
                    },
                    {
                        "name": "Changsheng Wang"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00761v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00761v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11331v1",
                "updated": "2025-10-13T12:25:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    25,
                    55,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:25:55Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    25,
                    55,
                    0,
                    286,
                    0
                ],
                "title": "Efficient LLM Inference over Heterogeneous Edge Networks with\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference over Heterogeneous Edge Networks with\n  Speculative Decoding"
                },
                "summary": "Large language model (LLM) inference at the network edge is a promising\nserving paradigm that leverages distributed edge resources to run inference\nnear users and enhance privacy. Existing edge-based LLM inference systems\ntypically adopt autoregressive decoding (AD), which only generates one token\nper forward pass. This iterative process, compounded by the limited\ncomputational resources of edge nodes, results in high serving latency and\nconstrains the system's ability to support multiple users under growing\ndemands.To address these challenges, we propose a speculative decoding\n(SD)-based LLM serving framework that deploys small and large models across\nheterogeneous edge nodes to collaboratively deliver inference services.\nSpecifically, the small model rapidly generates draft tokens that the large\nmodel verifies in parallel, enabling multi-token generation per forward pass\nand thus reducing serving latency. To improve resource utilization of edge\nnodes, we incorporate pipeline parallelism to overlap drafting and verification\nacross multiple inference tasks. Based on this framework, we analyze and derive\na comprehensive latency model incorporating both communication and inference\nlatency. Then, we formulate a joint optimization problem for speculation\nlength, task batching, and wireless communication resource allocation to\nminimize total serving latency. To address this problem, we derive the\nclosed-form solutions for wireless communication resource allocation, and\ndevelop a dynamic programming algorithm for joint batching and speculation\ncontrol strategies. Experimental results demonstrate that the proposed\nframework achieves lower serving latency compared to AD-based serving systems.\nIn addition,the proposed joint optimization method delivers up to 44.9% latency\nreduction compared to benchmark schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference at the network edge is a promising\nserving paradigm that leverages distributed edge resources to run inference\nnear users and enhance privacy. Existing edge-based LLM inference systems\ntypically adopt autoregressive decoding (AD), which only generates one token\nper forward pass. This iterative process, compounded by the limited\ncomputational resources of edge nodes, results in high serving latency and\nconstrains the system's ability to support multiple users under growing\ndemands.To address these challenges, we propose a speculative decoding\n(SD)-based LLM serving framework that deploys small and large models across\nheterogeneous edge nodes to collaboratively deliver inference services.\nSpecifically, the small model rapidly generates draft tokens that the large\nmodel verifies in parallel, enabling multi-token generation per forward pass\nand thus reducing serving latency. To improve resource utilization of edge\nnodes, we incorporate pipeline parallelism to overlap drafting and verification\nacross multiple inference tasks. Based on this framework, we analyze and derive\na comprehensive latency model incorporating both communication and inference\nlatency. Then, we formulate a joint optimization problem for speculation\nlength, task batching, and wireless communication resource allocation to\nminimize total serving latency. To address this problem, we derive the\nclosed-form solutions for wireless communication resource allocation, and\ndevelop a dynamic programming algorithm for joint batching and speculation\ncontrol strategies. Experimental results demonstrate that the proposed\nframework achieves lower serving latency compared to AD-based serving systems.\nIn addition,the proposed joint optimization method delivers up to 44.9% latency\nreduction compared to benchmark schemes."
                },
                "authors": [
                    {
                        "name": "Bingjie Zhu"
                    },
                    {
                        "name": "Zhixiong Chen"
                    },
                    {
                        "name": "Liqiang Zhao"
                    },
                    {
                        "name": "Hyundong Shin"
                    },
                    {
                        "name": "Arumugam Nallanathan"
                    }
                ],
                "author_detail": {
                    "name": "Arumugam Nallanathan"
                },
                "author": "Arumugam Nallanathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11330v1",
                "updated": "2025-10-13T12:25:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    25,
                    33,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:25:33Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    25,
                    33,
                    0,
                    286,
                    0
                ],
                "title": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the\n  Audio-Text Modality Gap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-Link: Diffusion Probabilistic Model for Bridging the\n  Audio-Text Modality Gap"
                },
                "summary": "Contrastive audio-language pretraining yields powerful joint representations,\nyet a persistent audio-text modality gap limits the benefits of coupling\nmultimodal encoders with large language models (LLMs). We present\nDiffusion-Link, a diffusion-based modality-bridging module that generatively\nmaps audio embeddings into the text-embedding distribution. The module is\ntrained at the output embedding from the frozen multimodal encoder and\nimplemented as a lightweight network with three residual MLP blocks. To assess\nthe effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on\nAutomatic Audio Captioning (AAC); to our knowledge, this is the first\napplication of diffusion-based modality bridging to AAC. We report two results.\n(1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link\nreduces the modality gap the most among prior diffusion-based methods and shows\na collective migration of audio embeddings toward the text distribution. (2)\nDownstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline\nachieves state-of-the-art on AudioCaps in both zero-shot and fully supervised\ncaptioning without external knowledge, with relative gains up to 52.5% and\n7.5%, respectively. These findings show that closing the modality gap is\npivotal for effective coupling between multimodal encoders and LLMs, and\ndiffusion-based modality bridging offers a promising direction beyond\nknowledge-retrieval-centric designs. Code will be released upon acceptance\nhttps://github.com/DevKiHyun/Diffusion-Link",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive audio-language pretraining yields powerful joint representations,\nyet a persistent audio-text modality gap limits the benefits of coupling\nmultimodal encoders with large language models (LLMs). We present\nDiffusion-Link, a diffusion-based modality-bridging module that generatively\nmaps audio embeddings into the text-embedding distribution. The module is\ntrained at the output embedding from the frozen multimodal encoder and\nimplemented as a lightweight network with three residual MLP blocks. To assess\nthe effect of Diffusion-Link on multimodal encoder-LLM coupling, we evaluate on\nAutomatic Audio Captioning (AAC); to our knowledge, this is the first\napplication of diffusion-based modality bridging to AAC. We report two results.\n(1) Modality-gap analysis: on similarity and geometric criteria, Diffusion-Link\nreduces the modality gap the most among prior diffusion-based methods and shows\na collective migration of audio embeddings toward the text distribution. (2)\nDownstream AAC: attaching Diffusion-Link to the same multimodal LLM baseline\nachieves state-of-the-art on AudioCaps in both zero-shot and fully supervised\ncaptioning without external knowledge, with relative gains up to 52.5% and\n7.5%, respectively. These findings show that closing the modality gap is\npivotal for effective coupling between multimodal encoders and LLMs, and\ndiffusion-based modality bridging offers a promising direction beyond\nknowledge-retrieval-centric designs. Code will be released upon acceptance\nhttps://github.com/DevKiHyun/Diffusion-Link"
                },
                "authors": [
                    {
                        "name": "KiHyun Nam"
                    },
                    {
                        "name": "Jongmin Choi"
                    },
                    {
                        "name": "Hyeongkeun Lee"
                    },
                    {
                        "name": "Jungwoo Heo"
                    },
                    {
                        "name": "Joon Son Chung"
                    }
                ],
                "author_detail": {
                    "name": "Joon Son Chung"
                },
                "author": "Joon Son Chung",
                "arxiv_comment": "5 pages. Submitted to IEEE ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11328v1",
                "updated": "2025-10-13T12:24:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    24,
                    24,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:24:24Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    24,
                    24,
                    0,
                    286,
                    0
                ],
                "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control"
                },
                "summary": "As the demand for emotional intelligence in large language models (LLMs)\ngrows, a key challenge lies in understanding the internal mechanisms that give\nrise to emotional expression and in controlling emotions in generated text.\nThis study addresses three core questions: (1) Do LLMs contain context-agnostic\nmechanisms shaping emotional expression? (2) What form do these mechanisms\ntake? (3) Can they be harnessed for universal emotion control? We first\nconstruct a controlled dataset, SEV (Scenario-Event with Valence), to elicit\ncomparable internal states across emotions. Subsequently, we extract\ncontext-agnostic emotion directions that reveal consistent, cross-context\nencoding of emotion (Q1). We identify neurons and attention heads that locally\nimplement emotional computation through analytical decomposition and causal\nanalysis, and validate their causal roles via ablation and enhancement\ninterventions. Next, we quantify each sublayer's causal influence on the\nmodel's final emotion representation and integrate the identified local\ncomponents into coherent global emotion circuits that drive emotional\nexpression (Q2). Directly modulating these circuits achieves 99.65%\nemotion-expression accuracy on the test set, surpassing prompting- and\nsteering-based methods (Q3). To our knowledge, this is the first systematic\nstudy to uncover and validate emotion circuits in LLMs, offering new insights\ninto interpretability and controllable emotional intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for emotional intelligence in large language models (LLMs)\ngrows, a key challenge lies in understanding the internal mechanisms that give\nrise to emotional expression and in controlling emotions in generated text.\nThis study addresses three core questions: (1) Do LLMs contain context-agnostic\nmechanisms shaping emotional expression? (2) What form do these mechanisms\ntake? (3) Can they be harnessed for universal emotion control? We first\nconstruct a controlled dataset, SEV (Scenario-Event with Valence), to elicit\ncomparable internal states across emotions. Subsequently, we extract\ncontext-agnostic emotion directions that reveal consistent, cross-context\nencoding of emotion (Q1). We identify neurons and attention heads that locally\nimplement emotional computation through analytical decomposition and causal\nanalysis, and validate their causal roles via ablation and enhancement\ninterventions. Next, we quantify each sublayer's causal influence on the\nmodel's final emotion representation and integrate the identified local\ncomponents into coherent global emotion circuits that drive emotional\nexpression (Q2). Directly modulating these circuits achieves 99.65%\nemotion-expression accuracy on the test set, surpassing prompting- and\nsteering-based methods (Q3). To our knowledge, this is the first systematic\nstudy to uncover and validate emotion circuits in LLMs, offering new insights\ninto interpretability and controllable emotional intelligence."
                },
                "authors": [
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Yixuan Zhang"
                    },
                    {
                        "name": "Ruiji Yu"
                    },
                    {
                        "name": "Yufei Zheng"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Zixiang Xu"
                    },
                    {
                        "name": "Gus Xia"
                    },
                    {
                        "name": "Huishuai Zhang"
                    },
                    {
                        "name": "Dongyan Zhao"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "19 pages, 8 figures, 8 tables. Code and dataset available at\n  https://github.com/Aurora-cx/EmotionCircuits-LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05195v2",
                "updated": "2025-10-13T12:21:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    21,
                    52,
                    0,
                    286,
                    0
                ],
                "published": "2025-07-07T16:54:18Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    16,
                    54,
                    18,
                    0,
                    188,
                    0
                ],
                "title": "Train-before-Test Harmonizes Language Model Rankings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train-before-Test Harmonizes Language Model Rankings"
                },
                "summary": "Existing language model benchmarks provide contradictory model rankings, even\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\nrankings hampers model selection, clouds model comparisons, and adds confusion\nto a growing ecosystem of competing models. In this paper, we take a different\nperspective on model comparison: instead of relying on out-of-the-box\nperformance via direct evaluation, we compare model potential by providing each\nmodel with identical benchmark-specific fine-tuning before evaluation. We call\nthis approach train-before-test. Our primary contribution is a comprehensive\nempirical evaluation of model potential across 24 benchmarks and 61 models.\nFirst, we demonstrate that model potential rankings obtained through\ntrain-before-test exhibit remarkable consistency across all benchmarks. Whereas\ntraditional rankings demonstrate little external validity under direct\nevaluation, they enjoy a significant degree of external validity when applying\ntrain-before-test: model potential rankings transfer gracefully from one\nbenchmark to another. Second, train-before-test restores the connection between\nperplexity and downstream task performance, lost under direct evaluation.\nRemarkably, even pre-finetuning perplexity of a base model predicts\npost-finetuning downstream performance, suggesting that ranking consistency\nreflects inherent model potential rather than fine-tuning artifacts. Finally,\ntrain-before-test reduces the model-score matrix to essentially rank one,\nindicating that model potential is dominated by one latent factor, uncovered by\ntrain-before-test. Our work supports the recommendation to make\ntrain-before-test a default component of LLM benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing language model benchmarks provide contradictory model rankings, even\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\nrankings hampers model selection, clouds model comparisons, and adds confusion\nto a growing ecosystem of competing models. In this paper, we take a different\nperspective on model comparison: instead of relying on out-of-the-box\nperformance via direct evaluation, we compare model potential by providing each\nmodel with identical benchmark-specific fine-tuning before evaluation. We call\nthis approach train-before-test. Our primary contribution is a comprehensive\nempirical evaluation of model potential across 24 benchmarks and 61 models.\nFirst, we demonstrate that model potential rankings obtained through\ntrain-before-test exhibit remarkable consistency across all benchmarks. Whereas\ntraditional rankings demonstrate little external validity under direct\nevaluation, they enjoy a significant degree of external validity when applying\ntrain-before-test: model potential rankings transfer gracefully from one\nbenchmark to another. Second, train-before-test restores the connection between\nperplexity and downstream task performance, lost under direct evaluation.\nRemarkably, even pre-finetuning perplexity of a base model predicts\npost-finetuning downstream performance, suggesting that ranking consistency\nreflects inherent model potential rather than fine-tuning artifacts. Finally,\ntrain-before-test reduces the model-score matrix to essentially rank one,\nindicating that model potential is dominated by one latent factor, uncovered by\ntrain-before-test. Our work supports the recommendation to make\ntrain-before-test a default component of LLM benchmarking."
                },
                "authors": [
                    {
                        "name": "Guanhua Zhang"
                    },
                    {
                        "name": "Ricardo Dominguez-Olmedo"
                    },
                    {
                        "name": "Moritz Hardt"
                    }
                ],
                "author_detail": {
                    "name": "Moritz Hardt"
                },
                "author": "Moritz Hardt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11321v1",
                "updated": "2025-10-13T12:19:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    19,
                    7,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:19:07Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    19,
                    7,
                    0,
                    286,
                    0
                ],
                "title": "HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled\n  Multi-Modal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled\n  Multi-Modal Data"
                },
                "summary": "Effective generalization in robotic manipulation requires representations\nthat capture invariant patterns of interaction across environments and tasks.\nWe present a self-supervised framework for learning hierarchical manipulation\nconcepts that encode these invariant patterns through cross-modal sensory\ncorrelations and multi-level temporal abstractions without requiring human\nannotation. Our approach combines a cross-modal correlation network that\nidentifies persistent patterns across sensory modalities with a multi-horizon\npredictor that organizes representations hierarchically across temporal scales.\nManipulation concepts learned through this dual structure enable policies to\nfocus on transferable relational patterns while maintaining awareness of both\nimmediate actions and longer-term goals. Empirical evaluation across simulated\nbenchmarks and real-world deployments demonstrates significant performance\nimprovements with our concept-enhanced policies. Analysis reveals that the\nlearned concepts resemble human-interpretable manipulation primitives despite\nreceiving no semantic supervision. This work advances both the understanding of\nrepresentation learning for manipulation and provides a practical approach to\nenhancing robotic performance in complex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective generalization in robotic manipulation requires representations\nthat capture invariant patterns of interaction across environments and tasks.\nWe present a self-supervised framework for learning hierarchical manipulation\nconcepts that encode these invariant patterns through cross-modal sensory\ncorrelations and multi-level temporal abstractions without requiring human\nannotation. Our approach combines a cross-modal correlation network that\nidentifies persistent patterns across sensory modalities with a multi-horizon\npredictor that organizes representations hierarchically across temporal scales.\nManipulation concepts learned through this dual structure enable policies to\nfocus on transferable relational patterns while maintaining awareness of both\nimmediate actions and longer-term goals. Empirical evaluation across simulated\nbenchmarks and real-world deployments demonstrates significant performance\nimprovements with our concept-enhanced policies. Analysis reveals that the\nlearned concepts resemble human-interpretable manipulation primitives despite\nreceiving no semantic supervision. This work advances both the understanding of\nrepresentation learning for manipulation and provides a practical approach to\nenhancing robotic performance in complex scenarios."
                },
                "authors": [
                    {
                        "name": "Ruizhe Liu"
                    },
                    {
                        "name": "Pei Zhou"
                    },
                    {
                        "name": "Qian Luo"
                    },
                    {
                        "name": "Li Sun"
                    },
                    {
                        "name": "Jun Cen"
                    },
                    {
                        "name": "Yibing Song"
                    },
                    {
                        "name": "Yanchao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yanchao Yang"
                },
                "author": "Yanchao Yang",
                "arxiv_comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01271v2",
                "updated": "2025-10-13T12:16:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    16,
                    25,
                    0,
                    286,
                    0
                ],
                "published": "2025-07-02T01:13:08Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    1,
                    13,
                    8,
                    2,
                    183,
                    0
                ],
                "title": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Practical Evaluation Scenarios for Large Multimodal Model\n  Unlearning"
                },
                "summary": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, unlearning techniques, which are methods for inducing a\nmodel to \"forget\" previously learned information, have attracted attention as a\nway to address privacy and copyright concerns in large language models (LLMs)\nand large multimodal models (LMMs). While several unlearning benchmarks have\nbeen established for LLMs, a practical evaluation framework for unlearning in\nLMMs has been less explored. Specifically, existing unlearning benchmark for\nLMMs considers only scenarios in which the model is required to unlearn\nfine-tuned knowledge through a single unlearning operation. In this study, we\nintroduce PULSE protocol for realistic unlearning scenarios for LMMs by\nintroducing two critical perspectives: (i) Pre-trained knowledge Unlearning for\nanalyzing the effect across different knowledge acquisition phases and (ii)\nLong-term Sustainability Evaluation to address sequential requests. We then\nevaluate existing unlearning methods along these dimensions. Our results reveal\nthat, although some techniques can successfully unlearn knowledge acquired\nthrough fine-tuning, they struggle to eliminate information learned during\npre-training. Moreover, methods that effectively unlearn a batch of target data\nin a single operation exhibit substantial performance degradation when the same\ndata are split and unlearned sequentially."
                },
                "authors": [
                    {
                        "name": "Tatsuki Kawakami"
                    },
                    {
                        "name": "Kazuki Egashira"
                    },
                    {
                        "name": "Atsuyuki Miyai"
                    },
                    {
                        "name": "Go Irie"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Kiyoharu Aizawa"
                },
                "author": "Kiyoharu Aizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11317v1",
                "updated": "2025-10-13T12:13:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    13,
                    17,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:13:17Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    13,
                    17,
                    0,
                    286,
                    0
                ],
                "title": "Next Interest Flow: A Generative Pre-training Paradigm for Recommender\n  Systems by Modeling All-domain Movelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Interest Flow: A Generative Pre-training Paradigm for Recommender\n  Systems by Modeling All-domain Movelines"
                },
                "summary": "Click-Through Rate (CTR) prediction, a cornerstone of modern recommender\nsystems, has been dominated by discriminative models that react to past user\nbehavior rather than proactively modeling user intent. Existing generative\nparadigms attempt to address this but suffer from critical limitations: Large\nLanguage Model (LLM) based methods create a semantic mismatch by forcing\ne-commerce signals into a linguistic space, while ID-based generation is\nconstrained by item memorization and cold-start issues. To overcome these\nlimitations, we propose a novel generative pre-training paradigm. Our model\nlearns to predict the Next Interest Flow, a dense vector sequence representing\na user's future intent, while simultaneously modeling its internal Interest\nDiversity and Interest Evolution Velocity to ensure the representation is both\nrich and coherent. However, this two-stage approach introduces a critical\nobjective mismatch between the generative and discriminative stages. We resolve\nthis via a bidirectional alignment strategy, which harmonizes the two stages\nthrough cross-stage weight initialization and a dynamic Semantic Alignment\nModule for fine-tuning. Additionally, we enhance the underlying discriminative\nmodel with a Temporal Sequential Pairwise (TSP) mechanism to better capture\ntemporal causality. We present the All-domain Moveline Evolution Network\n(AMEN), a unified framework implementing our entire pipeline. Extensive offline\nexperiments validate AMEN's superiority over strong baselines, and a\nlarge-scale online A/B test demonstrates its significant real-world impact,\ndelivering substantial improvements in key business metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-Through Rate (CTR) prediction, a cornerstone of modern recommender\nsystems, has been dominated by discriminative models that react to past user\nbehavior rather than proactively modeling user intent. Existing generative\nparadigms attempt to address this but suffer from critical limitations: Large\nLanguage Model (LLM) based methods create a semantic mismatch by forcing\ne-commerce signals into a linguistic space, while ID-based generation is\nconstrained by item memorization and cold-start issues. To overcome these\nlimitations, we propose a novel generative pre-training paradigm. Our model\nlearns to predict the Next Interest Flow, a dense vector sequence representing\na user's future intent, while simultaneously modeling its internal Interest\nDiversity and Interest Evolution Velocity to ensure the representation is both\nrich and coherent. However, this two-stage approach introduces a critical\nobjective mismatch between the generative and discriminative stages. We resolve\nthis via a bidirectional alignment strategy, which harmonizes the two stages\nthrough cross-stage weight initialization and a dynamic Semantic Alignment\nModule for fine-tuning. Additionally, we enhance the underlying discriminative\nmodel with a Temporal Sequential Pairwise (TSP) mechanism to better capture\ntemporal causality. We present the All-domain Moveline Evolution Network\n(AMEN), a unified framework implementing our entire pipeline. Extensive offline\nexperiments validate AMEN's superiority over strong baselines, and a\nlarge-scale online A/B test demonstrates its significant real-world impact,\ndelivering substantial improvements in key business metrics."
                },
                "authors": [
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Zixin Zhao"
                    },
                    {
                        "name": "Lv Shao"
                    },
                    {
                        "name": "Tong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Liu"
                },
                "author": "Tong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11313v1",
                "updated": "2025-10-13T12:03:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    3,
                    6,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T12:03:06Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    12,
                    3,
                    6,
                    0,
                    286,
                    0
                ],
                "title": "Automated Skill Decomposition Meets Expert Ontologies: Bridging the\n  Granularity Gap with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Skill Decomposition Meets Expert Ontologies: Bridging the\n  Granularity Gap with LLMs"
                },
                "summary": "This paper investigates automated skill decomposition using Large Language\nModels (LLMs) and proposes a rigorous, ontology-grounded evaluation framework.\nOur framework standardizes the pipeline from prompting and generation to\nnormalization and alignment with ontology nodes. To evaluate outputs, we\nintroduce two metrics: a semantic F1-score that uses optimal embedding-based\nmatching to assess content accuracy, and a hierarchy-aware F1-score that\ncredits structurally correct placements to assess granularity. We conduct\nexperiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing\ntwo prompting strategies: zero-shot and leakage-safe few-shot with exemplars.\nAcross diverse LLMs, zero-shot offers a strong baseline, while few-shot\nconsistently stabilizes phrasing and granularity and improves hierarchy-aware\nalignment. A latency analysis further shows that exemplar-guided prompts are\ncompetitive - and sometimes faster - than unguided zero-shot due to more\nschema-compliant completions. Together, the framework, benchmark, and metrics\nprovide a reproducible foundation for developing ontology-faithful skill\ndecomposition systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates automated skill decomposition using Large Language\nModels (LLMs) and proposes a rigorous, ontology-grounded evaluation framework.\nOur framework standardizes the pipeline from prompting and generation to\nnormalization and alignment with ontology nodes. To evaluate outputs, we\nintroduce two metrics: a semantic F1-score that uses optimal embedding-based\nmatching to assess content accuracy, and a hierarchy-aware F1-score that\ncredits structurally correct placements to assess granularity. We conduct\nexperiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing\ntwo prompting strategies: zero-shot and leakage-safe few-shot with exemplars.\nAcross diverse LLMs, zero-shot offers a strong baseline, while few-shot\nconsistently stabilizes phrasing and granularity and improves hierarchy-aware\nalignment. A latency analysis further shows that exemplar-guided prompts are\ncompetitive - and sometimes faster - than unguided zero-shot due to more\nschema-compliant completions. Together, the framework, benchmark, and metrics\nprovide a reproducible foundation for developing ontology-faithful skill\ndecomposition systems."
                },
                "authors": [
                    {
                        "name": "Le Ngoc Luyen"
                    },
                    {
                        "name": "Marie-Hélène Abel"
                    }
                ],
                "author_detail": {
                    "name": "Marie-Hélène Abel"
                },
                "author": "Marie-Hélène Abel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15524v2",
                "updated": "2025-10-13T11:56:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    56,
                    31,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-21T12:57:04Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    12,
                    57,
                    4,
                    3,
                    233,
                    0
                ],
                "title": "The Enemy from Within: A Study of Political Delegitimization Discourse\n  in Israeli Political Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Enemy from Within: A Study of Political Delegitimization Discourse\n  in Israeli Political Speech"
                },
                "summary": "We present the first large-scale computational study of political\ndelegitimization discourse (PDD), defined as symbolic attacks on the normative\nvalidity of political entities. We curate and manually annotate a novel\nHebrew-language corpus of 10,410 sentences drawn from Knesset speeches\n(1993-2023), Facebook posts (2018-2021), and leading news outlets, of which\n1,812 instances (17.4\\%) exhibit PDD and 642 carry additional annotations for\nintensity, incivility, target type, and affective framing. We introduce a\ntwo-stage classification pipeline combining finetuned encoder models and\ndecoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary\nPDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization\ncharacteristics. Applying this classifier to longitudinal and cross-platform\ndata, we see a marked rise in PDD over three decades, higher prevalence on\nsocial media versus parliamentary debate, greater use by male than female\npoliticians, and stronger tendencies among right-leaning actors - with\npronounced spikes during election campaigns and major political events. Our\nfindings demonstrate the feasibility and value of automated PDD analysis for\nunderstanding democratic discourse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first large-scale computational study of political\ndelegitimization discourse (PDD), defined as symbolic attacks on the normative\nvalidity of political entities. We curate and manually annotate a novel\nHebrew-language corpus of 10,410 sentences drawn from Knesset speeches\n(1993-2023), Facebook posts (2018-2021), and leading news outlets, of which\n1,812 instances (17.4\\%) exhibit PDD and 642 carry additional annotations for\nintensity, incivility, target type, and affective framing. We introduce a\ntwo-stage classification pipeline combining finetuned encoder models and\ndecoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary\nPDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization\ncharacteristics. Applying this classifier to longitudinal and cross-platform\ndata, we see a marked rise in PDD over three decades, higher prevalence on\nsocial media versus parliamentary debate, greater use by male than female\npoliticians, and stronger tendencies among right-leaning actors - with\npronounced spikes during election campaigns and major political events. Our\nfindings demonstrate the feasibility and value of automated PDD analysis for\nunderstanding democratic discourse."
                },
                "authors": [
                    {
                        "name": "Naama Rivlin-Angert"
                    },
                    {
                        "name": "Guy Mor-Lan"
                    }
                ],
                "author_detail": {
                    "name": "Guy Mor-Lan"
                },
                "author": "Guy Mor-Lan",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11302v1",
                "updated": "2025-10-13T11:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    48,
                    48,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:48:48Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    48,
                    48,
                    0,
                    286,
                    0
                ],
                "title": "When Does Supervised Training Pay Off? The Hidden Economics of Object\n  Detection in the Era of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Does Supervised Training Pay Off? The Hidden Economics of Object\n  Detection in the Era of Vision-Language Models"
                },
                "summary": "Object detection systems have traditionally relied on supervised learning\nwith manually annotated bounding boxes, achieving high accuracy at the cost of\nsubstantial annotation investment. The emergence of Vision-Language Models\n(VLMs) offers an alternative paradigm enabling zero-shot detection through\nnatural language queries, eliminating annotation requirements but operating\nwith reduced accuracy. This paper presents the first comprehensive\ncost-effectiveness analysis comparing supervised detection (YOLO) with\nzero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on\n1,000 stratified COCO images and 200 diverse product images spanning consumer\nelectronics and rare categories, combined with detailed Total Cost of Ownership\nmodeling, we establish quantitative break-even thresholds governing\narchitecture selection. Our findings reveal that supervised YOLO achieves 91.2%\naccuracy versus 68.5% for zero-shot Gemini on standard categories, representing\na 22.7 percentage point advantage that costs $10,800 in annotation for\n100-category systems. However, this advantage justifies investment only beyond\n55 million inferences, equivalent to 151,000 images daily for one year.\nZero-shot Gemini demonstrates 52.3% accuracy on diverse product categories\n(ranging from highly web-prevalent consumer electronics at 75-85% to rare\nspecialized equipment at 25-40%) where supervised YOLO achieves 0% due to\narchitectural constraints preventing detection of untrained classes. Cost per\nCorrect Detection analysis reveals substantially lower per-detection costs for\nGemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We\ndevelop decision frameworks demonstrating that optimal architecture selection\ndepends critically on deployment volume, category stability, budget\nconstraints, and accuracy requirements rather than purely technical performance\nmetrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detection systems have traditionally relied on supervised learning\nwith manually annotated bounding boxes, achieving high accuracy at the cost of\nsubstantial annotation investment. The emergence of Vision-Language Models\n(VLMs) offers an alternative paradigm enabling zero-shot detection through\nnatural language queries, eliminating annotation requirements but operating\nwith reduced accuracy. This paper presents the first comprehensive\ncost-effectiveness analysis comparing supervised detection (YOLO) with\nzero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on\n1,000 stratified COCO images and 200 diverse product images spanning consumer\nelectronics and rare categories, combined with detailed Total Cost of Ownership\nmodeling, we establish quantitative break-even thresholds governing\narchitecture selection. Our findings reveal that supervised YOLO achieves 91.2%\naccuracy versus 68.5% for zero-shot Gemini on standard categories, representing\na 22.7 percentage point advantage that costs $10,800 in annotation for\n100-category systems. However, this advantage justifies investment only beyond\n55 million inferences, equivalent to 151,000 images daily for one year.\nZero-shot Gemini demonstrates 52.3% accuracy on diverse product categories\n(ranging from highly web-prevalent consumer electronics at 75-85% to rare\nspecialized equipment at 25-40%) where supervised YOLO achieves 0% due to\narchitectural constraints preventing detection of untrained classes. Cost per\nCorrect Detection analysis reveals substantially lower per-detection costs for\nGemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We\ndevelop decision frameworks demonstrating that optimal architecture selection\ndepends critically on deployment volume, category stability, budget\nconstraints, and accuracy requirements rather than purely technical performance\nmetrics."
                },
                "authors": [
                    {
                        "name": "Samer Al-Hamadani"
                    }
                ],
                "author_detail": {
                    "name": "Samer Al-Hamadani"
                },
                "author": "Samer Al-Hamadani",
                "arxiv_comment": "23 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02689v2",
                "updated": "2025-10-13T11:42:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    42,
                    22,
                    0,
                    286,
                    0
                ],
                "published": "2024-12-03T18:58:11Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    18,
                    58,
                    11,
                    1,
                    338,
                    0
                ],
                "title": "Data Scaling Laws for Imitation Learning-Based End-to-End Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Scaling Laws for Imitation Learning-Based End-to-End Autonomous\n  Driving"
                },
                "summary": "The end-to-end autonomous driving paradigm has recently attracted lots of\nattention due to its scalability. However, existing methods are constrained by\nthe limited scale of real-world data, which hinders a comprehensive exploration\nof the scaling laws associated with end-to-end autonomous driving. To address\nthis issue, we collected substantial data from various driving scenarios and\nbehaviors and conducted an extensive study on the scaling laws of existing\nimitation learning-based end-to-end autonomous driving paradigms. Specifically,\napproximately 4 million demonstrations from 23 different scenario types were\ngathered, amounting to over 30,000 hours of driving demonstrations. We\nperformed open-loop evaluations and closed-loop simulation evaluations in 1,400\ndiverse driving demonstrations (1,300 for open-loop and 100 for closed-loop)\nunder stringent assessment conditions. Through experimental analysis, we\ndiscovered that (1) the performance of the driving model exhibits a power-law\nrelationship with the amount of data, but this is not the case in closed-loop\nevaluation. The inconsistency between the two assessments shifts our focus\ntoward the distribution of data rather than merely expanding its volume. (2) a\nsmall increase in the quantity of long-tailed data can significantly improve\nthe performance for the corresponding scenarios; (3) appropriate scaling of\ndata enables the model to achieve combinatorial generalization in novel scenes\nand actions. Our results highlight the critical role of data scaling in\nimproving the generalizability of models across diverse autonomous driving\nscenarios, assuring safe deployment in the real world.. Project repository:\nhttps://github.com/ucaszyp/Driving-Scaling-Law",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The end-to-end autonomous driving paradigm has recently attracted lots of\nattention due to its scalability. However, existing methods are constrained by\nthe limited scale of real-world data, which hinders a comprehensive exploration\nof the scaling laws associated with end-to-end autonomous driving. To address\nthis issue, we collected substantial data from various driving scenarios and\nbehaviors and conducted an extensive study on the scaling laws of existing\nimitation learning-based end-to-end autonomous driving paradigms. Specifically,\napproximately 4 million demonstrations from 23 different scenario types were\ngathered, amounting to over 30,000 hours of driving demonstrations. We\nperformed open-loop evaluations and closed-loop simulation evaluations in 1,400\ndiverse driving demonstrations (1,300 for open-loop and 100 for closed-loop)\nunder stringent assessment conditions. Through experimental analysis, we\ndiscovered that (1) the performance of the driving model exhibits a power-law\nrelationship with the amount of data, but this is not the case in closed-loop\nevaluation. The inconsistency between the two assessments shifts our focus\ntoward the distribution of data rather than merely expanding its volume. (2) a\nsmall increase in the quantity of long-tailed data can significantly improve\nthe performance for the corresponding scenarios; (3) appropriate scaling of\ndata enables the model to achieve combinatorial generalization in novel scenes\nand actions. Our results highlight the critical role of data scaling in\nimproving the generalizability of models across diverse autonomous driving\nscenarios, assuring safe deployment in the real world.. Project repository:\nhttps://github.com/ucaszyp/Driving-Scaling-Law"
                },
                "authors": [
                    {
                        "name": "Yupeng Zheng"
                    },
                    {
                        "name": "Pengxuan Yang"
                    },
                    {
                        "name": "Zhongpu Xia"
                    },
                    {
                        "name": "Qichao Zhang"
                    },
                    {
                        "name": "Yuhang Zheng"
                    },
                    {
                        "name": "Songen Gu"
                    },
                    {
                        "name": "Bu Jin"
                    },
                    {
                        "name": "Teng Zhang"
                    },
                    {
                        "name": "Ben Lu"
                    },
                    {
                        "name": "Chao Han"
                    },
                    {
                        "name": "Xianpeng Lang"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00743v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00743v3",
                "updated": "2025-10-13T11:42:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    42,
                    11,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-01T16:18:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    18,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Agentic large language models improve retrieval-based radiology question\n  answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic large language models improve retrieval-based radiology question\n  answering"
                },
                "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose radiology Retrieval\nand Reasoning (RaR), a multi-step retrieval and reasoning framework designed to\nimprove diagnostic accuracy, factual consistency, and clinical reliability of\nLLMs in radiology question answering. We evaluated 25 LLMs spanning diverse\narchitectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. To assess generalizability, we additionally tested on an\nunseen internal dataset of 65 real-world radiology board examination questions.\nRaR significantly improved mean diagnostic accuracy over zero-shot prompting\nand conventional online RAG. The greatest gains occurred in small-scale models,\nwhile very large models (>200B parameters) demonstrated minimal changes (<2%\nimprovement). Additionally, RaR retrieval reduced hallucinations (mean 9.4%)\nand retrieved clinically relevant context in 46% of cases, substantially aiding\nfactual grounding. Even clinically fine-tuned models showed gains from RaR\n(e.g., MedGemma-27B), indicating that retrieval remains beneficial despite\nembedded domain knowledge. These results highlight the potential of RaR to\nenhance factuality and diagnostic accuracy in radiology QA, warranting future\nstudies to validate their clinical utility. All datasets, code, and the full\nRaR framework are publicly available to support open research and clinical\ntranslation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose radiology Retrieval\nand Reasoning (RaR), a multi-step retrieval and reasoning framework designed to\nimprove diagnostic accuracy, factual consistency, and clinical reliability of\nLLMs in radiology question answering. We evaluated 25 LLMs spanning diverse\narchitectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. To assess generalizability, we additionally tested on an\nunseen internal dataset of 65 real-world radiology board examination questions.\nRaR significantly improved mean diagnostic accuracy over zero-shot prompting\nand conventional online RAG. The greatest gains occurred in small-scale models,\nwhile very large models (>200B parameters) demonstrated minimal changes (<2%\nimprovement). Additionally, RaR retrieval reduced hallucinations (mean 9.4%)\nand retrieved clinically relevant context in 46% of cases, substantially aiding\nfactual grounding. Even clinically fine-tuned models showed gains from RaR\n(e.g., MedGemma-27B), indicating that retrieval remains beneficial despite\nembedded domain knowledge. These results highlight the potential of RaR to\nenhance factuality and diagnostic accuracy in radiology QA, warranting future\nstudies to validate their clinical utility. All datasets, code, and the full\nRaR framework are publicly available to support open research and clinical\ntranslation."
                },
                "authors": [
                    {
                        "name": "Sebastian Wind"
                    },
                    {
                        "name": "Jeta Sopa"
                    },
                    {
                        "name": "Daniel Truhn"
                    },
                    {
                        "name": "Mahshad Lotfinia"
                    },
                    {
                        "name": "Tri-Thien Nguyen"
                    },
                    {
                        "name": "Keno Bressem"
                    },
                    {
                        "name": "Lisa Adams"
                    },
                    {
                        "name": "Mirabela Rusu"
                    },
                    {
                        "name": "Harald Köstler"
                    },
                    {
                        "name": "Gerhard Wellein"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Soroosh Tayebi Arasteh"
                    }
                ],
                "author_detail": {
                    "name": "Soroosh Tayebi Arasteh"
                },
                "author": "Soroosh Tayebi Arasteh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00743v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00743v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11297v1",
                "updated": "2025-10-13T11:37:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    37,
                    48,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:37:48Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    37,
                    48,
                    0,
                    286,
                    0
                ],
                "title": "Are Large Language Models Effective Knowledge Graph Constructors?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Effective Knowledge Graph Constructors?"
                },
                "summary": "Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown\npromise in reducing hallucinations in large language models (LLMs). However,\nconstructing high-quality KGs remains difficult, requiring accurate information\nextraction and structured representations that support interpretability and\ndownstream utility. Existing LLM-based approaches often focus narrowly on\nentity and relation extraction, limiting coverage to sentence-level contexts or\nrelying on predefined schemas. We propose a hierarchical extraction framework\nthat organizes information at multiple levels, enabling the creation of\nsemantically rich and well-structured KGs. Using state-of-the-art LLMs, we\nextract and construct knowledge graphs and evaluate them comprehensively from\nboth structural and semantic perspectives. Our results highlight the strengths\nand shortcomings of current LLMs in KG construction and identify key challenges\nfor future work. To advance research in this area, we also release a curated\ndataset of LLM-generated KGs derived from research papers on children's mental\nwell-being. This resource aims to foster more transparent, reliable, and\nimpactful applications in high-stakes domains such as healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown\npromise in reducing hallucinations in large language models (LLMs). However,\nconstructing high-quality KGs remains difficult, requiring accurate information\nextraction and structured representations that support interpretability and\ndownstream utility. Existing LLM-based approaches often focus narrowly on\nentity and relation extraction, limiting coverage to sentence-level contexts or\nrelying on predefined schemas. We propose a hierarchical extraction framework\nthat organizes information at multiple levels, enabling the creation of\nsemantically rich and well-structured KGs. Using state-of-the-art LLMs, we\nextract and construct knowledge graphs and evaluate them comprehensively from\nboth structural and semantic perspectives. Our results highlight the strengths\nand shortcomings of current LLMs in KG construction and identify key challenges\nfor future work. To advance research in this area, we also release a curated\ndataset of LLM-generated KGs derived from research papers on children's mental\nwell-being. This resource aims to foster more transparent, reliable, and\nimpactful applications in high-stakes domains such as healthcare."
                },
                "authors": [
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Bo Xiong"
                    },
                    {
                        "name": "Fiona Liausvia"
                    },
                    {
                        "name": "Dongkyu Choi"
                    },
                    {
                        "name": "Boon Kiat Quek"
                    }
                ],
                "author_detail": {
                    "name": "Boon Kiat Quek"
                },
                "author": "Boon Kiat Quek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16354v2",
                "updated": "2025-10-13T11:31:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    31,
                    20,
                    0,
                    286,
                    0
                ],
                "published": "2025-07-22T08:35:32Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    8,
                    35,
                    32,
                    1,
                    203,
                    0
                ],
                "title": "TARD: Test-time Domain Adaptation for Robust Fault Detection under\n  Evolving Operating Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TARD: Test-time Domain Adaptation for Robust Fault Detection under\n  Evolving Operating Conditions"
                },
                "summary": "Fault detection is essential in complex industrial systems to prevent\nfailures and optimize performance by distinguishing abnormal from normal\noperating conditions. With the growing availability of condition monitoring\ndata, data-driven approaches have increasingly applied in detecting system\nfaults. However, these methods typically require large, diverse, and\nrepresentative training datasets that capture the full range of operating\nscenarios, an assumption rarely met in practice, particularly in the early\nstages of deployment.\n  Industrial systems often operate under highly variable and evolving\nconditions, making it difficult to collect comprehensive training data. This\nvariability results in a distribution shift between training and testing data,\nas future operating conditions may diverge from those previously observed ones.\nSuch domain shifts hinder the generalization of traditional models, limiting\ntheir ability to transfer knowledge across time and system instances,\nultimately leading to performance degradation in practical deployments.\n  To address these challenges, we propose a novel method for continuous\ntest-time domain adaptation, designed to support robust early-stage fault\ndetection in the presence of domain shifts and limited representativeness of\ntraining data. Our proposed framework --Test-time domain Adaptation for Robust\nfault Detection (TARD) -- explicitly separates input features into system\nparameters and sensor measurements. It employs a dedicated domain adaptation\nmodule to adapt to each input type using different strategies, enabling more\ntargeted and effective adaptation to evolving operating conditions. We validate\nour approach on two real-world case studies from multi-phase flow facilities,\ndelivering substantial improvements in both fault detection accuracy and model\nrobustness over existing domain adaptation methods under real-world\nvariability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault detection is essential in complex industrial systems to prevent\nfailures and optimize performance by distinguishing abnormal from normal\noperating conditions. With the growing availability of condition monitoring\ndata, data-driven approaches have increasingly applied in detecting system\nfaults. However, these methods typically require large, diverse, and\nrepresentative training datasets that capture the full range of operating\nscenarios, an assumption rarely met in practice, particularly in the early\nstages of deployment.\n  Industrial systems often operate under highly variable and evolving\nconditions, making it difficult to collect comprehensive training data. This\nvariability results in a distribution shift between training and testing data,\nas future operating conditions may diverge from those previously observed ones.\nSuch domain shifts hinder the generalization of traditional models, limiting\ntheir ability to transfer knowledge across time and system instances,\nultimately leading to performance degradation in practical deployments.\n  To address these challenges, we propose a novel method for continuous\ntest-time domain adaptation, designed to support robust early-stage fault\ndetection in the presence of domain shifts and limited representativeness of\ntraining data. Our proposed framework --Test-time domain Adaptation for Robust\nfault Detection (TARD) -- explicitly separates input features into system\nparameters and sensor measurements. It employs a dedicated domain adaptation\nmodule to adapt to each input type using different strategies, enabling more\ntargeted and effective adaptation to evolving operating conditions. We validate\nour approach on two real-world case studies from multi-phase flow facilities,\ndelivering substantial improvements in both fault detection accuracy and model\nrobustness over existing domain adaptation methods under real-world\nvariability."
                },
                "authors": [
                    {
                        "name": "Han Sun"
                    },
                    {
                        "name": "Olga Fink"
                    }
                ],
                "author_detail": {
                    "name": "Olga Fink"
                },
                "author": "Olga Fink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11292v1",
                "updated": "2025-10-13T11:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences"
                },
                "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."
                },
                "authors": [
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11290v1",
                "updated": "2025-10-13T11:27:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    27,
                    53,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:27:53Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    27,
                    53,
                    0,
                    286,
                    0
                ],
                "title": "Evolution in Simulation: AI-Agent School with Dual Memory for\n  High-Fidelity Educational Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution in Simulation: AI-Agent School with Dual Memory for\n  High-Fidelity Educational Dynamics"
                },
                "summary": "Large language models (LLMs) based Agents are increasingly pivotal in\nsimulating and understanding complex human systems and interactions. We propose\nthe AI-Agent School (AAS) system, built around a self-evolving mechanism that\nleverages agents for simulating complex educational dynamics. Addressing the\nfragmented issues in teaching process modeling and the limitations of agents\nperformance in simulating diverse educational participants, AAS constructs the\nZero-Exp strategy, employs a continuous \"experience-reflection-optimization\"\ncycle, grounded in a dual memory base comprising experience and knowledge bases\nand incorporating short-term and long-term memory components. Through this\nmechanism, agents autonomously evolve via situated interactions within diverse\nsimulated school scenarios. This evolution enables agents to more accurately\nmodel the nuanced, multi-faceted teacher-student engagements and underlying\nlearning processes found in physical schools. Experiment confirms that AAS can\neffectively simulate intricate educational dynamics and is effective in\nfostering advanced agent cognitive abilities, providing a foundational stepping\nstone from the \"Era of Experience\" to the \"Era of Simulation\" by generating\nhigh-fidelity behavioral and interaction data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based Agents are increasingly pivotal in\nsimulating and understanding complex human systems and interactions. We propose\nthe AI-Agent School (AAS) system, built around a self-evolving mechanism that\nleverages agents for simulating complex educational dynamics. Addressing the\nfragmented issues in teaching process modeling and the limitations of agents\nperformance in simulating diverse educational participants, AAS constructs the\nZero-Exp strategy, employs a continuous \"experience-reflection-optimization\"\ncycle, grounded in a dual memory base comprising experience and knowledge bases\nand incorporating short-term and long-term memory components. Through this\nmechanism, agents autonomously evolve via situated interactions within diverse\nsimulated school scenarios. This evolution enables agents to more accurately\nmodel the nuanced, multi-faceted teacher-student engagements and underlying\nlearning processes found in physical schools. Experiment confirms that AAS can\neffectively simulate intricate educational dynamics and is effective in\nfostering advanced agent cognitive abilities, providing a foundational stepping\nstone from the \"Era of Experience\" to the \"Era of Simulation\" by generating\nhigh-fidelity behavioral and interaction data."
                },
                "authors": [
                    {
                        "name": "Sheng Jin"
                    },
                    {
                        "name": "Haoming Wang"
                    },
                    {
                        "name": "Zhiqi Gao"
                    },
                    {
                        "name": "Yongbo Yang"
                    },
                    {
                        "name": "Bao Chunjia"
                    },
                    {
                        "name": "Chengliang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chengliang Wang"
                },
                "author": "Chengliang Wang",
                "arxiv_comment": "9 pages, 7 figures, EMNLP conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11288v1",
                "updated": "2025-10-13T11:23:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    23,
                    56,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:23:56Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    23,
                    56,
                    0,
                    286,
                    0
                ],
                "title": "Emergent Misalignment via In-Context Learning: Narrow in-context\n  examples can produce broadly misaligned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Misalignment via In-Context Learning: Narrow in-context\n  examples can produce broadly misaligned LLMs"
                },
                "summary": "Recent work has shown that narrow finetuning can produce broadly misaligned\nLLMs, a phenomenon termed emergent misalignment (EM). While concerning, these\nfindings were limited to finetuning and activation steering, leaving out\nin-context learning (ICL). We therefore ask: does EM emerge in ICL? We find\nthat it does: across three datasets, three frontier models produce broadly\nmisaligned responses at rates between 2% and 17% given 64 narrow in-context\nexamples, and up to 58% with 256 examples. We also examine mechanisms of EM by\neliciting step-by-step reasoning (while leaving in-context examples unchanged).\nManual analysis of the resulting chain-of-thought shows that 67.5% of\nmisaligned traces explicitly rationalize harmful outputs by adopting a reckless\nor dangerous ''persona'', echoing prior results on finetuning-induced EM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that narrow finetuning can produce broadly misaligned\nLLMs, a phenomenon termed emergent misalignment (EM). While concerning, these\nfindings were limited to finetuning and activation steering, leaving out\nin-context learning (ICL). We therefore ask: does EM emerge in ICL? We find\nthat it does: across three datasets, three frontier models produce broadly\nmisaligned responses at rates between 2% and 17% given 64 narrow in-context\nexamples, and up to 58% with 256 examples. We also examine mechanisms of EM by\neliciting step-by-step reasoning (while leaving in-context examples unchanged).\nManual analysis of the resulting chain-of-thought shows that 67.5% of\nmisaligned traces explicitly rationalize harmful outputs by adopting a reckless\nor dangerous ''persona'', echoing prior results on finetuning-induced EM."
                },
                "authors": [
                    {
                        "name": "Nikita Afonin"
                    },
                    {
                        "name": "Nikita Andriyanov"
                    },
                    {
                        "name": "Nikhil Bageshpura"
                    },
                    {
                        "name": "Kyle Liu"
                    },
                    {
                        "name": "Kevin Zhu"
                    },
                    {
                        "name": "Sunishchal Dev"
                    },
                    {
                        "name": "Ashwinee Panda"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Oleg Rogov"
                    },
                    {
                        "name": "Elena Tutubalina"
                    },
                    {
                        "name": "Mikhail Seleznyov"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Seleznyov"
                },
                "author": "Mikhail Seleznyov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11282v1",
                "updated": "2025-10-13T11:15:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    15,
                    56,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:15:56Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    15,
                    56,
                    0,
                    286,
                    0
                ],
                "title": "Vision-LLMs for Spatiotemporal Traffic Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-LLMs for Spatiotemporal Traffic Forecasting"
                },
                "summary": "Accurate spatiotemporal traffic forecasting is a critical prerequisite for\nproactive resource management in dense urban mobile networks. While Large\nLanguage Models (LLMs) have shown promise in time series analysis, they\ninherently struggle to model the complex spatial dependencies of grid-based\ntraffic data. Effectively extending LLMs to this domain is challenging, as\nrepresenting the vast amount of information from dense geographical grids can\nbe inefficient and overwhelm the model's context. To address these challenges,\nwe propose ST-Vision-LLM, a novel framework that reframes spatiotemporal\nforecasting as a vision-language fusion problem. Our approach leverages a\nVision-LLM visual encoder to process historical global traffic matrices as\nimage sequences, providing the model with a comprehensive global view to inform\ncell-level predictions. To overcome the inefficiency of LLMs in handling\nnumerical data, we introduce an efficient encoding scheme that represents\nfloating-point values as single tokens via a specialized vocabulary, coupled\nwith a two-stage numerical alignment fine-tuning process. The model is first\ntrained with Supervised Fine-Tuning (SFT) and then further optimized for\npredictive accuracy using Group Relative Policy Optimization (GRPO), a\nmemory-efficient reinforcement learning method. Evaluations on real-world\nmobile traffic datasets demonstrate that ST-Vision-LLM outperforms existing\nmethods by 15.6% in long-term prediction accuracy and exceeds the second-best\nbaseline by over 30.04% in cross-domain few-shot scenarios. Our extensive\nexperiments validate the model's strong generalization capabilities across\nvarious data-scarce environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate spatiotemporal traffic forecasting is a critical prerequisite for\nproactive resource management in dense urban mobile networks. While Large\nLanguage Models (LLMs) have shown promise in time series analysis, they\ninherently struggle to model the complex spatial dependencies of grid-based\ntraffic data. Effectively extending LLMs to this domain is challenging, as\nrepresenting the vast amount of information from dense geographical grids can\nbe inefficient and overwhelm the model's context. To address these challenges,\nwe propose ST-Vision-LLM, a novel framework that reframes spatiotemporal\nforecasting as a vision-language fusion problem. Our approach leverages a\nVision-LLM visual encoder to process historical global traffic matrices as\nimage sequences, providing the model with a comprehensive global view to inform\ncell-level predictions. To overcome the inefficiency of LLMs in handling\nnumerical data, we introduce an efficient encoding scheme that represents\nfloating-point values as single tokens via a specialized vocabulary, coupled\nwith a two-stage numerical alignment fine-tuning process. The model is first\ntrained with Supervised Fine-Tuning (SFT) and then further optimized for\npredictive accuracy using Group Relative Policy Optimization (GRPO), a\nmemory-efficient reinforcement learning method. Evaluations on real-world\nmobile traffic datasets demonstrate that ST-Vision-LLM outperforms existing\nmethods by 15.6% in long-term prediction accuracy and exceeds the second-best\nbaseline by over 30.04% in cross-domain few-shot scenarios. Our extensive\nexperiments validate the model's strong generalization capabilities across\nvarious data-scarce environments."
                },
                "authors": [
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Hengyu Zhong"
                    },
                    {
                        "name": "Haijun Zhang"
                    },
                    {
                        "name": "Randall Berry"
                    }
                ],
                "author_detail": {
                    "name": "Randall Berry"
                },
                "author": "Randall Berry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11281v1",
                "updated": "2025-10-13T11:15:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    15,
                    49,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:15:49Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    15,
                    49,
                    0,
                    286,
                    0
                ],
                "title": "PADME: Procedure Aware DynaMic Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PADME: Procedure Aware DynaMic Execution"
                },
                "summary": "Learning to autonomously execute long-horizon procedures from natural\nlanguage remains a core challenge for intelligent agents. Free-form\ninstructions such as recipes, scientific protocols, or business workflows\nencode rich procedural knowledge, but their variability and lack of structure\ncause agents driven by large language models (LLMs) to drift or fail during\nexecution. We introduce Procedure Aware DynaMic Execution (PADME), an agent\nframework that produces and exploits a graph-based representation of\nprocedures. Unlike prior work that relies on manual graph construction or\nunstructured reasoning, PADME autonomously transforms procedural text into\nexecutable graphs that capture task dependencies, decision points, and reusable\nsubroutines. Central to PADME is a two-phase methodology; Teach phase, which\nfocuses on systematic structuring, enrichment with executable logic of\nprocedures, followed by Execute phase, which enables dynamic execution in\nresponse to real-time inputs and environment feedback. This separation ensures\nquality assurance and scalability, allowing expert knowledge to be encoded once\nand reliably reused across varying contexts. The graph representation also\nprovides an inductive bias that reduces error accumulation in long-horizon\nreasoning, underscoring the importance of structured procedure modeling for\nreliable agent-driven automation. Empirically, PADME achieves state-of-the-art\nperformance on four diverse benchmarks, including ALFWorld and ScienceWorld.\nThese results demonstrate that agents equipped with graph-based procedure\nrepresentations offer a powerful intermediate abstraction for robust and\ngeneralizable execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to autonomously execute long-horizon procedures from natural\nlanguage remains a core challenge for intelligent agents. Free-form\ninstructions such as recipes, scientific protocols, or business workflows\nencode rich procedural knowledge, but their variability and lack of structure\ncause agents driven by large language models (LLMs) to drift or fail during\nexecution. We introduce Procedure Aware DynaMic Execution (PADME), an agent\nframework that produces and exploits a graph-based representation of\nprocedures. Unlike prior work that relies on manual graph construction or\nunstructured reasoning, PADME autonomously transforms procedural text into\nexecutable graphs that capture task dependencies, decision points, and reusable\nsubroutines. Central to PADME is a two-phase methodology; Teach phase, which\nfocuses on systematic structuring, enrichment with executable logic of\nprocedures, followed by Execute phase, which enables dynamic execution in\nresponse to real-time inputs and environment feedback. This separation ensures\nquality assurance and scalability, allowing expert knowledge to be encoded once\nand reliably reused across varying contexts. The graph representation also\nprovides an inductive bias that reduces error accumulation in long-horizon\nreasoning, underscoring the importance of structured procedure modeling for\nreliable agent-driven automation. Empirically, PADME achieves state-of-the-art\nperformance on four diverse benchmarks, including ALFWorld and ScienceWorld.\nThese results demonstrate that agents equipped with graph-based procedure\nrepresentations offer a powerful intermediate abstraction for robust and\ngeneralizable execution."
                },
                "authors": [
                    {
                        "name": "Deepeka Garg"
                    },
                    {
                        "name": "Sihan Zeng"
                    },
                    {
                        "name": "Annapoorani L. Narayanan"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    },
                    {
                        "name": "Leo Ardon"
                    }
                ],
                "author_detail": {
                    "name": "Leo Ardon"
                },
                "author": "Leo Ardon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11278v1",
                "updated": "2025-10-13T11:13:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    13,
                    9,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:13:09Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    13,
                    9,
                    0,
                    286,
                    0
                ],
                "title": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models"
                },
                "summary": "We present Entropic Mutual-Information Geometry Large-Language Model\nAlignment (ENIGMA), a novel approach to Large-Language Model (LLM) training\nthat jointly improves reasoning, alignment and robustness by treating an\norganisation's policies/principles as directions to move on a model's\ninformation manifold. Our single-loop trainer combines Group-Relative Policy\nOptimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought\n(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information\n(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn\noptimal-transport regulariser on hidden-state distributions to bound geometry\ndrift. We also introduce infoNCE metrics that specialise to a standard MI lower\nbound under matched negatives to measure how strongly a model's CoT encodes\nthese policies. These metrics include a Sufficiency Index (SI) that enables the\nselection and creation of principles that maximise downstream performance prior\nto training. In our experiments using small (1B) LLMs, high-SI principles\npredict steadier training dynamics and improved benchmark performance over GRPO\nablations. Our information-geometry analysis of trained models validates\ndesirable structural change in the manifold. These results support our\nhypothesis that reasoning, alignment, and robustness are projections of a\nsingle informationgeometric objective, and that models trained using ENIGMA\ndemonstrate principled reasoning without the use of a reward model, offering a\npath to trusted capability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Entropic Mutual-Information Geometry Large-Language Model\nAlignment (ENIGMA), a novel approach to Large-Language Model (LLM) training\nthat jointly improves reasoning, alignment and robustness by treating an\norganisation's policies/principles as directions to move on a model's\ninformation manifold. Our single-loop trainer combines Group-Relative Policy\nOptimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought\n(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information\n(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn\noptimal-transport regulariser on hidden-state distributions to bound geometry\ndrift. We also introduce infoNCE metrics that specialise to a standard MI lower\nbound under matched negatives to measure how strongly a model's CoT encodes\nthese policies. These metrics include a Sufficiency Index (SI) that enables the\nselection and creation of principles that maximise downstream performance prior\nto training. In our experiments using small (1B) LLMs, high-SI principles\npredict steadier training dynamics and improved benchmark performance over GRPO\nablations. Our information-geometry analysis of trained models validates\ndesirable structural change in the manifold. These results support our\nhypothesis that reasoning, alignment, and robustness are projections of a\nsingle informationgeometric objective, and that models trained using ENIGMA\ndemonstrate principled reasoning without the use of a reward model, offering a\npath to trusted capability"
                },
                "authors": [
                    {
                        "name": "Gareth Seneque"
                    },
                    {
                        "name": "Lap-Hang Ho"
                    },
                    {
                        "name": "Nafise Erfanian Saeedi"
                    },
                    {
                        "name": "Jeffrey Molendijk"
                    },
                    {
                        "name": "Ariel Kupermann"
                    },
                    {
                        "name": "Tim Elson"
                    }
                ],
                "author_detail": {
                    "name": "Tim Elson"
                },
                "author": "Tim Elson",
                "arxiv_comment": "52 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11277v1",
                "updated": "2025-10-13T11:11:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    11,
                    46,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:11:46Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    11,
                    46,
                    0,
                    286,
                    0
                ],
                "title": "Towards Real-Time Fake News Detection under Evidence Scarcity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Real-Time Fake News Detection under Evidence Scarcity"
                },
                "summary": "Fake news detection becomes particularly challenging in real-time scenarios,\nwhere emerging events often lack sufficient supporting evidence. Existing\napproaches often rely heavily on external evidence and therefore struggle to\ngeneralize under evidence scarcity. To address this issue, we propose\nEvaluation-Aware Selection of Experts (EASE), a novel framework for real-time\nfake news detection that dynamically adapts its decision-making process\naccording to the assessed sufficiency of available evidence. EASE introduces a\nsequential evaluation mechanism comprising three independent perspectives: (1)\nEvidence-based evaluation, which assesses evidence and incorporates it into\ndecision-making only when the evidence is sufficiently supportive; (2)\nReasoning-based evaluation, which leverages the world knowledge of large\nlanguage models (LLMs) and applies them only when their reliability is\nadequately established; and (3) Sentiment-based fallback, which integrates\nsentiment cues when neither evidence nor reasoning is reliable. To enhance the\naccuracy of evaluation processes, EASE employs instruction tuning with pseudo\nlabels to guide each evaluator in justifying its perspective-specific knowledge\nthrough interpretable reasoning. Furthermore, the expert modules integrate the\nevaluators' justified assessments with the news content to enable\nevaluation-aware decision-making, thereby enhancing overall detection accuracy.\nMoreover, we introduce RealTimeNews-25, a new benchmark comprising recent news\nfor evaluating model generalization on emerging news with limited evidence.\nExtensive experiments demonstrate that EASE not only achieves state-of-the-art\nperformance across multiple benchmarks, but also significantly improves\ngeneralization to real-time news. The code and dataset are available:\nhttps://github.com/wgyhhhh/EASE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fake news detection becomes particularly challenging in real-time scenarios,\nwhere emerging events often lack sufficient supporting evidence. Existing\napproaches often rely heavily on external evidence and therefore struggle to\ngeneralize under evidence scarcity. To address this issue, we propose\nEvaluation-Aware Selection of Experts (EASE), a novel framework for real-time\nfake news detection that dynamically adapts its decision-making process\naccording to the assessed sufficiency of available evidence. EASE introduces a\nsequential evaluation mechanism comprising three independent perspectives: (1)\nEvidence-based evaluation, which assesses evidence and incorporates it into\ndecision-making only when the evidence is sufficiently supportive; (2)\nReasoning-based evaluation, which leverages the world knowledge of large\nlanguage models (LLMs) and applies them only when their reliability is\nadequately established; and (3) Sentiment-based fallback, which integrates\nsentiment cues when neither evidence nor reasoning is reliable. To enhance the\naccuracy of evaluation processes, EASE employs instruction tuning with pseudo\nlabels to guide each evaluator in justifying its perspective-specific knowledge\nthrough interpretable reasoning. Furthermore, the expert modules integrate the\nevaluators' justified assessments with the news content to enable\nevaluation-aware decision-making, thereby enhancing overall detection accuracy.\nMoreover, we introduce RealTimeNews-25, a new benchmark comprising recent news\nfor evaluating model generalization on emerging news with limited evidence.\nExtensive experiments demonstrate that EASE not only achieves state-of-the-art\nperformance across multiple benchmarks, but also significantly improves\ngeneralization to real-time news. The code and dataset are available:\nhttps://github.com/wgyhhhh/EASE."
                },
                "authors": [
                    {
                        "name": "Guangyu Wei"
                    },
                    {
                        "name": "Ke Han"
                    },
                    {
                        "name": "Yueming Lyu"
                    },
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Nicu Sebe"
                    }
                ],
                "author_detail": {
                    "name": "Nicu Sebe"
                },
                "author": "Nicu Sebe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10320v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10320v3",
                "updated": "2025-10-13T11:06:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    6,
                    51,
                    0,
                    286,
                    0
                ],
                "published": "2025-05-15T14:05:15Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    5,
                    15,
                    3,
                    135,
                    0
                ],
                "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning"
                },
                "summary": "The progress of AI is bottlenecked by the quality of evaluation, making\npowerful LLM-as-a-Judge models a core solution. The efficacy of these judges\ndepends on their chain-of-thought reasoning, creating a critical need for\nmethods that can effectively optimize this reasoning process. In this work, we\nintroduce J1, a reinforcement learning framework for teaching LLM judges to\nthink before making decisions. Our core contribution lies in converting all\njudgment tasks for non-verifiable and verifiable prompts into a unified format\nwith verifiable rewards, enabling direct optimization of evaluation quality\nwhile mitigating positional bias. We then use RL to train thinking-judges at\nscales of 8B, 32B, and 70B and show that they obtain state-of-the-art\nperformance across multiple benchmarks. In particular, J1-Qwen-32B, our\nmultitasked pointwise and pairwise judge also outperforms o1-mini, o3, and a\nmuch larger 671B DeepSeek-R1 on some benchmarks, while only training on\nsynthetic data. Through comprehensive ablations of pairwise, pointwise, and\nmultitask J1 variants, we demonstrate the effectiveness of our approach across\nseed prompts, reward strategies, and training recipes. Qualitative analysis\nreveals that J1 develops systematic evaluation strategies, including dynamic\ncriteria generation, reference answer creation, iterative self-correction of\ninitial assessments, and feedback generation for low-quality responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The progress of AI is bottlenecked by the quality of evaluation, making\npowerful LLM-as-a-Judge models a core solution. The efficacy of these judges\ndepends on their chain-of-thought reasoning, creating a critical need for\nmethods that can effectively optimize this reasoning process. In this work, we\nintroduce J1, a reinforcement learning framework for teaching LLM judges to\nthink before making decisions. Our core contribution lies in converting all\njudgment tasks for non-verifiable and verifiable prompts into a unified format\nwith verifiable rewards, enabling direct optimization of evaluation quality\nwhile mitigating positional bias. We then use RL to train thinking-judges at\nscales of 8B, 32B, and 70B and show that they obtain state-of-the-art\nperformance across multiple benchmarks. In particular, J1-Qwen-32B, our\nmultitasked pointwise and pairwise judge also outperforms o1-mini, o3, and a\nmuch larger 671B DeepSeek-R1 on some benchmarks, while only training on\nsynthetic data. Through comprehensive ablations of pairwise, pointwise, and\nmultitask J1 variants, we demonstrate the effectiveness of our approach across\nseed prompts, reward strategies, and training recipes. Qualitative analysis\nreveals that J1 develops systematic evaluation strategies, including dynamic\ncriteria generation, reference answer creation, iterative self-correction of\ninitial assessments, and feedback generation for low-quality responses."
                },
                "authors": [
                    {
                        "name": "Chenxi Whitehouse"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    }
                ],
                "author_detail": {
                    "name": "Swarnadeep Saha"
                },
                "author": "Swarnadeep Saha",
                "arxiv_comment": "10 pages, 13 tables, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10320v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10320v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11264v1",
                "updated": "2025-10-13T10:53:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    53,
                    29,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T10:53:29Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    53,
                    29,
                    0,
                    286,
                    0
                ],
                "title": "Learning Hanzi Character Through VR-Based Mortise-Tenon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Hanzi Character Through VR-Based Mortise-Tenon"
                },
                "summary": "This paper introduces a novel VR-based system that redefines the acquisition\nof Hanzi character literacy by integrating traditional mortise-tenon joinery\nprinciples (HVRMT).Addressing the challenge of abstract character memorization\nin digital learning,our system deconstructs Hanzi components into interactive\n\"structural radicals\"akin to wooden joint modules.Leveraging PICO's 6DoF\nspatial tracking and LLM's morphological analysis,learners assemble stroke\nsequences with haptic feedback simulating wood-to-wood friction.Our system also\nsupports multiplayer online experiences, enhancing engagement and memory\nretention while preserving intangible cultural heritage. This innovative\napproach not only enhances engagement and memory retention but also\nreconstructs the craft wisdom embedded in Chinese writing systems, offering new\npathways for preserving intangible cultural heritage in digital ecosystems.For\nthe demo,please refer to this link{https://youtu.be/oUwfFTRpFyo}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel VR-based system that redefines the acquisition\nof Hanzi character literacy by integrating traditional mortise-tenon joinery\nprinciples (HVRMT).Addressing the challenge of abstract character memorization\nin digital learning,our system deconstructs Hanzi components into interactive\n\"structural radicals\"akin to wooden joint modules.Leveraging PICO's 6DoF\nspatial tracking and LLM's morphological analysis,learners assemble stroke\nsequences with haptic feedback simulating wood-to-wood friction.Our system also\nsupports multiplayer online experiences, enhancing engagement and memory\nretention while preserving intangible cultural heritage. This innovative\napproach not only enhances engagement and memory retention but also\nreconstructs the craft wisdom embedded in Chinese writing systems, offering new\npathways for preserving intangible cultural heritage in digital ecosystems.For\nthe demo,please refer to this link{https://youtu.be/oUwfFTRpFyo}."
                },
                "authors": [
                    {
                        "name": "Conglin Ma"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Sen-Zhe Xu"
                    },
                    {
                        "name": "Ju Dai"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Feng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhou"
                },
                "author": "Feng Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]