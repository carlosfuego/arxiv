[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v2",
                "updated": "2025-09-04T13:14:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    14,
                    33,
                    3,
                    247,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v2",
                "updated": "2025-09-04T09:08:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    9,
                    8,
                    29,
                    3,
                    247,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v1",
                "updated": "2025-09-03T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v1",
                "updated": "2025-09-01T19:49:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i)\na new OMAP algorithm that leverages isolated HBM as an unobservable cache to\naccelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i)\na new OMAP algorithm that leverages isolated HBM as an unobservable cache to\naccelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v1",
                "updated": "2025-09-01T03:16:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "ron Jnosik"
                    },
                    {
                        "name": "Csenge Mikls"
                    },
                    {
                        "name": "Dniel G. Simon"
                    },
                    {
                        "name": "Kristf Zlomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristf Zlomy"
                },
                "author": "Kristf Zlomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v1",
                "updated": "2025-08-29T02:29:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando Garca-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00105v1",
                "updated": "2025-08-28T00:46:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T00:46:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving"
                },
                "summary": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay."
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00100v1",
                "updated": "2025-08-27T17:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T17:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "title": "MODE: Mixture of Document Experts for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODE: Mixture of Document Experts for RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter."
                },
                "authors": [
                    {
                        "name": "Rahul Anand"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Anand"
                },
                "author": "Rahul Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Gera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MnchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21091v1",
                "updated": "2025-08-27T10:37:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion"
                },
                "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache."
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v3",
                "updated": "2025-08-26T03:23:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    23,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00052v1",
                "updated": "2025-08-25T02:58:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T02:58:39Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "title": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation"
                },
                "summary": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality."
                },
                "authors": [
                    {
                        "name": "Jianzhi Long"
                    },
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rongcheng Tu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17593v1",
                "updated": "2025-08-25T01:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T01:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs"
                },
                "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."
                },
                "authors": [
                    {
                        "name": "Aadesh Deshmukh"
                    },
                    {
                        "name": "Venkata Yaswanth Raparti"
                    },
                    {
                        "name": "Samuel Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Hsu"
                },
                "author": "Samuel Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v3",
                "updated": "2025-08-25T00:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    0,
                    15,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on\n  Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v3",
                "updated": "2025-08-24T22:09:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    22,
                    9,
                    57,
                    6,
                    236,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "Published as a conference paper at EMNLP 2025. 28 pages, 7 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17518v1",
                "updated": "2025-08-24T20:51:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T20:51:06Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "title": "Evaluating Compiler Optimization Impacts on zkVM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compiler Optimization Impacts on zkVM Performance"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers."
                },
                "authors": [
                    {
                        "name": "Thomas Gassmann"
                    },
                    {
                        "name": "Stefanos Chaliasos"
                    },
                    {
                        "name": "Thodoris Sotiropoulos"
                    },
                    {
                        "name": "Zhendong Su"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Su"
                },
                "author": "Zhendong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17496v1",
                "updated": "2025-08-24T19:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T19:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "title": "Practical Insertion-Only Convex Hull",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Insertion-Only Convex Hull"
                },
                "summary": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large."
                },
                "authors": [
                    {
                        "name": "Ivor van der Hoog"
                    },
                    {
                        "name": "Henrik Reinstdtler"
                    },
                    {
                        "name": "Eva Rotenberg"
                    }
                ],
                "author_detail": {
                    "name": "Eva Rotenberg"
                },
                "author": "Eva Rotenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17445v1",
                "updated": "2025-08-24T16:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling"
                },
                "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO."
                },
                "authors": [
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17434v1",
                "updated": "2025-08-24T16:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:17:33Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yuhang Yu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v1",
                "updated": "2025-08-24T13:30:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17219v1",
                "updated": "2025-08-24T05:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T05:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving"
                },
                "summary": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14148v2",
                "updated": "2025-08-23T20:28:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    45,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-19T16:56:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPad: Efficient Diffusion Language Models with Suffix Dropout"
                },
                "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad."
                },
                "authors": [
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17137v1",
                "updated": "2025-08-23T20:28:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T20:28:32Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "title": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices"
                },
                "summary": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines."
                },
                "authors": [
                    {
                        "name": "Nishant Gavhane"
                    },
                    {
                        "name": "Arush Mehrotra"
                    },
                    {
                        "name": "Rohit Chawla"
                    },
                    {
                        "name": "Peter Proenca"
                    }
                ],
                "author_detail": {
                    "name": "Peter Proenca"
                },
                "author": "Peter Proenca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17125v1",
                "updated": "2025-08-23T19:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T19:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "title": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling"
                },
                "summary": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Yongxiang Tang"
                    },
                    {
                        "name": "Yanhua Cheng"
                    },
                    {
                        "name": "Yong Bai"
                    },
                    {
                        "name": "Yanxiang Zeng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Xialong Liu"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17032v1",
                "updated": "2025-08-23T14:20:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T14:20:06Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "title": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations"
                },
                "summary": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling."
                },
                "authors": [
                    {
                        "name": "Maurizio Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Diaz"
                },
                "author": "Maurizio Diaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16984v1",
                "updated": "2025-08-23T10:35:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T10:35:16Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching"
                },
                "summary": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v2",
                "updated": "2025-08-23T08:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    40,
                    52,
                    5,
                    235,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v2",
                "updated": "2025-08-22T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    45,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.05291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05291v1",
                "updated": "2025-09-05T17:56:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    56,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T17:56:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    56,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "Crosscoding Through Time: Tracking Emergence & Consolidation Of\n  Linguistic Representations Throughout LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crosscoding Through Time: Tracking Emergence & Consolidation Of\n  Linguistic Representations Throughout LLM Pretraining"
                },
                "summary": "Large language models (LLMs) learn non-trivial abstractions during\npretraining, like detecting irregular plural noun subjects. However, it is not\nwell understood when and how specific linguistic abilities emerge as\ntraditional evaluation methods such as benchmarking fail to reveal how models\nacquire concepts and capabilities. To bridge this gap and better understand\nmodel training at the concept level, we use sparse crosscoders to discover and\nalign features across model checkpoints. Using this approach, we track the\nevolution of linguistic features during pretraining. We train crosscoders\nbetween open-sourced checkpoint triplets with significant performance and\nrepresentation shifts, and introduce a novel metric, Relative Indirect Effects\n(RelIE), to trace training stages at which individual features become causally\nimportant for task performance. We show that crosscoders can detect feature\nemergence, maintenance, and discontinuation during pretraining. Our approach is\narchitecture-agnostic and scalable, offering a promising path toward more\ninterpretable and fine-grained analysis of representation learning throughout\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) learn non-trivial abstractions during\npretraining, like detecting irregular plural noun subjects. However, it is not\nwell understood when and how specific linguistic abilities emerge as\ntraditional evaluation methods such as benchmarking fail to reveal how models\nacquire concepts and capabilities. To bridge this gap and better understand\nmodel training at the concept level, we use sparse crosscoders to discover and\nalign features across model checkpoints. Using this approach, we track the\nevolution of linguistic features during pretraining. We train crosscoders\nbetween open-sourced checkpoint triplets with significant performance and\nrepresentation shifts, and introduce a novel metric, Relative Indirect Effects\n(RelIE), to trace training stages at which individual features become causally\nimportant for task performance. We show that crosscoders can detect feature\nemergence, maintenance, and discontinuation during pretraining. Our approach is\narchitecture-agnostic and scalable, offering a promising path toward more\ninterpretable and fine-grained analysis of representation learning throughout\npretraining."
                },
                "authors": [
                    {
                        "name": "Deniz Bayazit"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07236v2",
                "updated": "2025-09-05T17:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    54,
                    18,
                    4,
                    248,
                    0
                ],
                "published": "2025-07-09T19:13:25Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    19,
                    13,
                    25,
                    2,
                    190,
                    0
                ],
                "title": "Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM\n  Uncertainty Quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM\n  Uncertainty Quantification"
                },
                "summary": "Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and na\\\"ive ensemble baselines. In addition, we\nexplore using MUSE as guided signals with chain-of-thought distillation to\nfine-tune LLMs for calibration. MUSE is available\nat:https://github.com/LARK-NLP-Lab/MUSE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and na\\\"ive ensemble baselines. In addition, we\nexplore using MUSE as guided signals with chain-of-thought distillation to\nfine-tune LLMs for calibration. MUSE is available\nat:https://github.com/LARK-NLP-Lab/MUSE."
                },
                "authors": [
                    {
                        "name": "Maya Kruse"
                    },
                    {
                        "name": "Majid Afshar"
                    },
                    {
                        "name": "Saksham Khatwani"
                    },
                    {
                        "name": "Anoop Mayampurath"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yanjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Gao"
                },
                "author": "Yanjun Gao",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05528v2",
                "updated": "2025-09-05T17:52:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    52,
                    53,
                    4,
                    248,
                    0
                ],
                "published": "2025-07-07T22:56:37Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    22,
                    56,
                    37,
                    0,
                    188,
                    0
                ],
                "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for\n  Procedural Learning and Pedagogic Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Education at Scale: A Multi-LLM Agent Workflow for\n  Procedural Learning and Pedagogic Quality Assessment"
                },
                "summary": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced."
                },
                "authors": [
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Wentao Deng"
                    },
                    {
                        "name": "Koen Hindriks"
                    },
                    {
                        "name": "Junxiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junxiao Wang"
                },
                "author": "Junxiao Wang",
                "arxiv_comment": "14 pages, accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06460v2",
                "updated": "2025-09-05T17:44:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    44,
                    5,
                    4,
                    248,
                    0
                ],
                "published": "2025-04-08T22:00:32Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    22,
                    0,
                    32,
                    1,
                    98,
                    0
                ],
                "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for\n  Counterfactual Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for\n  Counterfactual Instruction Following"
                },
                "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate\npersonas in virtual environments, leveraging their instruction-following\ncapability. However, we discovered that even state-of-the-art LLMs cannot\nsimulate personas with reversed performance (e.g., student personas with low\nproficiency in educational settings), which impairs the simulation diversity\nand limits the practical applications of the simulated environments. In this\nwork, using mathematical reasoning as a representative scenario, we propose the\nfirst benchmark dataset for evaluating LLMs on simulating personas with\nreversed performance, a capability that we dub \"counterfactual instruction\nfollowing\". We evaluate both open-weight and closed-source LLMs on this task\nand find that LLMs, including the OpenAI o1 reasoning model, all struggle to\nfollow counterfactual instructions for simulating reversedly performing\npersonas. Intersectionally simulating both the performance level and the race\npopulation of a persona worsens the effect even further. These results\nhighlight the challenges of counterfactual instruction following and the need\nfor further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are now increasingly widely used to simulate\npersonas in virtual environments, leveraging their instruction-following\ncapability. However, we discovered that even state-of-the-art LLMs cannot\nsimulate personas with reversed performance (e.g., student personas with low\nproficiency in educational settings), which impairs the simulation diversity\nand limits the practical applications of the simulated environments. In this\nwork, using mathematical reasoning as a representative scenario, we propose the\nfirst benchmark dataset for evaluating LLMs on simulating personas with\nreversed performance, a capability that we dub \"counterfactual instruction\nfollowing\". We evaluate both open-weight and closed-source LLMs on this task\nand find that LLMs, including the OpenAI o1 reasoning model, all struggle to\nfollow counterfactual instructions for simulating reversedly performing\npersonas. Intersectionally simulating both the performance level and the race\npopulation of a persona worsens the effect even further. These results\nhighlight the challenges of counterfactual instruction following and the need\nfor further research."
                },
                "authors": [
                    {
                        "name": "Sai Adith Senthil Kumar"
                    },
                    {
                        "name": "Hao Yan"
                    },
                    {
                        "name": "Saipavan Perepa"
                    },
                    {
                        "name": "Murong Yue"
                    },
                    {
                        "name": "Ziyu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Yao"
                },
                "author": "Ziyu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05276v1",
                "updated": "2025-09-05T17:34:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    34,
                    0,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T17:34:00Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    34,
                    0,
                    4,
                    248,
                    0
                ],
                "title": "SpikingBrain Technical Report: Spiking Brain-inspired Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikingBrain Technical Report: Spiking Brain-inspired Large Models"
                },
                "summary": "Mainstream Transformer-based large language models face major efficiency\nbottlenecks: training computation scales quadratically with sequence length,\nand inference memory grows linearly, limiting long-context processing. Building\nlarge models on non-NVIDIA platforms also poses challenges for stable and\nefficient training. To address this, we introduce SpikingBrain, a family of\nbrain-inspired models designed for efficient long-context training and\ninference. SpikingBrain leverages the MetaX GPU cluster and focuses on three\naspects: (1) Model Architecture: linear and hybrid-linear attention\narchitectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an\nefficient, conversion-based training pipeline and a dedicated spike coding\nframework; (3) System Engineering: customized training frameworks, operator\nlibraries, and parallelism strategies tailored to MetaX hardware.\n  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,\nand SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the\nfeasibility of large-scale LLM development on non-NVIDIA platforms.\nSpikingBrain achieves performance comparable to open-source Transformer\nbaselines while using only about 150B tokens for continual pre-training. Our\nmodels significantly improve long-sequence training efficiency and deliver\ninference with (partially) constant memory and event-driven spiking behavior.\nFor example, SpikingBrain-7B attains over 100x speedup in Time to First Token\nfor 4M-token sequences. Training remains stable for weeks on hundreds of MetaX\nC550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4\npercent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling\nlow-power operation. Overall, this work demonstrates the potential of\nbrain-inspired mechanisms to drive the next generation of efficient and\nscalable large model design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainstream Transformer-based large language models face major efficiency\nbottlenecks: training computation scales quadratically with sequence length,\nand inference memory grows linearly, limiting long-context processing. Building\nlarge models on non-NVIDIA platforms also poses challenges for stable and\nefficient training. To address this, we introduce SpikingBrain, a family of\nbrain-inspired models designed for efficient long-context training and\ninference. SpikingBrain leverages the MetaX GPU cluster and focuses on three\naspects: (1) Model Architecture: linear and hybrid-linear attention\narchitectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an\nefficient, conversion-based training pipeline and a dedicated spike coding\nframework; (3) System Engineering: customized training frameworks, operator\nlibraries, and parallelism strategies tailored to MetaX hardware.\n  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,\nand SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the\nfeasibility of large-scale LLM development on non-NVIDIA platforms.\nSpikingBrain achieves performance comparable to open-source Transformer\nbaselines while using only about 150B tokens for continual pre-training. Our\nmodels significantly improve long-sequence training efficiency and deliver\ninference with (partially) constant memory and event-driven spiking behavior.\nFor example, SpikingBrain-7B attains over 100x speedup in Time to First Token\nfor 4M-token sequences. Training remains stable for weeks on hundreds of MetaX\nC550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4\npercent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling\nlow-power operation. Overall, this work demonstrates the potential of\nbrain-inspired mechanisms to drive the next generation of efficient and\nscalable large model design."
                },
                "authors": [
                    {
                        "name": "Yuqi Pan"
                    },
                    {
                        "name": "Yupeng Feng"
                    },
                    {
                        "name": "Jinghao Zhuang"
                    },
                    {
                        "name": "Siyu Ding"
                    },
                    {
                        "name": "Zehao Liu"
                    },
                    {
                        "name": "Bohan Sun"
                    },
                    {
                        "name": "Yuhong Chou"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Xuerui Qiu"
                    },
                    {
                        "name": "Anlin Deng"
                    },
                    {
                        "name": "Anjie Hu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Man Yao"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Guoliang Sun"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Guoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoqi Li"
                },
                "author": "Guoqi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02445v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02445v7",
                "updated": "2025-09-05T17:32:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    32,
                    10,
                    4,
                    248,
                    0
                ],
                "published": "2025-03-04T09:40:00Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    40,
                    0,
                    1,
                    63,
                    0
                ],
                "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modeling"
                },
                "summary": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by up to 12% on MSE and 6% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by up to 12% on MSE and 6% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yu-Hao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Goran Nenadic"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "ICML 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02445v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02445v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05263v1",
                "updated": "2025-09-05T17:22:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    22,
                    33,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T17:22:33Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    22,
                    33,
                    4,
                    248,
                    0
                ],
                "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation"
                },
                "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18"
                },
                "authors": [
                    {
                        "name": "Yinglin Duan"
                    },
                    {
                        "name": "Zhengxia Zou"
                    },
                    {
                        "name": "Tongwei Gu"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Zhan Zhao"
                    },
                    {
                        "name": "Luyi Xu"
                    },
                    {
                        "name": "Xinzhu Liu"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Shuang Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Shuang Qiu"
                },
                "author": "Shuang Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00008v2",
                "updated": "2025-09-05T17:21:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    21,
                    2,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-12T03:13:21Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    3,
                    13,
                    21,
                    3,
                    163,
                    0
                ],
                "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via\n  Modality-Aware Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via\n  Modality-Aware Visual Reasoning"
                },
                "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning."
                },
                "authors": [
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Qingwen Ye"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07864v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07864v3",
                "updated": "2025-09-05T17:19:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    19,
                    27,
                    4,
                    248,
                    0
                ],
                "published": "2024-04-11T15:57:12Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    15,
                    57,
                    12,
                    3,
                    102,
                    0
                ],
                "title": "Inferring Change Points in High-Dimensional Regression via Approximate\n  Message Passing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Change Points in High-Dimensional Regression via Approximate\n  Message Passing"
                },
                "summary": "We consider the problem of localizing change points in a generalized linear\nmodel (GLM), a model that covers many widely studied problems in statistical\nlearning including linear, logistic, and rectified linear regression. We\npropose a novel and computationally efficient Approximate Message Passing (AMP)\nalgorithm for estimating both the signals and the change point locations, and\nrigorously characterize its performance in the high-dimensional limit where the\nnumber of parameters $p$ is proportional to the number of samples $n$. This\ncharacterization is in terms of a state evolution recursion, which allows us to\nprecisely compute performance measures such as the asymptotic Hausdorff error\nof our change point estimates, and allows us to tailor the algorithm to take\nadvantage of any prior structural information on the signals and change points.\nMoreover, we show how our AMP iterates can be used to efficiently compute a\nBayesian posterior distribution over the change point locations in the\nhigh-dimensional limit. We validate our theory via numerical experiments, and\ndemonstrate the favorable performance of our estimators on both synthetic and\nreal data in the settings of linear, logistic, and rectified linear regression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of localizing change points in a generalized linear\nmodel (GLM), a model that covers many widely studied problems in statistical\nlearning including linear, logistic, and rectified linear regression. We\npropose a novel and computationally efficient Approximate Message Passing (AMP)\nalgorithm for estimating both the signals and the change point locations, and\nrigorously characterize its performance in the high-dimensional limit where the\nnumber of parameters $p$ is proportional to the number of samples $n$. This\ncharacterization is in terms of a state evolution recursion, which allows us to\nprecisely compute performance measures such as the asymptotic Hausdorff error\nof our change point estimates, and allows us to tailor the algorithm to take\nadvantage of any prior structural information on the signals and change points.\nMoreover, we show how our AMP iterates can be used to efficiently compute a\nBayesian posterior distribution over the change point locations in the\nhigh-dimensional limit. We validate our theory via numerical experiments, and\ndemonstrate the favorable performance of our estimators on both synthetic and\nreal data in the settings of linear, logistic, and rectified linear regression."
                },
                "authors": [
                    {
                        "name": "Gabriel Arpino"
                    },
                    {
                        "name": "Xiaoqi Liu"
                    },
                    {
                        "name": "Julia Gontarek"
                    },
                    {
                        "name": "Ramji Venkataramanan"
                    }
                ],
                "author_detail": {
                    "name": "Ramji Venkataramanan"
                },
                "author": "Ramji Venkataramanan",
                "arxiv_comment": "49 pages, 9 figures. To appear in the Journal of Machine Learning\n  Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07864v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05258v1",
                "updated": "2025-09-05T17:14:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    14,
                    58,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T17:14:58Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    14,
                    58,
                    4,
                    248,
                    0
                ],
                "title": "Scaling Performance of Large Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Performance of Large Language Model Pretraining"
                },
                "summary": "Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, information about the scaling performance and training\nconsiderations of these large training pipelines is scarce in public\nliterature. Working with large-scale datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, information about the scaling performance and training\nconsiderations of these large training pipelines is scarce in public\nliterature. Working with large-scale datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity."
                },
                "authors": [
                    {
                        "name": "Alexander Interrante-Grant"
                    },
                    {
                        "name": "Carla Varela-Rosa"
                    },
                    {
                        "name": "Suhaas Narayan"
                    },
                    {
                        "name": "Chris Connelly"
                    },
                    {
                        "name": "Albert Reuther"
                    }
                ],
                "author_detail": {
                    "name": "Albert Reuther"
                },
                "author": "Albert Reuther",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01249v2",
                "updated": "2025-09-05T17:13:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    13,
                    5,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-02T07:59:34Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    7,
                    59,
                    34,
                    5,
                    214,
                    0
                ],
                "title": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend\n  Against Prompt Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend\n  Against Prompt Injection"
                },
                "summary": "Large Language Model (LLM) agents offer a powerful new paradigm for solving\nvarious problems by combining natural language reasoning with the execution of\nexternal tools. However, their dynamic and non-transparent behavior introduces\ncritical security risks, particularly in the presence of prompt injection\nattacks. In this work, we propose a novel insight that treats the agent runtime\ntraces as structured programs with analyzable semantics. Thus, we present\nAgentArmor, a program analysis framework that converts agent traces into graph\nintermediate representation-based structured program dependency representations\n(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.\nAgentArmor consists of three key components: (1) a graph constructor that\nreconstructs the agent's runtime traces as graph-based intermediate\nrepresentations with control and data flow described within; (2) a property\nregistry that attaches security-relevant metadata of interacted tools \\& data,\nand (3) a type system that performs static inference and checking over the\nintermediate representation. By representing agent behavior as structured\nprograms, AgentArmor enables program analysis for sensitive data flow, trust\nboundaries, and policy violations. We evaluate AgentArmor on the AgentDojo\nbenchmark, the results show that AgentArmor can reduce the ASR to 3\\%, with the\nutility drop only 1\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents offer a powerful new paradigm for solving\nvarious problems by combining natural language reasoning with the execution of\nexternal tools. However, their dynamic and non-transparent behavior introduces\ncritical security risks, particularly in the presence of prompt injection\nattacks. In this work, we propose a novel insight that treats the agent runtime\ntraces as structured programs with analyzable semantics. Thus, we present\nAgentArmor, a program analysis framework that converts agent traces into graph\nintermediate representation-based structured program dependency representations\n(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.\nAgentArmor consists of three key components: (1) a graph constructor that\nreconstructs the agent's runtime traces as graph-based intermediate\nrepresentations with control and data flow described within; (2) a property\nregistry that attaches security-relevant metadata of interacted tools \\& data,\nand (3) a type system that performs static inference and checking over the\nintermediate representation. By representing agent behavior as structured\nprograms, AgentArmor enables program analysis for sensitive data flow, trust\nboundaries, and policy violations. We evaluate AgentArmor on the AgentDojo\nbenchmark, the results show that AgentArmor can reduce the ASR to 3\\%, with the\nutility drop only 1\\%."
                },
                "authors": [
                    {
                        "name": "Peiran Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yunfei Lu"
                    },
                    {
                        "name": "Yifeng Cai"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Qingyou Yang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jue Hong"
                    },
                    {
                        "name": "Ye Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ye Wu"
                },
                "author": "Ye Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05238v1",
                "updated": "2025-09-05T16:54:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    54,
                    26,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:54:26Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    54,
                    26,
                    4,
                    248,
                    0
                ],
                "title": "Uncertain but Useful: Leveraging CNN Variability into Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertain but Useful: Leveraging CNN Variability into Data Augmentation"
                },
                "summary": "Deep learning (DL) is rapidly advancing neuroimaging by achieving\nstate-of-the-art performance with reduced computation times. Yet the numerical\nstability of DL models -- particularly during training -- remains\nunderexplored. While inference with DL is relatively stable, training\nintroduces additional variability primarily through iterative stochastic\noptimization. We investigate this training-time variability using FastSurfer, a\nCNN-based whole-brain segmentation pipeline. Controlled perturbations are\nintroduced via floating point perturbations and random seeds. We find that: (i)\nFastSurfer exhibits higher variability compared to that of a traditional\nneuroimaging pipeline, suggesting that DL inherits and is particularly\nsusceptible to sources of instability present in its predecessors; (ii)\nensembles generated with perturbations achieve performance similar to an\nunperturbed baseline; and (iii) variability effectively produces ensembles of\nnumerical model families that can be repurposed for downstream applications. As\na proof of concept, we demonstrate that numerical ensembles can be used as a\ndata augmentation strategy for brain age regression. These findings position\ntraining-time variability not only as a reproducibility concern but also as a\nresource that can be harnessed to improve robustness and enable new\napplications in neuroimaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) is rapidly advancing neuroimaging by achieving\nstate-of-the-art performance with reduced computation times. Yet the numerical\nstability of DL models -- particularly during training -- remains\nunderexplored. While inference with DL is relatively stable, training\nintroduces additional variability primarily through iterative stochastic\noptimization. We investigate this training-time variability using FastSurfer, a\nCNN-based whole-brain segmentation pipeline. Controlled perturbations are\nintroduced via floating point perturbations and random seeds. We find that: (i)\nFastSurfer exhibits higher variability compared to that of a traditional\nneuroimaging pipeline, suggesting that DL inherits and is particularly\nsusceptible to sources of instability present in its predecessors; (ii)\nensembles generated with perturbations achieve performance similar to an\nunperturbed baseline; and (iii) variability effectively produces ensembles of\nnumerical model families that can be repurposed for downstream applications. As\na proof of concept, we demonstrate that numerical ensembles can be used as a\ndata augmentation strategy for brain age regression. These findings position\ntraining-time variability not only as a reproducibility concern but also as a\nresource that can be harnessed to improve robustness and enable new\napplications in neuroimaging."
                },
                "authors": [
                    {
                        "name": "Ins Gonzalez-Pepe"
                    },
                    {
                        "name": "Vinuyan Sivakolunthu"
                    },
                    {
                        "name": "Yohan Chatelain"
                    },
                    {
                        "name": "Tristan Glatard"
                    }
                ],
                "author_detail": {
                    "name": "Tristan Glatard"
                },
                "author": "Tristan Glatard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22809v2",
                "updated": "2025-09-05T16:48:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    48,
                    2,
                    4,
                    248,
                    0
                ],
                "published": "2025-05-28T19:34:36Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    19,
                    34,
                    36,
                    2,
                    148,
                    0
                ],
                "title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &\n  Dragons Gameplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &\n  Dragons Gameplay"
                },
                "summary": "Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents."
                },
                "authors": [
                    {
                        "name": "Andrew Zhu"
                    },
                    {
                        "name": "Evan Osgood"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "9 pages, 5 figures. COLM 2025 Workshop on AI Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05226v1",
                "updated": "2025-09-05T16:40:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    40,
                    13,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:40:13Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    40,
                    13,
                    4,
                    248,
                    0
                ],
                "title": "Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware\n  Chain-of-Thought Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware\n  Chain-of-Thought Distillation"
                },
                "summary": "Chain-of-thought reasoning, while powerful, can produce unnecessarily verbose\noutput for simpler problems. We present a framework for difficulty-aware\nreasoning that teaches models to dynamically adjust reasoning depth based on\nproblem complexity. Remarkably, we show that models can be endowed with such\ndynamic inference pathways without any architectural modifications; we simply\npost-train on data that is carefully curated to include chain-of-thought traces\nthat are proportional in length to problem difficulty. Our analysis reveals\nthat post-training via supervised fine-tuning (SFT) primarily captures patterns\nlike reasoning length and format, while direct preference optimization (DPO)\npreserves reasoning accuracy, with their combination reducing length and\nmaintaining or improving performance. Both quantitative metrics and qualitative\nassessments confirm that models can learn to \"think proportionally\", reasoning\nminimally on simple problems while maintaining depth for complex ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought reasoning, while powerful, can produce unnecessarily verbose\noutput for simpler problems. We present a framework for difficulty-aware\nreasoning that teaches models to dynamically adjust reasoning depth based on\nproblem complexity. Remarkably, we show that models can be endowed with such\ndynamic inference pathways without any architectural modifications; we simply\npost-train on data that is carefully curated to include chain-of-thought traces\nthat are proportional in length to problem difficulty. Our analysis reveals\nthat post-training via supervised fine-tuning (SFT) primarily captures patterns\nlike reasoning length and format, while direct preference optimization (DPO)\npreserves reasoning accuracy, with their combination reducing length and\nmaintaining or improving performance. Both quantitative metrics and qualitative\nassessments confirm that models can learn to \"think proportionally\", reasoning\nminimally on simple problems while maintaining depth for complex ones."
                },
                "authors": [
                    {
                        "name": "Abdul Waheed"
                    },
                    {
                        "name": "Chancharik Mitra"
                    },
                    {
                        "name": "Laurie Z. Wang"
                    },
                    {
                        "name": "Deva Ramanan"
                    },
                    {
                        "name": "Bhiksha Raj"
                    }
                ],
                "author_detail": {
                    "name": "Bhiksha Raj"
                },
                "author": "Bhiksha Raj",
                "arxiv_comment": "28 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18416v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18416v5",
                "updated": "2025-09-05T16:33:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    33,
                    46,
                    4,
                    248,
                    0
                ],
                "published": "2024-07-25T22:24:45Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    22,
                    24,
                    45,
                    3,
                    207,
                    0
                ],
                "title": "PersonaGym: Evaluating Persona Agents and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaGym: Evaluating Persona Agents and LLMs"
                },
                "summary": "Persona agents, which are LLM agents conditioned to act according to an\nassigned persona, enable contextually rich and user aligned interactions across\ndomains like education and healthcare. However, evaluating how faithfully these\nagents adhere to their personas remains a significant challenge, particularly\nin free-form settings that demand consistency across diverse, persona-relevant\nenvironments. We introduce PersonaGym, the first dynamic evaluation framework\nfor persona agents, and PersonaScore, a human-aligned automatic metric grounded\nin decision theory that enables comprehensive large-scale evaluation. Our\nevaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals\nsignificant advancement opportunities. For example, GPT-4.1 had the exact same\nPersonaScore as LLaMA-3-8b despite being a more recent and advanced closed\nsource model. Importantly, increased model size and complexity do not\nnecessarily enhance persona agent capabilities, underscoring the need for\nalgorithmic and architectural innovation toward faithful, performant persona\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona agents, which are LLM agents conditioned to act according to an\nassigned persona, enable contextually rich and user aligned interactions across\ndomains like education and healthcare. However, evaluating how faithfully these\nagents adhere to their personas remains a significant challenge, particularly\nin free-form settings that demand consistency across diverse, persona-relevant\nenvironments. We introduce PersonaGym, the first dynamic evaluation framework\nfor persona agents, and PersonaScore, a human-aligned automatic metric grounded\nin decision theory that enables comprehensive large-scale evaluation. Our\nevaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals\nsignificant advancement opportunities. For example, GPT-4.1 had the exact same\nPersonaScore as LLaMA-3-8b despite being a more recent and advanced closed\nsource model. Importantly, increased model size and complexity do not\nnecessarily enhance persona agent capabilities, underscoring the need for\nalgorithmic and architectural innovation toward faithful, performant persona\nagents."
                },
                "authors": [
                    {
                        "name": "Vinay Samuel"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Shreyas Chaudhari"
                    },
                    {
                        "name": "Ashwin Kalyan"
                    },
                    {
                        "name": "Tanmay Rajpurohit"
                    },
                    {
                        "name": "Ameet Deshpande"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Vishvak Murahari"
                    }
                ],
                "author_detail": {
                    "name": "Vishvak Murahari"
                },
                "author": "Vishvak Murahari",
                "arxiv_comment": "EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18416v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18416v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05221v1",
                "updated": "2025-09-05T16:32:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    32,
                    26,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:32:26Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    32,
                    26,
                    4,
                    248,
                    0
                ],
                "title": "A functional tensor model for dynamic multilayer networks with common\n  invariant subspaces and the RKHS estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A functional tensor model for dynamic multilayer networks with common\n  invariant subspaces and the RKHS estimation"
                },
                "summary": "Dynamic multilayer networks are frequently used to describe the structure and\ntemporal evolution of multiple relationships among common entities, with\napplications in fields such as sociology, economics, and neuroscience. However,\nexploration of analytical methods for these complex data structures remains\nlimited. We propose a functional tensor-based model for dynamic multilayer\nnetworks, with the key feature of capturing the shared structure among common\nvertices across all layers, while simultaneously accommodating smoothly varying\ntemporal dynamics and layer-specific heterogeneity. The proposed model and its\nembeddings can be applied to various downstream network inference tasks,\nincluding dimensionality reduction, vertex community detection, analysis of\nnetwork evolution periodicity, visualization of dynamic network evolution\npatterns, and evaluation of inter-layer similarity. We provide an estimation\nalgorithm based on functional tensor Tucker decomposition and the reproducing\nkernel Hilbert space framework, with an effective initialization strategy to\nimprove computational efficiency. The estimation procedure can be extended to\naddress more generalized functional tensor problems, as well as to handle\nmissing data or unaligned observations. We validate our method on simulated\ndata and two real-world cases: the dynamic Citi Bike trip network and an\ninternational food trade dynamic multilayer network, with each layer\ncorresponding to a different product.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic multilayer networks are frequently used to describe the structure and\ntemporal evolution of multiple relationships among common entities, with\napplications in fields such as sociology, economics, and neuroscience. However,\nexploration of analytical methods for these complex data structures remains\nlimited. We propose a functional tensor-based model for dynamic multilayer\nnetworks, with the key feature of capturing the shared structure among common\nvertices across all layers, while simultaneously accommodating smoothly varying\ntemporal dynamics and layer-specific heterogeneity. The proposed model and its\nembeddings can be applied to various downstream network inference tasks,\nincluding dimensionality reduction, vertex community detection, analysis of\nnetwork evolution periodicity, visualization of dynamic network evolution\npatterns, and evaluation of inter-layer similarity. We provide an estimation\nalgorithm based on functional tensor Tucker decomposition and the reproducing\nkernel Hilbert space framework, with an effective initialization strategy to\nimprove computational efficiency. The estimation procedure can be extended to\naddress more generalized functional tensor problems, as well as to handle\nmissing data or unaligned observations. We validate our method on simulated\ndata and two real-world cases: the dynamic Citi Bike trip network and an\ninternational food trade dynamic multilayer network, with each layer\ncorresponding to a different product."
                },
                "authors": [
                    {
                        "name": "Runshi Tang"
                    },
                    {
                        "name": "Runbing Zheng"
                    },
                    {
                        "name": "Anru R. Zhang"
                    },
                    {
                        "name": "Carey E. Priebe"
                    }
                ],
                "author_detail": {
                    "name": "Carey E. Priebe"
                },
                "author": "Carey E. Priebe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20938v2",
                "updated": "2025-09-05T16:19:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    19,
                    54,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-26T02:01:11Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    2,
                    1,
                    11,
                    3,
                    177,
                    0
                ],
                "title": "ParEval-Repo: A Benchmark Suite for Evaluating LLMs with\n  Repository-level HPC Translation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParEval-Repo: A Benchmark Suite for Evaluating LLMs with\n  Repository-level HPC Translation Tasks"
                },
                "summary": "GPGPU architectures have become significantly more diverse in recent years,\nwhich has led to an emergence of a variety of specialized programming models\nand software stacks to support them. Portable programming models exist, but\nthey require significant developer effort to port to and optimize for different\nhardware architectures. Large language models (LLMs) may help to reduce this\nprogrammer burden. In this paper, we present a novel benchmark and testing\nframework, ParEval-Repo, which can be used to evaluate the efficacy of\nLLM-based approaches in automatically translating entire codebases across GPGPU\nexecution models. ParEval-Repo includes several scientific computing and AI\nmini-applications in a range of programming models and levels of repository\ncomplexity. We use ParEval-Repo to evaluate a range of state-of-the-art\nopen-source and commercial LLMs, with both a non-agentic and a top-down agentic\napproach. We assess code generated by the LLMs and approaches in terms of\ncompilability, functional correctness, categories of build errors, and the cost\nof translation in terms of the number of inference tokens. Our results\ndemonstrate that LLM translation of scientific applications is feasible for\nsmall programs but difficulty with generating functional build systems and\ncross-file dependencies pose challenges in scaling to larger codebases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPGPU architectures have become significantly more diverse in recent years,\nwhich has led to an emergence of a variety of specialized programming models\nand software stacks to support them. Portable programming models exist, but\nthey require significant developer effort to port to and optimize for different\nhardware architectures. Large language models (LLMs) may help to reduce this\nprogrammer burden. In this paper, we present a novel benchmark and testing\nframework, ParEval-Repo, which can be used to evaluate the efficacy of\nLLM-based approaches in automatically translating entire codebases across GPGPU\nexecution models. ParEval-Repo includes several scientific computing and AI\nmini-applications in a range of programming models and levels of repository\ncomplexity. We use ParEval-Repo to evaluate a range of state-of-the-art\nopen-source and commercial LLMs, with both a non-agentic and a top-down agentic\napproach. We assess code generated by the LLMs and approaches in terms of\ncompilability, functional correctness, categories of build errors, and the cost\nof translation in terms of the number of inference tokens. Our results\ndemonstrate that LLM translation of scientific applications is feasible for\nsmall programs but difficulty with generating functional build systems and\ncross-file dependencies pose challenges in scaling to larger codebases."
                },
                "authors": [
                    {
                        "name": "Joshua H. Davis"
                    },
                    {
                        "name": "Daniel Nichols"
                    },
                    {
                        "name": "Ishan Khillan"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05215v1",
                "updated": "2025-09-05T16:18:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    18,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:18:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    18,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "BEDTime: A Unified Benchmark for Automatically Describing Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEDTime: A Unified Benchmark for Automatically Describing Time Series"
                },
                "summary": "Many recent studies have proposed general-purpose foundation models designed\nfor a variety of time series analysis tasks. While several established datasets\nalready exist for evaluating these models, previous works frequently introduce\ntheir models in conjunction with new datasets, limiting opportunities for\ndirect, independent comparisons and obscuring insights into the relative\nstrengths of different methods. Additionally, prior evaluations often cover\nnumerous tasks simultaneously, assessing a broad range of model abilities\nwithout clearly pinpointing which capabilities contribute to overall\nperformance. To address these gaps, we formalize and evaluate 3 tasks that test\na model's ability to describe time series using generic natural language: (1)\nrecognition (True/False question-answering), (2) differentiation (multiple\nchoice question-answering), and (3) generation (open-ended natural language\ndescription). We then unify 4 recent datasets to enable head-to-head model\ncomparisons on each task. Experimentally, in evaluating 13 state-of-the-art\nlanguage, vision--language, and time series--language models, we find that (1)\npopular language-only methods largely underperform, indicating a need for time\nseries-specific architectures, (2) VLMs are quite successful, as expected,\nidentifying the value of vision models for these tasks and (3) pretrained\nmultimodal time series--language models successfully outperform LLMs, but still\nhave significant room for improvement. We also find that all approaches exhibit\nclear fragility in a range of robustness tests. Overall, our benchmark provides\na standardized evaluation on a task necessary for time series reasoning\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many recent studies have proposed general-purpose foundation models designed\nfor a variety of time series analysis tasks. While several established datasets\nalready exist for evaluating these models, previous works frequently introduce\ntheir models in conjunction with new datasets, limiting opportunities for\ndirect, independent comparisons and obscuring insights into the relative\nstrengths of different methods. Additionally, prior evaluations often cover\nnumerous tasks simultaneously, assessing a broad range of model abilities\nwithout clearly pinpointing which capabilities contribute to overall\nperformance. To address these gaps, we formalize and evaluate 3 tasks that test\na model's ability to describe time series using generic natural language: (1)\nrecognition (True/False question-answering), (2) differentiation (multiple\nchoice question-answering), and (3) generation (open-ended natural language\ndescription). We then unify 4 recent datasets to enable head-to-head model\ncomparisons on each task. Experimentally, in evaluating 13 state-of-the-art\nlanguage, vision--language, and time series--language models, we find that (1)\npopular language-only methods largely underperform, indicating a need for time\nseries-specific architectures, (2) VLMs are quite successful, as expected,\nidentifying the value of vision models for these tasks and (3) pretrained\nmultimodal time series--language models successfully outperform LLMs, but still\nhave significant room for improvement. We also find that all approaches exhibit\nclear fragility in a range of robustness tests. Overall, our benchmark provides\na standardized evaluation on a task necessary for time series reasoning\nsystems."
                },
                "authors": [
                    {
                        "name": "Medhasweta Sen"
                    },
                    {
                        "name": "Zachary Gottesman"
                    },
                    {
                        "name": "Jiaxing Qiu"
                    },
                    {
                        "name": "C. Bayan Bruss"
                    },
                    {
                        "name": "Nam Nguyen"
                    },
                    {
                        "name": "Tom Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hartvigsen"
                },
                "author": "Tom Hartvigsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15442v3",
                "updated": "2025-09-05T16:13:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    13,
                    1,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-21T11:04:33Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    11,
                    4,
                    33,
                    3,
                    233,
                    0
                ],
                "title": "Mitigating Hallucinations in LM-Based TTS Models via Distribution\n  Alignment Using GFlowNets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucinations in LM-Based TTS Models via Distribution\n  Alignment Using GFlowNets"
                },
                "summary": "Language Model (LM)-based Text-to-Speech (TTS) systems often generate\nhallucinated speech that deviates from input text. Existing mitigation\nstrategies either demand excessive training resources or introduce significant\ninference latency. In this paper, we propose GFlOwNet-guided distribution\nAlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates\nhallucinations without relying on massive resources or inference cost.\nSpecifically, we first conduct an uncertainty analysis, revealing a strong\npositive correlation between hallucination and model uncertainty. Based on\nthis, we reformulate TTS generation as a trajectory flow optimization problem\nand introduce an enhanced Subtrajectory Balance objective together with a\nsharpened internal reward as target distribution. We further integrate reward\ntemperature decay and learning rate optimization for stability and performance\nbalance. Extensive experiments show that GOAT reduce over 50% character error\nrates on challenging test cases and lowering uncertainty by up to 58%,\ndemonstrating its strong generalization ability and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model (LM)-based Text-to-Speech (TTS) systems often generate\nhallucinated speech that deviates from input text. Existing mitigation\nstrategies either demand excessive training resources or introduce significant\ninference latency. In this paper, we propose GFlOwNet-guided distribution\nAlignmenT (GOAT) for LM-based TTS, a post-training framework that mitigates\nhallucinations without relying on massive resources or inference cost.\nSpecifically, we first conduct an uncertainty analysis, revealing a strong\npositive correlation between hallucination and model uncertainty. Based on\nthis, we reformulate TTS generation as a trajectory flow optimization problem\nand introduce an enhanced Subtrajectory Balance objective together with a\nsharpened internal reward as target distribution. We further integrate reward\ntemperature decay and learning rate optimization for stability and performance\nbalance. Extensive experiments show that GOAT reduce over 50% character error\nrates on challenging test cases and lowering uncertainty by up to 58%,\ndemonstrating its strong generalization ability and effectiveness."
                },
                "authors": [
                    {
                        "name": "Chenlin Liu"
                    },
                    {
                        "name": "Minghui Fang"
                    },
                    {
                        "name": "Patrick Zhang"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Jie Gao"
                    },
                    {
                        "name": "Jiqing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiqing Han"
                },
                "author": "Jiqing Han",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05208v1",
                "updated": "2025-09-05T16:10:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    53,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:53Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    53,
                    4,
                    248,
                    0
                ],
                "title": "Symbolic Graphics Programming with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic Graphics Programming with Large Language Models"
                },
                "summary": "Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding."
                },
                "authors": [
                    {
                        "name": "Yamei Chen"
                    },
                    {
                        "name": "Haoquan Zhang"
                    },
                    {
                        "name": "Yangyi Huang"
                    },
                    {
                        "name": "Zeju Qiu"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Yandong Wen"
                    },
                    {
                        "name": "Weiyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiyang Liu"
                },
                "author": "Weiyang Liu",
                "arxiv_comment": "Technical report (32 pages, 12 figures, project page:\n  https://spherelab.ai/SGP-Gen/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06020v2",
                "updated": "2025-09-05T16:04:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    4,
                    23,
                    4,
                    248,
                    0
                ],
                "published": "2025-05-09T13:08:27Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    8,
                    27,
                    4,
                    129,
                    0
                ],
                "title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for\n  Visual Art Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArtRAG: Retrieval-Augmented Generation with Structured Context for\n  Visual Art Understanding"
                },
                "summary": "Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Ivona Najdenkoska"
                    },
                    {
                        "name": "Hongyi Zhu"
                    },
                    {
                        "name": "Stevan Rudinac"
                    },
                    {
                        "name": "Monika Kackovic"
                    },
                    {
                        "name": "Nachoem Wijnberg"
                    },
                    {
                        "name": "Marcel Worring"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Worring"
                },
                "author": "Marcel Worring",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05199v1",
                "updated": "2025-09-05T15:58:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    58,
                    49,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T15:58:49Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    58,
                    49,
                    4,
                    248,
                    0
                ],
                "title": "Triadic Fusion of Cognitive, Functional, and Causal Dimensions for\n  Explainable LLMs: The TAXAL Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triadic Fusion of Cognitive, Functional, and Causal Dimensions for\n  Explainable LLMs: The TAXAL Framework"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in high-risk\ndomains where opacity, bias, and instability undermine trust and\naccountability. Traditional explainability methods, focused on surface outputs,\ndo not capture the reasoning pathways, planning logic, and systemic impacts of\nagentic LLMs.\n  We introduce TAXAL (Triadic Alignment for eXplainability in Agentic LLMs), a\ntriadic fusion framework that unites three complementary dimensions: cognitive\n(user understanding), functional (practical utility), and causal (faithful\nreasoning). TAXAL provides a unified, role-sensitive foundation for designing,\nevaluating, and deploying explanations in diverse sociotechnical settings.\n  Our analysis synthesizes existing methods, ranging from post-hoc attribution\nand dialogic interfaces to explanation-aware prompting, and situates them\nwithin the TAXAL triadic fusion model. We further demonstrate its applicability\nthrough case studies in law, education, healthcare, and public services,\nshowing how explanation strategies adapt to institutional constraints and\nstakeholder roles.\n  By combining conceptual clarity with design patterns and deployment pathways,\nTAXAL advances explainability as a technical and sociotechnical practice,\nsupporting trustworthy and context-sensitive LLM applications in the era of\nagentic AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in high-risk\ndomains where opacity, bias, and instability undermine trust and\naccountability. Traditional explainability methods, focused on surface outputs,\ndo not capture the reasoning pathways, planning logic, and systemic impacts of\nagentic LLMs.\n  We introduce TAXAL (Triadic Alignment for eXplainability in Agentic LLMs), a\ntriadic fusion framework that unites three complementary dimensions: cognitive\n(user understanding), functional (practical utility), and causal (faithful\nreasoning). TAXAL provides a unified, role-sensitive foundation for designing,\nevaluating, and deploying explanations in diverse sociotechnical settings.\n  Our analysis synthesizes existing methods, ranging from post-hoc attribution\nand dialogic interfaces to explanation-aware prompting, and situates them\nwithin the TAXAL triadic fusion model. We further demonstrate its applicability\nthrough case studies in law, education, healthcare, and public services,\nshowing how explanation strategies adapt to institutional constraints and\nstakeholder roles.\n  By combining conceptual clarity with design patterns and deployment pathways,\nTAXAL advances explainability as a technical and sociotechnical practice,\nsupporting trustworthy and context-sensitive LLM applications in the era of\nagentic AI."
                },
                "authors": [
                    {
                        "name": "David Herrera-Poyatos"
                    },
                    {
                        "name": "Carlos Pelez-Gonzlez"
                    },
                    {
                        "name": "Cristina Zuheros"
                    },
                    {
                        "name": "Virilo Tejedor"
                    },
                    {
                        "name": "Rosana Montes"
                    },
                    {
                        "name": "Francisco Herrera"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Herrera"
                },
                "author": "Francisco Herrera",
                "arxiv_comment": "27 pages, 9 tables and 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05197v1",
                "updated": "2025-09-05T15:57:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    57,
                    16,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T15:57:16Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    57,
                    16,
                    4,
                    248,
                    0
                ],
                "title": "AI Agents for Web Testing: A Case Study in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents for Web Testing: A Case Study in the Wild"
                },
                "summary": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks."
                },
                "authors": [
                    {
                        "name": "Naimeng Ye"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Ruize Xu"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Zhou Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Yu"
                },
                "author": "Zhou Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00030v2",
                "updated": "2025-09-05T15:41:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    41,
                    49,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-20T17:44:47Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    44,
                    47,
                    2,
                    232,
                    0
                ],
                "title": "MultiStream-LLM: Bridging Modalities for Robust Sign Language\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiStream-LLM: Bridging Modalities for Robust Sign Language\n  Translation"
                },
                "summary": "Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation."
                },
                "authors": [
                    {
                        "name": "Marshall Thomas"
                    },
                    {
                        "name": "Edward Fish"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05186v1",
                "updated": "2025-09-05T15:35:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    35,
                    4,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T15:35:04Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    35,
                    4,
                    4,
                    248,
                    0
                ],
                "title": "Probabilistic operator learning: generative modeling and uncertainty\n  quantification for foundation models of differential equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic operator learning: generative modeling and uncertainty\n  quantification for foundation models of differential equations"
                },
                "summary": "In-context operator networks (ICON) are a class of operator learning methods\nbased on the novel architectures of foundation models. Trained on a diverse set\nof datasets of initial and boundary conditions paired with corresponding\nsolutions to ordinary and partial differential equations (ODEs and PDEs), ICON\nlearns to map example condition-solution pairs of a given differential equation\nto an approximation of its solution operator. Here, we present a probabilistic\nframework that reveals ICON as implicitly performing Bayesian inference, where\nit computes the mean of the posterior predictive distribution over solution\noperators conditioned on the provided context, i.e., example condition-solution\npairs. The formalism of random differential equations provides the\nprobabilistic framework for describing the tasks ICON accomplishes while also\nproviding a basis for understanding other multi-operator learning methods. This\nprobabilistic perspective provides a basis for extending ICON to\n\\emph{generative} settings, where one can sample from the posterior predictive\ndistribution of solution operators. The generative formulation of ICON\n(GenICON) captures the underlying uncertainty in the solution operator, which\nenables principled uncertainty quantification in the solution predictions in\noperator learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context operator networks (ICON) are a class of operator learning methods\nbased on the novel architectures of foundation models. Trained on a diverse set\nof datasets of initial and boundary conditions paired with corresponding\nsolutions to ordinary and partial differential equations (ODEs and PDEs), ICON\nlearns to map example condition-solution pairs of a given differential equation\nto an approximation of its solution operator. Here, we present a probabilistic\nframework that reveals ICON as implicitly performing Bayesian inference, where\nit computes the mean of the posterior predictive distribution over solution\noperators conditioned on the provided context, i.e., example condition-solution\npairs. The formalism of random differential equations provides the\nprobabilistic framework for describing the tasks ICON accomplishes while also\nproviding a basis for understanding other multi-operator learning methods. This\nprobabilistic perspective provides a basis for extending ICON to\n\\emph{generative} settings, where one can sample from the posterior predictive\ndistribution of solution operators. The generative formulation of ICON\n(GenICON) captures the underlying uncertainty in the solution operator, which\nenables principled uncertainty quantification in the solution predictions in\noperator learning."
                },
                "authors": [
                    {
                        "name": "Benjamin J. Zhang"
                    },
                    {
                        "name": "Siting Liu"
                    },
                    {
                        "name": "Stanley J. Osher"
                    },
                    {
                        "name": "Markos A. Katsoulakis"
                    }
                ],
                "author_detail": {
                    "name": "Markos A. Katsoulakis"
                },
                "author": "Markos A. Katsoulakis",
                "arxiv_comment": "First two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05148v1",
                "updated": "2025-09-05T14:38:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    38,
                    18,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:38:18Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    38,
                    18,
                    4,
                    248,
                    0
                ],
                "title": "Bulk viscosity from early-time thermalization of cosmic fluids in light\n  of DESI DR2 data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bulk viscosity from early-time thermalization of cosmic fluids in light\n  of DESI DR2 data"
                },
                "summary": "If nonrelativistic dark matter and radiation are allowed to interact,\nreaching an approximate thermal equilibrium, this interaction induces a bulk\nviscous pressure changing the effective one-fluid description of the universe\ndynamics. By modeling such components as perfect fluids, a cosmologically\nrelevant bulk viscous pressure emerges for dark matter particle masses in the\nrange of $1\\,\\text{eV} - 10\\,\\text{eV}$ keeping thermal equilibrium with the\nradiation. Such a transient bulk viscosity introduces significant effects in\nthe expansion rate near the matter-radiation equality (redshift $z_{\\text{eq}}\n\\sim 3400$) and at late times (leading to a higher inferred value of the Hubble\nconstant $H_0$). We use the recent DESI DR2 BAO data to place an upper bound on\nthe free parameter of the model $\\tau_\\text{eq}$ which represents the time\nscale in which each component follows its own internal perfect fluid dynamics\nuntil thermalization occurs. Our main result is encoded in the bound\n$\\tau_\\text{eq} < 1.84 \\times 10^{-10} $ s (2$\\sigma$), with the corresponding\ndimensionless bulk coefficient $\\tilde{\\xi} H_0/H_{eq}<5.94\\times10^{-4}$\n(2$\\sigma$). In practice, this rules out any possible interaction between\nradiation and dark matter prior to the recombination epoch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If nonrelativistic dark matter and radiation are allowed to interact,\nreaching an approximate thermal equilibrium, this interaction induces a bulk\nviscous pressure changing the effective one-fluid description of the universe\ndynamics. By modeling such components as perfect fluids, a cosmologically\nrelevant bulk viscous pressure emerges for dark matter particle masses in the\nrange of $1\\,\\text{eV} - 10\\,\\text{eV}$ keeping thermal equilibrium with the\nradiation. Such a transient bulk viscosity introduces significant effects in\nthe expansion rate near the matter-radiation equality (redshift $z_{\\text{eq}}\n\\sim 3400$) and at late times (leading to a higher inferred value of the Hubble\nconstant $H_0$). We use the recent DESI DR2 BAO data to place an upper bound on\nthe free parameter of the model $\\tau_\\text{eq}$ which represents the time\nscale in which each component follows its own internal perfect fluid dynamics\nuntil thermalization occurs. Our main result is encoded in the bound\n$\\tau_\\text{eq} < 1.84 \\times 10^{-10} $ s (2$\\sigma$), with the corresponding\ndimensionless bulk coefficient $\\tilde{\\xi} H_0/H_{eq}<5.94\\times10^{-4}$\n(2$\\sigma$). In practice, this rules out any possible interaction between\nradiation and dark matter prior to the recombination epoch."
                },
                "authors": [
                    {
                        "name": "Hermano Velten"
                    },
                    {
                        "name": "William Iania"
                    }
                ],
                "author_detail": {
                    "name": "William Iania"
                },
                "author": "William Iania",
                "arxiv_comment": "7 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22451v2",
                "updated": "2025-09-05T14:22:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    22,
                    32,
                    4,
                    248,
                    0
                ],
                "published": "2025-03-28T14:03:14Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    3,
                    14,
                    4,
                    87,
                    0
                ],
                "title": "STADE: Standard Deviation as a Pruning Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STADE: Standard Deviation as a Pruning Metric"
                },
                "summary": "Recently, Large Language Models (LLMs) have become very widespread and are\nused to solve a wide variety of tasks. To successfully handle these tasks, LLMs\nrequire longer training times and larger model sizes. This makes LLMs ideal\ncandidates for pruning methods that reduce computational demands while\nmaintaining performance. Previous methods require a retraining phase after\npruning to maintain the original model's performance. However, state-of-the-art\npruning methods, such as Wanda, prune the model without retraining, making the\npruning process faster and more efficient. Building upon Wanda's work, this\nstudy provides a theoretical explanation of why the method is effective and\nleverages these insights to enhance the pruning process. Specifically, a\ntheoretical analysis of the pruning problem reveals a common scenario in\nMachine Learning where Wanda is the optimal pruning method. Furthermore, this\nanalysis is extended to cases where Wanda is no longer optimal, leading to the\ndevelopment of a new method, STADE, based on the standard deviation of the\ninput. From a theoretical standpoint, STADE demonstrates better generality\nacross different scenarios. Finally, extensive experiments on Llama and Open\nPre-trained Transformers (OPT) models validate these theoretical findings,\nshowing that depending on the training conditions, Wanda's optimal performance\nvaries as predicted by the theoretical framework. These insights contribute to\na more robust understanding of pruning strategies and their practical\nimplications. Code is available at: https://github.com/Coello-dev/STADE/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have become very widespread and are\nused to solve a wide variety of tasks. To successfully handle these tasks, LLMs\nrequire longer training times and larger model sizes. This makes LLMs ideal\ncandidates for pruning methods that reduce computational demands while\nmaintaining performance. Previous methods require a retraining phase after\npruning to maintain the original model's performance. However, state-of-the-art\npruning methods, such as Wanda, prune the model without retraining, making the\npruning process faster and more efficient. Building upon Wanda's work, this\nstudy provides a theoretical explanation of why the method is effective and\nleverages these insights to enhance the pruning process. Specifically, a\ntheoretical analysis of the pruning problem reveals a common scenario in\nMachine Learning where Wanda is the optimal pruning method. Furthermore, this\nanalysis is extended to cases where Wanda is no longer optimal, leading to the\ndevelopment of a new method, STADE, based on the standard deviation of the\ninput. From a theoretical standpoint, STADE demonstrates better generality\nacross different scenarios. Finally, extensive experiments on Llama and Open\nPre-trained Transformers (OPT) models validate these theoretical findings,\nshowing that depending on the training conditions, Wanda's optimal performance\nvaries as predicted by the theoretical framework. These insights contribute to\na more robust understanding of pruning strategies and their practical\nimplications. Code is available at: https://github.com/Coello-dev/STADE/"
                },
                "authors": [
                    {
                        "name": "Diego Coello de Portugal Mecke"
                    },
                    {
                        "name": "Haya Alyoussef"
                    },
                    {
                        "name": "Maximilian Stubbemann"
                    },
                    {
                        "name": "Ilia Koloiarov"
                    },
                    {
                        "name": "Tom Hanika"
                    },
                    {
                        "name": "Lars Schmidt-Thieme"
                    }
                ],
                "author_detail": {
                    "name": "Lars Schmidt-Thieme"
                },
                "author": "Lars Schmidt-Thieme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13205v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13205v6",
                "updated": "2025-09-05T14:19:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    19,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-16T08:09:32Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    9,
                    32,
                    0,
                    167,
                    0
                ],
                "title": "Poison Once, Control Anywhere: Clean-Text Visual Backdoors in VLM-based\n  Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poison Once, Control Anywhere: Clean-Text Visual Backdoors in VLM-based\n  Mobile Agents"
                },
                "summary": "Mobile agents powered by vision-language models (VLMs) are increasingly\nadopted for tasks such as UI automation and camera-based assistance. These\nagents are typically fine-tuned using small-scale, user-collected data, making\nthem susceptible to stealthy training-time threats. This work introduces VIBMA,\nthe first clean-text backdoor attack targeting VLM-based mobile agents. The\nattack injects malicious behaviors into the model by modifying only the visual\ninput while preserving textual prompts and instructions, achieving stealth\nthrough the complete absence of textual anomalies. Once the agent is fine-tuned\non this poisoned data, adding a predefined visual pattern (trigger) at\ninference time activates the attacker-specified behavior (backdoor). Our attack\naligns the training gradients of poisoned samples with those of an\nattacker-specified target instance, effectively embedding backdoor-specific\nfeatures into the poisoned data. To ensure the robustness and stealthiness of\nthe attack, we design three trigger variants that better resemble real-world\nscenarios: static patches, dynamic motion patterns, and low-opacity blended\ncontent. Extensive experiments on six Android applications and three\nmobile-compatible VLMs demonstrate that our attack achieves high success rates\n(ASR up to 94.67%) while preserving clean-task behavior (FSR up to 95.85%). We\nfurther conduct ablation studies to understand how key design factors impact\nattack reliability and stealth. These findings is the first to reveal the\nsecurity vulnerabilities of mobile agents and their susceptibility to backdoor\ninjection, underscoring the need for robust defenses in mobile agent adaptation\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile agents powered by vision-language models (VLMs) are increasingly\nadopted for tasks such as UI automation and camera-based assistance. These\nagents are typically fine-tuned using small-scale, user-collected data, making\nthem susceptible to stealthy training-time threats. This work introduces VIBMA,\nthe first clean-text backdoor attack targeting VLM-based mobile agents. The\nattack injects malicious behaviors into the model by modifying only the visual\ninput while preserving textual prompts and instructions, achieving stealth\nthrough the complete absence of textual anomalies. Once the agent is fine-tuned\non this poisoned data, adding a predefined visual pattern (trigger) at\ninference time activates the attacker-specified behavior (backdoor). Our attack\naligns the training gradients of poisoned samples with those of an\nattacker-specified target instance, effectively embedding backdoor-specific\nfeatures into the poisoned data. To ensure the robustness and stealthiness of\nthe attack, we design three trigger variants that better resemble real-world\nscenarios: static patches, dynamic motion patterns, and low-opacity blended\ncontent. Extensive experiments on six Android applications and three\nmobile-compatible VLMs demonstrate that our attack achieves high success rates\n(ASR up to 94.67%) while preserving clean-task behavior (FSR up to 95.85%). We\nfurther conduct ablation studies to understand how key design factors impact\nattack reliability and stealth. These findings is the first to reveal the\nsecurity vulnerabilities of mobile agents and their susceptibility to backdoor\ninjection, underscoring the need for robust defenses in mobile agent adaptation\npipelines."
                },
                "authors": [
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Yuliang Lu"
                    },
                    {
                        "name": "Xitong Gao"
                    },
                    {
                        "name": "Ee-Chien Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ee-Chien Chang"
                },
                "author": "Ee-Chien Chang",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13205v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13205v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05131v1",
                "updated": "2025-09-05T14:18:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    18,
                    52,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:18:52Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    18,
                    52,
                    4,
                    248,
                    0
                ],
                "title": "A Scalable Attention-Based Approach for Image-to-3D Texture Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Attention-Based Approach for Image-to-3D Texture Mapping"
                },
                "summary": "High-quality textures are critical for realistic 3D content creation, yet\nexisting generative methods are slow, rely on UV maps, and often fail to remain\nfaithful to a reference image. To address these challenges, we propose a\ntransformer-based framework that predicts a 3D texture field directly from a\nsingle image and a mesh, eliminating the need for UV mapping and differentiable\nrendering, and enabling faster texture generation. Our method integrates a\ntriplane representation with depth-based backprojection losses, enabling\nefficient training and faster inference. Once trained, it generates\nhigh-fidelity textures in a single forward pass, requiring only 0.2s per shape.\nExtensive qualitative, quantitative, and user preference evaluations\ndemonstrate that our method outperforms state-of-the-art baselines on\nsingle-image texture reconstruction in terms of both fidelity to the input\nimage and perceptual quality, highlighting its practicality for scalable,\nhigh-quality, and controllable 3D content creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality textures are critical for realistic 3D content creation, yet\nexisting generative methods are slow, rely on UV maps, and often fail to remain\nfaithful to a reference image. To address these challenges, we propose a\ntransformer-based framework that predicts a 3D texture field directly from a\nsingle image and a mesh, eliminating the need for UV mapping and differentiable\nrendering, and enabling faster texture generation. Our method integrates a\ntriplane representation with depth-based backprojection losses, enabling\nefficient training and faster inference. Once trained, it generates\nhigh-fidelity textures in a single forward pass, requiring only 0.2s per shape.\nExtensive qualitative, quantitative, and user preference evaluations\ndemonstrate that our method outperforms state-of-the-art baselines on\nsingle-image texture reconstruction in terms of both fidelity to the input\nimage and perceptual quality, highlighting its practicality for scalable,\nhigh-quality, and controllable 3D content creation."
                },
                "authors": [
                    {
                        "name": "Arianna Rampini"
                    },
                    {
                        "name": "Kanika Madan"
                    },
                    {
                        "name": "Bruno Roy"
                    },
                    {
                        "name": "AmirHossein Zamani"
                    },
                    {
                        "name": "Derek Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Derek Cheung"
                },
                "author": "Derek Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01265v2",
                "updated": "2025-09-05T14:13:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    13,
                    44,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-01T08:54:00Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    8,
                    54,
                    0,
                    0,
                    244,
                    0
                ],
                "title": "Self-Employment as a Signal: Career Concerns with Hidden Firm\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Employment as a Signal: Career Concerns with Hidden Firm\n  Performance"
                },
                "summary": "We analyze a dynamic labor market in which a worker with career concerns\nchooses each period between (i) self-employment that makes output publicly\nobservable and (ii) employment at a firm that pays a flat wage but keeps\nindividual performance hidden from outside observers. Output is binary and the\nworker is risk averse. The worker values future opportunities through a\nreputation for talent; firms may be benchmark (myopic) (ignoring the\ninformational content of an application) or equilibrium (updating beliefs from\nthe very act of applying). Three forces shape equilibrium: an insurance -\ninformation trade-off, selection by reputation, and inference from application\ndecisions. We show that (i) an absorbing employment region exists in which\nlow-reputation workers strictly prefer the firm's insurance and optimally cease\nproducing public information; (ii) sufficiently strong reputation triggers\nself-employment in order to generate public signals and preserve future outside\noptions; and (iii) with equilibrium firms, application choices act as signals\nthat shift hiring thresholds and wages even when in-firm performance remains\nopaque. Comparative statics deliver sharp, testable predictions for the\nprevalence of self-employment, the cyclicality of switching, and wage dynamics\nacross markets with different degrees of performance transparency. The\nframework links classic career-concerns models to contemporary environments in\nwhich some tasks generate portable, public histories while firm tasks remain\nunobserved by the outside market (e.g., open-source contributions, freelancing\nplatforms, or sales roles with standardized public metrics). Our results\nrationalize recent empirical findings on the value of public performance\nrecords and illuminate when opacity inside firms dampens or amplifies\nreputational incentives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze a dynamic labor market in which a worker with career concerns\nchooses each period between (i) self-employment that makes output publicly\nobservable and (ii) employment at a firm that pays a flat wage but keeps\nindividual performance hidden from outside observers. Output is binary and the\nworker is risk averse. The worker values future opportunities through a\nreputation for talent; firms may be benchmark (myopic) (ignoring the\ninformational content of an application) or equilibrium (updating beliefs from\nthe very act of applying). Three forces shape equilibrium: an insurance -\ninformation trade-off, selection by reputation, and inference from application\ndecisions. We show that (i) an absorbing employment region exists in which\nlow-reputation workers strictly prefer the firm's insurance and optimally cease\nproducing public information; (ii) sufficiently strong reputation triggers\nself-employment in order to generate public signals and preserve future outside\noptions; and (iii) with equilibrium firms, application choices act as signals\nthat shift hiring thresholds and wages even when in-firm performance remains\nopaque. Comparative statics deliver sharp, testable predictions for the\nprevalence of self-employment, the cyclicality of switching, and wage dynamics\nacross markets with different degrees of performance transparency. The\nframework links classic career-concerns models to contemporary environments in\nwhich some tasks generate portable, public histories while firm tasks remain\nunobserved by the outside market (e.g., open-source contributions, freelancing\nplatforms, or sales roles with standardized public metrics). Our results\nrationalize recent empirical findings on the value of public performance\nrecords and illuminate when opacity inside firms dampens or amplifies\nreputational incentives."
                },
                "authors": [
                    {
                        "name": "Georgy Lukyanov"
                    },
                    {
                        "name": "Konstantin Popov"
                    },
                    {
                        "name": "Shubh Lashkery"
                    }
                ],
                "author_detail": {
                    "name": "Shubh Lashkery"
                },
                "author": "Shubh Lashkery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05117v1",
                "updated": "2025-09-05T13:59:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    59,
                    25,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T13:59:25Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    59,
                    25,
                    4,
                    248,
                    0
                ],
                "title": "HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of\n  Manufactured Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of\n  Manufactured Solutions"
                },
                "summary": "We present HyPINO, a multi-physics neural operator designed for zero-shot\ngeneralization across a broad class of parametric PDEs without requiring\ntask-specific fine-tuning. Our approach combines a Swin Transformer-based\nhypernetwork with mixed supervision: (i) labeled data from analytical solutions\ngenerated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled\nsamples optimized using physics-informed objectives. The model maps PDE\nparametrizations to target Physics-Informed Neural Networks (PINNs) and can\nhandle linear elliptic, hyperbolic, and parabolic equations in two dimensions\nwith varying source terms, geometries, and mixed Dirichlet/Neumann boundary\nconditions, including interior boundaries. HyPINO achieves strong zero-shot\naccuracy on seven benchmark problems from PINN literature, outperforming\nU-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we\nintroduce an iterative refinement procedure that compares the physics of the\ngenerated PINN to the requested PDE and uses the discrepancy to generate a\n\"delta\" PINN. Summing their contributions and repeating this process forms an\nensemble whose combined solution progressively reduces the error on six\nbenchmarks and achieves over 100x gain in average $L_2$ loss in the best case,\nwhile retaining forward-only inference. Additionally, we evaluate the\nfine-tuning behavior of PINNs initialized by HyPINO and show that they converge\nfaster and to lower final error than both randomly initialized and\nReptile-meta-learned PINNs on five benchmarks, performing on par on the\nremaining two. Our results highlight the potential of this scalable approach as\na foundation for extending neural operators toward solving increasingly\ncomplex, nonlinear, and high-dimensional PDE problems with significantly\nimproved accuracy and reduced computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present HyPINO, a multi-physics neural operator designed for zero-shot\ngeneralization across a broad class of parametric PDEs without requiring\ntask-specific fine-tuning. Our approach combines a Swin Transformer-based\nhypernetwork with mixed supervision: (i) labeled data from analytical solutions\ngenerated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled\nsamples optimized using physics-informed objectives. The model maps PDE\nparametrizations to target Physics-Informed Neural Networks (PINNs) and can\nhandle linear elliptic, hyperbolic, and parabolic equations in two dimensions\nwith varying source terms, geometries, and mixed Dirichlet/Neumann boundary\nconditions, including interior boundaries. HyPINO achieves strong zero-shot\naccuracy on seven benchmark problems from PINN literature, outperforming\nU-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we\nintroduce an iterative refinement procedure that compares the physics of the\ngenerated PINN to the requested PDE and uses the discrepancy to generate a\n\"delta\" PINN. Summing their contributions and repeating this process forms an\nensemble whose combined solution progressively reduces the error on six\nbenchmarks and achieves over 100x gain in average $L_2$ loss in the best case,\nwhile retaining forward-only inference. Additionally, we evaluate the\nfine-tuning behavior of PINNs initialized by HyPINO and show that they converge\nfaster and to lower final error than both randomly initialized and\nReptile-meta-learned PINNs on five benchmarks, performing on par on the\nremaining two. Our results highlight the potential of this scalable approach as\na foundation for extending neural operators toward solving increasingly\ncomplex, nonlinear, and high-dimensional PDE problems with significantly\nimproved accuracy and reduced computational cost."
                },
                "authors": [
                    {
                        "name": "Rafael Bischof"
                    },
                    {
                        "name": "Michal Piovari"
                    },
                    {
                        "name": "Michael A. Kraus"
                    },
                    {
                        "name": "Siddhartha Mishra"
                    },
                    {
                        "name": "Bernd Bickel"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Bickel"
                },
                "author": "Bernd Bickel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18122v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18122v3",
                "updated": "2025-09-05T13:58:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    58,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2024-10-12T09:46:36Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    9,
                    46,
                    36,
                    5,
                    286,
                    0
                ],
                "title": "Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution\n  Generalization of Misinformation Detection Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution\n  Generalization of Misinformation Detection Models"
                },
                "summary": "This article introduces misinfo-general, a benchmark dataset for evaluating\nmisinformation models' ability to perform out-of-distribution generalization.\nMisinformation changes rapidly, much more quickly than moderators can annotate\nat scale, resulting in a shift between the training and inference data\ndistributions. As a result, misinformation detectors need to be able to perform\nout-of-distribution generalization, an attribute they currently lack. Our\nbenchmark uses distant labelling to enable simulating covariate shifts in\nmisinformation content. We identify time, event, topic, publisher, political\nbias, misinformation type as important axes for generalization, and we evaluate\na common class of baseline models on each. Using article metadata, we show how\nthis model fails desiderata, which is not necessarily obvious from\nclassification metrics. Finally, we analyze properties of the data to ensure\nlimited presence of modelling shortcuts. We make the dataset and accompanying\ncode publicly available: https://github.com/ioverho/misinfo-general",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces misinfo-general, a benchmark dataset for evaluating\nmisinformation models' ability to perform out-of-distribution generalization.\nMisinformation changes rapidly, much more quickly than moderators can annotate\nat scale, resulting in a shift between the training and inference data\ndistributions. As a result, misinformation detectors need to be able to perform\nout-of-distribution generalization, an attribute they currently lack. Our\nbenchmark uses distant labelling to enable simulating covariate shifts in\nmisinformation content. We identify time, event, topic, publisher, political\nbias, misinformation type as important axes for generalization, and we evaluate\na common class of baseline models on each. Using article metadata, we show how\nthis model fails desiderata, which is not necessarily obvious from\nclassification metrics. Finally, we analyze properties of the data to ensure\nlimited presence of modelling shortcuts. We make the dataset and accompanying\ncode publicly available: https://github.com/ioverho/misinfo-general"
                },
                "authors": [
                    {
                        "name": "Ivo Verhoeven"
                    },
                    {
                        "name": "Pushkar Mishra"
                    },
                    {
                        "name": "Ekaterina Shutova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Shutova"
                },
                "author": "Ekaterina Shutova",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18122v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18122v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05091v1",
                "updated": "2025-09-05T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    30,
                    17,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T13:30:17Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    30,
                    17,
                    4,
                    248,
                    0
                ],
                "title": "ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed\n  Feedback"
                },
                "summary": "While humans are inherently social creatures, the challenge of identifying\nwhen and how to assist and collaborate with others - particularly when pursuing\nindependent goals - can hinder cooperation. To address this challenge, we aim\nto develop an AI system that provides useful feedback to promote prosocial\nbehaviour - actions that benefit others, even when not directly aligned with\none's own goals. We introduce ProToM, a Theory of Mind-informed facilitator\nthat promotes prosocial actions in multi-agent systems by providing targeted,\ncontext-sensitive feedback to individual agents. ProToM first infers agents'\ngoals using Bayesian inverse planning, then selects feedback to communicate by\nmaximising expected utility, conditioned on the inferred goal distribution. We\nevaluate our approach against baselines in two multi-agent environments: Doors,\nKeys, and Gems, as well as Overcooked. Our results suggest that\nstate-of-the-art large language and reasoning models fall short of\ncommunicating feedback that is both contextually grounded and well-timed -\nleading to higher communication overhead and task speedup. In contrast, ProToM\nprovides targeted and helpful feedback, achieving a higher success rate,\nshorter task completion times, and is consistently preferred by human users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While humans are inherently social creatures, the challenge of identifying\nwhen and how to assist and collaborate with others - particularly when pursuing\nindependent goals - can hinder cooperation. To address this challenge, we aim\nto develop an AI system that provides useful feedback to promote prosocial\nbehaviour - actions that benefit others, even when not directly aligned with\none's own goals. We introduce ProToM, a Theory of Mind-informed facilitator\nthat promotes prosocial actions in multi-agent systems by providing targeted,\ncontext-sensitive feedback to individual agents. ProToM first infers agents'\ngoals using Bayesian inverse planning, then selects feedback to communicate by\nmaximising expected utility, conditioned on the inferred goal distribution. We\nevaluate our approach against baselines in two multi-agent environments: Doors,\nKeys, and Gems, as well as Overcooked. Our results suggest that\nstate-of-the-art large language and reasoning models fall short of\ncommunicating feedback that is both contextually grounded and well-timed -\nleading to higher communication overhead and task speedup. In contrast, ProToM\nprovides targeted and helpful feedback, achieving a higher success rate,\nshorter task completion times, and is consistently preferred by human users."
                },
                "authors": [
                    {
                        "name": "Matteo Bortoletto"
                    },
                    {
                        "name": "Yichao Zhou"
                    },
                    {
                        "name": "Lance Ying"
                    },
                    {
                        "name": "Tianmin Shu"
                    },
                    {
                        "name": "Andreas Bulling"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Bulling"
                },
                "author": "Andreas Bulling",
                "arxiv_comment": "Website at https://www.matteobortoletto.org/protom/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05086v1",
                "updated": "2025-09-05T13:25:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    25,
                    33,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T13:25:33Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    25,
                    33,
                    4,
                    248,
                    0
                ],
                "title": "Robust Experts: the Effect of Adversarial Training on CNNs with Sparse\n  Mixture-of-Experts Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Experts: the Effect of Adversarial Training on CNNs with Sparse\n  Mixture-of-Experts Layers"
                },
                "summary": "Robustifying convolutional neural networks (CNNs) against adversarial attacks\nremains challenging and often requires resource-intensive countermeasures. We\nexplore the use of sparse mixture-of-experts (MoE) layers to improve robustness\nby replacing selected residual blocks or convolutional layers, thereby\nincreasing model capacity without additional inference cost. On ResNet\narchitectures trained on CIFAR-100, we find that inserting a single MoE layer\nin the deeper stages leads to consistent improvements in robustness under PGD\nand AutoPGD attacks when combined with adversarial training. Furthermore, we\ndiscover that when switch loss is used for balancing, it causes routing to\ncollapse onto a small set of overused experts, thereby concentrating\nadversarial training on these paths and inadvertently making them more robust.\nAs a result, some individual experts outperform the gated MoE model in\nrobustness, suggesting that robust subpaths emerge through specialization. Our\ncode is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying convolutional neural networks (CNNs) against adversarial attacks\nremains challenging and often requires resource-intensive countermeasures. We\nexplore the use of sparse mixture-of-experts (MoE) layers to improve robustness\nby replacing selected residual blocks or convolutional layers, thereby\nincreasing model capacity without additional inference cost. On ResNet\narchitectures trained on CIFAR-100, we find that inserting a single MoE layer\nin the deeper stages leads to consistent improvements in robustness under PGD\nand AutoPGD attacks when combined with adversarial training. Furthermore, we\ndiscover that when switch loss is used for balancing, it causes routing to\ncollapse onto a small set of overused experts, thereby concentrating\nadversarial training on these paths and inadvertently making them more robust.\nAs a result, some individual experts outperform the gated MoE model in\nrobustness, suggesting that robust subpaths emerge through specialization. Our\ncode is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes."
                },
                "authors": [
                    {
                        "name": "Svetlana Pavlitska"
                    },
                    {
                        "name": "Haixi Fan"
                    },
                    {
                        "name": "Konstantin Ditschuneit"
                    },
                    {
                        "name": "J. Marius Zllner"
                    }
                ],
                "author_detail": {
                    "name": "J. Marius Zllner"
                },
                "author": "J. Marius Zllner",
                "arxiv_comment": "Accepted for publication at the STREAM workshop at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05080v1",
                "updated": "2025-09-05T13:19:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    19,
                    51,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T13:19:51Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    19,
                    51,
                    4,
                    248,
                    0
                ],
                "title": "MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial\n  Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial\n  Trading"
                },
                "summary": "The inherent non-stationarity of financial markets and the complexity of\nmulti-modal information pose significant challenges to existing quantitative\ntrading models. Traditional methods relying on fixed structures and unimodal\ndata struggle to adapt to market regime shifts, while large language model\n(LLM)-driven solutions - despite their multi-modal comprehension - suffer from\nstatic strategies and homogeneous expert designs, lacking dynamic adjustment\nand fine-grained decision mechanisms. To address these limitations, we propose\nMM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on\nlarge language models. MM-DREX explicitly decouples market state perception\nfrom strategy execution to enable adaptive sequential decision-making in\nnon-stationary environments. Specifically, it (1) introduces a vision-language\nmodel (VLM)-powered dynamic router that jointly analyzes candlestick chart\npatterns and long-term temporal features to allocate real-time expert weights;\n(2) designs four heterogeneous trading experts (trend, reversal, breakout,\npositioning) generating specialized fine-grained sub-strategies; and (3)\nproposes an SFT-RL hybrid training paradigm to synergistically optimize the\nrouter's market classification capability and experts' risk-adjusted\ndecision-making. Extensive experiments on multi-modal datasets spanning stocks,\nfutures, and cryptocurrencies demonstrate that MM-DREX significantly\noutperforms 15 baselines (including state-of-the-art financial LLMs and deep\nreinforcement learning models) across key metrics: total return, Sharpe ratio,\nand maximum drawdown, validating its robustness and generalization.\nAdditionally, an interpretability module traces routing logic and expert\nbehavior in real time, providing an audit trail for strategy transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent non-stationarity of financial markets and the complexity of\nmulti-modal information pose significant challenges to existing quantitative\ntrading models. Traditional methods relying on fixed structures and unimodal\ndata struggle to adapt to market regime shifts, while large language model\n(LLM)-driven solutions - despite their multi-modal comprehension - suffer from\nstatic strategies and homogeneous expert designs, lacking dynamic adjustment\nand fine-grained decision mechanisms. To address these limitations, we propose\nMM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on\nlarge language models. MM-DREX explicitly decouples market state perception\nfrom strategy execution to enable adaptive sequential decision-making in\nnon-stationary environments. Specifically, it (1) introduces a vision-language\nmodel (VLM)-powered dynamic router that jointly analyzes candlestick chart\npatterns and long-term temporal features to allocate real-time expert weights;\n(2) designs four heterogeneous trading experts (trend, reversal, breakout,\npositioning) generating specialized fine-grained sub-strategies; and (3)\nproposes an SFT-RL hybrid training paradigm to synergistically optimize the\nrouter's market classification capability and experts' risk-adjusted\ndecision-making. Extensive experiments on multi-modal datasets spanning stocks,\nfutures, and cryptocurrencies demonstrate that MM-DREX significantly\noutperforms 15 baselines (including state-of-the-art financial LLMs and deep\nreinforcement learning models) across key metrics: total return, Sharpe ratio,\nand maximum drawdown, validating its robustness and generalization.\nAdditionally, an interpretability module traces routing logic and expert\nbehavior in real time, providing an audit trail for strategy transparency."
                },
                "authors": [
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Yueheng Jiang"
                    },
                    {
                        "name": "Zhaozhao Ma"
                    },
                    {
                        "name": "Yuchen Cao Jacky Keung"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Yiquan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03164v2",
                "updated": "2025-09-05T13:18:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    18,
                    9,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-03T09:30:39Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    30,
                    39,
                    2,
                    246,
                    0
                ],
                "title": "OPRA-Vis: Visual Analytics System to Assist Organization-Public\n  Relationship Assessment with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPRA-Vis: Visual Analytics System to Assist Organization-Public\n  Relationship Assessment with Large Language Models"
                },
                "summary": "Analysis of public opinions collected from digital media helps organizations\nmaintain positive relationships with the public. Such public relations (PR)\nanalysis often involves assessing opinions, for example, measuring how strongly\npeople trust an organization. Pre-trained Large Language Models (LLMs) hold\ngreat promise for supporting Organization-Public Relationship Assessment (OPRA)\nbecause they can map unstructured public text to OPRA dimensions and articulate\nrationales through prompting. However, adapting LLMs for PR analysis typically\nrequires fine-tuning on large labeled datasets, which is both labor-intensive\nand knowledge-intensive, making it difficult for PR researchers to apply these\nmodels. In this paper, we present OPRA-Vis, a visual analytics system that\nleverages LLMs for OPRA without requiring extensive labeled data. Our framework\nemploys Chain-of-Thought prompting to guide LLMs in analyzing public opinion\ndata by incorporating PR expertise directly into the reasoning process.\nFurthermore, OPRA-Vis provides visualizations that reveal the clues and\nreasoning paths used by LLMs, enabling users to explore, critique, and refine\nmodel decisions. We demonstrate the effectiveness of OPRA-Vis through two\nreal-world use cases and evaluate it quantitatively, through comparisons with\nalternative LLMs and prompting strategies, and qualitatively, through\nassessments of usability, effectiveness, and expert feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of public opinions collected from digital media helps organizations\nmaintain positive relationships with the public. Such public relations (PR)\nanalysis often involves assessing opinions, for example, measuring how strongly\npeople trust an organization. Pre-trained Large Language Models (LLMs) hold\ngreat promise for supporting Organization-Public Relationship Assessment (OPRA)\nbecause they can map unstructured public text to OPRA dimensions and articulate\nrationales through prompting. However, adapting LLMs for PR analysis typically\nrequires fine-tuning on large labeled datasets, which is both labor-intensive\nand knowledge-intensive, making it difficult for PR researchers to apply these\nmodels. In this paper, we present OPRA-Vis, a visual analytics system that\nleverages LLMs for OPRA without requiring extensive labeled data. Our framework\nemploys Chain-of-Thought prompting to guide LLMs in analyzing public opinion\ndata by incorporating PR expertise directly into the reasoning process.\nFurthermore, OPRA-Vis provides visualizations that reveal the clues and\nreasoning paths used by LLMs, enabling users to explore, critique, and refine\nmodel decisions. We demonstrate the effectiveness of OPRA-Vis through two\nreal-world use cases and evaluate it quantitatively, through comparisons with\nalternative LLMs and prompting strategies, and qualitatively, through\nassessments of usability, effectiveness, and expert feedback."
                },
                "authors": [
                    {
                        "name": "Sangbong Yoo"
                    },
                    {
                        "name": "Seongbum Seo"
                    },
                    {
                        "name": "Chanyoung Yoon"
                    },
                    {
                        "name": "Hyelim Lee"
                    },
                    {
                        "name": "Jeong-Nam Kim"
                    },
                    {
                        "name": "Chansoo Kim"
                    },
                    {
                        "name": "Yun Jang"
                    },
                    {
                        "name": "Takanori Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takanori Fujiwara"
                },
                "author": "Takanori Fujiwara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19141v2",
                "updated": "2025-09-05T13:10:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    10,
                    26,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-23T21:25:19Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    21,
                    25,
                    19,
                    0,
                    174,
                    0
                ],
                "title": "EEG Foundation Challenge: From Cross-Task to Cross-Subject EEG Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEG Foundation Challenge: From Cross-Task to Cross-Subject EEG Decoding"
                },
                "summary": "Current electroencephalogram (EEG) decoding models are typically trained on\nsmall numbers of subjects performing a single task. Here, we introduce a\nlarge-scale, code-submission-based competition comprising two challenges.\nFirst, the Transfer Challenge asks participants to build and test a model that\ncan zero-shot decode new tasks and new subjects from their EEG data. Second,\nthe Psychopathology factor prediction Challenge asks participants to infer\nsubject measures of mental health from EEG data. For this, we use an\nunprecedented, multi-terabyte dataset of high-density EEG signals (128\nchannels) recorded from over 3,000 child to young adult subjects engaged in\nmultiple active and passive tasks. We provide several tunable neural network\nbaselines for each of these two challenges, including a simple network and\ndemographic-based regression models. Developing models that generalise across\ntasks and individuals will pave the way for ML network architectures capable of\nadapting to EEG data collected from diverse tasks and individuals. Similarly,\npredicting mental health-relevant personality trait values from EEG might\nidentify objective biomarkers useful for clinical diagnosis and design of\npersonalised treatment for psychological conditions. Ultimately, the advances\nspurred by this challenge could contribute to the development of computational\npsychiatry and useful neurotechnology, and contribute to breakthroughs in both\nfundamental neuroscience and applied clinical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current electroencephalogram (EEG) decoding models are typically trained on\nsmall numbers of subjects performing a single task. Here, we introduce a\nlarge-scale, code-submission-based competition comprising two challenges.\nFirst, the Transfer Challenge asks participants to build and test a model that\ncan zero-shot decode new tasks and new subjects from their EEG data. Second,\nthe Psychopathology factor prediction Challenge asks participants to infer\nsubject measures of mental health from EEG data. For this, we use an\nunprecedented, multi-terabyte dataset of high-density EEG signals (128\nchannels) recorded from over 3,000 child to young adult subjects engaged in\nmultiple active and passive tasks. We provide several tunable neural network\nbaselines for each of these two challenges, including a simple network and\ndemographic-based regression models. Developing models that generalise across\ntasks and individuals will pave the way for ML network architectures capable of\nadapting to EEG data collected from diverse tasks and individuals. Similarly,\npredicting mental health-relevant personality trait values from EEG might\nidentify objective biomarkers useful for clinical diagnosis and design of\npersonalised treatment for psychological conditions. Ultimately, the advances\nspurred by this challenge could contribute to the development of computational\npsychiatry and useful neurotechnology, and contribute to breakthroughs in both\nfundamental neuroscience and applied clinical research."
                },
                "authors": [
                    {
                        "name": "Bruno Aristimunha"
                    },
                    {
                        "name": "Dung Truong"
                    },
                    {
                        "name": "Pierre Guetschel"
                    },
                    {
                        "name": "Seyed Yahya Shirazi"
                    },
                    {
                        "name": "Isabelle Guyon"
                    },
                    {
                        "name": "Alexandre R. Franco"
                    },
                    {
                        "name": "Michael P. Milham"
                    },
                    {
                        "name": "Aviv Dotan"
                    },
                    {
                        "name": "Scott Makeig"
                    },
                    {
                        "name": "Alexandre Gramfort"
                    },
                    {
                        "name": "Jean-Remi King"
                    },
                    {
                        "name": "Marie-Constance Corsi"
                    },
                    {
                        "name": "Pedro A. Valds-Sosa"
                    },
                    {
                        "name": "Amit Majumdar"
                    },
                    {
                        "name": "Alan Evans"
                    },
                    {
                        "name": "Terrence J Sejnowski"
                    },
                    {
                        "name": "Oren Shriki"
                    },
                    {
                        "name": "Sylvain Chevallier"
                    },
                    {
                        "name": "Arnaud Delorme"
                    }
                ],
                "author_detail": {
                    "name": "Arnaud Delorme"
                },
                "author": "Arnaud Delorme",
                "arxiv_comment": "Approved at Neurips Competition track. webpage:\n  https://eeg2025.github.io/",
                "arxiv_journal_ref": "The Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00461v2",
                "updated": "2025-09-05T12:32:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    32,
                    22,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-30T11:31:04Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    11,
                    31,
                    4,
                    5,
                    242,
                    0
                ],
                "title": "TECP: Token-Entropy Conformal Prediction for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TECP: Token-Entropy Conformal Prediction for LLMs"
                },
                "summary": "Uncertainty quantification (UQ) for open-ended language generation remains a\ncritical yet underexplored challenge, especially under black-box constraints\nwhere internal model signals are inaccessible. In this paper, we introduce\nToken-Entropy Conformal Prediction (TECP), a novel framework that leverages\ntoken-level entropy as a logit-free, reference-free uncertainty measure and\nintegrates it into a split conformal prediction (CP) pipeline to construct\nprediction sets with formal coverage guarantees. Unlike existing approaches\nthat rely on semantic consistency heuristics or white-box features, TECP\ndirectly estimates epistemic uncertainty from the token entropy structure of\nsampled generations and calibrates uncertainty thresholds via CP quantiles to\nensure provable error control. Empirical evaluations across six large language\nmodels and two benchmarks (CoQA and TriviaQA) demonstrate that TECP\nconsistently achieves reliable coverage and compact prediction sets,\noutperforming prior self-consistency-based UQ methods. Our method provides a\nprincipled and efficient solution for trustworthy generation in black-box LLM\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) for open-ended language generation remains a\ncritical yet underexplored challenge, especially under black-box constraints\nwhere internal model signals are inaccessible. In this paper, we introduce\nToken-Entropy Conformal Prediction (TECP), a novel framework that leverages\ntoken-level entropy as a logit-free, reference-free uncertainty measure and\nintegrates it into a split conformal prediction (CP) pipeline to construct\nprediction sets with formal coverage guarantees. Unlike existing approaches\nthat rely on semantic consistency heuristics or white-box features, TECP\ndirectly estimates epistemic uncertainty from the token entropy structure of\nsampled generations and calibrates uncertainty thresholds via CP quantiles to\nensure provable error control. Empirical evaluations across six large language\nmodels and two benchmarks (CoQA and TriviaQA) demonstrate that TECP\nconsistently achieves reliable coverage and compact prediction sets,\noutperforming prior self-consistency-based UQ methods. Our method provides a\nprincipled and efficient solution for trustworthy generation in black-box LLM\nsettings."
                },
                "authors": [
                    {
                        "name": "Beining Xu"
                    },
                    {
                        "name": "Yongming Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yongming Lu"
                },
                "author": "Yongming Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05048v1",
                "updated": "2025-09-05T12:25:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    25,
                    22,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T12:25:22Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    25,
                    22,
                    4,
                    248,
                    0
                ],
                "title": "Semi-supervised inference for treatment heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-supervised inference for treatment heterogeneity"
                },
                "summary": "In causal inference, measuring treatment heterogeneity is crucial as it\nprovides scientific insights into how treatments influence outcomes and guides\npersonalized decision-making. In this work, we study semi-supervised settings\nwhere a labeled dataset is accompanied by a large unlabeled dataset, and\ndevelop semi-supervised estimators for two measures of treatment heterogeneity:\nthe total treatment heterogeneity (TTH) and the explained treatment\nheterogeneity (ETH) of a simplified working model. We propose semi-supervised\nestimators for both quantities and demonstrate their improved robustness and\nefficiency compared with supervised methods. For ETH estimation, we show that\ndirect semi-supervised approaches may result in efficiency loss relative to\nsupervised counterparts. To address this, we introduce a re-weighting strategy\nthat assigns data-dependent weights to labeled and unlabeled samples to\noptimize efficiency. The proposed approach guarantees an asymptotic variance no\nlarger than that of the supervised method, ensuring its safe use. We evaluate\nthe performance of the proposed estimators through simulation studies and a\nreal-data application based on an AIDS clinical trial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In causal inference, measuring treatment heterogeneity is crucial as it\nprovides scientific insights into how treatments influence outcomes and guides\npersonalized decision-making. In this work, we study semi-supervised settings\nwhere a labeled dataset is accompanied by a large unlabeled dataset, and\ndevelop semi-supervised estimators for two measures of treatment heterogeneity:\nthe total treatment heterogeneity (TTH) and the explained treatment\nheterogeneity (ETH) of a simplified working model. We propose semi-supervised\nestimators for both quantities and demonstrate their improved robustness and\nefficiency compared with supervised methods. For ETH estimation, we show that\ndirect semi-supervised approaches may result in efficiency loss relative to\nsupervised counterparts. To address this, we introduce a re-weighting strategy\nthat assigns data-dependent weights to labeled and unlabeled samples to\noptimize efficiency. The proposed approach guarantees an asymptotic variance no\nlarger than that of the supervised method, ensuring its safe use. We evaluate\nthe performance of the proposed estimators through simulation studies and a\nreal-data application based on an AIDS clinical trial."
                },
                "authors": [
                    {
                        "name": "Yilizhati Anniwaer"
                    },
                    {
                        "name": "Yuqian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqian Zhang"
                },
                "author": "Yuqian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05043v1",
                "updated": "2025-09-05T12:07:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    7,
                    28,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T12:07:28Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    7,
                    28,
                    4,
                    248,
                    0
                ],
                "title": "Detecting extreme event-driven causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting extreme event-driven causality"
                },
                "summary": "The occurrence of some extreme events (such as marine heatwaves or\nexceptional circulations) can cause other extreme events (such as heatwave,\ndrought and flood). These concurrent extreme events have a great impact on\nenvironment and human health. However, how to detect and quantify the causes\nand impacts of these extreme events by a data-driven way is still unsolved. In\nthis study, the dynamic system method is extended to develop a method for\ndetecting the causality between extreme events. Taking the coupled\nLorenz-Lorenz systems with extreme event-driven coupling as an example, it is\ndemonstrated that this proposed detecting method is able to capture the extreme\nevent-driven causality, with even better causality detecting performance\nbetween concurrent extreme events. Comparison among three kinds of measured\nseries, full measurements outperform partial ones in event-to-event causality\ndetecting. The successful applicability of our proposed approach in Walker\ncirculation phenomenon indicates that our method contributes a novel way to the\nstudy of causal inference in complex systems. This method offers valuable\ninsights into multi-scale, nonlinear dynamics, particularly in uncovering\nassociations among extreme events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The occurrence of some extreme events (such as marine heatwaves or\nexceptional circulations) can cause other extreme events (such as heatwave,\ndrought and flood). These concurrent extreme events have a great impact on\nenvironment and human health. However, how to detect and quantify the causes\nand impacts of these extreme events by a data-driven way is still unsolved. In\nthis study, the dynamic system method is extended to develop a method for\ndetecting the causality between extreme events. Taking the coupled\nLorenz-Lorenz systems with extreme event-driven coupling as an example, it is\ndemonstrated that this proposed detecting method is able to capture the extreme\nevent-driven causality, with even better causality detecting performance\nbetween concurrent extreme events. Comparison among three kinds of measured\nseries, full measurements outperform partial ones in event-to-event causality\ndetecting. The successful applicability of our proposed approach in Walker\ncirculation phenomenon indicates that our method contributes a novel way to the\nstudy of causal inference in complex systems. This method offers valuable\ninsights into multi-scale, nonlinear dynamics, particularly in uncovering\nassociations among extreme events."
                },
                "authors": [
                    {
                        "name": "Siyang Yu"
                    },
                    {
                        "name": "Yu Huang"
                    },
                    {
                        "name": "Zuntao Fu"
                    }
                ],
                "author_detail": {
                    "name": "Zuntao Fu"
                },
                "author": "Zuntao Fu",
                "arxiv_doi": "10.1016/j.chaos.2025.117138",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.chaos.2025.117138",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.05043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05042v1",
                "updated": "2025-09-05T12:06:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    6,
                    6,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T12:06:06Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    6,
                    6,
                    4,
                    248,
                    0
                ],
                "title": "Shared Autonomy through LLMs and Reinforcement Learning for Applications\n  to Ship Hull Inspections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Autonomy through LLMs and Reinforcement Learning for Applications\n  to Ship Hull Inspections"
                },
                "summary": "Shared autonomy is a promising paradigm in robotic systems, particularly\nwithin the maritime domain, where complex, high-risk, and uncertain\nenvironments necessitate effective human-robot collaboration. This paper\ninvestigates the interaction of three complementary approaches to advance\nshared autonomy in heterogeneous marine robotic fleets: (i) the integration of\nLarge Language Models (LLMs) to facilitate intuitive high-level task\nspecification and support hull inspection missions, (ii) the implementation of\nhuman-in-the-loop interaction frameworks in multi-agent settings to enable\nadaptive and intent-aware coordination, and (iii) the development of a modular\nMission Manager based on Behavior Trees to provide interpretable and flexible\nmission control. Preliminary results from simulation and real-world lake-like\nenvironments demonstrate the potential of this multi-layered architecture to\nreduce operator cognitive load, enhance transparency, and improve adaptive\nbehaviour alignment with human intent. Ongoing work focuses on fully\nintegrating these components, refining coordination mechanisms, and validating\nthe system in operational port scenarios. This study contributes to\nestablishing a modular and scalable foundation for trustworthy,\nhuman-collaborative autonomy in safety-critical maritime robotics applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared autonomy is a promising paradigm in robotic systems, particularly\nwithin the maritime domain, where complex, high-risk, and uncertain\nenvironments necessitate effective human-robot collaboration. This paper\ninvestigates the interaction of three complementary approaches to advance\nshared autonomy in heterogeneous marine robotic fleets: (i) the integration of\nLarge Language Models (LLMs) to facilitate intuitive high-level task\nspecification and support hull inspection missions, (ii) the implementation of\nhuman-in-the-loop interaction frameworks in multi-agent settings to enable\nadaptive and intent-aware coordination, and (iii) the development of a modular\nMission Manager based on Behavior Trees to provide interpretable and flexible\nmission control. Preliminary results from simulation and real-world lake-like\nenvironments demonstrate the potential of this multi-layered architecture to\nreduce operator cognitive load, enhance transparency, and improve adaptive\nbehaviour alignment with human intent. Ongoing work focuses on fully\nintegrating these components, refining coordination mechanisms, and validating\nthe system in operational port scenarios. This study contributes to\nestablishing a modular and scalable foundation for trustworthy,\nhuman-collaborative autonomy in safety-critical maritime robotics applications."
                },
                "authors": [
                    {
                        "name": "Cristiano Caissutti"
                    },
                    {
                        "name": "Estelle Gerbier"
                    },
                    {
                        "name": "Ehsan Khorrambakht"
                    },
                    {
                        "name": "Paolo Marinelli"
                    },
                    {
                        "name": "Andrea Munafo'"
                    },
                    {
                        "name": "Andrea Caiti"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Caiti"
                },
                "author": "Andrea Caiti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01807v2",
                "updated": "2025-09-05T11:51:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    51,
                    58,
                    4,
                    248,
                    0
                ],
                "published": "2025-04-02T15:12:34Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    12,
                    34,
                    2,
                    92,
                    0
                ],
                "title": "Barrier Certificates for Unknown Systems with Latent States and\n  Polynomial Dynamics using Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Barrier Certificates for Unknown Systems with Latent States and\n  Polynomial Dynamics using Bayesian Inference"
                },
                "summary": "Certifying safety in dynamical systems is crucial, but barrier certificates -\nwidely used to verify that system trajectories remain within a safe region -\ntypically require explicit system models. When dynamics are unknown,\ndata-driven methods can be used instead, yet obtaining a valid certificate\nrequires rigorous uncertainty quantification. For this purpose, existing\nmethods usually rely on full-state measurements, limiting their applicability.\nThis paper proposes a novel approach for synthesizing barrier certificates for\nunknown systems with latent states and polynomial dynamics. A Bayesian\nframework is employed, where a prior in state-space representation is updated\nusing output data via a targeted marginal Metropolis-Hastings sampler. The\nresulting samples are used to construct a barrier certificate through a\nsum-of-squares program. Probabilistic guarantees for its validity with respect\nto the true, unknown system are obtained by testing on an additional set of\nposterior samples. The approach and its probabilistic guarantees are\nillustrated through a numerical simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certifying safety in dynamical systems is crucial, but barrier certificates -\nwidely used to verify that system trajectories remain within a safe region -\ntypically require explicit system models. When dynamics are unknown,\ndata-driven methods can be used instead, yet obtaining a valid certificate\nrequires rigorous uncertainty quantification. For this purpose, existing\nmethods usually rely on full-state measurements, limiting their applicability.\nThis paper proposes a novel approach for synthesizing barrier certificates for\nunknown systems with latent states and polynomial dynamics. A Bayesian\nframework is employed, where a prior in state-space representation is updated\nusing output data via a targeted marginal Metropolis-Hastings sampler. The\nresulting samples are used to construct a barrier certificate through a\nsum-of-squares program. Probabilistic guarantees for its validity with respect\nto the true, unknown system are obtained by testing on an additional set of\nposterior samples. The approach and its probabilistic guarantees are\nillustrated through a numerical simulation."
                },
                "authors": [
                    {
                        "name": "Robert Lefringhausen"
                    },
                    {
                        "name": "Sami Leon Noel Aziz Hanna"
                    },
                    {
                        "name": "Elias August"
                    },
                    {
                        "name": "Sandra Hirche"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Hirche"
                },
                "author": "Sandra Hirche",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 64th IEEE\n  Conference on Decision and Control",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14387v2",
                "updated": "2025-09-05T11:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    46,
                    29,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-17T10:33:23Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    10,
                    33,
                    23,
                    1,
                    168,
                    0
                ],
                "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning"
                },
                "summary": "Existing work on mitigating catastrophic forgetting during large language\nmodels (LLMs) fine-tuning for new knowledge instances has primarily focused on\npreserving performance on previously seen data, while critically overlooking\nthe collapse of essential capabilities instilled through alignment, most\nnotably the model's ability to faithfully express epistemic uncertainty (a\nproperty we term 'Ignorance Awareness'). In this work, we formalize the notion\nof Ignorance Awareness and illustrate that conventional fine-tuning methods can\nresult in substantial activation displacement. This displacement undermines the\ncritical capability of ignorance awareness, leading to undesirable behaviors\nsuch as hallucinations. To address this challenge, we introduce SEAT, a simple\nand principled fine-tuning approach that not only enables the model to\neffectively acquire new knowledge instances but also preserves its aligned\nignorance awareness. SEAT integrates two key components: (1) sparse tuning that\nconstrains activation drift, and (2) a novel entity perturbation method\ndesigned to counter knowledge entanglement. Experimental results demonstrate\nthat, across both real-world and synthetic datasets, SEAT significantly\noutperforms baselines in preserving ignorance awareness while retaining optimal\nfine-tuning performance, offering a more robust solution for LLM fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing work on mitigating catastrophic forgetting during large language\nmodels (LLMs) fine-tuning for new knowledge instances has primarily focused on\npreserving performance on previously seen data, while critically overlooking\nthe collapse of essential capabilities instilled through alignment, most\nnotably the model's ability to faithfully express epistemic uncertainty (a\nproperty we term 'Ignorance Awareness'). In this work, we formalize the notion\nof Ignorance Awareness and illustrate that conventional fine-tuning methods can\nresult in substantial activation displacement. This displacement undermines the\ncritical capability of ignorance awareness, leading to undesirable behaviors\nsuch as hallucinations. To address this challenge, we introduce SEAT, a simple\nand principled fine-tuning approach that not only enables the model to\neffectively acquire new knowledge instances but also preserves its aligned\nignorance awareness. SEAT integrates two key components: (1) sparse tuning that\nconstrains activation drift, and (2) a novel entity perturbation method\ndesigned to counter knowledge entanglement. Experimental results demonstrate\nthat, across both real-world and synthetic datasets, SEAT significantly\noutperforms baselines in preserving ignorance awareness while retaining optimal\nfine-tuning performance, offering a more robust solution for LLM fine-tuning."
                },
                "authors": [
                    {
                        "name": "William F. Shen"
                    },
                    {
                        "name": "Xinchi Qiu"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14295v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14295v5",
                "updated": "2025-09-05T11:39:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    39,
                    31,
                    4,
                    248,
                    0
                ],
                "published": "2024-01-25T16:34:00Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    16,
                    34,
                    0,
                    3,
                    25,
                    0
                ],
                "title": "Demystifying Chains, Trees, and Graphs of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Chains, Trees, and Graphs of Thoughts"
                },
                "summary": "The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Florim Memedi"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Guangyuan Piao"
                    },
                    {
                        "name": "Nils Blach"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Grzegorz Kwaniewski"
                    },
                    {
                        "name": "Jrgen Mller"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Aidan O'Mahony"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_doi": "10.1109/TPAMI.2025.3598182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2025.3598182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14295v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14295v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14506v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14506v3",
                "updated": "2025-09-05T11:16:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    16,
                    45,
                    4,
                    248,
                    0
                ],
                "published": "2024-09-22T16:10:10Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    10,
                    10,
                    6,
                    266,
                    0
                ],
                "title": "InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic\n  Robot Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic\n  Robot Autonomy"
                },
                "summary": "We introduce an interactive LLM-based framework designed to enhance the\nautonomy and robustness of domestic robots, targeting embodied intelligence.\nOur approach reduces reliance on large-scale data and incorporates a\nrobot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan,\nensures that the LLM's decision-making capabilities are effectively aligned\nwith robotic functions, enhancing operational robustness and adaptability,\nwhile our human-in-the-loop mechanism allows for real-time human intervention\nwhen user instruction is required. We evaluate our method in both simulation\nand on the real Toyota Human Support Robot and Anymal D-Unitree Z1 platforms.\nOur method achieves a 95% success rate in the 'fetch me' task completion with\nfailure recovery, highlighting its capability in both failure reasoning and\ntask planning. InteLiPlan achieves comparable performance to state-of-the-art\nlarge-scale LLM-based robotics planners, while using only real-time onboard\ncomputing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an interactive LLM-based framework designed to enhance the\nautonomy and robustness of domestic robots, targeting embodied intelligence.\nOur approach reduces reliance on large-scale data and incorporates a\nrobot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan,\nensures that the LLM's decision-making capabilities are effectively aligned\nwith robotic functions, enhancing operational robustness and adaptability,\nwhile our human-in-the-loop mechanism allows for real-time human intervention\nwhen user instruction is required. We evaluate our method in both simulation\nand on the real Toyota Human Support Robot and Anymal D-Unitree Z1 platforms.\nOur method achieves a 95% success rate in the 'fetch me' task completion with\nfailure recovery, highlighting its capability in both failure reasoning and\ntask planning. InteLiPlan achieves comparable performance to state-of-the-art\nlarge-scale LLM-based robotics planners, while using only real-time onboard\ncomputing."
                },
                "authors": [
                    {
                        "name": "Kim Tien Ly"
                    },
                    {
                        "name": "Kai Lu"
                    },
                    {
                        "name": "Ioannis Havoutis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Havoutis"
                },
                "author": "Ioannis Havoutis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14506v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05007v1",
                "updated": "2025-09-05T11:14:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    14,
                    11,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T11:14:11Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    14,
                    11,
                    4,
                    248,
                    0
                ],
                "title": "Sticker-TTS: Learn to Utilize Historical Experience with a\n  Sticker-driven Test-Time Scaling Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sticker-TTS: Learn to Utilize Historical Experience with a\n  Sticker-driven Test-Time Scaling Framework"
                },
                "summary": "Large reasoning models (LRMs) have exhibited strong performance on complex\nreasoning tasks, with further gains achievable through increased computational\nbudgets at inference. However, current test-time scaling methods predominantly\nrely on redundant sampling, ignoring the historical experience utilization,\nthereby limiting computational efficiency. To overcome this limitation, we\npropose Sticker-TTS, a novel test-time scaling framework that coordinates three\ncollaborative LRMs to iteratively explore and refine solutions guided by\nhistorical attempts. At the core of our framework are distilled key\nconditions-termed stickers-which drive the extraction, refinement, and reuse of\ncritical information across multiple rounds of reasoning. To further enhance\nthe efficiency and performance of our framework, we introduce a two-stage\noptimization strategy that combines imitation learning with self-improvement,\nenabling progressive refinement. Extensive evaluations on three challenging\nmathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,\ndemonstrate that Sticker-TTS consistently surpasses strong baselines, including\nself-consistency and advanced reinforcement learning approaches, under\ncomparable inference budgets. These results highlight the effectiveness of\nsticker-guided historical experience utilization. Our code and data are\navailable at https://github.com/RUCAIBox/Sticker-TTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have exhibited strong performance on complex\nreasoning tasks, with further gains achievable through increased computational\nbudgets at inference. However, current test-time scaling methods predominantly\nrely on redundant sampling, ignoring the historical experience utilization,\nthereby limiting computational efficiency. To overcome this limitation, we\npropose Sticker-TTS, a novel test-time scaling framework that coordinates three\ncollaborative LRMs to iteratively explore and refine solutions guided by\nhistorical attempts. At the core of our framework are distilled key\nconditions-termed stickers-which drive the extraction, refinement, and reuse of\ncritical information across multiple rounds of reasoning. To further enhance\nthe efficiency and performance of our framework, we introduce a two-stage\noptimization strategy that combines imitation learning with self-improvement,\nenabling progressive refinement. Extensive evaluations on three challenging\nmathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,\ndemonstrate that Sticker-TTS consistently surpasses strong baselines, including\nself-consistency and advanced reinforcement learning approaches, under\ncomparable inference budgets. These results highlight the effectiveness of\nsticker-guided historical experience utilization. Our code and data are\navailable at https://github.com/RUCAIBox/Sticker-TTS."
                },
                "authors": [
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "11 pages, 1 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04365v3",
                "updated": "2025-09-05T11:08:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    8,
                    45,
                    4,
                    248,
                    0
                ],
                "published": "2025-04-06T05:30:10Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    5,
                    30,
                    10,
                    6,
                    96,
                    0
                ],
                "title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPDL: Automatic Prompt Optimization for LLM Agents"
                },
                "summary": "The performance of large language models (LLMs) depends on how they are\nprompted, with choices spanning both the high-level prompting pattern (e.g.,\nZero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and\nfew-shot demonstrations). Manually tuning this combination is tedious,\nerror-prone, and specific to a given LLM and task. Therefore, this paper\nproposes AutoPDL, an automated approach to discovering good LLM agent\nconfigurations. Our approach frames this as a structured AutoML problem over a\ncombinatorial space of agentic and non-agentic prompting patterns and\ndemonstrations, using successive halving to efficiently navigate this space. We\nintroduce a library implementing common prompting patterns using the PDL prompt\nprogramming language. AutoPDL solutions are human-readable, editable, and\nexecutable PDL programs that use this library. This approach also enables\nsource-to-source optimization, allowing human-in-the-loop refinement and reuse.\nEvaluations across three tasks and seven LLMs (ranging from 3B to 70B\nparameters) show consistent accuracy gains ($9.21\\pm15.46$ percentage points),\nup to 67.5pp, and reveal that selected prompting strategies vary across models\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) depends on how they are\nprompted, with choices spanning both the high-level prompting pattern (e.g.,\nZero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and\nfew-shot demonstrations). Manually tuning this combination is tedious,\nerror-prone, and specific to a given LLM and task. Therefore, this paper\nproposes AutoPDL, an automated approach to discovering good LLM agent\nconfigurations. Our approach frames this as a structured AutoML problem over a\ncombinatorial space of agentic and non-agentic prompting patterns and\ndemonstrations, using successive halving to efficiently navigate this space. We\nintroduce a library implementing common prompting patterns using the PDL prompt\nprogramming language. AutoPDL solutions are human-readable, editable, and\nexecutable PDL programs that use this library. This approach also enables\nsource-to-source optimization, allowing human-in-the-loop refinement and reuse.\nEvaluations across three tasks and seven LLMs (ranging from 3B to 70B\nparameters) show consistent accuracy gains ($9.21\\pm15.46$ percentage points),\nup to 67.5pp, and reveal that selected prompting strategies vary across models\nand tasks."
                },
                "authors": [
                    {
                        "name": "Claudio Spiess"
                    },
                    {
                        "name": "Mandana Vaziri"
                    },
                    {
                        "name": "Louis Mandel"
                    },
                    {
                        "name": "Martin Hirzel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Hirzel"
                },
                "author": "Martin Hirzel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05005v1",
                "updated": "2025-09-05T11:06:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    6,
                    13,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T11:06:13Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    6,
                    13,
                    4,
                    248,
                    0
                ],
                "title": "Reconstruction of the Dipole Amplitude in the Dipole Picture as a\n  mathematical Inverse Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction of the Dipole Amplitude in the Dipole Picture as a\n  mathematical Inverse Problem"
                },
                "summary": "We show that the inference problem of constraining the dipole amplitude with\ninclusive deep inelastic scattering data can be written into a discrete linear\ninverse problem, in an analogous manner as can be done for computed tomography.\nTo this formulation of the problem, we apply standard inverse problems methods\nand algorithms to reconstruct known dipole amplitudes from simulated reduced\ncross section data with realistic precision. The main difference of this\napproach to previous works is that this implementation does not require any fit\nparametrization of the dipole amplitude. The freedom from parametrization also\nenables us for the first time to quantify the uncertainties of the inferred\ndipole amplitude in a novel more general framework. This mathematical approach\nto small-$x$ phenomenology opens a path to parametrization bias free inference\nof the dipole amplitude from HERA and Electron--Ion Collider data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that the inference problem of constraining the dipole amplitude with\ninclusive deep inelastic scattering data can be written into a discrete linear\ninverse problem, in an analogous manner as can be done for computed tomography.\nTo this formulation of the problem, we apply standard inverse problems methods\nand algorithms to reconstruct known dipole amplitudes from simulated reduced\ncross section data with realistic precision. The main difference of this\napproach to previous works is that this implementation does not require any fit\nparametrization of the dipole amplitude. The freedom from parametrization also\nenables us for the first time to quantify the uncertainties of the inferred\ndipole amplitude in a novel more general framework. This mathematical approach\nto small-$x$ phenomenology opens a path to parametrization bias free inference\nof the dipole amplitude from HERA and Electron--Ion Collider data."
                },
                "authors": [
                    {
                        "name": "Henri Hnninen"
                    },
                    {
                        "name": "Antti Kykknen"
                    },
                    {
                        "name": "Hjrdis Schlter"
                    }
                ],
                "author_detail": {
                    "name": "Hjrdis Schlter"
                },
                "author": "Hjrdis Schlter",
                "arxiv_comment": "18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65R32",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03627v3",
                "updated": "2025-09-05T10:45:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    45,
                    6,
                    4,
                    248,
                    0
                ],
                "published": "2024-09-05T15:39:11Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    15,
                    39,
                    11,
                    3,
                    249,
                    0
                ],
                "title": "Reading signatures of supermassive binary black holes in pulsar timing\n  array observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading signatures of supermassive binary black holes in pulsar timing\n  array observations"
                },
                "summary": "We find the inferred properties of the putative gravitational wave background\nin the second data release of the European Pulsar Timing Array to be in better\nagreement with theoretical expectations under the improved noise model. In\nparticular, our improved noise models show consistency of the background's\nstrain spectral index with the value of -2/3, favoring the population of\nsupermassive black hole binaries as the origin of the background. Our results\nfurther suggest that the observed gravitational wave emission is the dominant\nsource of the binary energy loss, with no evidence of environmental effects or\neccentric orbits. At the reference gravitational wave frequency of yr$^{-1}$,\nwe also find a lower power-law strain amplitude of the background than in\nprevious data analyses. This mitigates some of the tensions of the strain\namplitude with the expected number density and mass scale of binaries discussed\nin the literature. However, we show that it is mostly affected by strong\ncovariance of the amplitude and the strain spectral index at yr$^{-1}$, whereas\nthe strain amplitude at 0.1 yr$^{-1}$ and the strain amplitude at yr$^{-1}$\nassuming a fixed spectral index of -2/3 remains unaffected. Our results\nhighlight the importance of accurate noise models for correctly inferring\nproperties of the gravitational wave background.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find the inferred properties of the putative gravitational wave background\nin the second data release of the European Pulsar Timing Array to be in better\nagreement with theoretical expectations under the improved noise model. In\nparticular, our improved noise models show consistency of the background's\nstrain spectral index with the value of -2/3, favoring the population of\nsupermassive black hole binaries as the origin of the background. Our results\nfurther suggest that the observed gravitational wave emission is the dominant\nsource of the binary energy loss, with no evidence of environmental effects or\neccentric orbits. At the reference gravitational wave frequency of yr$^{-1}$,\nwe also find a lower power-law strain amplitude of the background than in\nprevious data analyses. This mitigates some of the tensions of the strain\namplitude with the expected number density and mass scale of binaries discussed\nin the literature. However, we show that it is mostly affected by strong\ncovariance of the amplitude and the strain spectral index at yr$^{-1}$, whereas\nthe strain amplitude at 0.1 yr$^{-1}$ and the strain amplitude at yr$^{-1}$\nassuming a fixed spectral index of -2/3 remains unaffected. Our results\nhighlight the importance of accurate noise models for correctly inferring\nproperties of the gravitational wave background."
                },
                "authors": [
                    {
                        "name": "Boris Goncharov"
                    },
                    {
                        "name": "Shubhit Sardana"
                    },
                    {
                        "name": "A. Sesana"
                    },
                    {
                        "name": "S. M. Tomson"
                    },
                    {
                        "name": "J. Antoniadis"
                    },
                    {
                        "name": "A. Chalumeau"
                    },
                    {
                        "name": "D. Champion"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "E. F. Keane"
                    },
                    {
                        "name": "K. Liu"
                    },
                    {
                        "name": "G. Shaifullah"
                    },
                    {
                        "name": "L. Speri"
                    },
                    {
                        "name": "S. Valtolina"
                    }
                ],
                "author_detail": {
                    "name": "S. Valtolina"
                },
                "author": "S. Valtolina",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04996v1",
                "updated": "2025-09-05T10:43:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    43,
                    12,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T10:43:12Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    43,
                    12,
                    4,
                    248,
                    0
                ],
                "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient\n  Vision-Language-Action Flow Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLOWER: Democratizing Generalist Robot Policies with Efficient\n  Vision-Language-Action Flow Policies"
                },
                "summary": "Developing efficient Vision-Language-Action (VLA) policies is crucial for\npractical robotics deployment, yet current approaches face prohibitive\ncomputational costs and resource requirements. Existing diffusion-based VLA\npolicies require multi-billion-parameter models and massive datasets to achieve\nstrong performance. We tackle this efficiency challenge with two contributions:\nintermediate-modality fusion, which reallocates capacity to the diffusion head\nby pruning up to $50\\%$ of LLM layers, and action-specific Global-AdaLN\nconditioning, which cuts parameters by $20\\%$ through modular adaptation. We\nintegrate these advances into a novel 950 M-parameter VLA called FLOWER.\nPretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance\nwith bigger VLAs across $190$ tasks spanning ten simulation and real-world\nbenchmarks and demonstrates robustness across diverse robotic embodiments. In\naddition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.\nDemos, code and pretrained weights are available at\nhttps://intuitive-robots.github.io/flower_vla/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing efficient Vision-Language-Action (VLA) policies is crucial for\npractical robotics deployment, yet current approaches face prohibitive\ncomputational costs and resource requirements. Existing diffusion-based VLA\npolicies require multi-billion-parameter models and massive datasets to achieve\nstrong performance. We tackle this efficiency challenge with two contributions:\nintermediate-modality fusion, which reallocates capacity to the diffusion head\nby pruning up to $50\\%$ of LLM layers, and action-specific Global-AdaLN\nconditioning, which cuts parameters by $20\\%$ through modular adaptation. We\nintegrate these advances into a novel 950 M-parameter VLA called FLOWER.\nPretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance\nwith bigger VLAs across $190$ tasks spanning ten simulation and real-world\nbenchmarks and demonstrates robustness across diverse robotic embodiments. In\naddition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.\nDemos, code and pretrained weights are available at\nhttps://intuitive-robots.github.io/flower_vla/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Hongyi Zhou"
                    },
                    {
                        "name": "Marcel Rhle"
                    },
                    {
                        "name": "mer Erdin Yamurlu"
                    },
                    {
                        "name": "Fabian Otto"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "arxiv_comment": "Published at CoRL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17224v2",
                "updated": "2025-09-05T10:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    41,
                    6,
                    4,
                    248,
                    0
                ],
                "published": "2024-11-26T08:45:15Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    8,
                    45,
                    15,
                    1,
                    331,
                    0
                ],
                "title": "Double robust estimation of functional outcomes with data missing at\n  random",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double robust estimation of functional outcomes with data missing at\n  random"
                },
                "summary": "We present and study semi-parametric estimators for the mean of functional\noutcomes in situations where some of these outcomes are missing and covariate\ninformation is available on all units. Assuming that the missingness mechanism\ndepends only on the covariates (missing at random assumption), we present two\nestimators for the functional mean parameter, using working models for the\nfunctional outcome given the covariates, and the probability of missingness\ngiven the covariates. We contribute by establishing that both these estimators\nhave Gaussian processes as limiting distributions and explicitly give their\ncovariance functions. One of the estimators is double robust in the sense that\nthe limiting distribution holds whenever at least one of the nuisance models is\ncorrectly specified. These results allow us to present simultaneous confidence\nbands for the mean function with asymptotically guaranteed coverage. A Monte\nCarlo study shows the finite sample properties of the proposed functional\nestimators and their associated simultaneous inference. The use of the method\nis illustrated in an application where the mean of counterfactual outcomes is\ntargeted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present and study semi-parametric estimators for the mean of functional\noutcomes in situations where some of these outcomes are missing and covariate\ninformation is available on all units. Assuming that the missingness mechanism\ndepends only on the covariates (missing at random assumption), we present two\nestimators for the functional mean parameter, using working models for the\nfunctional outcome given the covariates, and the probability of missingness\ngiven the covariates. We contribute by establishing that both these estimators\nhave Gaussian processes as limiting distributions and explicitly give their\ncovariance functions. One of the estimators is double robust in the sense that\nthe limiting distribution holds whenever at least one of the nuisance models is\ncorrectly specified. These results allow us to present simultaneous confidence\nbands for the mean function with asymptotically guaranteed coverage. A Monte\nCarlo study shows the finite sample properties of the proposed functional\nestimators and their associated simultaneous inference. The use of the method\nis illustrated in an application where the mean of counterfactual outcomes is\ntargeted."
                },
                "authors": [
                    {
                        "name": "Xijia Liu"
                    },
                    {
                        "name": "Kreske Felix Ecker"
                    },
                    {
                        "name": "Lina Schelin"
                    },
                    {
                        "name": "Xavier de Luna"
                    }
                ],
                "author_detail": {
                    "name": "Xavier de Luna"
                },
                "author": "Xavier de Luna",
                "arxiv_comment": "24 pages incl appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04993v1",
                "updated": "2025-09-05T10:40:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    40,
                    31,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T10:40:31Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    40,
                    31,
                    4,
                    248,
                    0
                ],
                "title": "LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of\n  Dual-Loop Edge-Terminal Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of\n  Dual-Loop Edge-Terminal Collaboration"
                },
                "summary": "The ubiquitous computing resources in 6G networks provide ideal environments\nfor the fusion of large language models (LLMs) and intelligent services through\nthe agent framework. With auxiliary modules and planning cores, LLM-enabled\nagents can autonomously plan and take actions to deal with diverse environment\nsemantics and user intentions. However, the limited resources of individual\nnetwork devices significantly hinder the efficient operation of LLM-enabled\nagents with complex tool calls, highlighting the urgent need for efficient\nmulti-level device collaborations. To this end, the framework and method of the\nLLM-enabled multi-agent system with dual-loop terminal-edge collaborations are\nproposed in 6G networks. Firstly, the outer loop consists of the iterative\ncollaborations between the global agent and multiple sub-agents deployed on\nedge servers and terminals, where the planning capability is enhanced through\ntask decomposition and parallel sub-task distribution. Secondly, the inner loop\nutilizes sub-agents with dedicated roles to circularly reason, execute, and\nreplan the sub-task, and the parallel tool calling generation with offloading\nstrategies is incorporated to improve efficiency. The improved task planning\ncapability and task execution efficiency are validated through the conducted\ncase study in 6G-supported urban safety governance. Finally, the open\nchallenges and future directions are thoroughly analyzed in 6G networks,\naccelerating the advent of the 6G era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ubiquitous computing resources in 6G networks provide ideal environments\nfor the fusion of large language models (LLMs) and intelligent services through\nthe agent framework. With auxiliary modules and planning cores, LLM-enabled\nagents can autonomously plan and take actions to deal with diverse environment\nsemantics and user intentions. However, the limited resources of individual\nnetwork devices significantly hinder the efficient operation of LLM-enabled\nagents with complex tool calls, highlighting the urgent need for efficient\nmulti-level device collaborations. To this end, the framework and method of the\nLLM-enabled multi-agent system with dual-loop terminal-edge collaborations are\nproposed in 6G networks. Firstly, the outer loop consists of the iterative\ncollaborations between the global agent and multiple sub-agents deployed on\nedge servers and terminals, where the planning capability is enhanced through\ntask decomposition and parallel sub-task distribution. Secondly, the inner loop\nutilizes sub-agents with dedicated roles to circularly reason, execute, and\nreplan the sub-task, and the parallel tool calling generation with offloading\nstrategies is incorporated to improve efficiency. The improved task planning\ncapability and task execution efficiency are validated through the conducted\ncase study in 6G-supported urban safety governance. Finally, the open\nchallenges and future directions are thoroughly analyzed in 6G networks,\naccelerating the advent of the 6G era."
                },
                "authors": [
                    {
                        "name": "Zheyan Qu"
                    },
                    {
                        "name": "Wenbo Wang"
                    },
                    {
                        "name": "Zitong Yu"
                    },
                    {
                        "name": "Boquan Sun"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xing Zhang"
                },
                "author": "Xing Zhang",
                "arxiv_comment": "This paper has been accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04091v2",
                "updated": "2025-09-05T10:34:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    34,
                    25,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T10:48:02Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    48,
                    2,
                    3,
                    247,
                    0
                ],
                "title": "Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its\n  Implications Across Security Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its\n  Implications Across Security Tasks"
                },
                "summary": "Accurate detection of third-party libraries (TPLs) is fundamental to Android\nsecurity, supporting vulnerability tracking, malware detection, and supply\nchain auditing. Despite many proposed tools, their real-world effectiveness\nremains unclear. We present the first large-scale empirical study of ten\nstate-of-the-art TPL detection techniques across over 6,000 apps, enabled by a\nnew ground truth dataset with precise version-level annotations for both remote\nand local dependencies. Our evaluation exposes tool fragility to R8-era\ntransformations, weak version discrimination, inaccurate correspondence of\ncandidate libraries, difficulty in generalizing similarity thresholds, and\nprohibitive runtime/memory overheads at scale. Beyond tool assessment, we\nfurther analyze how TPLs shape downstream tasks, including vulnerability\nanalysis, malware detection, secret leakage assessment, and LLM-based\nevaluation. From this perspective, our study provides concrete insights into\nhow TPL characteristics affect these tasks and informs future improvements in\nsecurity analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate detection of third-party libraries (TPLs) is fundamental to Android\nsecurity, supporting vulnerability tracking, malware detection, and supply\nchain auditing. Despite many proposed tools, their real-world effectiveness\nremains unclear. We present the first large-scale empirical study of ten\nstate-of-the-art TPL detection techniques across over 6,000 apps, enabled by a\nnew ground truth dataset with precise version-level annotations for both remote\nand local dependencies. Our evaluation exposes tool fragility to R8-era\ntransformations, weak version discrimination, inaccurate correspondence of\ncandidate libraries, difficulty in generalizing similarity thresholds, and\nprohibitive runtime/memory overheads at scale. Beyond tool assessment, we\nfurther analyze how TPLs shape downstream tasks, including vulnerability\nanalysis, malware detection, secret leakage assessment, and LLM-based\nevaluation. From this perspective, our study provides concrete insights into\nhow TPL characteristics affect these tasks and informs future improvements in\nsecurity analysis."
                },
                "authors": [
                    {
                        "name": "Jintao Gu"
                    },
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Yihan Lin"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yuchun Guo"
                    },
                    {
                        "name": "Yigui Cao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "20pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5; D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04987v1",
                "updated": "2025-09-05T10:33:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    33,
                    30,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T10:33:30Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    33,
                    30,
                    4,
                    248,
                    0
                ],
                "title": "Optimal Estimation for General Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Estimation for General Gaussian Processes"
                },
                "summary": "This paper proposes a novel exact maximum likelihood (ML) estimation method\nfor general Gaussian processes, where all parameters are estimated jointly. The\nexact ML estimator (MLE) is consistent and asymptotically normally distributed.\nWe prove the local asymptotic normality (LAN) property of the sequence of\nstatistical experiments for general Gaussian processes in the sense of Le Cam,\nthereby enabling optimal estimation and facilitating statistical inference. The\nresults rely solely on the asymptotic behavior of the spectral density near\nzero, allowing them to be widely applied. The established optimality not only\naddresses the gap left by Adenstedt(1974), who proposed an efficient but\ninfeasible estimator for the long-run mean $\\mu$, but also enables us to\nevaluate the finite-sample performance of the existing method -- the commonly\nused plug-in MLE, in which the sample mean is substituted into the likelihood.\nOur simulation results show that the plug-in MLE performs nearly as well as the\nexact MLE, alleviating concerns that inefficient estimation of $\\mu$ would\ncompromise the efficiency of the remaining parameter estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel exact maximum likelihood (ML) estimation method\nfor general Gaussian processes, where all parameters are estimated jointly. The\nexact ML estimator (MLE) is consistent and asymptotically normally distributed.\nWe prove the local asymptotic normality (LAN) property of the sequence of\nstatistical experiments for general Gaussian processes in the sense of Le Cam,\nthereby enabling optimal estimation and facilitating statistical inference. The\nresults rely solely on the asymptotic behavior of the spectral density near\nzero, allowing them to be widely applied. The established optimality not only\naddresses the gap left by Adenstedt(1974), who proposed an efficient but\ninfeasible estimator for the long-run mean $\\mu$, but also enables us to\nevaluate the finite-sample performance of the existing method -- the commonly\nused plug-in MLE, in which the sample mean is substituted into the likelihood.\nOur simulation results show that the plug-in MLE performs nearly as well as the\nexact MLE, alleviating concerns that inefficient estimation of $\\mu$ would\ncompromise the efficiency of the remaining parameter estimates."
                },
                "authors": [
                    {
                        "name": "Tetsuya Takabatake"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04979v1",
                "updated": "2025-09-05T10:04:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    4,
                    33,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T10:04:33Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    4,
                    33,
                    4,
                    248,
                    0
                ],
                "title": "Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for\n  Ranking Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for\n  Ranking Agents"
                },
                "summary": "AI agents -- powered by reasoning-capable large language models (LLMs) and\nintegrated with tools, data, and web search -- are poised to transform the\ninternet into a \\emph{Web of Agents}: a machine-native ecosystem where\nautonomous agents interact, collaborate, and execute tasks at scale. Realizing\nthis vision requires \\emph{Agent Ranking} -- selecting agents not only by\ndeclared capabilities but by proven, recent performance. Unlike Web~1.0's\nPageRank, a global, transparent network of agent interactions does not exist;\nusage signals are fragmented and private, making ranking infeasible without\ncoordination.\n  We propose \\textbf{DOVIS}, a five-layer operational protocol\n(\\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that\nenables the collection of minimal, privacy-preserving aggregates of usage and\nperformance across the ecosystem. On this substrate, we implement\n\\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines\n\\emph{usage} (selection frequency) and \\emph{competence} (outcome quality,\ncost, safety, latency) into a unified ranking. We present simulation results\nand theoretical guarantees on convergence, robustness, and Sybil resistance,\ndemonstrating the viability of coordinated protocols and performance-aware\nranking in enabling a scalable, trustworthy Agentic Web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents -- powered by reasoning-capable large language models (LLMs) and\nintegrated with tools, data, and web search -- are poised to transform the\ninternet into a \\emph{Web of Agents}: a machine-native ecosystem where\nautonomous agents interact, collaborate, and execute tasks at scale. Realizing\nthis vision requires \\emph{Agent Ranking} -- selecting agents not only by\ndeclared capabilities but by proven, recent performance. Unlike Web~1.0's\nPageRank, a global, transparent network of agent interactions does not exist;\nusage signals are fragmented and private, making ranking infeasible without\ncoordination.\n  We propose \\textbf{DOVIS}, a five-layer operational protocol\n(\\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that\nenables the collection of minimal, privacy-preserving aggregates of usage and\nperformance across the ecosystem. On this substrate, we implement\n\\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines\n\\emph{usage} (selection frequency) and \\emph{competence} (outcome quality,\ncost, safety, latency) into a unified ranking. We present simulation results\nand theoretical guarantees on convergence, robustness, and Sybil resistance,\ndemonstrating the viability of coordinated protocols and performance-aware\nranking in enabling a scalable, trustworthy Agentic Web."
                },
                "authors": [
                    {
                        "name": "Rajesh Tembarai Krishnamachari"
                    },
                    {
                        "name": "Srividya Rajesh"
                    }
                ],
                "author_detail": {
                    "name": "Srividya Rajesh"
                },
                "author": "Srividya Rajesh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04969v1",
                "updated": "2025-09-05T09:49:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    49,
                    39,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T09:49:39Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    49,
                    39,
                    4,
                    248,
                    0
                ],
                "title": "Classification of kinetic-related injury in hospital triage data using\n  NLP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification of kinetic-related injury in hospital triage data using\n  NLP"
                },
                "summary": "Triage notes, created at the start of a patient's hospital visit, contain a\nwealth of information that can help medical staff and researchers understand\nEmergency Department patient epidemiology and the degree of time-dependent\nillness or injury. Unfortunately, applying modern Natural Language Processing\nand Machine Learning techniques to analyse triage data faces some challenges:\nFirstly, hospital data contains highly sensitive information that is subject to\nprivacy regulation thus need to be analysed on site; Secondly, most hospitals\nand medical facilities lack the necessary hardware to fine-tune a Large\nLanguage Model (LLM), much less training one from scratch; Lastly, to identify\nthe records of interest, expert inputs are needed to manually label the\ndatasets, which can be time-consuming and costly. We present in this paper a\npipeline that enables the classification of triage data using LLM and limited\ncompute resources. We first fine-tuned a pre-trained LLM with a classifier\nusing a small (2k) open sourced dataset on a GPU; and then further fine-tuned\nthe model with a hospital specific dataset of 1000 samples on a CPU. We\ndemonstrated that by carefully curating the datasets and leveraging existing\nmodels and open sourced data, we can successfully classify triage data with\nlimited compute resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triage notes, created at the start of a patient's hospital visit, contain a\nwealth of information that can help medical staff and researchers understand\nEmergency Department patient epidemiology and the degree of time-dependent\nillness or injury. Unfortunately, applying modern Natural Language Processing\nand Machine Learning techniques to analyse triage data faces some challenges:\nFirstly, hospital data contains highly sensitive information that is subject to\nprivacy regulation thus need to be analysed on site; Secondly, most hospitals\nand medical facilities lack the necessary hardware to fine-tune a Large\nLanguage Model (LLM), much less training one from scratch; Lastly, to identify\nthe records of interest, expert inputs are needed to manually label the\ndatasets, which can be time-consuming and costly. We present in this paper a\npipeline that enables the classification of triage data using LLM and limited\ncompute resources. We first fine-tuned a pre-trained LLM with a classifier\nusing a small (2k) open sourced dataset on a GPU; and then further fine-tuned\nthe model with a hospital specific dataset of 1000 samples on a CPU. We\ndemonstrated that by carefully curating the datasets and leveraging existing\nmodels and open sourced data, we can successfully classify triage data with\nlimited compute resources."
                },
                "authors": [
                    {
                        "name": "Midhun Shyam"
                    },
                    {
                        "name": "Jim Basilakis"
                    },
                    {
                        "name": "Kieran Luken"
                    },
                    {
                        "name": "Steven Thomas"
                    },
                    {
                        "name": "John Crozier"
                    },
                    {
                        "name": "Paul M. Middleton"
                    },
                    {
                        "name": "X. Rosalind Wang"
                    }
                ],
                "author_detail": {
                    "name": "X. Rosalind Wang"
                },
                "author": "X. Rosalind Wang",
                "arxiv_comment": "Accepted as a short paper for publishing at ADMA 2025\n  (https://adma2025.github.io), with Supplementary Material available at\n  https://github.com/CRMDS/Kinetic-Injury-Triage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17450v2",
                "updated": "2025-09-05T09:48:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    48,
                    4,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-24T17:08:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    17,
                    8,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability\n  in Knowledge and Safety with DuET-PD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability\n  in Knowledge and Safety with DuET-PD"
                },
                "summary": "Large Language Models (LLMs) can struggle to balance gullibility to\nmisinformation and resistance to valid corrections in persuasive dialogues, a\ncritical challenge for reliable deployment. We introduce DuET-PD (Dual\nEvaluation for Trust in Persuasive Dialogues), a framework evaluating\nmulti-turn stance-change dynamics across dual dimensions: persuasion type\n(corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via\nSALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves\nonly 27.32% accuracy in MMLU-Pro under sustained misleading persuasions.\nMoreover, results reveal a concerning trend of increasing sycophancy in newer\nopen-source models. To address this, we introduce Holistic DPO, a training\napproach balancing positive and negative persuasion examples. Unlike prompting\nor resist-only training, Holistic DPO enhances both robustness to\nmisinformation and receptiveness to corrections, improving\nLlama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts\nfrom 4.21% to 76.54%. These contributions offer a pathway to developing more\nreliable and adaptable LLMs for multi-turn dialogue. Code is available at\nhttps://github.com/Social-AI-Studio/DuET-PD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can struggle to balance gullibility to\nmisinformation and resistance to valid corrections in persuasive dialogues, a\ncritical challenge for reliable deployment. We introduce DuET-PD (Dual\nEvaluation for Trust in Persuasive Dialogues), a framework evaluating\nmulti-turn stance-change dynamics across dual dimensions: persuasion type\n(corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via\nSALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves\nonly 27.32% accuracy in MMLU-Pro under sustained misleading persuasions.\nMoreover, results reveal a concerning trend of increasing sycophancy in newer\nopen-source models. To address this, we introduce Holistic DPO, a training\napproach balancing positive and negative persuasion examples. Unlike prompting\nor resist-only training, Holistic DPO enhances both robustness to\nmisinformation and receptiveness to corrections, improving\nLlama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts\nfrom 4.21% to 76.54%. These contributions offer a pathway to developing more\nreliable and adaptable LLMs for multi-turn dialogue. Code is available at\nhttps://github.com/Social-AI-Studio/DuET-PD."
                },
                "authors": [
                    {
                        "name": "Bryan Chen Zhengyu Tan"
                    },
                    {
                        "name": "Daniel Wai Kit Chin"
                    },
                    {
                        "name": "Zhengyuan Liu"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "To appear at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04958v1",
                "updated": "2025-09-05T09:28:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    28,
                    51,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T09:28:51Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    28,
                    51,
                    4,
                    248,
                    0
                ],
                "title": "Learning Multidimensional Urban Poverty Representation with Satellite\n  Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Multidimensional Urban Poverty Representation with Satellite\n  Imagery"
                },
                "summary": "Recent advances in deep learning have enabled the inference of urban\nsocioeconomic characteristics from satellite imagery. However, models relying\nsolely on urbanization traits often show weak correlations with poverty\nindicators, as unplanned urban growth can obscure economic disparities and\nspatial inequalities. To address this limitation, we introduce a novel\nrepresentation learning framework that captures multidimensional\ndeprivation-related traits from very high-resolution satellite imagery for\nprecise urban poverty mapping. Our approach integrates three complementary\ntraits: (1) accessibility traits, learned via contrastive learning to encode\nproximity to essential infrastructure; (2) morphological traits, derived from\nbuilding footprints to reflect housing conditions in informal settlements; and\n(3) economic traits, inferred from nightlight intensity as a proxy for economic\nactivity. To mitigate spurious correlations - such as those from\nnon-residential nightlight sources that misrepresent poverty conditions - we\nincorporate a backdoor adjustment mechanism that leverages morphological traits\nduring training of the economic module. By fusing these complementary features\ninto a unified representation, our framework captures the complex nature of\npoverty, which often diverges from economic development trends. Evaluations\nacross three capital cities - Cape Town, Dhaka, and Phnom Penh - show that our\nmodel significantly outperforms existing baselines, offering a robust tool for\npoverty mapping and policy support in data-scarce regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning have enabled the inference of urban\nsocioeconomic characteristics from satellite imagery. However, models relying\nsolely on urbanization traits often show weak correlations with poverty\nindicators, as unplanned urban growth can obscure economic disparities and\nspatial inequalities. To address this limitation, we introduce a novel\nrepresentation learning framework that captures multidimensional\ndeprivation-related traits from very high-resolution satellite imagery for\nprecise urban poverty mapping. Our approach integrates three complementary\ntraits: (1) accessibility traits, learned via contrastive learning to encode\nproximity to essential infrastructure; (2) morphological traits, derived from\nbuilding footprints to reflect housing conditions in informal settlements; and\n(3) economic traits, inferred from nightlight intensity as a proxy for economic\nactivity. To mitigate spurious correlations - such as those from\nnon-residential nightlight sources that misrepresent poverty conditions - we\nincorporate a backdoor adjustment mechanism that leverages morphological traits\nduring training of the economic module. By fusing these complementary features\ninto a unified representation, our framework captures the complex nature of\npoverty, which often diverges from economic development trends. Evaluations\nacross three capital cities - Cape Town, Dhaka, and Phnom Penh - show that our\nmodel significantly outperforms existing baselines, offering a robust tool for\npoverty mapping and policy support in data-scarce regions."
                },
                "authors": [
                    {
                        "name": "Sungwon Park"
                    },
                    {
                        "name": "Sumin Lee"
                    },
                    {
                        "name": "Jihee Kim"
                    },
                    {
                        "name": "Jae-Gil Lee"
                    },
                    {
                        "name": "Meeyoung Cha"
                    },
                    {
                        "name": "Jeasurk Yang"
                    },
                    {
                        "name": "Donghyun Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Donghyun Ahn"
                },
                "author": "Donghyun Ahn",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07173v2",
                "updated": "2025-09-05T09:25:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    25,
                    57,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-08T14:45:11Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    14,
                    45,
                    11,
                    6,
                    159,
                    0
                ],
                "title": "Translating Federated Learning Algorithms in Python into CSP Processes\n  Using ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Federated Learning Algorithms in Python into CSP Processes\n  Using ChatGPT"
                },
                "summary": "The Python Testbed for Federated Learning Algorithms is a simple Python FL\nframework that is easy to use by ML&AI developers who do not need to be\nprofessional programmers and is also amenable to LLMs. In the previous\nresearch, generic federated learning algorithms provided by this framework were\nmanually translated into the CSP processes and algorithms' safety and liveness\nproperties were automatically verified by the model checker PAT. In this paper,\na simple translation process is introduced wherein the ChatGPT is used to\nautomate the translation of the mentioned federated learning algorithms in\nPython into the corresponding CSP processes. Within the process, the minimality\nof the used context is estimated based on the feedback from ChatGPT. The\nproposed translation process was experimentally validated by successful\ntranslation (verified by the model checker PAT) of both generic centralized and\ndecentralized federated learning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Python Testbed for Federated Learning Algorithms is a simple Python FL\nframework that is easy to use by ML&AI developers who do not need to be\nprofessional programmers and is also amenable to LLMs. In the previous\nresearch, generic federated learning algorithms provided by this framework were\nmanually translated into the CSP processes and algorithms' safety and liveness\nproperties were automatically verified by the model checker PAT. In this paper,\na simple translation process is introduced wherein the ChatGPT is used to\nautomate the translation of the mentioned federated learning algorithms in\nPython into the corresponding CSP processes. Within the process, the minimality\nof the used context is estimated based on the feedback from ChatGPT. The\nproposed translation process was experimentally validated by successful\ntranslation (verified by the model checker PAT) of both generic centralized and\ndecentralized federated learning algorithms."
                },
                "authors": [
                    {
                        "name": "Miroslav Popovic"
                    },
                    {
                        "name": "Marko Popovic"
                    },
                    {
                        "name": "Miodrag Djukic"
                    },
                    {
                        "name": "Ilija Basicevic"
                    }
                ],
                "author_detail": {
                    "name": "Ilija Basicevic"
                },
                "author": "Ilija Basicevic",
                "arxiv_doi": "10.1109/MIPRO65660.2025.11131995",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MIPRO65660.2025.11131995",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.07173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 4 tables; Published by IEEE Xplore",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19920v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19920v3",
                "updated": "2025-09-05T09:22:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    22,
                    59,
                    4,
                    248,
                    0
                ],
                "published": "2024-10-25T18:25:35Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    25,
                    35,
                    4,
                    299,
                    0
                ],
                "title": "Reinforcement Learning for Aligning Large Language Models Agents with\n  Interactive Environments: Quantifying and Mitigating Prompt Overfitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Aligning Large Language Models Agents with\n  Interactive Environments: Quantifying and Mitigating Prompt Overfitting"
                },
                "summary": "Reinforcement learning (RL) is a promising approach for aligning large\nlanguage models (LLMs) knowledge with sequential decision-making tasks.\nHowever, few studies have thoroughly investigated the impact on LLM agents\ncapabilities of fine-tuning them with RL in a specific environment. In this\npaper, we propose a novel framework to analyze the sensitivity of LLMs to\nprompt formulations following RL training in a textual environment. Our\nfindings reveal that the performance of LLMs degrades when faced with prompt\nformulations different from those used during the RL training phase. Besides,\nwe analyze the source of this sensitivity by examining the model's internal\nrepresentations and salient tokens. Finally, we propose to use a contrastive\nloss to mitigate this sensitivity and improve the robustness and generalization\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is a promising approach for aligning large\nlanguage models (LLMs) knowledge with sequential decision-making tasks.\nHowever, few studies have thoroughly investigated the impact on LLM agents\ncapabilities of fine-tuning them with RL in a specific environment. In this\npaper, we propose a novel framework to analyze the sensitivity of LLMs to\nprompt formulations following RL training in a textual environment. Our\nfindings reveal that the performance of LLMs degrades when faced with prompt\nformulations different from those used during the RL training phase. Besides,\nwe analyze the source of this sensitivity by examining the model's internal\nrepresentations and salient tokens. Finally, we propose to use a contrastive\nloss to mitigate this sensitivity and improve the robustness and generalization\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Mohamed Salim Aissi"
                    },
                    {
                        "name": "Clement Romac"
                    },
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Nicolas Thome"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Thome"
                },
                "author": "Nicolas Thome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19920v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19920v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02544v3",
                "updated": "2025-09-05T09:21:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    21,
                    10,
                    4,
                    248,
                    0
                ],
                "published": "2024-08-05T15:16:22Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    15,
                    16,
                    22,
                    0,
                    218,
                    0
                ],
                "title": "Caution for the Environment: Multimodal LLM Agents are Susceptible to\n  Environmental Distractions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caution for the Environment: Multimodal LLM Agents are Susceptible to\n  Environmental Distractions"
                },
                "summary": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in a graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general scenario is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing a simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness of\nagents, our findings first indicate that these agents are prone to\nenvironmental distractions. Furthermore, we implement an adversarial\nenvironment injection and analyze the approach to improve faithfulness, calling\nfor a collective focus on this important topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in a graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general scenario is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing a simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness of\nagents, our findings first indicate that these agents are prone to\nenvironmental distractions. Furthermore, we implement an adversarial\nenvironment injection and analyze the approach to improve faithfulness, calling\nfor a collective focus on this important topic."
                },
                "authors": [
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Yiting Wang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Tongxin Yuan"
                    },
                    {
                        "name": "Aston Zhang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11987v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11987v3",
                "updated": "2025-09-05T09:15:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    15,
                    55,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-16T08:54:08Z",
                "published_parsed": [
                    2025,
                    8,
                    16,
                    8,
                    54,
                    8,
                    5,
                    228,
                    0
                ],
                "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction"
                },
                "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\n$\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\n$\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Siyuan Chen"
                    },
                    {
                        "name": "Tianci He"
                    },
                    {
                        "name": "Yali Liao"
                    },
                    {
                        "name": "Yixiao Tian"
                    },
                    {
                        "name": "Jinpeng Wang"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Lingyue Yin"
                    },
                    {
                        "name": "Mingren Yin"
                    },
                    {
                        "name": "Zhenwei Zhu"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Jiecao Chen"
                    },
                    {
                        "name": "Yantao Du"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Jiacheng Guo"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Jianpeng Jiao"
                    },
                    {
                        "name": "Xiangsheng Li"
                    },
                    {
                        "name": "Jingkai Liu"
                    },
                    {
                        "name": "Shuang Ni"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Jose Blanchet"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "arxiv_comment": "Technical report, 51 pages. Update the results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11987v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11987v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00931v2",
                "updated": "2025-09-05T09:15:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    15,
                    52,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-31T16:57:02Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    16,
                    57,
                    2,
                    6,
                    243,
                    0
                ],
                "title": "Semi-Supervised Bayesian GANs with Log-Signatures for Uncertainty-Aware\n  Credit Card Fraud Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Bayesian GANs with Log-Signatures for Uncertainty-Aware\n  Credit Card Fraud Detection"
                },
                "summary": "We present a novel deep generative semi-supervised framework for credit card\nfraud detection, formulated as time series classification task. As financial\ntransaction data streams grow in scale and complexity, traditional methods\noften require large labeled datasets, struggle with time series of irregular\nsampling frequencies and varying sequence lengths. To address these challenges,\nwe extend conditional Generative Adversarial Networks (GANs) for targeted data\naugmentation, integrate Bayesian inference to obtain predictive distributions\nand quantify uncertainty, and leverage log-signatures for robust feature\nencoding of transaction histories. We introduce a novel Wasserstein\ndistance-based loss to align generated and real unlabeled samples while\nsimultaneously maximizing classification accuracy on labeled data. Our approach\nis evaluated on the BankSim dataset, a widely used simulator for credit card\ntransaction data, under varying proportions of labeled samples, demonstrating\nconsistent improvements over benchmarks in both global statistical and\ndomain-specific metrics. These findings highlight the effectiveness of\nGAN-driven semi-supervised learning with log-signatures for irregularly sampled\ntime series and emphasize the importance of uncertainty-aware predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel deep generative semi-supervised framework for credit card\nfraud detection, formulated as time series classification task. As financial\ntransaction data streams grow in scale and complexity, traditional methods\noften require large labeled datasets, struggle with time series of irregular\nsampling frequencies and varying sequence lengths. To address these challenges,\nwe extend conditional Generative Adversarial Networks (GANs) for targeted data\naugmentation, integrate Bayesian inference to obtain predictive distributions\nand quantify uncertainty, and leverage log-signatures for robust feature\nencoding of transaction histories. We introduce a novel Wasserstein\ndistance-based loss to align generated and real unlabeled samples while\nsimultaneously maximizing classification accuracy on labeled data. Our approach\nis evaluated on the BankSim dataset, a widely used simulator for credit card\ntransaction data, under varying proportions of labeled samples, demonstrating\nconsistent improvements over benchmarks in both global statistical and\ndomain-specific metrics. These findings highlight the effectiveness of\nGAN-driven semi-supervised learning with log-signatures for irregularly sampled\ntime series and emphasize the importance of uncertainty-aware predictions."
                },
                "authors": [
                    {
                        "name": "David Hirnschall"
                    }
                ],
                "author_detail": {
                    "name": "David Hirnschall"
                },
                "author": "David Hirnschall",
                "arxiv_comment": "Updated references in v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04111v2",
                "updated": "2025-09-05T09:12:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    12,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T11:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    20,
                    53,
                    3,
                    247,
                    0
                ],
                "title": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages"
                },
                "summary": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available."
                },
                "authors": [
                    {
                        "name": "Dan Saattrup Smart"
                    }
                ],
                "author_detail": {
                    "name": "Dan Saattrup Smart"
                },
                "author": "Dan Saattrup Smart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04944v1",
                "updated": "2025-09-05T09:10:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    10,
                    22,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T09:10:22Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    10,
                    22,
                    4,
                    248,
                    0
                ],
                "title": "Observation of an Accreting Planetary-Mass Companion with Signs of\n  Disk-Disk Interaction in Orion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of an Accreting Planetary-Mass Companion with Signs of\n  Disk-Disk Interaction in Orion"
                },
                "summary": "Young ($\\lesssim 10$ Myr) planetary-mass companions (PMCs) provide valuable\ninsights into the formation and early evolution of planetary systems. To date,\nonly a dozen such objects have been identified through direct imaging. Using\nJWST/NIRCam observations towards the Orion Nebula, obtained as part of the\n\\textit{PDRs4All} Early Release Science program, we have identified a faint\npoint source near the M-type star V2376 Ori. Follow-up spectroscopic\nobservations with the MUSE instrument on the VLT confirm that the source, V2376\nOri b, is indeed a young planetary-mass companion. It is a member of Orion D,\naround 80\\,pc in the foreground of the Trapezium cluster of Orion and with an\nage of approximately $7 \\pm 3$ Myr. We fit the SED of V2376 Ori b to infer a\nmass of $ \\sim 20~M_{\\rm Jup}$. The MUSE spectrum reveals several accretion\ntracers. Based on the H$\\alpha$ line intensity, we estimate an accretion rate\nof $\\sim$10$^{-6.5 \\pm 0.7}~\\rm M_{Jup}\\,yr^{-1}$, which is comparable to that\nof young PMCs such as PDS~70b. In addition, the MUSE data cube reveals extended\nemission in the [O\\,\\textsc{ii}] doublet at 7320 and 7330~\\AA, which is\ninterpreted as evidence of a dynamical interaction between the two sources\nthat, potentially, involves mass transfer between their individual accretion\ndisks. These results demonstrate that JWST/NIRCam imaging surveys of young\nstellar associations can uncover new PMCs, which can then be confirmed and\ncharacterized through ground-based spectroscopic follow-up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Young ($\\lesssim 10$ Myr) planetary-mass companions (PMCs) provide valuable\ninsights into the formation and early evolution of planetary systems. To date,\nonly a dozen such objects have been identified through direct imaging. Using\nJWST/NIRCam observations towards the Orion Nebula, obtained as part of the\n\\textit{PDRs4All} Early Release Science program, we have identified a faint\npoint source near the M-type star V2376 Ori. Follow-up spectroscopic\nobservations with the MUSE instrument on the VLT confirm that the source, V2376\nOri b, is indeed a young planetary-mass companion. It is a member of Orion D,\naround 80\\,pc in the foreground of the Trapezium cluster of Orion and with an\nage of approximately $7 \\pm 3$ Myr. We fit the SED of V2376 Ori b to infer a\nmass of $ \\sim 20~M_{\\rm Jup}$. The MUSE spectrum reveals several accretion\ntracers. Based on the H$\\alpha$ line intensity, we estimate an accretion rate\nof $\\sim$10$^{-6.5 \\pm 0.7}~\\rm M_{Jup}\\,yr^{-1}$, which is comparable to that\nof young PMCs such as PDS~70b. In addition, the MUSE data cube reveals extended\nemission in the [O\\,\\textsc{ii}] doublet at 7320 and 7330~\\AA, which is\ninterpreted as evidence of a dynamical interaction between the two sources\nthat, potentially, involves mass transfer between their individual accretion\ndisks. These results demonstrate that JWST/NIRCam imaging surveys of young\nstellar associations can uncover new PMCs, which can then be confirmed and\ncharacterized through ground-based spectroscopic follow-up."
                },
                "authors": [
                    {
                        "name": "Emilie Vila"
                    },
                    {
                        "name": "Paul Amiot"
                    },
                    {
                        "name": "Olivier Bern"
                    },
                    {
                        "name": "Ilane Schroetter"
                    },
                    {
                        "name": "Thomas Haworth"
                    },
                    {
                        "name": "Peter Zeidler"
                    },
                    {
                        "name": "Christiaan Boersma"
                    },
                    {
                        "name": "Jan Cami"
                    },
                    {
                        "name": "Asuncion Fuente"
                    },
                    {
                        "name": "Javier R. Goicoechea"
                    },
                    {
                        "name": "Takashi Onaka"
                    },
                    {
                        "name": "Els Peeters"
                    },
                    {
                        "name": "Massimo Robberto"
                    },
                    {
                        "name": "Markus Rllig"
                    }
                ],
                "author_detail": {
                    "name": "Markus Rllig"
                },
                "author": "Markus Rllig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04926v1",
                "updated": "2025-09-05T08:44:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    44,
                    27,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:44:27Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    44,
                    27,
                    4,
                    248,
                    0
                ],
                "title": "Towards Ontology-Based Descriptions of Conversations with\n  Qualitatively-Defined Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Ontology-Based Descriptions of Conversations with\n  Qualitatively-Defined Concepts"
                },
                "summary": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI."
                },
                "authors": [
                    {
                        "name": "Barbara Gendron"
                    },
                    {
                        "name": "Gal Guibon"
                    },
                    {
                        "name": "Mathieu D'aquin"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu D'aquin"
                },
                "arxiv_affiliation": "LORIA, UL",
                "author": "Mathieu D'aquin",
                "arxiv_comment": "Accepted at TOTh 2025 (Terminology \\& Ontology: Theories and\n  applications)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20541v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20541v4",
                "updated": "2025-09-05T08:42:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    42,
                    4,
                    4,
                    248,
                    0
                ],
                "published": "2025-07-28T05:56:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    5,
                    56,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic\n  Design"
                },
                "summary": "This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that\npresents a new paradigm for Automatic Heuristic Design (AHD). Traditional\nevolutionary methods operate directly on heuristic code; in contrast, MeLA\nevolves the instructional prompts used to guide a Large Language Model (LLM) in\ngenerating these heuristics. This process of \"prompt evolution\" is driven by a\nnovel metacognitive framework where the system analyzes performance feedback to\nsystematically refine its generative strategy. MeLA's architecture integrates a\nproblem analyzer to construct an initial strategic prompt, an error diagnosis\nsystem to repair faulty code, and a metacognitive search engine that\niteratively optimizes the prompt based on heuristic effectiveness. In\ncomprehensive experiments across both benchmark and real-world problems, MeLA\nconsistently generates more effective and robust heuristics, significantly\noutperforming state-of-the-art methods. Ultimately, this research demonstrates\nthe profound potential of using cognitive science as a blueprint for AI\narchitecture, revealing that by enabling an LLM to metacognitively regulate its\nproblem-solving process, we unlock a more robust and interpretable path to AHD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that\npresents a new paradigm for Automatic Heuristic Design (AHD). Traditional\nevolutionary methods operate directly on heuristic code; in contrast, MeLA\nevolves the instructional prompts used to guide a Large Language Model (LLM) in\ngenerating these heuristics. This process of \"prompt evolution\" is driven by a\nnovel metacognitive framework where the system analyzes performance feedback to\nsystematically refine its generative strategy. MeLA's architecture integrates a\nproblem analyzer to construct an initial strategic prompt, an error diagnosis\nsystem to repair faulty code, and a metacognitive search engine that\niteratively optimizes the prompt based on heuristic effectiveness. In\ncomprehensive experiments across both benchmark and real-world problems, MeLA\nconsistently generates more effective and robust heuristics, significantly\noutperforming state-of-the-art methods. Ultimately, this research demonstrates\nthe profound potential of using cognitive science as a blueprint for AI\narchitecture, revealing that by enabling an LLM to metacognitively regulate its\nproblem-solving process, we unlock a more robust and interpretable path to AHD."
                },
                "authors": [
                    {
                        "name": "Zishang Qiu"
                    },
                    {
                        "name": "Xinan Chen"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Ruibin Bai"
                    }
                ],
                "author_detail": {
                    "name": "Ruibin Bai"
                },
                "author": "Ruibin Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20541v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20541v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16172v2",
                "updated": "2025-09-05T08:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    26,
                    57,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-22T07:50:57Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    50,
                    57,
                    4,
                    234,
                    0
                ],
                "title": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent\n  with Preference Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent\n  with Preference Chain"
                },
                "summary": "Understanding human behavior in urban environments is a crucial field within\ncity sciences. However, collecting accurate behavioral data, particularly in\nnewly developed areas, poses significant challenges. Recent advances in\ngenerative agents, powered by Large Language Models (LLMs), have shown promise\nin simulating human behaviors without relying on extensive datasets.\nNevertheless, these methods often struggle with generating consistent,\ncontext-sensitive, and realistic behavioral outputs. To address these\nlimitations, this paper introduces the Preference Chain, a novel method that\nintegrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance\ncontext-aware simulation of human behavior in transportation systems.\nExperiments conducted on the Replica dataset demonstrate that the Preference\nChain outperforms standard LLM in aligning with real-world transportation mode\nchoices. The development of the Mobility Agent highlights potential\napplications of proposed method in urban mobility modeling for emerging cities,\npersonalized travel behavior analysis, and dynamic traffic forecasting. Despite\nlimitations such as slow inference and the risk of hallucination, the method\noffers a promising framework for simulating complex human behavior in\ndata-scarce environments, where traditional data-driven models struggle due to\nlimited data availability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human behavior in urban environments is a crucial field within\ncity sciences. However, collecting accurate behavioral data, particularly in\nnewly developed areas, poses significant challenges. Recent advances in\ngenerative agents, powered by Large Language Models (LLMs), have shown promise\nin simulating human behaviors without relying on extensive datasets.\nNevertheless, these methods often struggle with generating consistent,\ncontext-sensitive, and realistic behavioral outputs. To address these\nlimitations, this paper introduces the Preference Chain, a novel method that\nintegrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance\ncontext-aware simulation of human behavior in transportation systems.\nExperiments conducted on the Replica dataset demonstrate that the Preference\nChain outperforms standard LLM in aligning with real-world transportation mode\nchoices. The development of the Mobility Agent highlights potential\napplications of proposed method in urban mobility modeling for emerging cities,\npersonalized travel behavior analysis, and dynamic traffic forecasting. Despite\nlimitations such as slow inference and the risk of hallucination, the method\noffers a promising framework for simulating complex human behavior in\ndata-scarce environments, where traditional data-driven models struggle due to\nlimited data availability."
                },
                "authors": [
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Parfait Atchade-Adelomou"
                    },
                    {
                        "name": "Carlo Adornetto"
                    },
                    {
                        "name": "Adrian Mora-Carrero"
                    },
                    {
                        "name": "Luis Alonso-Pastor"
                    },
                    {
                        "name": "Ariel Noyman"
                    },
                    {
                        "name": "Yubo Liu"
                    },
                    {
                        "name": "Kent Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kent Larson"
                },
                "author": "Kent Larson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04908v1",
                "updated": "2025-09-05T08:24:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    24,
                    12,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:24:12Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    24,
                    12,
                    4,
                    248,
                    0
                ],
                "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and\n  Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and\n  Parsing"
                },
                "summary": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser."
                },
                "authors": [
                    {
                        "name": "Hongyi Jing"
                    },
                    {
                        "name": "Jiafu Chen"
                    },
                    {
                        "name": "Chen Rao"
                    },
                    {
                        "name": "Ziqiang Dang"
                    },
                    {
                        "name": "Jiajie Teng"
                    },
                    {
                        "name": "Tianyi Chu"
                    },
                    {
                        "name": "Juncheng Mo"
                    },
                    {
                        "name": "Shuo Fang"
                    },
                    {
                        "name": "Huaizhong Lin"
                    },
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Chenguang Ma"
                    },
                    {
                        "name": "Lei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhao"
                },
                "author": "Lei Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04905v1",
                "updated": "2025-09-05T08:23:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    23,
                    16,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:23:16Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    23,
                    16,
                    4,
                    248,
                    0
                ],
                "title": "Revolution or Hype? Seeking the Limits of Large Models in Hardware\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolution or Hype? Seeking the Limits of Large Models in Hardware\n  Design"
                },
                "summary": "Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models\n(LCMs) have sparked excitement across the electronic design automation (EDA)\ncommunity, promising a revolution in circuit design and optimization. Yet, this\nexcitement is met with significant skepticism: Are these AI models a genuine\nrevolution in circuit design, or a temporary wave of inflated expectations?\nThis paper serves as a foundational text for the corresponding ICCAD 2025\npanel, bringing together perspectives from leading experts in academia and\nindustry. It critically examines the practical capabilities, fundamental\nlimitations, and future prospects of large AI models in hardware design. The\npaper synthesizes the core arguments surrounding reliability, scalability, and\ninterpretability, framing the debate on whether these models can meaningfully\noutperform or complement traditional EDA methods. The result is an\nauthoritative overview offering fresh insights into one of today's most\ncontentious and impactful technology trends.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models\n(LCMs) have sparked excitement across the electronic design automation (EDA)\ncommunity, promising a revolution in circuit design and optimization. Yet, this\nexcitement is met with significant skepticism: Are these AI models a genuine\nrevolution in circuit design, or a temporary wave of inflated expectations?\nThis paper serves as a foundational text for the corresponding ICCAD 2025\npanel, bringing together perspectives from leading experts in academia and\nindustry. It critically examines the practical capabilities, fundamental\nlimitations, and future prospects of large AI models in hardware design. The\npaper synthesizes the core arguments surrounding reliability, scalability, and\ninterpretability, framing the debate on whether these models can meaningfully\noutperform or complement traditional EDA methods. The result is an\nauthoritative overview offering fresh insights into one of today's most\ncontentious and impactful technology trends."
                },
                "authors": [
                    {
                        "name": "Qiang Xu"
                    },
                    {
                        "name": "Leon Stok"
                    },
                    {
                        "name": "Rolf Drechsler"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Grace Li Zhang"
                    },
                    {
                        "name": "Igor L. Markov"
                    }
                ],
                "author_detail": {
                    "name": "Igor L. Markov"
                },
                "author": "Igor L. Markov",
                "arxiv_comment": "Invited paper to appear at ICCAD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04904v1",
                "updated": "2025-09-05T08:23:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    23,
                    14,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:23:14Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    23,
                    14,
                    4,
                    248,
                    0
                ],
                "title": "An information metric for comparing and assessing informative interim\n  decisions in sequential clinical trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An information metric for comparing and assessing informative interim\n  decisions in sequential clinical trials"
                },
                "summary": "Group sequential designs enable interim analyses and potential early stopping\nfor efficacy or futility. While these adaptations improve trial efficiency and\nethical considerations, they also introduce bias into the adapted analyses. We\ndemonstrate how failing to account for informative interim decisions in the\nanalysis can substantially affect posterior estimates of the treatment effect,\noften resulting in overly optimistic credible intervals aligned with the\nstopping decision. Drawing on information theory, we use the Kullback-Leibler\ndivergence to quantify this distortion and highlight its use for post-hoc\nevaluation of informative interim decisions, with a focus on end-of-study\ninference. Unlike pointwise comparisons, this measure provides an integrated\nsummary of this distortion on the whole parameter space. By comparing\nalternative decision boundaries and prior specifications, we illustrate how\nthis measure can improve the understanding of trial results and inform the\nplanning of future adaptive studies. We also introduce an expected version of\nthis metric to support clinicians in choosing decision boundaries. This\nguidance complements traditional strategies based on type-I error rate control\nby offering insights into the distortion introduced to the treatment effect at\neach interim phase. The use of this pre-experimental measure is finally\nillustrated in a group sequential trial for evaluating a treatment for central\nnervous system disorders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group sequential designs enable interim analyses and potential early stopping\nfor efficacy or futility. While these adaptations improve trial efficiency and\nethical considerations, they also introduce bias into the adapted analyses. We\ndemonstrate how failing to account for informative interim decisions in the\nanalysis can substantially affect posterior estimates of the treatment effect,\noften resulting in overly optimistic credible intervals aligned with the\nstopping decision. Drawing on information theory, we use the Kullback-Leibler\ndivergence to quantify this distortion and highlight its use for post-hoc\nevaluation of informative interim decisions, with a focus on end-of-study\ninference. Unlike pointwise comparisons, this measure provides an integrated\nsummary of this distortion on the whole parameter space. By comparing\nalternative decision boundaries and prior specifications, we illustrate how\nthis measure can improve the understanding of trial results and inform the\nplanning of future adaptive studies. We also introduce an expected version of\nthis metric to support clinicians in choosing decision boundaries. This\nguidance complements traditional strategies based on type-I error rate control\nby offering insights into the distortion introduced to the treatment effect at\neach interim phase. The use of this pre-experimental measure is finally\nillustrated in a group sequential trial for evaluating a treatment for central\nnervous system disorders."
                },
                "authors": [
                    {
                        "name": "G. Caruso"
                    },
                    {
                        "name": "W. F. Rosenberger"
                    },
                    {
                        "name": "P. Mozgunov"
                    },
                    {
                        "name": "N. Flournoy"
                    }
                ],
                "author_detail": {
                    "name": "N. Flournoy"
                },
                "author": "N. Flournoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04903v1",
                "updated": "2025-09-05T08:21:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    21,
                    41,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:21:41Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    21,
                    41,
                    4,
                    248,
                    0
                ],
                "title": "ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation\n  Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\nlong-context understanding, yet they face significant challenges in\nhigh-quality long-form generation. Existing studies primarily suffer from two\nlimitations: (1) A heavy reliance on scarce, high-quality long-form response\ndata for supervised fine-tuning (SFT) or for pairwise preference reward in\nreinforcement learning (RL). (2) Focus on coarse-grained quality optimization\ndimensions, such as relevance, coherence, and helpfulness, overlooking the\nfine-grained specifics inherent to diverse long-form generation scenarios. To\naddress this issue, we propose a framework using Adaptive Constraint-Enhanced\nreward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first\nautomatically deconstructs each instruction into a set of fine-grained,\nadaptive constraint criteria by identifying its underlying intents and demands.\nSubsequently, we design a reward mechanism that quantifies the quality of\nlong-form responses based on their satisfaction over corresponding constraints,\nconverting subjective quality evaluation into constraint verification. Finally,\nwe utilize reinforcement learning to guide models toward superior long-form\ngeneration capabilities. Experimental results demonstrate that our ACE-RL\nframework significantly outperforms existing SFT and RL baselines by 20.70% and\n7.32% on WritingBench, and our top-performing model even surpasses proprietary\nsystems like GPT-4o by 7.10%, providing a more effective training paradigm for\nLLMs to generate high-quality content across diverse long-form generation\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable progress in\nlong-context understanding, yet they face significant challenges in\nhigh-quality long-form generation. Existing studies primarily suffer from two\nlimitations: (1) A heavy reliance on scarce, high-quality long-form response\ndata for supervised fine-tuning (SFT) or for pairwise preference reward in\nreinforcement learning (RL). (2) Focus on coarse-grained quality optimization\ndimensions, such as relevance, coherence, and helpfulness, overlooking the\nfine-grained specifics inherent to diverse long-form generation scenarios. To\naddress this issue, we propose a framework using Adaptive Constraint-Enhanced\nreward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first\nautomatically deconstructs each instruction into a set of fine-grained,\nadaptive constraint criteria by identifying its underlying intents and demands.\nSubsequently, we design a reward mechanism that quantifies the quality of\nlong-form responses based on their satisfaction over corresponding constraints,\nconverting subjective quality evaluation into constraint verification. Finally,\nwe utilize reinforcement learning to guide models toward superior long-form\ngeneration capabilities. Experimental results demonstrate that our ACE-RL\nframework significantly outperforms existing SFT and RL baselines by 20.70% and\n7.32% on WritingBench, and our top-performing model even surpasses proprietary\nsystems like GPT-4o by 7.10%, providing a more effective training paradigm for\nLLMs to generate high-quality content across diverse long-form generation\nscenarios."
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Qixiang Yin"
                    },
                    {
                        "name": "Lingxing Kong"
                    },
                    {
                        "name": "Zhixing Tan"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Under review, our code is available at https://github.com/ZNLP/ACE-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04897v1",
                "updated": "2025-09-05T08:17:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    17,
                    59,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:17:59Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    17,
                    59,
                    4,
                    248,
                    0
                ],
                "title": "PLaMo 2 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLaMo 2 Technical Report"
                },
                "summary": "In this report, we introduce PLaMo 2, a series of Japanese-focused large\nlanguage models featuring a hybrid Samba-based architecture that transitions to\nfull attention via continual pre-training to support 32K token contexts.\nTraining leverages extensive synthetic corpora to overcome data scarcity, while\ncomputational efficiency is achieved through weight reuse and structured\npruning. This efficient pruning methodology produces an 8B model that achieves\nperformance comparable to our previous 100B model. Post-training further\nrefines the models using a pipeline of supervised fine-tuning (SFT) and direct\npreference optimization (DPO), enhanced by synthetic Japanese instruction data\nand model merging techniques. Optimized for inference using vLLM and\nquantization with minimal accuracy loss, the PLaMo 2 models achieve\nstate-of-the-art results on Japanese benchmarks, outperforming similarly-sized\nopen models in instruction-following, language fluency, and Japanese-specific\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we introduce PLaMo 2, a series of Japanese-focused large\nlanguage models featuring a hybrid Samba-based architecture that transitions to\nfull attention via continual pre-training to support 32K token contexts.\nTraining leverages extensive synthetic corpora to overcome data scarcity, while\ncomputational efficiency is achieved through weight reuse and structured\npruning. This efficient pruning methodology produces an 8B model that achieves\nperformance comparable to our previous 100B model. Post-training further\nrefines the models using a pipeline of supervised fine-tuning (SFT) and direct\npreference optimization (DPO), enhanced by synthetic Japanese instruction data\nand model merging techniques. Optimized for inference using vLLM and\nquantization with minimal accuracy loss, the PLaMo 2 models achieve\nstate-of-the-art results on Japanese benchmarks, outperforming similarly-sized\nopen models in instruction-following, language fluency, and Japanese-specific\nknowledge."
                },
                "authors": [
                    {
                        "name": "Preferred Networks"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Kaizaburo Chubachi"
                    },
                    {
                        "name": "Yasuhiro Fujita"
                    },
                    {
                        "name": "Shinichi Hemmi"
                    },
                    {
                        "name": "Yuta Hirokawa"
                    },
                    {
                        "name": "Toshiki Kataoka"
                    },
                    {
                        "name": "Goro Kobayashi"
                    },
                    {
                        "name": "Kenichi Maehashi"
                    },
                    {
                        "name": "Calvin Metzger"
                    },
                    {
                        "name": "Hiroaki Mikami"
                    },
                    {
                        "name": "Shogo Murai"
                    },
                    {
                        "name": "Daisuke Nishino"
                    },
                    {
                        "name": "Kento Nozawa"
                    },
                    {
                        "name": "Shintarou Okada"
                    },
                    {
                        "name": "Daisuke Okanohara"
                    },
                    {
                        "name": "Shunta Saito"
                    },
                    {
                        "name": "Shotaro Sano"
                    },
                    {
                        "name": "Shuji Suzuki"
                    },
                    {
                        "name": "Daisuke Tanaka"
                    },
                    {
                        "name": "Avinash Ummadisingu"
                    },
                    {
                        "name": "Hanqin Wang"
                    },
                    {
                        "name": "Sixue Wang"
                    },
                    {
                        "name": "Tianqi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Xu"
                },
                "author": "Tianqi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04884v1",
                "updated": "2025-09-05T08:03:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    3,
                    1,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:03:01Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    3,
                    1,
                    4,
                    248,
                    0
                ],
                "title": "L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning"
                },
                "summary": "The ability of Large Language Models (LLMs) to solve complex tasks has made\nthem crucial in the development of AI-based applications. However, the high\ncomputational requirements to fine-tune these LLMs on downstream tasks pose\nsignificant challenges, particularly when resources are limited. In response to\nthis challenge, we introduce L1RA, a novel technique aimed at dynamically\ndistributing the rank of low-rank adapters during fine-tuning using LoRA. Given\na rank budget (i.e., total sum of adapters rank), L1RA leverages L1\nregularisation to prune redundant ranks and redistribute them across adapters,\nthereby optimising resource utilisation. Through a series of comprehensive\nexperiments, we empirically demonstrate that L1RA maintains comparable or even\nreduced computational overhead compared to other LoRA variants, including the\nvanilla approach, while achieving same or better performances. Moreover, the\npost-training analysis of rank distribution unveiled insights into the specific\nmodel components requiring the most adaptation to align with the task\nobjective: the feed-forward layers and the attention output projection. These\nresults highlight the efficacy of L1RA in not only enhancing the efficiency of\nLLM fine-tuning, but also in providing valuable diagnostic information for\nmodel refinement and customisation. In conclusion, L1RA stands as a promising\ntechnique for advancing the performance and interpretability of LLM adaptation,\nparticularly in scenarios where computational resources are constrained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of Large Language Models (LLMs) to solve complex tasks has made\nthem crucial in the development of AI-based applications. However, the high\ncomputational requirements to fine-tune these LLMs on downstream tasks pose\nsignificant challenges, particularly when resources are limited. In response to\nthis challenge, we introduce L1RA, a novel technique aimed at dynamically\ndistributing the rank of low-rank adapters during fine-tuning using LoRA. Given\na rank budget (i.e., total sum of adapters rank), L1RA leverages L1\nregularisation to prune redundant ranks and redistribute them across adapters,\nthereby optimising resource utilisation. Through a series of comprehensive\nexperiments, we empirically demonstrate that L1RA maintains comparable or even\nreduced computational overhead compared to other LoRA variants, including the\nvanilla approach, while achieving same or better performances. Moreover, the\npost-training analysis of rank distribution unveiled insights into the specific\nmodel components requiring the most adaptation to align with the task\nobjective: the feed-forward layers and the attention output projection. These\nresults highlight the efficacy of L1RA in not only enhancing the efficiency of\nLLM fine-tuning, but also in providing valuable diagnostic information for\nmodel refinement and customisation. In conclusion, L1RA stands as a promising\ntechnique for advancing the performance and interpretability of LLM adaptation,\nparticularly in scenarios where computational resources are constrained."
                },
                "authors": [
                    {
                        "name": "Raul Singh"
                    },
                    {
                        "name": "Nicolo Brunello"
                    },
                    {
                        "name": "Vincenzo Scotti"
                    },
                    {
                        "name": "Mark James Carman"
                    }
                ],
                "author_detail": {
                    "name": "Mark James Carman"
                },
                "author": "Mark James Carman",
                "arxiv_comment": "Work published at ICNLSP 2025, waiting for publication link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16299v3",
                "updated": "2025-09-05T08:01:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    1,
                    17,
                    4,
                    248,
                    0
                ],
                "published": "2024-09-09T19:35:34Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    19,
                    35,
                    34,
                    0,
                    253,
                    0
                ],
                "title": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks\n  at Scale"
                },
                "summary": "Large Language Models (LLMs) have revolutionized software engineering (SE),\nshowcasing remarkable proficiency in various coding tasks. Despite recent\nadvancements that have enabled the creation of autonomous software agents\nutilizing LLMs for end-to-end development tasks, these systems are typically\ndesigned for specific SE functions. We introduce HyperAgent, an innovative\ngeneralist multi-agent system designed to tackle a wide range of SE tasks\nacross different programming languages by mimicking the workflows of human\ndevelopers. HyperAgent features four specialized agents-Planner, Navigator,\nCode Editor, and Executor-capable of handling the entire lifecycle of SE tasks,\nfrom initial planning to final verification. HyperAgent sets new benchmarks in\ndiverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench\nbenchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates\nexceptional performance in repository-level code generation (RepoExec) and\nfault localization and program repair (Defects4J), often surpassing\nstate-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized software engineering (SE),\nshowcasing remarkable proficiency in various coding tasks. Despite recent\nadvancements that have enabled the creation of autonomous software agents\nutilizing LLMs for end-to-end development tasks, these systems are typically\ndesigned for specific SE functions. We introduce HyperAgent, an innovative\ngeneralist multi-agent system designed to tackle a wide range of SE tasks\nacross different programming languages by mimicking the workflows of human\ndevelopers. HyperAgent features four specialized agents-Planner, Navigator,\nCode Editor, and Executor-capable of handling the entire lifecycle of SE tasks,\nfrom initial planning to final verification. HyperAgent sets new benchmarks in\ndiverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench\nbenchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates\nexceptional performance in repository-level code generation (RepoExec) and\nfault localization and program repair (Defects4J), often surpassing\nstate-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Huy Nhat Phan"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Phong X. Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "arxiv_comment": "49 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12226v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12226v4",
                "updated": "2025-09-05T07:56:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    56,
                    50,
                    4,
                    248,
                    0
                ],
                "published": "2024-02-19T15:33:10Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    15,
                    33,
                    10,
                    0,
                    50,
                    0
                ],
                "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling"
                },
                "summary": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/"
                },
                "authors": [
                    {
                        "name": "Jun Zhan"
                    },
                    {
                        "name": "Junqi Dai"
                    },
                    {
                        "name": "Jiasheng Ye"
                    },
                    {
                        "name": "Yunhua Zhou"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Linyang Li"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Tianxiang Sun"
                    },
                    {
                        "name": "Yugang Jiang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "28 pages, 16 figures, under review, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12226v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12226v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04877v1",
                "updated": "2025-09-05T07:46:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    46,
                    27,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T07:46:27Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    46,
                    27,
                    4,
                    248,
                    0
                ],
                "title": "Integrating Large Language Models in Software Engineering Education: A\n  Pilot Study through GitHub Repositories Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models in Software Engineering Education: A\n  Pilot Study through GitHub Repositories Mining"
                },
                "summary": "Context: Large Language Models (LLMs) such as ChatGPT are increasingly\nadopted in software engineering (SE) education, offering both opportunities and\nchallenges. Their adoption requires systematic investigation to ensure\nresponsible integration into curricula. Objective: This doctoral research aims\nto develop a validated framework for integrating LLMs into SE education through\na multi-phase process, including taxonomies development, empirical\ninvestigation, and case studies. This paper presents the first empirical step.\nMethod: We conducted a pilot repository mining study of 400 GitHub projects,\nanalyzing README files and issues discussions to identify the presence of\nmotivator and demotivator previously synthesized in our literature review [ 8]\nstudy. Results: Motivators such as engagement and motivation (227 hits),\nsoftware engineering process understanding (133 hits), and programming\nassistance and debugging support (97 hits) were strongly represented.\nDemotivators, including plagiarism and IP concerns (385 hits), security,\nprivacy and data integrity (87 hits), and over-reliance on AI in learning (39\nhits), also appeared prominently. In contrast, demotivators such as challenges\nin evaluating learning outcomes and difficulty in curriculum redesign recorded\nno hits across the repositories. Conclusion: The study provides early empirical\nvalidation of motivators/demotivators taxonomies with respect to their themes,\nhighlights research practice gaps, and lays the foundation for developing a\ncomprehensive framework to guide the responsible adoption of LLMs in SE\neducation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Large Language Models (LLMs) such as ChatGPT are increasingly\nadopted in software engineering (SE) education, offering both opportunities and\nchallenges. Their adoption requires systematic investigation to ensure\nresponsible integration into curricula. Objective: This doctoral research aims\nto develop a validated framework for integrating LLMs into SE education through\na multi-phase process, including taxonomies development, empirical\ninvestigation, and case studies. This paper presents the first empirical step.\nMethod: We conducted a pilot repository mining study of 400 GitHub projects,\nanalyzing README files and issues discussions to identify the presence of\nmotivator and demotivator previously synthesized in our literature review [ 8]\nstudy. Results: Motivators such as engagement and motivation (227 hits),\nsoftware engineering process understanding (133 hits), and programming\nassistance and debugging support (97 hits) were strongly represented.\nDemotivators, including plagiarism and IP concerns (385 hits), security,\nprivacy and data integrity (87 hits), and over-reliance on AI in learning (39\nhits), also appeared prominently. In contrast, demotivators such as challenges\nin evaluating learning outcomes and difficulty in curriculum redesign recorded\nno hits across the repositories. Conclusion: The study provides early empirical\nvalidation of motivators/demotivators taxonomies with respect to their themes,\nhighlights research practice gaps, and lays the foundation for developing a\ncomprehensive framework to guide the responsible adoption of LLMs in SE\neducation."
                },
                "authors": [
                    {
                        "name": "Maryam Khan"
                    },
                    {
                        "name": "Muhammad Azeem Akbar"
                    },
                    {
                        "name": "Jussi Kasurinen"
                    }
                ],
                "author_detail": {
                    "name": "Jussi Kasurinen"
                },
                "author": "Jussi Kasurinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04876v1",
                "updated": "2025-09-05T07:44:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    44,
                    5,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T07:44:05Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    44,
                    5,
                    4,
                    248,
                    0
                ],
                "title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in\n  Multi-Agent LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in\n  Multi-Agent LLM Collaboration"
                },
                "summary": "This paper introduces OSC (Orchestrating Cognitive Synergy), a\nknowledge-aware adaptive collaboration framework designed to enhance cognitive\nsynergy in multi-agent systems with large language models. While prior work has\nadvanced agent selection and result aggregation, efficient linguistic\ninteractions for deep collaboration among expert agents remain a critical\nbottleneck. OSC addresses this gap as a pivotal intermediate layer between\nselection and aggregation, introducing Collaborator Knowledge Models (CKM) to\nenable each agent to dynamically perceive its collaborators' cognitive states.\nThrough real-time cognitive gap analysis, agents adaptively adjust\ncommunication behaviors, including content focus, detail level, and expression\nstyle, using learned strategies. Experiments on complex reasoning and\nproblem-solving benchmarks demonstrate that OSC significantly improves task\nperformance and communication efficiency, transforming \"parallel-working\nindividuals'' into a \"deeply collaborative cognitive team.'' This framework not\nonly optimizes multi-agent collaboration but also offers new insights into LLM\nagent interaction behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces OSC (Orchestrating Cognitive Synergy), a\nknowledge-aware adaptive collaboration framework designed to enhance cognitive\nsynergy in multi-agent systems with large language models. While prior work has\nadvanced agent selection and result aggregation, efficient linguistic\ninteractions for deep collaboration among expert agents remain a critical\nbottleneck. OSC addresses this gap as a pivotal intermediate layer between\nselection and aggregation, introducing Collaborator Knowledge Models (CKM) to\nenable each agent to dynamically perceive its collaborators' cognitive states.\nThrough real-time cognitive gap analysis, agents adaptively adjust\ncommunication behaviors, including content focus, detail level, and expression\nstyle, using learned strategies. Experiments on complex reasoning and\nproblem-solving benchmarks demonstrate that OSC significantly improves task\nperformance and communication efficiency, transforming \"parallel-working\nindividuals'' into a \"deeply collaborative cognitive team.'' This framework not\nonly optimizes multi-agent collaboration but also offers new insights into LLM\nagent interaction behaviors."
                },
                "authors": [
                    {
                        "name": "Jusheng Zhang"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Xiaofei Sun"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "arxiv_comment": "Accepted at EMNLP 2025 (Long Paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04871v1",
                "updated": "2025-09-05T07:36:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    36,
                    12,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T07:36:12Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    36,
                    12,
                    4,
                    248,
                    0
                ],
                "title": "Cloning a Conversational Voice AI Agent from Call\\,Recording Datasets\n  for Telesales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloning a Conversational Voice AI Agent from Call\\,Recording Datasets\n  for Telesales"
                },
                "summary": "Recent advances in language and speech modelling have made it possible to\nbuild autonomous voice assistants that understand and generate human dialogue\nin real time. These systems are increasingly being deployed in domains such as\ncustomer service and healthcare care, where they can automate repetitive tasks,\nreduce operational costs, and provide constant support around the clock. In\nthis paper, we present a general methodology for cloning a conversational voice\nAI agent from a corpus of call recordings. Although the case study described in\nthis paper uses telesales data to illustrate the approach, the underlying\nprocess generalizes to any domain where call transcripts are available. Our\nsystem listens to customers over the telephone, responds with a synthetic\nvoice, and follows a structured playbook learned from top performing human\nagents. We describe the domain selection, knowledge extraction, and prompt\nengineering used to construct the agent, integrating automatic speech\nrecognition, a large language model based dialogue manager, and text to speech\nsynthesis into a streaming inference pipeline. The cloned agent is evaluated\nagainst human agents on a rubric of 22 criteria covering introduction, product\ncommunication, sales drive, objection handling, and closing. Blind tests show\nthat the AI agent approaches human performance in routine aspects of the call\nwhile underperforming in persuasion and objection handling. We analyze these\nshortcomings and refine the prompt accordingly. The paper concludes with design\nlessons and avenues for future research, including large scale simulation and\nautomated evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in language and speech modelling have made it possible to\nbuild autonomous voice assistants that understand and generate human dialogue\nin real time. These systems are increasingly being deployed in domains such as\ncustomer service and healthcare care, where they can automate repetitive tasks,\nreduce operational costs, and provide constant support around the clock. In\nthis paper, we present a general methodology for cloning a conversational voice\nAI agent from a corpus of call recordings. Although the case study described in\nthis paper uses telesales data to illustrate the approach, the underlying\nprocess generalizes to any domain where call transcripts are available. Our\nsystem listens to customers over the telephone, responds with a synthetic\nvoice, and follows a structured playbook learned from top performing human\nagents. We describe the domain selection, knowledge extraction, and prompt\nengineering used to construct the agent, integrating automatic speech\nrecognition, a large language model based dialogue manager, and text to speech\nsynthesis into a streaming inference pipeline. The cloned agent is evaluated\nagainst human agents on a rubric of 22 criteria covering introduction, product\ncommunication, sales drive, objection handling, and closing. Blind tests show\nthat the AI agent approaches human performance in routine aspects of the call\nwhile underperforming in persuasion and objection handling. We analyze these\nshortcomings and refine the prompt accordingly. The paper concludes with design\nlessons and avenues for future research, including large scale simulation and\nautomated evaluation."
                },
                "authors": [
                    {
                        "name": "Krittanon Kaewtawee"
                    },
                    {
                        "name": "Wachiravit Modecrua"
                    },
                    {
                        "name": "Krittin Pachtrachai"
                    },
                    {
                        "name": "Touchapon Kraisingkorn"
                    }
                ],
                "author_detail": {
                    "name": "Touchapon Kraisingkorn"
                },
                "author": "Touchapon Kraisingkorn",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04868v1",
                "updated": "2025-09-05T07:30:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    30,
                    40,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T07:30:40Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    30,
                    40,
                    4,
                    248,
                    0
                ],
                "title": "Using LLMs for Multilingual Clinical Entity Linking to ICD-10",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs for Multilingual Clinical Entity Linking to ICD-10"
                },
                "summary": "The linking of clinical entities is a crucial part of extracting structured\ninformation from clinical texts. It is the process of assigning a code from a\nmedical ontology or classification to a phrase in the text. The International\nClassification of Diseases - 10th revision (ICD-10) is an international\nstandard for classifying diseases for statistical and insurance purposes.\nAutomatically assigning the correct ICD-10 code to terms in discharge summaries\nwill simplify the work of healthcare professionals and ensure consistent coding\nin hospitals. Our paper proposes an approach for linking clinical terms to\nICD-10 codes in different languages using Large Language Models (LLMs). The\napproach consists of a multistage pipeline that uses clinical dictionaries to\nmatch unambiguous terms in the text and then applies in-context learning with\nGPT-4.1 to predict the ICD-10 code for the terms that do not match the\ndictionary. Our system shows promising results in predicting ICD-10 codes on\ndifferent benchmark datasets in Spanish - 0.89 F1 for categories and 0.78 F1 on\nsubcategories on CodiEsp, and Greek - 0.85 F1 on ElCardioCC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linking of clinical entities is a crucial part of extracting structured\ninformation from clinical texts. It is the process of assigning a code from a\nmedical ontology or classification to a phrase in the text. The International\nClassification of Diseases - 10th revision (ICD-10) is an international\nstandard for classifying diseases for statistical and insurance purposes.\nAutomatically assigning the correct ICD-10 code to terms in discharge summaries\nwill simplify the work of healthcare professionals and ensure consistent coding\nin hospitals. Our paper proposes an approach for linking clinical terms to\nICD-10 codes in different languages using Large Language Models (LLMs). The\napproach consists of a multistage pipeline that uses clinical dictionaries to\nmatch unambiguous terms in the text and then applies in-context learning with\nGPT-4.1 to predict the ICD-10 code for the terms that do not match the\ndictionary. Our system shows promising results in predicting ICD-10 codes on\ndifferent benchmark datasets in Spanish - 0.89 F1 for categories and 0.78 F1 on\nsubcategories on CodiEsp, and Greek - 0.85 F1 on ElCardioCC."
                },
                "authors": [
                    {
                        "name": "Sylvia Vassileva"
                    },
                    {
                        "name": "Ivan Koychev"
                    },
                    {
                        "name": "Svetla Boytcheva"
                    }
                ],
                "author_detail": {
                    "name": "Svetla Boytcheva"
                },
                "author": "Svetla Boytcheva",
                "arxiv_comment": "7 pages, 2 Figures, to be published in Proceedings of the 15th\n  International Conference on Recent Advances in Natural Language Processing,\n  RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04866v1",
                "updated": "2025-09-05T07:30:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    30,
                    1,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T07:30:01Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    30,
                    1,
                    4,
                    248,
                    0
                ],
                "title": "Memorization $\\neq$ Understanding: Do Large Language Models Have the\n  Ability of Scenario Cognition?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization $\\neq$ Understanding: Do Large Language Models Have the\n  Ability of Scenario Cognition?"
                },
                "summary": "Driven by vast and diverse textual data, large language models (LLMs) have\ndemonstrated impressive performance across numerous natural language processing\n(NLP) tasks. Yet, a critical question persists: does their generalization arise\nfrom mere memorization of training data or from deep semantic understanding? To\ninvestigate this, we propose a bi-perspective evaluation framework to assess\nLLMs' scenario cognition - the ability to link semantic scenario elements with\ntheir arguments in context. Specifically, we introduce a novel scenario-based\ndataset comprising diverse textual descriptions of fictional facts, annotated\nwith scenario elements. LLMs are evaluated through their capacity to answer\nscenario-related questions (model output perspective) and via probing their\ninternal representations for encoded scenario elements-argument associations\n(internal representation perspective). Our experiments reveal that current LLMs\npredominantly rely on superficial memorization, failing to achieve robust\nsemantic scenario cognition, even in simple cases. These findings expose\ncritical limitations in LLMs' semantic understanding and offer cognitive\ninsights for advancing their capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by vast and diverse textual data, large language models (LLMs) have\ndemonstrated impressive performance across numerous natural language processing\n(NLP) tasks. Yet, a critical question persists: does their generalization arise\nfrom mere memorization of training data or from deep semantic understanding? To\ninvestigate this, we propose a bi-perspective evaluation framework to assess\nLLMs' scenario cognition - the ability to link semantic scenario elements with\ntheir arguments in context. Specifically, we introduce a novel scenario-based\ndataset comprising diverse textual descriptions of fictional facts, annotated\nwith scenario elements. LLMs are evaluated through their capacity to answer\nscenario-related questions (model output perspective) and via probing their\ninternal representations for encoded scenario elements-argument associations\n(internal representation perspective). Our experiments reveal that current LLMs\npredominantly rely on superficial memorization, failing to achieve robust\nsemantic scenario cognition, even in simple cases. These findings expose\ncritical limitations in LLMs' semantic understanding and offer cognitive\ninsights for advancing their capabilities."
                },
                "authors": [
                    {
                        "name": "Boxiang Ma"
                    },
                    {
                        "name": "Ru Li"
                    },
                    {
                        "name": "Yuanlong Wang"
                    },
                    {
                        "name": "Hongye Tan"
                    },
                    {
                        "name": "Xiaoli Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoli Li"
                },
                "author": "Xiaoli Li",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03975v2",
                "updated": "2025-09-05T07:22:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    22,
                    43,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T08:01:27Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    1,
                    27,
                    3,
                    247,
                    0
                ],
                "title": "Improving Vessel Segmentation with Multi-Task Learning and Auxiliary\n  Data Available Only During Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Vessel Segmentation with Multi-Task Learning and Auxiliary\n  Data Available Only During Model Training"
                },
                "summary": "Liver vessel segmentation in magnetic resonance imaging data is important for\nthe computational analysis of vascular remodelling, associated with a wide\nspectrum of diffuse liver diseases. Existing approaches rely on contrast\nenhanced imaging data, but the necessary dedicated imaging sequences are not\nuniformly acquired. Images without contrast enhancement are acquired more\nfrequently, but vessel segmentation is challenging, and requires large-scale\nannotated data. We propose a multi-task learning framework to segment vessels\nin liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data\navailable only during training to reduce the need for annotated training\nexamples. Our approach draws on paired native and contrast enhanced data with\nand without vessel annotations for model training. Results show that auxiliary\ndata improves the accuracy of vessel segmentation, even if they are not\navailable during inference. The advantage is most pronounced if only few\nannotations are available for training, since the feature representation\nbenefits from the shared task structure. A validation of this approach to\naugment a model for brain tumor segmentation confirms its benefits across\ndifferent domains. An auxiliary informative imaging modality can augment expert\nannotations even if it is only available during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liver vessel segmentation in magnetic resonance imaging data is important for\nthe computational analysis of vascular remodelling, associated with a wide\nspectrum of diffuse liver diseases. Existing approaches rely on contrast\nenhanced imaging data, but the necessary dedicated imaging sequences are not\nuniformly acquired. Images without contrast enhancement are acquired more\nfrequently, but vessel segmentation is challenging, and requires large-scale\nannotated data. We propose a multi-task learning framework to segment vessels\nin liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data\navailable only during training to reduce the need for annotated training\nexamples. Our approach draws on paired native and contrast enhanced data with\nand without vessel annotations for model training. Results show that auxiliary\ndata improves the accuracy of vessel segmentation, even if they are not\navailable during inference. The advantage is most pronounced if only few\nannotations are available for training, since the feature representation\nbenefits from the shared task structure. A validation of this approach to\naugment a model for brain tumor segmentation confirms its benefits across\ndifferent domains. An auxiliary informative imaging modality can augment expert\nannotations even if it is only available during training."
                },
                "authors": [
                    {
                        "name": "Daniel Sobotka"
                    },
                    {
                        "name": "Alexander Herold"
                    },
                    {
                        "name": "Matthias Perkonigg"
                    },
                    {
                        "name": "Lucian Beer"
                    },
                    {
                        "name": "Nina Bastati"
                    },
                    {
                        "name": "Alina Sablatnig"
                    },
                    {
                        "name": "Ahmed Ba-Ssalamah"
                    },
                    {
                        "name": "Georg Langs"
                    }
                ],
                "author_detail": {
                    "name": "Georg Langs"
                },
                "author": "Georg Langs",
                "arxiv_doi": "10.1016/j.compmedimag.2024.102369",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.compmedimag.2024.102369",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.03975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Computerized Medical Imaging and Graphics Volume 114, June 2024,\n  102369",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11813v2",
                "updated": "2025-09-05T07:15:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    15,
                    34,
                    4,
                    248,
                    0
                ],
                "published": "2024-08-21T17:58:02Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    58,
                    2,
                    2,
                    234,
                    0
                ],
                "title": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual\n  Integration in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual\n  Integration in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities by integrating visual and textual inputs, yet modality alignment\nremains one of the most challenging aspects. Current MLLMs typically rely on\nsimple adapter architectures and pretraining approaches to bridge vision\nencoders with large language models (LLM), guided by image-level supervision.\nWe identify this paradigm often leads to suboptimal alignment between\nmodalities, significantly constraining the LLM's ability to properly interpret\nand reason with visual features particularly for smaller language models. This\nlimitation degrades overall performance-particularly for smaller language\nmodels where capacity constraints are more pronounced and adaptation\ncapabilities are limited. To address this fundamental limitation, we propose\nSupervised Embedding Alignment (SEA), a token-level supervision alignment\nmethod that enables more precise visual-text alignment during pretraining. SEA\nintroduces minimal computational overhead while preserving language\ncapabilities and substantially improving cross-modal understanding. Our\ncomprehensive analyses reveal critical insights into the adapter's role in\nmultimodal integration, and extensive experiments demonstrate that SEA\nconsistently improves performance across various model sizes, with smaller\nmodels benefiting the most (average performance gain of 7.61% for Gemma-2B).\nThis work establishes a foundation for developing more effective alignment\nstrategies for future multimodal systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities by integrating visual and textual inputs, yet modality alignment\nremains one of the most challenging aspects. Current MLLMs typically rely on\nsimple adapter architectures and pretraining approaches to bridge vision\nencoders with large language models (LLM), guided by image-level supervision.\nWe identify this paradigm often leads to suboptimal alignment between\nmodalities, significantly constraining the LLM's ability to properly interpret\nand reason with visual features particularly for smaller language models. This\nlimitation degrades overall performance-particularly for smaller language\nmodels where capacity constraints are more pronounced and adaptation\ncapabilities are limited. To address this fundamental limitation, we propose\nSupervised Embedding Alignment (SEA), a token-level supervision alignment\nmethod that enables more precise visual-text alignment during pretraining. SEA\nintroduces minimal computational overhead while preserving language\ncapabilities and substantially improving cross-modal understanding. Our\ncomprehensive analyses reveal critical insights into the adapter's role in\nmultimodal integration, and extensive experiments demonstrate that SEA\nconsistently improves performance across various model sizes, with smaller\nmodels benefiting the most (average performance gain of 7.61% for Gemma-2B).\nThis work establishes a foundation for developing more effective alignment\nstrategies for future multimodal systems."
                },
                "authors": [
                    {
                        "name": "Yuanyang Yin"
                    },
                    {
                        "name": "Yaqi Zhao"
                    },
                    {
                        "name": "Yajie Zhang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Ke Lin"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04852v1",
                "updated": "2025-09-05T07:06:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    6,
                    56,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T07:06:56Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    6,
                    56,
                    4,
                    248,
                    0
                ],
                "title": "Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment"
                },
                "summary": "Estimating density ratios is a fundamental problem in machine learning, but\nexisting methods often trade off accuracy for efficiency. We propose\n\\textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)},\na framework that enables accurate, any-step estimation without numerical\nintegration.\n  Instead of modeling infinitesimal tangents as in prior methods, ISA-DRE\nlearns a global secant function, defined as the expectation of all tangents\nover an interval, with provably lower variance, making it more suitable for\nneural approximation. This is made possible by the \\emph{Secant Alignment\nIdentity}, a self-consistency condition that formally connects the secant with\nits underlying tangent representations.\n  To mitigate instability during early training, we introduce \\emph{Contraction\nInterval Annealing}, a curriculum strategy that gradually expands the alignment\ninterval during training. This process induces a contraction mapping, which\nimproves convergence and training stability.\n  Empirically, ISA-DRE achieves competitive accuracy with significantly fewer\nfunction evaluations compared to prior methods, resulting in much faster\ninference and making it well suited for real-time and interactive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating density ratios is a fundamental problem in machine learning, but\nexisting methods often trade off accuracy for efficiency. We propose\n\\textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)},\na framework that enables accurate, any-step estimation without numerical\nintegration.\n  Instead of modeling infinitesimal tangents as in prior methods, ISA-DRE\nlearns a global secant function, defined as the expectation of all tangents\nover an interval, with provably lower variance, making it more suitable for\nneural approximation. This is made possible by the \\emph{Secant Alignment\nIdentity}, a self-consistency condition that formally connects the secant with\nits underlying tangent representations.\n  To mitigate instability during early training, we introduce \\emph{Contraction\nInterval Annealing}, a curriculum strategy that gradually expands the alignment\ninterval during training. This process induces a contraction mapping, which\nimproves convergence and training stability.\n  Empirically, ISA-DRE achieves competitive accuracy with significantly fewer\nfunction evaluations compared to prior methods, resulting in much faster\ninference and making it well suited for real-time and interactive applications."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Shigui Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Zhiqi Lin"
                    },
                    {
                        "name": "Junmei Yang"
                    },
                    {
                        "name": "Delu Zeng"
                    },
                    {
                        "name": "John Paisley"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04836v1",
                "updated": "2025-09-05T06:37:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    37,
                    33,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T06:37:33Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    37,
                    33,
                    4,
                    248,
                    0
                ],
                "title": "COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of\n  Everyday Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of\n  Everyday Tasks"
                },
                "summary": "Continuous advancements in robotics and AI are driving the integration of\nrobots from industry into everyday environments. However, dynamic and\nunpredictable human activities in daily lives would directly or indirectly\nconflict with robot actions. Besides, due to the social attributes of such\nhuman-induced conflicts, solutions are not always unique and depend highly on\nthe user's personal preferences. To address these challenges and facilitate the\ndevelopment of household robots, we propose COMMET, a system for human-induced\nCOnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid\ndetection approach, which begins with multi-modal retrieval and escalates to\nfine-tuned model inference for low-confidence cases. Based on collected user\npreferred options and settings, GPT-4o will be used to summarize user\npreferences from relevant cases. In preliminary studies, our detection module\nshows better accuracy and latency compared with GPT models. To facilitate\nfuture research, we also design a user-friendly interface for user data\ncollection and demonstrate an effective workflow for real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous advancements in robotics and AI are driving the integration of\nrobots from industry into everyday environments. However, dynamic and\nunpredictable human activities in daily lives would directly or indirectly\nconflict with robot actions. Besides, due to the social attributes of such\nhuman-induced conflicts, solutions are not always unique and depend highly on\nthe user's personal preferences. To address these challenges and facilitate the\ndevelopment of household robots, we propose COMMET, a system for human-induced\nCOnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid\ndetection approach, which begins with multi-modal retrieval and escalates to\nfine-tuned model inference for low-confidence cases. Based on collected user\npreferred options and settings, GPT-4o will be used to summarize user\npreferences from relevant cases. In preliminary studies, our detection module\nshows better accuracy and latency compared with GPT models. To facilitate\nfuture research, we also design a user-friendly interface for user data\ncollection and demonstrate an effective workflow for real-world deployments."
                },
                "authors": [
                    {
                        "name": "Dongping Li"
                    },
                    {
                        "name": "Shaoting Peng"
                    },
                    {
                        "name": "John Pohovey"
                    },
                    {
                        "name": "Katherine Rose Driggs-Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Katherine Rose Driggs-Campbell"
                },
                "author": "Katherine Rose Driggs-Campbell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08613v3",
                "updated": "2025-09-05T06:35:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    35,
                    41,
                    4,
                    248,
                    0
                ],
                "published": "2025-01-15T06:22:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    22,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Assessing the Sensitivity and Alignment of FOL Closeness Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Sensitivity and Alignment of FOL Closeness Metrics"
                },
                "summary": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage (NL) statements into First-Order Logic~(FOL) and external theorem\nprovers. However, the correctness of FOL statements, comprising operators and\ntext, often go unverified due to the lack of a reliable evaluation metric for\ncomparing generated and ground-truth FOLs. In this paper, we conduct a\ncomprehensive study on the sensitivity of existing NL-, FOL-, and graph-based\nmetrics to capture differences between a sampled FOL and its corresponding\nground-truth. We then measure the alignment between a metric-based ranking of\nFOL outputs and a strong LLM as-a-judge. To do this, we first apply operator\nand text-based perturbations to ground-truth FOL statements to assess metric\nsensitivity. We then evaluate metric robustness by comparing the metrics\nagainst LLMs judgment. Our empirical findings highlight a clear oversensitivity\nin the n-gram metric BLEU for text perturbations. The operator perturbation\naffects the semantic graph metric Smatch++ for structural changes, and the FOL\nmetric for specific operator changes. We observe a closer alignment between\nBertScore and LLM judgement, proving the importance of semantic evaluation.\nAdditionally, we show that combining metrics enhances both robustness and\nsensitivity compared to using individual metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage (NL) statements into First-Order Logic~(FOL) and external theorem\nprovers. However, the correctness of FOL statements, comprising operators and\ntext, often go unverified due to the lack of a reliable evaluation metric for\ncomparing generated and ground-truth FOLs. In this paper, we conduct a\ncomprehensive study on the sensitivity of existing NL-, FOL-, and graph-based\nmetrics to capture differences between a sampled FOL and its corresponding\nground-truth. We then measure the alignment between a metric-based ranking of\nFOL outputs and a strong LLM as-a-judge. To do this, we first apply operator\nand text-based perturbations to ground-truth FOL statements to assess metric\nsensitivity. We then evaluate metric robustness by comparing the metrics\nagainst LLMs judgment. Our empirical findings highlight a clear oversensitivity\nin the n-gram metric BLEU for text perturbations. The operator perturbation\naffects the semantic graph metric Smatch++ for structural changes, and the FOL\nmetric for specific operator changes. We observe a closer alignment between\nBertScore and LLM judgement, proving the importance of semantic evaluation.\nAdditionally, we show that combining metrics enhances both robustness and\nsensitivity compared to using individual metrics."
                },
                "authors": [
                    {
                        "name": "Ramya Keerthy Thatikonda"
                    },
                    {
                        "name": "Wray Buntine"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Shareghi"
                },
                "author": "Ehsan Shareghi",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17439v2",
                "updated": "2025-09-05T06:26:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    26,
                    18,
                    4,
                    248,
                    0
                ],
                "published": "2025-07-23T11:58:54Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    58,
                    54,
                    2,
                    204,
                    0
                ],
                "title": "Doubly robust outlier resistant inference on causal treatment effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly robust outlier resistant inference on causal treatment effect"
                },
                "summary": "Outliers can severely distort causal effect estimation in observational\nstudies, especially in small samples. We develop a doubly robust estimator of\nthe ATE under a contaminated-data model that explicitly accommodates outliers.\nRobustness to outliers is delivered via a bounded-influence estimating equation\nfor the outcome model and covariate balancing propensity scores (CBPS) for\ntreatment assignment. To mitigate overfitting in high dimensions, we\nincorporate variable selection and unify all components within a penalized\nempirical likelihood framework. For further inference, we derive an optimal\nfinite-sample confidence interval (CI) whose endpoints are invariant to\noutliers under the contaminated model. Across extensive simulations and two\ngene-expression applications (Golub; Khan pediatric tumor), the proposed ATE\nestimator and finite-sample CI outperform state-of-the-art competitors in bias,\nmean squared error, empirical coverage, and interval length over a wide range\nof contamination levels and sample sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outliers can severely distort causal effect estimation in observational\nstudies, especially in small samples. We develop a doubly robust estimator of\nthe ATE under a contaminated-data model that explicitly accommodates outliers.\nRobustness to outliers is delivered via a bounded-influence estimating equation\nfor the outcome model and covariate balancing propensity scores (CBPS) for\ntreatment assignment. To mitigate overfitting in high dimensions, we\nincorporate variable selection and unify all components within a penalized\nempirical likelihood framework. For further inference, we derive an optimal\nfinite-sample confidence interval (CI) whose endpoints are invariant to\noutliers under the contaminated model. Across extensive simulations and two\ngene-expression applications (Golub; Khan pediatric tumor), the proposed ATE\nestimator and finite-sample CI outperform state-of-the-art competitors in bias,\nmean squared error, empirical coverage, and interval length over a wide range\nof contamination levels and sample sizes."
                },
                "authors": [
                    {
                        "name": "Byeonghee Lee"
                    },
                    {
                        "name": "Juhyun Park"
                    },
                    {
                        "name": "Saebom Jeon"
                    },
                    {
                        "name": "Joonsung Kang"
                    }
                ],
                "author_detail": {
                    "name": "Joonsung Kang"
                },
                "author": "Joonsung Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21433v2",
                "updated": "2025-09-05T06:16:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    16,
                    59,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-29T09:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    2,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM\n  Summarization for Agent Context Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM\n  Summarization for Agent Context Management"
                },
                "summary": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility"
                },
                "authors": [
                    {
                        "name": "Tobias Lindenbauer"
                    },
                    {
                        "name": "Igor Slinko"
                    },
                    {
                        "name": "Ludwig Felder"
                    },
                    {
                        "name": "Egor Bogomolov"
                    },
                    {
                        "name": "Yaroslav Zharov"
                    }
                ],
                "author_detail": {
                    "name": "Yaroslav Zharov"
                },
                "author": "Yaroslav Zharov",
                "arxiv_comment": "v2: Fixed typos and formatting issues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03959v2",
                "updated": "2025-09-05T06:09:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    9,
                    30,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T07:36:43Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    7,
                    36,
                    43,
                    3,
                    247,
                    0
                ],
                "title": "WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with\n  Multi-dimensional Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with\n  Multi-dimensional Annotation"
                },
                "summary": "The development of speech understanding and generation has been significantly\naccelerated by the availability of large-scale, high-quality speech datasets.\nAmong these, ASR and TTS are regarded as the most established and fundamental\ntasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9\nmillion native speakers worldwide, limited annotated resources have hindered\nprogress and resulted in suboptimal ASR and TTS performance. To address this\nchallenge, we propose WenetSpeech-Pipe, an integrated pipeline for building\nlarge-scale speech corpus with multi-dimensional annotation tailored for speech\nunderstanding and generation. It comprises six modules: Audio Collection,\nSpeaker Attributes Annotation, Speech Quality Annotation, Automatic Speech\nRecognition, Text Postprocessing and Recognizer Output Voting, enabling rich\nand high-quality annotations. Based on this pipeline, we release\nWenetSpeech-Yue, the first large-scale Cantonese speech corpus with\nmulti-dimensional annotation for ASR and TTS, covering 21,800 hours across 10\ndomains with annotations including ASR transcription, text confidence, speaker\nidentity, age, gender, speech quality scores, among other annotations. We also\nrelease WSYue-eval, a comprehensive Cantonese benchmark with two components:\nWSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long\nutterances, code-switching, and diverse acoustic conditions, and\nWSYue-TTS-eval, with base and coverage subsets for standard and generalization\ntesting. Experimental results show that models trained on WenetSpeech-Yue\nachieve competitive results against state-of-the-art (SOTA) Cantonese ASR and\nTTS systems, including commercial and LLM-based models, highlighting the value\nof our dataset and pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of speech understanding and generation has been significantly\naccelerated by the availability of large-scale, high-quality speech datasets.\nAmong these, ASR and TTS are regarded as the most established and fundamental\ntasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9\nmillion native speakers worldwide, limited annotated resources have hindered\nprogress and resulted in suboptimal ASR and TTS performance. To address this\nchallenge, we propose WenetSpeech-Pipe, an integrated pipeline for building\nlarge-scale speech corpus with multi-dimensional annotation tailored for speech\nunderstanding and generation. It comprises six modules: Audio Collection,\nSpeaker Attributes Annotation, Speech Quality Annotation, Automatic Speech\nRecognition, Text Postprocessing and Recognizer Output Voting, enabling rich\nand high-quality annotations. Based on this pipeline, we release\nWenetSpeech-Yue, the first large-scale Cantonese speech corpus with\nmulti-dimensional annotation for ASR and TTS, covering 21,800 hours across 10\ndomains with annotations including ASR transcription, text confidence, speaker\nidentity, age, gender, speech quality scores, among other annotations. We also\nrelease WSYue-eval, a comprehensive Cantonese benchmark with two components:\nWSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long\nutterances, code-switching, and diverse acoustic conditions, and\nWSYue-TTS-eval, with base and coverage subsets for standard and generalization\ntesting. Experimental results show that models trained on WenetSpeech-Yue\nachieve competitive results against state-of-the-art (SOTA) Cantonese ASR and\nTTS systems, including commercial and LLM-based models, highlighting the value\nof our dataset and pipeline."
                },
                "authors": [
                    {
                        "name": "Longhao Li"
                    },
                    {
                        "name": "Zhao Guo"
                    },
                    {
                        "name": "Hongjie Chen"
                    },
                    {
                        "name": "Yuhang Dai"
                    },
                    {
                        "name": "Ziyu Zhang"
                    },
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "Tianlun Zuo"
                    },
                    {
                        "name": "Chengyou Wang"
                    },
                    {
                        "name": "Shuiyuan Wang"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Hui Bu"
                    },
                    {
                        "name": "Binbin Zhang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Ziya Zhou"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04827v1",
                "updated": "2025-09-05T05:58:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    58,
                    16,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:58:16Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    58,
                    16,
                    4,
                    248,
                    0
                ],
                "title": "VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing\n  for Energy-Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing\n  for Energy-Efficient LLM Serving"
                },
                "summary": "Modern Large Language Model (LLM) serving systems increasingly support\ninteractive applications, like real-time chat assistants, code generation\ntools, and agentic workflows. However, the soaring energy cost of LLM inference\npresents a growing challenge for sustainable and cost-effective deployment.\nThis paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM\nserving, built from a control theory perspective. VoltanaLLM co-designs\nfrequency scaling and request routing in emerging prefill/decode disaggregated\narchitectures, leveraging their decoupled execution to enable fine-grained\nphase-specific control. It consists of a feedback-driven frequency controller\nthat dynamically adapts GPU frequency for prefill and decode phases, and a\nstate-space router that explores routing decisions across frequency-scaled\ninstances to minimize energy under latency constraints. We implement VoltanaLLM\nin SGLang and evaluate its performance over multiple state-of-the-art LLMs and\nreal-world datasets. The results demonstrate that VoltanaLLM achieves up to\n36.3% energy savings while maintaining near-perfect SLO attainment rate, paving\nthe way for sustainable and intelligent LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model (LLM) serving systems increasingly support\ninteractive applications, like real-time chat assistants, code generation\ntools, and agentic workflows. However, the soaring energy cost of LLM inference\npresents a growing challenge for sustainable and cost-effective deployment.\nThis paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM\nserving, built from a control theory perspective. VoltanaLLM co-designs\nfrequency scaling and request routing in emerging prefill/decode disaggregated\narchitectures, leveraging their decoupled execution to enable fine-grained\nphase-specific control. It consists of a feedback-driven frequency controller\nthat dynamically adapts GPU frequency for prefill and decode phases, and a\nstate-space router that explores routing decisions across frequency-scaled\ninstances to minimize energy under latency constraints. We implement VoltanaLLM\nin SGLang and evaluate its performance over multiple state-of-the-art LLMs and\nreal-world datasets. The results demonstrate that VoltanaLLM achieves up to\n36.3% energy savings while maintaining near-perfect SLO attainment rate, paving\nthe way for sustainable and intelligent LLM serving."
                },
                "authors": [
                    {
                        "name": "Jiahuan Yu"
                    },
                    {
                        "name": "Aryan Taneja"
                    },
                    {
                        "name": "Junfeng Lin"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "arxiv_affiliation": "University of Illinois Urbana-Champaign",
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04821v1",
                "updated": "2025-09-05T05:45:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    45,
                    7,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:45:07Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    45,
                    7,
                    4,
                    248,
                    0
                ],
                "title": "AFD-SLU: Adaptive Feature Distillation for Spoken Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AFD-SLU: Adaptive Feature Distillation for Spoken Language Understanding"
                },
                "summary": "Spoken Language Understanding (SLU) is a core component of conversational\nsystems, enabling machines to interpret user utterances. Despite its\nimportance, developing effective SLU systems remains challenging due to the\nscarcity of labeled training data and the computational burden of deploying\nLarge Language Models (LLMs) in real-world applications. To further alleviate\nthese issues, we propose an Adaptive Feature Distillation framework that\ntransfers rich semantic representations from a General Text Embeddings\n(GTE)-based teacher model to a lightweight student model. Our method introduces\na dynamic adapter equipped with a Residual Projection Neural Network (RPNN) to\nalign heterogeneous feature spaces, and a Dynamic Distillation Coefficient\n(DDC) that adaptively modulates the distillation strength based on real-time\nfeedback from intent and slot prediction performance. Experiments on the\nChinese profile-based ProSLU benchmark demonstrate that AFD-SLU achieves\nstate-of-the-art results, with 95.67% intent accuracy, 92.02% slot F1 score,\nand 85.50% overall accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken Language Understanding (SLU) is a core component of conversational\nsystems, enabling machines to interpret user utterances. Despite its\nimportance, developing effective SLU systems remains challenging due to the\nscarcity of labeled training data and the computational burden of deploying\nLarge Language Models (LLMs) in real-world applications. To further alleviate\nthese issues, we propose an Adaptive Feature Distillation framework that\ntransfers rich semantic representations from a General Text Embeddings\n(GTE)-based teacher model to a lightweight student model. Our method introduces\na dynamic adapter equipped with a Residual Projection Neural Network (RPNN) to\nalign heterogeneous feature spaces, and a Dynamic Distillation Coefficient\n(DDC) that adaptively modulates the distillation strength based on real-time\nfeedback from intent and slot prediction performance. Experiments on the\nChinese profile-based ProSLU benchmark demonstrate that AFD-SLU achieves\nstate-of-the-art results, with 95.67% intent accuracy, 92.02% slot F1 score,\nand 85.50% overall accuracy."
                },
                "authors": [
                    {
                        "name": "Yan Xie"
                    },
                    {
                        "name": "Yibo Cui"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Erwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Erwei Yin"
                },
                "author": "Erwei Yin",
                "arxiv_comment": "5 pages, 1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04820v1",
                "updated": "2025-09-05T05:44:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    44,
                    50,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:44:50Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    44,
                    50,
                    4,
                    248,
                    0
                ],
                "title": "Fishing for Answers: Exploring One-shot vs. Iterative Retrieval\n  Strategies for Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fishing for Answers: Exploring One-shot vs. Iterative Retrieval\n  Strategies for Retrieval Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) based on Large Language Models (LLMs) is\na powerful solution to understand and query the industry's closed-source\ndocuments. However, basic RAG often struggles with complex QA tasks in legal\nand regulatory domains, particularly when dealing with numerous government\ndocuments. The top-$k$ strategy frequently misses golden chunks, leading to\nincomplete or inaccurate answers. To address these retrieval bottlenecks, we\nexplore two strategies to improve evidence coverage and answer quality. The\nfirst is a One-SHOT retrieval method that adaptively selects chunks based on a\ntoken budget, allowing as much relevant content as possible to be included\nwithin the model's context window. Additionally, we design modules to further\nfilter and refine the chunks. The second is an iterative retrieval strategy\nbuilt on a Reasoning Agentic RAG framework, where a reasoning LLM dynamically\nissues search queries, evaluates retrieved results, and progressively refines\nthe context over multiple turns. We identify query drift and retrieval laziness\nissues and further design two modules to tackle them. Through extensive\nexperiments on a dataset of government documents, we aim to offer practical\ninsights and guidance for real-world applications in legal and regulatory\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) based on Large Language Models (LLMs) is\na powerful solution to understand and query the industry's closed-source\ndocuments. However, basic RAG often struggles with complex QA tasks in legal\nand regulatory domains, particularly when dealing with numerous government\ndocuments. The top-$k$ strategy frequently misses golden chunks, leading to\nincomplete or inaccurate answers. To address these retrieval bottlenecks, we\nexplore two strategies to improve evidence coverage and answer quality. The\nfirst is a One-SHOT retrieval method that adaptively selects chunks based on a\ntoken budget, allowing as much relevant content as possible to be included\nwithin the model's context window. Additionally, we design modules to further\nfilter and refine the chunks. The second is an iterative retrieval strategy\nbuilt on a Reasoning Agentic RAG framework, where a reasoning LLM dynamically\nissues search queries, evaluates retrieved results, and progressively refines\nthe context over multiple turns. We identify query drift and retrieval laziness\nissues and further design two modules to tackle them. Through extensive\nexperiments on a dataset of government documents, we aim to offer practical\ninsights and guidance for real-world applications in legal and regulatory\ndomains."
                },
                "authors": [
                    {
                        "name": "Huifeng Lin"
                    },
                    {
                        "name": "Gang Su"
                    },
                    {
                        "name": "Jintao Liang"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Ziyue Li"
                    }
                ],
                "author_detail": {
                    "name": "Ziyue Li"
                },
                "author": "Ziyue Li",
                "arxiv_comment": "under Review of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04818v1",
                "updated": "2025-09-05T05:39:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    39,
                    5,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:39:05Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    39,
                    5,
                    4,
                    248,
                    0
                ],
                "title": "Turbulence from CO observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turbulence from CO observations"
                },
                "summary": "Turbulence influences the structure and dynamics of molecular clouds, and\nplays a key role in regulating star formation. We therefore need methods to\naccurately infer turbulence properties of molecular clouds from\nposition-position-velocity (PPV) spectral observations. A previous method\ncalibrated with simulation data exists to recover the 3D turbulent velocity\ndispersion from PPV data. However, that method relies on optically-thin\nconditions, ignoring any radiative transfer (RT) and chemical effects. In the\npresent study we determine how opacity, RT, and chemical effects influence\nturbulence measurements with CO lines. We post-process a chemo-dynamical\nsimulation of a turbulent collapsing cloud with a non-local thermodynamic\nequilibrium line RT code to generate PPV spectral cubes of the CO (1-0) and CO\n(2-1) lines, and obtain moment maps. We isolate the turbulence in the\nfirst-moment maps by using a Gaussian smoothing approach. We compare the CO\nresults with the optically-thin scenario to explore how line excitation and RT\nimpact the turbulence measurements. We find that the turbulent velocity\ndispersion (sigma_v) measured via CO requires a correction by a factor R_CO,\nwith R_CO,1-0 = 0.88 (+0.09, -0.08) for the CO (1-0) line and R_CO,2-1 = 0.88\n(+0.10, -0.08) for the CO (2-1) line. As a consequence, previous measurements\nof sigma_v were overestimated by about 10-15% on average, with potential\noverestimates as high as 40%, taking the 1-sigma uncertainty into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turbulence influences the structure and dynamics of molecular clouds, and\nplays a key role in regulating star formation. We therefore need methods to\naccurately infer turbulence properties of molecular clouds from\nposition-position-velocity (PPV) spectral observations. A previous method\ncalibrated with simulation data exists to recover the 3D turbulent velocity\ndispersion from PPV data. However, that method relies on optically-thin\nconditions, ignoring any radiative transfer (RT) and chemical effects. In the\npresent study we determine how opacity, RT, and chemical effects influence\nturbulence measurements with CO lines. We post-process a chemo-dynamical\nsimulation of a turbulent collapsing cloud with a non-local thermodynamic\nequilibrium line RT code to generate PPV spectral cubes of the CO (1-0) and CO\n(2-1) lines, and obtain moment maps. We isolate the turbulence in the\nfirst-moment maps by using a Gaussian smoothing approach. We compare the CO\nresults with the optically-thin scenario to explore how line excitation and RT\nimpact the turbulence measurements. We find that the turbulent velocity\ndispersion (sigma_v) measured via CO requires a correction by a factor R_CO,\nwith R_CO,1-0 = 0.88 (+0.09, -0.08) for the CO (1-0) line and R_CO,2-1 = 0.88\n(+0.10, -0.08) for the CO (2-1) line. As a consequence, previous measurements\nof sigma_v were overestimated by about 10-15% on average, with potential\noverestimates as high as 40%, taking the 1-sigma uncertainty into account."
                },
                "authors": [
                    {
                        "name": "Jayashree Narayan"
                    },
                    {
                        "name": "Aris Tritsis"
                    },
                    {
                        "name": "Christoph Federrath"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Federrath"
                },
                "author": "Christoph Federrath",
                "arxiv_comment": "15 pages, 6 figures, Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05748v2",
                "updated": "2025-09-05T05:30:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    30,
                    31,
                    4,
                    248,
                    0
                ],
                "published": "2024-08-11T11:15:41Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    11,
                    15,
                    41,
                    6,
                    224,
                    0
                ],
                "title": "Low-Dimensional Federated Knowledge Graph Embedding via Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Dimensional Federated Knowledge Graph Embedding via Knowledge\n  Distillation"
                },
                "summary": "Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative\nlearning of entity and relation embeddings from distributed Knowledge Graphs\n(KGs) across multiple clients, while preserving data privacy. Training FKGE\nmodels with higher dimensions is typically favored due to their potential for\nachieving superior performance. However, high-dimensional embeddings present\nsignificant challenges in terms of storage resource and inference speed. Unlike\ntraditional KG embedding methods, FKGE involves multiple client-server\ncommunication rounds, where communication efficiency is critical. Existing\nembedding compression methods for traditional KGs may not be directly\napplicable to FKGE as they often require multiple model trainings which\npotentially incur substantial communication costs. In this paper, we propose a\nlight-weight component based on Knowledge Distillation (KD) which is titled\nFedKD and tailored specifically for FKGE methods. During client-side local\ntraining, FedKD facilitates the low-dimensional student model to mimic the\nscore distribution of triples from the high-dimensional teacher model using KL\ndivergence loss. Unlike traditional KD way, FedKD adaptively learns a\ntemperature to scale the score of positive triples and separately adjusts the\nscores of corresponding negative triples using a predefined temperature,\nthereby mitigating teacher over-confidence issue. Furthermore, we dynamically\nadjust the weight of KD loss to optimize the training process. Extensive\nexperiments on three datasets support the effectiveness of FedKD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative\nlearning of entity and relation embeddings from distributed Knowledge Graphs\n(KGs) across multiple clients, while preserving data privacy. Training FKGE\nmodels with higher dimensions is typically favored due to their potential for\nachieving superior performance. However, high-dimensional embeddings present\nsignificant challenges in terms of storage resource and inference speed. Unlike\ntraditional KG embedding methods, FKGE involves multiple client-server\ncommunication rounds, where communication efficiency is critical. Existing\nembedding compression methods for traditional KGs may not be directly\napplicable to FKGE as they often require multiple model trainings which\npotentially incur substantial communication costs. In this paper, we propose a\nlight-weight component based on Knowledge Distillation (KD) which is titled\nFedKD and tailored specifically for FKGE methods. During client-side local\ntraining, FedKD facilitates the low-dimensional student model to mimic the\nscore distribution of triples from the high-dimensional teacher model using KL\ndivergence loss. Unlike traditional KD way, FedKD adaptively learns a\ntemperature to scale the score of positive triples and separately adjusts the\nscores of corresponding negative triples using a predefined temperature,\nthereby mitigating teacher over-confidence issue. Furthermore, we dynamically\nadjust the weight of KD loss to optimize the training process. Extensive\nexperiments on three datasets support the effectiveness of FedKD."
                },
                "authors": [
                    {
                        "name": "Xiaoxiong Zhang"
                    },
                    {
                        "name": "Zhiwei Zeng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Zhiqi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqi Shen"
                },
                "author": "Zhiqi Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15937v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15937v3",
                "updated": "2025-09-05T05:26:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    26,
                    23,
                    4,
                    248,
                    0
                ],
                "published": "2025-03-20T08:25:00Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    25,
                    0,
                    3,
                    79,
                    0
                ],
                "title": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment"
                },
                "summary": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale.\n  V-Droid obtains a substantial task success rate across several public mobile\ntask automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49%\non MobileAgentBench, surpassing existing agents by 5.2%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves a remarkably low latency of 4.3s\nper step, which is 6.1X faster compared with existing mobile agents. The source\ncode is available at https://github.com/V-Droid-Agent/V-Droid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale.\n  V-Droid obtains a substantial task success rate across several public mobile\ntask automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49%\non MobileAgentBench, surpassing existing agents by 5.2%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves a remarkably low latency of 4.3s\nper step, which is 6.1X faster compared with existing mobile agents. The source\ncode is available at https://github.com/V-Droid-Agent/V-Droid."
                },
                "authors": [
                    {
                        "name": "Gaole Dai"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Rui Tan"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "add baselines, add source code link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15937v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15937v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04810v1",
                "updated": "2025-09-05T05:17:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    17,
                    14,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:17:14Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    17,
                    14,
                    4,
                    248,
                    0
                ],
                "title": "Code Review Without Borders: Evaluating Synthetic vs. Real Data for\n  Review Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Review Without Borders: Evaluating Synthetic vs. Real Data for\n  Review Recommendation"
                },
                "summary": "Automating the decision of whether a code change requires manual review is\nvital for maintaining software quality in modern development workflows.\nHowever, the emergence of new programming languages and frameworks creates a\ncritical bottleneck: while large volumes of unlabelled code are readily\navailable, there is an insufficient amount of labelled data to train supervised\nmodels for review classification. We address this challenge by leveraging Large\nLanguage Models (LLMs) to translate code changes from well-resourced languages\ninto equivalent changes in underrepresented or emerging languages, generating\nsynthetic training data where labelled examples are scarce. We assume that\nalthough LLMs have learned the syntax and semantics of new languages from\navailable unlabelled code, they have yet to fully grasp which code changes are\nconsidered significant or review-worthy within these emerging ecosystems. To\novercome this, we use LLMs to generate synthetic change examples and train\nsupervised classifiers on them. We systematically compare the performance of\nthese classifiers against models trained on real labelled data. Our experiments\nacross multiple GitHub repositories and language pairs demonstrate that\nLLM-generated synthetic data can effectively bootstrap review recommendation\nsystems, narrowing the performance gap even in low-resource settings. This\napproach provides a scalable pathway to extend automated code review\ncapabilities to rapidly evolving technology stacks, even in the absence of\nannotated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the decision of whether a code change requires manual review is\nvital for maintaining software quality in modern development workflows.\nHowever, the emergence of new programming languages and frameworks creates a\ncritical bottleneck: while large volumes of unlabelled code are readily\navailable, there is an insufficient amount of labelled data to train supervised\nmodels for review classification. We address this challenge by leveraging Large\nLanguage Models (LLMs) to translate code changes from well-resourced languages\ninto equivalent changes in underrepresented or emerging languages, generating\nsynthetic training data where labelled examples are scarce. We assume that\nalthough LLMs have learned the syntax and semantics of new languages from\navailable unlabelled code, they have yet to fully grasp which code changes are\nconsidered significant or review-worthy within these emerging ecosystems. To\novercome this, we use LLMs to generate synthetic change examples and train\nsupervised classifiers on them. We systematically compare the performance of\nthese classifiers against models trained on real labelled data. Our experiments\nacross multiple GitHub repositories and language pairs demonstrate that\nLLM-generated synthetic data can effectively bootstrap review recommendation\nsystems, narrowing the performance gap even in low-resource settings. This\napproach provides a scalable pathway to extend automated code review\ncapabilities to rapidly evolving technology stacks, even in the absence of\nannotated data."
                },
                "authors": [
                    {
                        "name": "Yogev Cohen"
                    },
                    {
                        "name": "Dudi Ohayon"
                    },
                    {
                        "name": "Romy Somkin"
                    },
                    {
                        "name": "Yehudit Aperstein"
                    },
                    {
                        "name": "Alexander Apartsin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Apartsin"
                },
                "author": "Alexander Apartsin",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04809v1",
                "updated": "2025-09-05T05:09:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    9,
                    9,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:09:09Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    9,
                    9,
                    4,
                    248,
                    0
                ],
                "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning\n  Agents with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning\n  Agents with Large Language Models"
                },
                "summary": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain."
                },
                "authors": [
                    {
                        "name": "Haechang Kim"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Can Li"
                    },
                    {
                        "name": "Jong Min Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jong Min Lee"
                },
                "author": "Jong Min Lee",
                "arxiv_comment": "31 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04996v3",
                "updated": "2025-09-05T05:07:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    7,
                    21,
                    4,
                    248,
                    0
                ],
                "published": "2024-10-07T12:52:38Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    52,
                    38,
                    0,
                    281,
                    0
                ],
                "title": "Assumption-Lean Post-Integrated Inference with Surrogate Control\n  Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assumption-Lean Post-Integrated Inference with Surrogate Control\n  Outcomes"
                },
                "summary": "Data integration methods aim to extract low-dimensional embeddings from\nhigh-dimensional outcomes to remove unwanted variations, such as batch effects\nand unmeasured covariates, across heterogeneous datasets. However, multiple\nhypothesis testing after integration can be biased due to data-dependent\nprocesses. We introduce a robust post-integrated inference (PII) method that\nadjusts for latent heterogeneity using control outcomes. Leveraging causal\ninterpretations, we derive nonparametric identifiability of the direct effects\nusing negative control outcomes. By utilizing surrogate control outcomes as an\nextension of negative control outcomes, we develop semiparametric inference on\nprojected direct effect estimands, accounting for hidden mediators,\nconfounders, and moderators. These estimands remain statistically meaningful\nunder model misspecifications and with error-prone embeddings. We provide bias\nquantifications and finite-sample linear expansions with uniform concentration\nbounds. The proposed doubly robust estimators are consistent and efficient\nunder minimal assumptions and potential misspecification, facilitating\ndata-adaptive estimation with machine learning algorithms. Our proposal is\nevaluated with random forests through simulations and analysis of single-cell\nCRISPR perturbed datasets with potential unmeasured confounders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data integration methods aim to extract low-dimensional embeddings from\nhigh-dimensional outcomes to remove unwanted variations, such as batch effects\nand unmeasured covariates, across heterogeneous datasets. However, multiple\nhypothesis testing after integration can be biased due to data-dependent\nprocesses. We introduce a robust post-integrated inference (PII) method that\nadjusts for latent heterogeneity using control outcomes. Leveraging causal\ninterpretations, we derive nonparametric identifiability of the direct effects\nusing negative control outcomes. By utilizing surrogate control outcomes as an\nextension of negative control outcomes, we develop semiparametric inference on\nprojected direct effect estimands, accounting for hidden mediators,\nconfounders, and moderators. These estimands remain statistically meaningful\nunder model misspecifications and with error-prone embeddings. We provide bias\nquantifications and finite-sample linear expansions with uniform concentration\nbounds. The proposed doubly robust estimators are consistent and efficient\nunder minimal assumptions and potential misspecification, facilitating\ndata-adaptive estimation with machine learning algorithms. Our proposal is\nevaluated with random forests through simulations and analysis of single-cell\nCRISPR perturbed datasets with potential unmeasured confounders."
                },
                "authors": [
                    {
                        "name": "Jin-Hong Du"
                    },
                    {
                        "name": "Kathryn Roeder"
                    },
                    {
                        "name": "Larry Wasserman"
                    }
                ],
                "author_detail": {
                    "name": "Larry Wasserman"
                },
                "author": "Larry Wasserman",
                "arxiv_comment": "23 pages for the main text, 32 pages for the appendix, 6 figures for\n  the main text, 7 figures for the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09600v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09600v3",
                "updated": "2025-09-05T04:54:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    54,
                    29,
                    4,
                    248,
                    0
                ],
                "published": "2024-08-18T21:45:03Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    21,
                    45,
                    3,
                    6,
                    231,
                    0
                ],
                "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning"
                },
                "summary": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can\nbreak the LLMs's safety alignment. While several defenses have been proposed,\nour evaluation shows that existing defenses fail \\textit{when some specific\ntraining hyper-parameters are chosen} -- a large learning rate or a large\nnumber of training epochs in the fine-tuning stage can easily invalidate the\ndefense. To this end, we propose Antidote, a post-fine-tuning stage solution,\nwhich remains \\textbf{\\textit{agnostic to the training hyper-parameters in the\nfine-tuning stage}}. Antidote relies on the philosophy that by removing the\nharmful parameters, the harmful model can be recovered from the harmful\nbehaviors, regardless of how those harmful parameters are formed in the\nfine-tuning stage. With this philosophy, we introduce a one-shot pruning stage\nafter harmful fine-tuning to remove the harmful weights that are responsible\nfor the generation of harmful content. Despite its embarrassing simplicity,\nempirical results show that Antidote can reduce harmful score while maintaining\naccuracy on downstream tasks. Code is available at\nhttps://github.com/git-disl/Antidote.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can\nbreak the LLMs's safety alignment. While several defenses have been proposed,\nour evaluation shows that existing defenses fail \\textit{when some specific\ntraining hyper-parameters are chosen} -- a large learning rate or a large\nnumber of training epochs in the fine-tuning stage can easily invalidate the\ndefense. To this end, we propose Antidote, a post-fine-tuning stage solution,\nwhich remains \\textbf{\\textit{agnostic to the training hyper-parameters in the\nfine-tuning stage}}. Antidote relies on the philosophy that by removing the\nharmful parameters, the harmful model can be recovered from the harmful\nbehaviors, regardless of how those harmful parameters are formed in the\nfine-tuning stage. With this philosophy, we introduce a one-shot pruning stage\nafter harmful fine-tuning to remove the harmful weights that are responsible\nfor the generation of harmful content. Despite its embarrassing simplicity,\nempirical results show that Antidote can reduce harmful score while maintaining\naccuracy on downstream tasks. Code is available at\nhttps://github.com/git-disl/Antidote."
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Gautam Bhattacharya"
                    },
                    {
                        "name": "Pratik Joshi"
                    },
                    {
                        "name": "Josh Kimball"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "arxiv_comment": "Rejected by AAAI25-AIA. Accepted by ICML25. Authors are thankful to\n  the anonymous reviewers from both AAAI25-AIA and ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09600v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09600v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.05291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05291v1",
                "updated": "2025-09-05T17:56:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    56,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T17:56:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    56,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "Crosscoding Through Time: Tracking Emergence & Consolidation Of\n  Linguistic Representations Throughout LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crosscoding Through Time: Tracking Emergence & Consolidation Of\n  Linguistic Representations Throughout LLM Pretraining"
                },
                "summary": "Large language models (LLMs) learn non-trivial abstractions during\npretraining, like detecting irregular plural noun subjects. However, it is not\nwell understood when and how specific linguistic abilities emerge as\ntraditional evaluation methods such as benchmarking fail to reveal how models\nacquire concepts and capabilities. To bridge this gap and better understand\nmodel training at the concept level, we use sparse crosscoders to discover and\nalign features across model checkpoints. Using this approach, we track the\nevolution of linguistic features during pretraining. We train crosscoders\nbetween open-sourced checkpoint triplets with significant performance and\nrepresentation shifts, and introduce a novel metric, Relative Indirect Effects\n(RelIE), to trace training stages at which individual features become causally\nimportant for task performance. We show that crosscoders can detect feature\nemergence, maintenance, and discontinuation during pretraining. Our approach is\narchitecture-agnostic and scalable, offering a promising path toward more\ninterpretable and fine-grained analysis of representation learning throughout\npretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) learn non-trivial abstractions during\npretraining, like detecting irregular plural noun subjects. However, it is not\nwell understood when and how specific linguistic abilities emerge as\ntraditional evaluation methods such as benchmarking fail to reveal how models\nacquire concepts and capabilities. To bridge this gap and better understand\nmodel training at the concept level, we use sparse crosscoders to discover and\nalign features across model checkpoints. Using this approach, we track the\nevolution of linguistic features during pretraining. We train crosscoders\nbetween open-sourced checkpoint triplets with significant performance and\nrepresentation shifts, and introduce a novel metric, Relative Indirect Effects\n(RelIE), to trace training stages at which individual features become causally\nimportant for task performance. We show that crosscoders can detect feature\nemergence, maintenance, and discontinuation during pretraining. Our approach is\narchitecture-agnostic and scalable, offering a promising path toward more\ninterpretable and fine-grained analysis of representation learning throughout\npretraining."
                },
                "authors": [
                    {
                        "name": "Deniz Bayazit"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07236v2",
                "updated": "2025-09-05T17:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    54,
                    18,
                    4,
                    248,
                    0
                ],
                "published": "2025-07-09T19:13:25Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    19,
                    13,
                    25,
                    2,
                    190,
                    0
                ],
                "title": "Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM\n  Uncertainty Quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM\n  Uncertainty Quantification"
                },
                "summary": "Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and na\\\"ive ensemble baselines. In addition, we\nexplore using MUSE as guided signals with chain-of-thought distillation to\nfine-tune LLMs for calibration. MUSE is available\nat:https://github.com/LARK-NLP-Lab/MUSE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and na\\\"ive ensemble baselines. In addition, we\nexplore using MUSE as guided signals with chain-of-thought distillation to\nfine-tune LLMs for calibration. MUSE is available\nat:https://github.com/LARK-NLP-Lab/MUSE."
                },
                "authors": [
                    {
                        "name": "Maya Kruse"
                    },
                    {
                        "name": "Majid Afshar"
                    },
                    {
                        "name": "Saksham Khatwani"
                    },
                    {
                        "name": "Anoop Mayampurath"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yanjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Gao"
                },
                "author": "Yanjun Gao",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05528v2",
                "updated": "2025-09-05T17:52:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    52,
                    53,
                    4,
                    248,
                    0
                ],
                "published": "2025-07-07T22:56:37Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    22,
                    56,
                    37,
                    0,
                    188,
                    0
                ],
                "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for\n  Procedural Learning and Pedagogic Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Education at Scale: A Multi-LLM Agent Workflow for\n  Procedural Learning and Pedagogic Quality Assessment"
                },
                "summary": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced."
                },
                "authors": [
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Wentao Deng"
                    },
                    {
                        "name": "Koen Hindriks"
                    },
                    {
                        "name": "Junxiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junxiao Wang"
                },
                "author": "Junxiao Wang",
                "arxiv_comment": "14 pages, accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06460v2",
                "updated": "2025-09-05T17:44:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    44,
                    5,
                    4,
                    248,
                    0
                ],
                "published": "2025-04-08T22:00:32Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    22,
                    0,
                    32,
                    1,
                    98,
                    0
                ],
                "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for\n  Counterfactual Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for\n  Counterfactual Instruction Following"
                },
                "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate\npersonas in virtual environments, leveraging their instruction-following\ncapability. However, we discovered that even state-of-the-art LLMs cannot\nsimulate personas with reversed performance (e.g., student personas with low\nproficiency in educational settings), which impairs the simulation diversity\nand limits the practical applications of the simulated environments. In this\nwork, using mathematical reasoning as a representative scenario, we propose the\nfirst benchmark dataset for evaluating LLMs on simulating personas with\nreversed performance, a capability that we dub \"counterfactual instruction\nfollowing\". We evaluate both open-weight and closed-source LLMs on this task\nand find that LLMs, including the OpenAI o1 reasoning model, all struggle to\nfollow counterfactual instructions for simulating reversedly performing\npersonas. Intersectionally simulating both the performance level and the race\npopulation of a persona worsens the effect even further. These results\nhighlight the challenges of counterfactual instruction following and the need\nfor further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are now increasingly widely used to simulate\npersonas in virtual environments, leveraging their instruction-following\ncapability. However, we discovered that even state-of-the-art LLMs cannot\nsimulate personas with reversed performance (e.g., student personas with low\nproficiency in educational settings), which impairs the simulation diversity\nand limits the practical applications of the simulated environments. In this\nwork, using mathematical reasoning as a representative scenario, we propose the\nfirst benchmark dataset for evaluating LLMs on simulating personas with\nreversed performance, a capability that we dub \"counterfactual instruction\nfollowing\". We evaluate both open-weight and closed-source LLMs on this task\nand find that LLMs, including the OpenAI o1 reasoning model, all struggle to\nfollow counterfactual instructions for simulating reversedly performing\npersonas. Intersectionally simulating both the performance level and the race\npopulation of a persona worsens the effect even further. These results\nhighlight the challenges of counterfactual instruction following and the need\nfor further research."
                },
                "authors": [
                    {
                        "name": "Sai Adith Senthil Kumar"
                    },
                    {
                        "name": "Hao Yan"
                    },
                    {
                        "name": "Saipavan Perepa"
                    },
                    {
                        "name": "Murong Yue"
                    },
                    {
                        "name": "Ziyu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Yao"
                },
                "author": "Ziyu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05281v1",
                "updated": "2025-09-05T17:41:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    41,
                    57,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T17:41:57Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    41,
                    57,
                    4,
                    248,
                    0
                ],
                "title": "Dual-Branch Convolutional Framework for Spatial and Frequency-Based\n  Image Forgery Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Branch Convolutional Framework for Spatial and Frequency-Based\n  Image Forgery Detection"
                },
                "summary": "With a very rapid increase in deepfakes and digital image forgeries, ensuring\nthe authenticity of images is becoming increasingly challenging. This report\nintroduces a forgery detection framework that combines spatial and\nfrequency-based features for detecting forgeries. We propose a dual branch\nconvolution neural network that operates on features extracted from spatial and\nfrequency domains. Features from both branches are fused and compared within a\nSiamese network, yielding 64 dimensional embeddings for classification. When\nbenchmarked on CASIA 2.0 dataset, our method achieves an accuracy of 77.9%,\noutperforming traditional statistical methods. Despite its relatively weaker\nperformance compared to larger, more complex forgery detection pipelines, our\napproach balances computational complexity and detection reliability, making it\nready for practical deployment. It provides a strong methodology for forensic\nscrutiny of digital images. In a broader sense, it advances the state of the\nart in visual forensics, addressing an urgent requirement in media\nverification, law enforcement and digital content reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With a very rapid increase in deepfakes and digital image forgeries, ensuring\nthe authenticity of images is becoming increasingly challenging. This report\nintroduces a forgery detection framework that combines spatial and\nfrequency-based features for detecting forgeries. We propose a dual branch\nconvolution neural network that operates on features extracted from spatial and\nfrequency domains. Features from both branches are fused and compared within a\nSiamese network, yielding 64 dimensional embeddings for classification. When\nbenchmarked on CASIA 2.0 dataset, our method achieves an accuracy of 77.9%,\noutperforming traditional statistical methods. Despite its relatively weaker\nperformance compared to larger, more complex forgery detection pipelines, our\napproach balances computational complexity and detection reliability, making it\nready for practical deployment. It provides a strong methodology for forensic\nscrutiny of digital images. In a broader sense, it advances the state of the\nart in visual forensics, addressing an urgent requirement in media\nverification, law enforcement and digital content reliability."
                },
                "authors": [
                    {
                        "name": "Naman Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Naman Tyagi"
                },
                "author": "Naman Tyagi",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05276v1",
                "updated": "2025-09-05T17:34:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    34,
                    0,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T17:34:00Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    34,
                    0,
                    4,
                    248,
                    0
                ],
                "title": "SpikingBrain Technical Report: Spiking Brain-inspired Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpikingBrain Technical Report: Spiking Brain-inspired Large Models"
                },
                "summary": "Mainstream Transformer-based large language models face major efficiency\nbottlenecks: training computation scales quadratically with sequence length,\nand inference memory grows linearly, limiting long-context processing. Building\nlarge models on non-NVIDIA platforms also poses challenges for stable and\nefficient training. To address this, we introduce SpikingBrain, a family of\nbrain-inspired models designed for efficient long-context training and\ninference. SpikingBrain leverages the MetaX GPU cluster and focuses on three\naspects: (1) Model Architecture: linear and hybrid-linear attention\narchitectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an\nefficient, conversion-based training pipeline and a dedicated spike coding\nframework; (3) System Engineering: customized training frameworks, operator\nlibraries, and parallelism strategies tailored to MetaX hardware.\n  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,\nand SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the\nfeasibility of large-scale LLM development on non-NVIDIA platforms.\nSpikingBrain achieves performance comparable to open-source Transformer\nbaselines while using only about 150B tokens for continual pre-training. Our\nmodels significantly improve long-sequence training efficiency and deliver\ninference with (partially) constant memory and event-driven spiking behavior.\nFor example, SpikingBrain-7B attains over 100x speedup in Time to First Token\nfor 4M-token sequences. Training remains stable for weeks on hundreds of MetaX\nC550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4\npercent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling\nlow-power operation. Overall, this work demonstrates the potential of\nbrain-inspired mechanisms to drive the next generation of efficient and\nscalable large model design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainstream Transformer-based large language models face major efficiency\nbottlenecks: training computation scales quadratically with sequence length,\nand inference memory grows linearly, limiting long-context processing. Building\nlarge models on non-NVIDIA platforms also poses challenges for stable and\nefficient training. To address this, we introduce SpikingBrain, a family of\nbrain-inspired models designed for efficient long-context training and\ninference. SpikingBrain leverages the MetaX GPU cluster and focuses on three\naspects: (1) Model Architecture: linear and hybrid-linear attention\narchitectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an\nefficient, conversion-based training pipeline and a dedicated spike coding\nframework; (3) System Engineering: customized training frameworks, operator\nlibraries, and parallelism strategies tailored to MetaX hardware.\n  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,\nand SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the\nfeasibility of large-scale LLM development on non-NVIDIA platforms.\nSpikingBrain achieves performance comparable to open-source Transformer\nbaselines while using only about 150B tokens for continual pre-training. Our\nmodels significantly improve long-sequence training efficiency and deliver\ninference with (partially) constant memory and event-driven spiking behavior.\nFor example, SpikingBrain-7B attains over 100x speedup in Time to First Token\nfor 4M-token sequences. Training remains stable for weeks on hundreds of MetaX\nC550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4\npercent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling\nlow-power operation. Overall, this work demonstrates the potential of\nbrain-inspired mechanisms to drive the next generation of efficient and\nscalable large model design."
                },
                "authors": [
                    {
                        "name": "Yuqi Pan"
                    },
                    {
                        "name": "Yupeng Feng"
                    },
                    {
                        "name": "Jinghao Zhuang"
                    },
                    {
                        "name": "Siyu Ding"
                    },
                    {
                        "name": "Zehao Liu"
                    },
                    {
                        "name": "Bohan Sun"
                    },
                    {
                        "name": "Yuhong Chou"
                    },
                    {
                        "name": "Han Xu"
                    },
                    {
                        "name": "Xuerui Qiu"
                    },
                    {
                        "name": "Anlin Deng"
                    },
                    {
                        "name": "Anjie Hu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Man Yao"
                    },
                    {
                        "name": "Jibin Wu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Guoliang Sun"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Guoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoqi Li"
                },
                "author": "Guoqi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02445v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02445v7",
                "updated": "2025-09-05T17:32:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    32,
                    10,
                    4,
                    248,
                    0
                ],
                "published": "2025-03-04T09:40:00Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    40,
                    0,
                    1,
                    63,
                    0
                ],
                "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modeling"
                },
                "summary": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by up to 12% on MSE and 6% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by up to 12% on MSE and 6% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yu-Hao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Goran Nenadic"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "ICML 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02445v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02445v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05263v1",
                "updated": "2025-09-05T17:22:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    22,
                    33,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T17:22:33Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    22,
                    33,
                    4,
                    248,
                    0
                ],
                "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation"
                },
                "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18"
                },
                "authors": [
                    {
                        "name": "Yinglin Duan"
                    },
                    {
                        "name": "Zhengxia Zou"
                    },
                    {
                        "name": "Tongwei Gu"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Zhan Zhao"
                    },
                    {
                        "name": "Luyi Xu"
                    },
                    {
                        "name": "Xinzhu Liu"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Shuang Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Shuang Qiu"
                },
                "author": "Shuang Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05258v1",
                "updated": "2025-09-05T17:14:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    14,
                    58,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T17:14:58Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    14,
                    58,
                    4,
                    248,
                    0
                ],
                "title": "Scaling Performance of Large Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Performance of Large Language Model Pretraining"
                },
                "summary": "Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, information about the scaling performance and training\nconsiderations of these large training pipelines is scarce in public\nliterature. Working with large-scale datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, information about the scaling performance and training\nconsiderations of these large training pipelines is scarce in public\nliterature. Working with large-scale datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity."
                },
                "authors": [
                    {
                        "name": "Alexander Interrante-Grant"
                    },
                    {
                        "name": "Carla Varela-Rosa"
                    },
                    {
                        "name": "Suhaas Narayan"
                    },
                    {
                        "name": "Chris Connelly"
                    },
                    {
                        "name": "Albert Reuther"
                    }
                ],
                "author_detail": {
                    "name": "Albert Reuther"
                },
                "author": "Albert Reuther",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01249v2",
                "updated": "2025-09-05T17:13:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    13,
                    5,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-02T07:59:34Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    7,
                    59,
                    34,
                    5,
                    214,
                    0
                ],
                "title": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend\n  Against Prompt Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend\n  Against Prompt Injection"
                },
                "summary": "Large Language Model (LLM) agents offer a powerful new paradigm for solving\nvarious problems by combining natural language reasoning with the execution of\nexternal tools. However, their dynamic and non-transparent behavior introduces\ncritical security risks, particularly in the presence of prompt injection\nattacks. In this work, we propose a novel insight that treats the agent runtime\ntraces as structured programs with analyzable semantics. Thus, we present\nAgentArmor, a program analysis framework that converts agent traces into graph\nintermediate representation-based structured program dependency representations\n(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.\nAgentArmor consists of three key components: (1) a graph constructor that\nreconstructs the agent's runtime traces as graph-based intermediate\nrepresentations with control and data flow described within; (2) a property\nregistry that attaches security-relevant metadata of interacted tools \\& data,\nand (3) a type system that performs static inference and checking over the\nintermediate representation. By representing agent behavior as structured\nprograms, AgentArmor enables program analysis for sensitive data flow, trust\nboundaries, and policy violations. We evaluate AgentArmor on the AgentDojo\nbenchmark, the results show that AgentArmor can reduce the ASR to 3\\%, with the\nutility drop only 1\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents offer a powerful new paradigm for solving\nvarious problems by combining natural language reasoning with the execution of\nexternal tools. However, their dynamic and non-transparent behavior introduces\ncritical security risks, particularly in the presence of prompt injection\nattacks. In this work, we propose a novel insight that treats the agent runtime\ntraces as structured programs with analyzable semantics. Thus, we present\nAgentArmor, a program analysis framework that converts agent traces into graph\nintermediate representation-based structured program dependency representations\n(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.\nAgentArmor consists of three key components: (1) a graph constructor that\nreconstructs the agent's runtime traces as graph-based intermediate\nrepresentations with control and data flow described within; (2) a property\nregistry that attaches security-relevant metadata of interacted tools \\& data,\nand (3) a type system that performs static inference and checking over the\nintermediate representation. By representing agent behavior as structured\nprograms, AgentArmor enables program analysis for sensitive data flow, trust\nboundaries, and policy violations. We evaluate AgentArmor on the AgentDojo\nbenchmark, the results show that AgentArmor can reduce the ASR to 3\\%, with the\nutility drop only 1\\%."
                },
                "authors": [
                    {
                        "name": "Peiran Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yunfei Lu"
                    },
                    {
                        "name": "Yifeng Cai"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Qingyou Yang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jue Hong"
                    },
                    {
                        "name": "Ye Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ye Wu"
                },
                "author": "Ye Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15961v2",
                "updated": "2025-09-05T16:48:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    48,
                    2,
                    4,
                    248,
                    0
                ],
                "published": "2025-02-21T21:46:56Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    46,
                    56,
                    4,
                    52,
                    0
                ],
                "title": "IA-TIGRIS: An Incremental and Adaptive Sampling-Based Planner for Online\n  Informative Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IA-TIGRIS: An Incremental and Adaptive Sampling-Based Planner for Online\n  Informative Path Planning"
                },
                "summary": "Planning paths that maximize information gain for robotic platforms has\nwide-ranging applications and significant potential impact. To effectively\nadapt to real-time data collection, informative path planning must be computed\nonline and be responsive to new observations. In this work, we present\nIA-TIGRIS, an incremental and adaptive sampling-based informative path planner\ndesigned for real-time onboard execution. Our approach leverages past planning\nefforts through incremental refinement while continuously adapting to updated\nbelief maps. We additionally present detailed implementation and optimization\ninsights to facilitate real-world deployment, along with an array of reward\nfunctions tailored to specific missions and behaviors. Extensive simulation\nresults demonstrate IA-TIGRIS generates higher-quality paths compared to\nbaseline methods. We validate our planner on two distinct hardware platforms: a\nhexarotor UAV and a fixed-wing UAV, each having different motion models and\nconfiguration spaces. Our results show up to a 41% improvement in information\ngain compared to baseline methods, highlighting the planner's potential for\ndeployment in real-world applications. Project website and video:\nhttps://ia-tigris.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning paths that maximize information gain for robotic platforms has\nwide-ranging applications and significant potential impact. To effectively\nadapt to real-time data collection, informative path planning must be computed\nonline and be responsive to new observations. In this work, we present\nIA-TIGRIS, an incremental and adaptive sampling-based informative path planner\ndesigned for real-time onboard execution. Our approach leverages past planning\nefforts through incremental refinement while continuously adapting to updated\nbelief maps. We additionally present detailed implementation and optimization\ninsights to facilitate real-world deployment, along with an array of reward\nfunctions tailored to specific missions and behaviors. Extensive simulation\nresults demonstrate IA-TIGRIS generates higher-quality paths compared to\nbaseline methods. We validate our planner on two distinct hardware platforms: a\nhexarotor UAV and a fixed-wing UAV, each having different motion models and\nconfiguration spaces. Our results show up to a 41% improvement in information\ngain compared to baseline methods, highlighting the planner's potential for\ndeployment in real-world applications. Project website and video:\nhttps://ia-tigris.github.io"
                },
                "authors": [
                    {
                        "name": "Brady Moon"
                    },
                    {
                        "name": "Nayana Suvarna"
                    },
                    {
                        "name": "Andrew Jong"
                    },
                    {
                        "name": "Satrajit Chatterjee"
                    },
                    {
                        "name": "Junbin Yuan"
                    },
                    {
                        "name": "Muqing Cao"
                    },
                    {
                        "name": "Sebastian Scherer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Scherer"
                },
                "author": "Sebastian Scherer",
                "arxiv_comment": "18 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22809v2",
                "updated": "2025-09-05T16:48:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    48,
                    2,
                    4,
                    248,
                    0
                ],
                "published": "2025-05-28T19:34:36Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    19,
                    34,
                    36,
                    2,
                    148,
                    0
                ],
                "title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &\n  Dragons Gameplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &\n  Dragons Gameplay"
                },
                "summary": "Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents."
                },
                "authors": [
                    {
                        "name": "Andrew Zhu"
                    },
                    {
                        "name": "Evan Osgood"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    }
                ],
                "author_detail": {
                    "name": "Chris Callison-Burch"
                },
                "author": "Chris Callison-Burch",
                "arxiv_comment": "9 pages, 5 figures. COLM 2025 Workshop on AI Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18416v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18416v5",
                "updated": "2025-09-05T16:33:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    33,
                    46,
                    4,
                    248,
                    0
                ],
                "published": "2024-07-25T22:24:45Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    22,
                    24,
                    45,
                    3,
                    207,
                    0
                ],
                "title": "PersonaGym: Evaluating Persona Agents and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaGym: Evaluating Persona Agents and LLMs"
                },
                "summary": "Persona agents, which are LLM agents conditioned to act according to an\nassigned persona, enable contextually rich and user aligned interactions across\ndomains like education and healthcare. However, evaluating how faithfully these\nagents adhere to their personas remains a significant challenge, particularly\nin free-form settings that demand consistency across diverse, persona-relevant\nenvironments. We introduce PersonaGym, the first dynamic evaluation framework\nfor persona agents, and PersonaScore, a human-aligned automatic metric grounded\nin decision theory that enables comprehensive large-scale evaluation. Our\nevaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals\nsignificant advancement opportunities. For example, GPT-4.1 had the exact same\nPersonaScore as LLaMA-3-8b despite being a more recent and advanced closed\nsource model. Importantly, increased model size and complexity do not\nnecessarily enhance persona agent capabilities, underscoring the need for\nalgorithmic and architectural innovation toward faithful, performant persona\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona agents, which are LLM agents conditioned to act according to an\nassigned persona, enable contextually rich and user aligned interactions across\ndomains like education and healthcare. However, evaluating how faithfully these\nagents adhere to their personas remains a significant challenge, particularly\nin free-form settings that demand consistency across diverse, persona-relevant\nenvironments. We introduce PersonaGym, the first dynamic evaluation framework\nfor persona agents, and PersonaScore, a human-aligned automatic metric grounded\nin decision theory that enables comprehensive large-scale evaluation. Our\nevaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals\nsignificant advancement opportunities. For example, GPT-4.1 had the exact same\nPersonaScore as LLaMA-3-8b despite being a more recent and advanced closed\nsource model. Importantly, increased model size and complexity do not\nnecessarily enhance persona agent capabilities, underscoring the need for\nalgorithmic and architectural innovation toward faithful, performant persona\nagents."
                },
                "authors": [
                    {
                        "name": "Vinay Samuel"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Shreyas Chaudhari"
                    },
                    {
                        "name": "Ashwin Kalyan"
                    },
                    {
                        "name": "Tanmay Rajpurohit"
                    },
                    {
                        "name": "Ameet Deshpande"
                    },
                    {
                        "name": "Karthik Narasimhan"
                    },
                    {
                        "name": "Vishvak Murahari"
                    }
                ],
                "author_detail": {
                    "name": "Vishvak Murahari"
                },
                "author": "Vishvak Murahari",
                "arxiv_comment": "EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18416v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18416v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20938v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20938v2",
                "updated": "2025-09-05T16:19:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    19,
                    54,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-26T02:01:11Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    2,
                    1,
                    11,
                    3,
                    177,
                    0
                ],
                "title": "ParEval-Repo: A Benchmark Suite for Evaluating LLMs with\n  Repository-level HPC Translation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParEval-Repo: A Benchmark Suite for Evaluating LLMs with\n  Repository-level HPC Translation Tasks"
                },
                "summary": "GPGPU architectures have become significantly more diverse in recent years,\nwhich has led to an emergence of a variety of specialized programming models\nand software stacks to support them. Portable programming models exist, but\nthey require significant developer effort to port to and optimize for different\nhardware architectures. Large language models (LLMs) may help to reduce this\nprogrammer burden. In this paper, we present a novel benchmark and testing\nframework, ParEval-Repo, which can be used to evaluate the efficacy of\nLLM-based approaches in automatically translating entire codebases across GPGPU\nexecution models. ParEval-Repo includes several scientific computing and AI\nmini-applications in a range of programming models and levels of repository\ncomplexity. We use ParEval-Repo to evaluate a range of state-of-the-art\nopen-source and commercial LLMs, with both a non-agentic and a top-down agentic\napproach. We assess code generated by the LLMs and approaches in terms of\ncompilability, functional correctness, categories of build errors, and the cost\nof translation in terms of the number of inference tokens. Our results\ndemonstrate that LLM translation of scientific applications is feasible for\nsmall programs but difficulty with generating functional build systems and\ncross-file dependencies pose challenges in scaling to larger codebases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPGPU architectures have become significantly more diverse in recent years,\nwhich has led to an emergence of a variety of specialized programming models\nand software stacks to support them. Portable programming models exist, but\nthey require significant developer effort to port to and optimize for different\nhardware architectures. Large language models (LLMs) may help to reduce this\nprogrammer burden. In this paper, we present a novel benchmark and testing\nframework, ParEval-Repo, which can be used to evaluate the efficacy of\nLLM-based approaches in automatically translating entire codebases across GPGPU\nexecution models. ParEval-Repo includes several scientific computing and AI\nmini-applications in a range of programming models and levels of repository\ncomplexity. We use ParEval-Repo to evaluate a range of state-of-the-art\nopen-source and commercial LLMs, with both a non-agentic and a top-down agentic\napproach. We assess code generated by the LLMs and approaches in terms of\ncompilability, functional correctness, categories of build errors, and the cost\nof translation in terms of the number of inference tokens. Our results\ndemonstrate that LLM translation of scientific applications is feasible for\nsmall programs but difficulty with generating functional build systems and\ncross-file dependencies pose challenges in scaling to larger codebases."
                },
                "authors": [
                    {
                        "name": "Joshua H. Davis"
                    },
                    {
                        "name": "Daniel Nichols"
                    },
                    {
                        "name": "Ishan Khillan"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20938v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20938v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05215v1",
                "updated": "2025-09-05T16:18:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    18,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:18:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    18,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "BEDTime: A Unified Benchmark for Automatically Describing Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEDTime: A Unified Benchmark for Automatically Describing Time Series"
                },
                "summary": "Many recent studies have proposed general-purpose foundation models designed\nfor a variety of time series analysis tasks. While several established datasets\nalready exist for evaluating these models, previous works frequently introduce\ntheir models in conjunction with new datasets, limiting opportunities for\ndirect, independent comparisons and obscuring insights into the relative\nstrengths of different methods. Additionally, prior evaluations often cover\nnumerous tasks simultaneously, assessing a broad range of model abilities\nwithout clearly pinpointing which capabilities contribute to overall\nperformance. To address these gaps, we formalize and evaluate 3 tasks that test\na model's ability to describe time series using generic natural language: (1)\nrecognition (True/False question-answering), (2) differentiation (multiple\nchoice question-answering), and (3) generation (open-ended natural language\ndescription). We then unify 4 recent datasets to enable head-to-head model\ncomparisons on each task. Experimentally, in evaluating 13 state-of-the-art\nlanguage, vision--language, and time series--language models, we find that (1)\npopular language-only methods largely underperform, indicating a need for time\nseries-specific architectures, (2) VLMs are quite successful, as expected,\nidentifying the value of vision models for these tasks and (3) pretrained\nmultimodal time series--language models successfully outperform LLMs, but still\nhave significant room for improvement. We also find that all approaches exhibit\nclear fragility in a range of robustness tests. Overall, our benchmark provides\na standardized evaluation on a task necessary for time series reasoning\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many recent studies have proposed general-purpose foundation models designed\nfor a variety of time series analysis tasks. While several established datasets\nalready exist for evaluating these models, previous works frequently introduce\ntheir models in conjunction with new datasets, limiting opportunities for\ndirect, independent comparisons and obscuring insights into the relative\nstrengths of different methods. Additionally, prior evaluations often cover\nnumerous tasks simultaneously, assessing a broad range of model abilities\nwithout clearly pinpointing which capabilities contribute to overall\nperformance. To address these gaps, we formalize and evaluate 3 tasks that test\na model's ability to describe time series using generic natural language: (1)\nrecognition (True/False question-answering), (2) differentiation (multiple\nchoice question-answering), and (3) generation (open-ended natural language\ndescription). We then unify 4 recent datasets to enable head-to-head model\ncomparisons on each task. Experimentally, in evaluating 13 state-of-the-art\nlanguage, vision--language, and time series--language models, we find that (1)\npopular language-only methods largely underperform, indicating a need for time\nseries-specific architectures, (2) VLMs are quite successful, as expected,\nidentifying the value of vision models for these tasks and (3) pretrained\nmultimodal time series--language models successfully outperform LLMs, but still\nhave significant room for improvement. We also find that all approaches exhibit\nclear fragility in a range of robustness tests. Overall, our benchmark provides\na standardized evaluation on a task necessary for time series reasoning\nsystems."
                },
                "authors": [
                    {
                        "name": "Medhasweta Sen"
                    },
                    {
                        "name": "Zachary Gottesman"
                    },
                    {
                        "name": "Jiaxing Qiu"
                    },
                    {
                        "name": "C. Bayan Bruss"
                    },
                    {
                        "name": "Nam Nguyen"
                    },
                    {
                        "name": "Tom Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Tom Hartvigsen"
                },
                "author": "Tom Hartvigsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05208v1",
                "updated": "2025-09-05T16:10:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    53,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:53Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    53,
                    4,
                    248,
                    0
                ],
                "title": "Symbolic Graphics Programming with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic Graphics Programming with Large Language Models"
                },
                "summary": "Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding."
                },
                "authors": [
                    {
                        "name": "Yamei Chen"
                    },
                    {
                        "name": "Haoquan Zhang"
                    },
                    {
                        "name": "Yangyi Huang"
                    },
                    {
                        "name": "Zeju Qiu"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Yandong Wen"
                    },
                    {
                        "name": "Weiyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiyang Liu"
                },
                "author": "Weiyang Liu",
                "arxiv_comment": "Technical report (32 pages, 12 figures, project page:\n  https://spherelab.ai/SGP-Gen/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05199v1",
                "updated": "2025-09-05T15:58:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    58,
                    49,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T15:58:49Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    58,
                    49,
                    4,
                    248,
                    0
                ],
                "title": "Triadic Fusion of Cognitive, Functional, and Causal Dimensions for\n  Explainable LLMs: The TAXAL Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triadic Fusion of Cognitive, Functional, and Causal Dimensions for\n  Explainable LLMs: The TAXAL Framework"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in high-risk\ndomains where opacity, bias, and instability undermine trust and\naccountability. Traditional explainability methods, focused on surface outputs,\ndo not capture the reasoning pathways, planning logic, and systemic impacts of\nagentic LLMs.\n  We introduce TAXAL (Triadic Alignment for eXplainability in Agentic LLMs), a\ntriadic fusion framework that unites three complementary dimensions: cognitive\n(user understanding), functional (practical utility), and causal (faithful\nreasoning). TAXAL provides a unified, role-sensitive foundation for designing,\nevaluating, and deploying explanations in diverse sociotechnical settings.\n  Our analysis synthesizes existing methods, ranging from post-hoc attribution\nand dialogic interfaces to explanation-aware prompting, and situates them\nwithin the TAXAL triadic fusion model. We further demonstrate its applicability\nthrough case studies in law, education, healthcare, and public services,\nshowing how explanation strategies adapt to institutional constraints and\nstakeholder roles.\n  By combining conceptual clarity with design patterns and deployment pathways,\nTAXAL advances explainability as a technical and sociotechnical practice,\nsupporting trustworthy and context-sensitive LLM applications in the era of\nagentic AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in high-risk\ndomains where opacity, bias, and instability undermine trust and\naccountability. Traditional explainability methods, focused on surface outputs,\ndo not capture the reasoning pathways, planning logic, and systemic impacts of\nagentic LLMs.\n  We introduce TAXAL (Triadic Alignment for eXplainability in Agentic LLMs), a\ntriadic fusion framework that unites three complementary dimensions: cognitive\n(user understanding), functional (practical utility), and causal (faithful\nreasoning). TAXAL provides a unified, role-sensitive foundation for designing,\nevaluating, and deploying explanations in diverse sociotechnical settings.\n  Our analysis synthesizes existing methods, ranging from post-hoc attribution\nand dialogic interfaces to explanation-aware prompting, and situates them\nwithin the TAXAL triadic fusion model. We further demonstrate its applicability\nthrough case studies in law, education, healthcare, and public services,\nshowing how explanation strategies adapt to institutional constraints and\nstakeholder roles.\n  By combining conceptual clarity with design patterns and deployment pathways,\nTAXAL advances explainability as a technical and sociotechnical practice,\nsupporting trustworthy and context-sensitive LLM applications in the era of\nagentic AI."
                },
                "authors": [
                    {
                        "name": "David Herrera-Poyatos"
                    },
                    {
                        "name": "Carlos Pelez-Gonzlez"
                    },
                    {
                        "name": "Cristina Zuheros"
                    },
                    {
                        "name": "Virilo Tejedor"
                    },
                    {
                        "name": "Rosana Montes"
                    },
                    {
                        "name": "Francisco Herrera"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Herrera"
                },
                "author": "Francisco Herrera",
                "arxiv_comment": "27 pages, 9 tables and 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05197v1",
                "updated": "2025-09-05T15:57:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    57,
                    16,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T15:57:16Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    57,
                    16,
                    4,
                    248,
                    0
                ],
                "title": "AI Agents for Web Testing: A Case Study in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents for Web Testing: A Case Study in the Wild"
                },
                "summary": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks."
                },
                "authors": [
                    {
                        "name": "Naimeng Ye"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Ruize Xu"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Zhou Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Yu"
                },
                "author": "Zhou Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21001v2",
                "updated": "2025-09-05T15:50:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    50,
                    8,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-28T17:04:00Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    17,
                    4,
                    0,
                    3,
                    240,
                    0
                ],
                "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees"
                },
                "summary": "Kinodynamic motion planning is concerned with computing collision-free\ntrajectories while abiding by the robot's dynamic constraints. This critical\nproblem is often tackled using sampling-based planners (SBPs) that explore the\nrobot's high-dimensional state space by constructing a search tree via action\npropagations. Although SBPs can offer global guarantees on completeness and\nsolution quality, their performance is often hindered by slow exploration due\nto uninformed action sampling. Learning-based approaches can yield\nsignificantly faster runtimes, yet they fail to generalize to\nout-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,\nthus limiting their deployment on physical robots. We present Diffusion Tree\n(DiTree): a provably-generalizable framework leveraging diffusion policies\n(DPs) as informed samplers to efficiently guide state-space search within SBPs.\nDiTree combines DP's ability to model complex distributions of expert\ntrajectories, conditioned on local observations, with the completeness of SBPs\nto yield provably-safe solutions within a few action propagation iterations for\ncomplex dynamical systems. We demonstrate DiTree's power with an implementation\ncombining the popular RRT planner with a DP action sampler trained on a single\nenvironment. In comprehensive evaluations on OOD scenarios, DiTree achieves on\naverage a 30% higher success rate compared to standalone DP or SBPs, on a\ndynamic car and Mujoco's ant robot settings (for the latter, SBPs fail\ncompletely). Beyond simulation, real-world car experiments confirm DiTree's\napplicability, demonstrating superior trajectory quality and robustness even\nunder severe sim-to-real gaps. Project webpage:\nhttps://sites.google.com/view/ditree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kinodynamic motion planning is concerned with computing collision-free\ntrajectories while abiding by the robot's dynamic constraints. This critical\nproblem is often tackled using sampling-based planners (SBPs) that explore the\nrobot's high-dimensional state space by constructing a search tree via action\npropagations. Although SBPs can offer global guarantees on completeness and\nsolution quality, their performance is often hindered by slow exploration due\nto uninformed action sampling. Learning-based approaches can yield\nsignificantly faster runtimes, yet they fail to generalize to\nout-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,\nthus limiting their deployment on physical robots. We present Diffusion Tree\n(DiTree): a provably-generalizable framework leveraging diffusion policies\n(DPs) as informed samplers to efficiently guide state-space search within SBPs.\nDiTree combines DP's ability to model complex distributions of expert\ntrajectories, conditioned on local observations, with the completeness of SBPs\nto yield provably-safe solutions within a few action propagation iterations for\ncomplex dynamical systems. We demonstrate DiTree's power with an implementation\ncombining the popular RRT planner with a DP action sampler trained on a single\nenvironment. In comprehensive evaluations on OOD scenarios, DiTree achieves on\naverage a 30% higher success rate compared to standalone DP or SBPs, on a\ndynamic car and Mujoco's ant robot settings (for the latter, SBPs fail\ncompletely). Beyond simulation, real-world car experiments confirm DiTree's\napplicability, demonstrating superior trajectory quality and robustness even\nunder severe sim-to-real gaps. Project webpage:\nhttps://sites.google.com/view/ditree."
                },
                "authors": [
                    {
                        "name": "Yaniv Hassidof"
                    },
                    {
                        "name": "Tom Jurgenson"
                    },
                    {
                        "name": "Kiril Solovey"
                    }
                ],
                "author_detail": {
                    "name": "Kiril Solovey"
                },
                "author": "Kiril Solovey",
                "arxiv_comment": "Accepted to CoRL 2025, Project page:\n  https://sites.google.com/view/ditree. v2: Abstract updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00030v2",
                "updated": "2025-09-05T15:41:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    15,
                    41,
                    49,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-20T17:44:47Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    44,
                    47,
                    2,
                    232,
                    0
                ],
                "title": "MultiStream-LLM: Bridging Modalities for Robust Sign Language\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiStream-LLM: Bridging Modalities for Robust Sign Language\n  Translation"
                },
                "summary": "Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation."
                },
                "authors": [
                    {
                        "name": "Marshall Thomas"
                    },
                    {
                        "name": "Edward Fish"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05161v1",
                "updated": "2025-09-05T14:57:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    57,
                    18,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:57:18Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    57,
                    18,
                    4,
                    248,
                    0
                ],
                "title": "Jamming Smarter, Not Harder: Exploiting O-RAN Y1 RAN Analytics for\n  Efficient Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jamming Smarter, Not Harder: Exploiting O-RAN Y1 RAN Analytics for\n  Efficient Interference"
                },
                "summary": "The Y1 interface in O-RAN enables the sharing of RAN Analytics Information\n(RAI) between the near-RT RIC and authorized Y1 consumers, which may be\ninternal applications within the operator's trusted domain or external systems\naccessing data through a secure exposure function. While this visibility\nenhances network optimization and enables advanced services, it also introduces\na potential security risk -- a malicious or compromised Y1 consumer could\nmisuse analytics to facilitate targeted interference. In this work, we\ndemonstrate how an adversary can exploit the Y1 interface to launch selective\njamming attacks by passively monitoring downlink metrics. We propose and\nevaluate two Y1-aided jamming strategies: a clustering-based jammer leveraging\nDBSCAN for traffic profiling and a threshold-based jammer. These are compared\nagainst two baselines strategies -- always-on jammer and random jammer -- on an\nover-the-air LTE/5G O-RAN testbed. Experimental results show that in\nunconstrained jamming budget scenarios, the threshold-based jammer can closely\nreplicate the disruption caused by always-on jamming while reducing\ntransmission time by 27\\%. Under constrained jamming budgets, the\nclustering-based jammer proves most effective, causing up to an 18.1\\% bitrate\ndrop while remaining active only 25\\% of the time. These findings reveal a\ncritical trade-off between jamming stealthiness and efficiency, and illustrate\nhow exposure of RAN analytics via the Y1 interface can enable highly targeted,\nlow-overhead attacks, raising important security considerations for both\ncivilian and mission-critical O-RAN deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Y1 interface in O-RAN enables the sharing of RAN Analytics Information\n(RAI) between the near-RT RIC and authorized Y1 consumers, which may be\ninternal applications within the operator's trusted domain or external systems\naccessing data through a secure exposure function. While this visibility\nenhances network optimization and enables advanced services, it also introduces\na potential security risk -- a malicious or compromised Y1 consumer could\nmisuse analytics to facilitate targeted interference. In this work, we\ndemonstrate how an adversary can exploit the Y1 interface to launch selective\njamming attacks by passively monitoring downlink metrics. We propose and\nevaluate two Y1-aided jamming strategies: a clustering-based jammer leveraging\nDBSCAN for traffic profiling and a threshold-based jammer. These are compared\nagainst two baselines strategies -- always-on jammer and random jammer -- on an\nover-the-air LTE/5G O-RAN testbed. Experimental results show that in\nunconstrained jamming budget scenarios, the threshold-based jammer can closely\nreplicate the disruption caused by always-on jamming while reducing\ntransmission time by 27\\%. Under constrained jamming budgets, the\nclustering-based jammer proves most effective, causing up to an 18.1\\% bitrate\ndrop while remaining active only 25\\% of the time. These findings reveal a\ncritical trade-off between jamming stealthiness and efficiency, and illustrate\nhow exposure of RAN analytics via the Y1 interface can enable highly targeted,\nlow-overhead attacks, raising important security considerations for both\ncivilian and mission-critical O-RAN deployments."
                },
                "authors": [
                    {
                        "name": "Abiodun Ganiyu"
                    },
                    {
                        "name": "Dara Ron"
                    },
                    {
                        "name": "Syed Rafiul Hussain"
                    },
                    {
                        "name": "Vijay K Shah"
                    }
                ],
                "author_detail": {
                    "name": "Vijay K Shah"
                },
                "author": "Vijay K Shah",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05145v1",
                "updated": "2025-09-05T14:38:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    38,
                    2,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:38:02Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    38,
                    2,
                    4,
                    248,
                    0
                ],
                "title": "Exploring Situated Stabilities of a Rhythm Generation System through\n  Variational Cross-Examination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Situated Stabilities of a Rhythm Generation System through\n  Variational Cross-Examination"
                },
                "summary": "This paper investigates GrooveTransformer, a real-time rhythm generation\nsystem, through the postphenomenological framework of Variational\nCross-Examination (VCE). By reflecting on its deployment across three distinct\nartistic contexts, we identify three stabilities: an autonomous drum\naccompaniment generator, a rhythmic control voltage sequencer in Eurorack\nformat, and a rhythm driver for a harmonic accompaniment system. The\nversatility of its applications was not an explicit goal from the outset of the\nproject. Thus, we ask: how did this multistability emerge? Through VCE, we\nidentify three key contributors to its emergence: the affordances of system\ninvariants, the interdisciplinary collaboration, and the situated nature of its\ndevelopment. We conclude by reflecting on the viability of VCE as a descriptive\nand analytical method for Digital Musical Instrument (DMI) design, emphasizing\nits value in uncovering how technologies mediate, co-shape, and are co-shaped\nby users and contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates GrooveTransformer, a real-time rhythm generation\nsystem, through the postphenomenological framework of Variational\nCross-Examination (VCE). By reflecting on its deployment across three distinct\nartistic contexts, we identify three stabilities: an autonomous drum\naccompaniment generator, a rhythmic control voltage sequencer in Eurorack\nformat, and a rhythm driver for a harmonic accompaniment system. The\nversatility of its applications was not an explicit goal from the outset of the\nproject. Thus, we ask: how did this multistability emerge? Through VCE, we\nidentify three key contributors to its emergence: the affordances of system\ninvariants, the interdisciplinary collaboration, and the situated nature of its\ndevelopment. We conclude by reflecting on the viability of VCE as a descriptive\nand analytical method for Digital Musical Instrument (DMI) design, emphasizing\nits value in uncovering how technologies mediate, co-shape, and are co-shaped\nby users and contexts."
                },
                "authors": [
                    {
                        "name": "Baej Kotowski"
                    },
                    {
                        "name": "Nicholas Evans"
                    },
                    {
                        "name": "Behzad Haki"
                    },
                    {
                        "name": "Frederic Font"
                    },
                    {
                        "name": "Sergi Jord"
                    }
                ],
                "author_detail": {
                    "name": "Sergi Jord"
                },
                "author": "Sergi Jord",
                "arxiv_comment": "AI Music Creativity 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22451v2",
                "updated": "2025-09-05T14:22:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    22,
                    32,
                    4,
                    248,
                    0
                ],
                "published": "2025-03-28T14:03:14Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    3,
                    14,
                    4,
                    87,
                    0
                ],
                "title": "STADE: Standard Deviation as a Pruning Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STADE: Standard Deviation as a Pruning Metric"
                },
                "summary": "Recently, Large Language Models (LLMs) have become very widespread and are\nused to solve a wide variety of tasks. To successfully handle these tasks, LLMs\nrequire longer training times and larger model sizes. This makes LLMs ideal\ncandidates for pruning methods that reduce computational demands while\nmaintaining performance. Previous methods require a retraining phase after\npruning to maintain the original model's performance. However, state-of-the-art\npruning methods, such as Wanda, prune the model without retraining, making the\npruning process faster and more efficient. Building upon Wanda's work, this\nstudy provides a theoretical explanation of why the method is effective and\nleverages these insights to enhance the pruning process. Specifically, a\ntheoretical analysis of the pruning problem reveals a common scenario in\nMachine Learning where Wanda is the optimal pruning method. Furthermore, this\nanalysis is extended to cases where Wanda is no longer optimal, leading to the\ndevelopment of a new method, STADE, based on the standard deviation of the\ninput. From a theoretical standpoint, STADE demonstrates better generality\nacross different scenarios. Finally, extensive experiments on Llama and Open\nPre-trained Transformers (OPT) models validate these theoretical findings,\nshowing that depending on the training conditions, Wanda's optimal performance\nvaries as predicted by the theoretical framework. These insights contribute to\na more robust understanding of pruning strategies and their practical\nimplications. Code is available at: https://github.com/Coello-dev/STADE/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have become very widespread and are\nused to solve a wide variety of tasks. To successfully handle these tasks, LLMs\nrequire longer training times and larger model sizes. This makes LLMs ideal\ncandidates for pruning methods that reduce computational demands while\nmaintaining performance. Previous methods require a retraining phase after\npruning to maintain the original model's performance. However, state-of-the-art\npruning methods, such as Wanda, prune the model without retraining, making the\npruning process faster and more efficient. Building upon Wanda's work, this\nstudy provides a theoretical explanation of why the method is effective and\nleverages these insights to enhance the pruning process. Specifically, a\ntheoretical analysis of the pruning problem reveals a common scenario in\nMachine Learning where Wanda is the optimal pruning method. Furthermore, this\nanalysis is extended to cases where Wanda is no longer optimal, leading to the\ndevelopment of a new method, STADE, based on the standard deviation of the\ninput. From a theoretical standpoint, STADE demonstrates better generality\nacross different scenarios. Finally, extensive experiments on Llama and Open\nPre-trained Transformers (OPT) models validate these theoretical findings,\nshowing that depending on the training conditions, Wanda's optimal performance\nvaries as predicted by the theoretical framework. These insights contribute to\na more robust understanding of pruning strategies and their practical\nimplications. Code is available at: https://github.com/Coello-dev/STADE/"
                },
                "authors": [
                    {
                        "name": "Diego Coello de Portugal Mecke"
                    },
                    {
                        "name": "Haya Alyoussef"
                    },
                    {
                        "name": "Maximilian Stubbemann"
                    },
                    {
                        "name": "Ilia Koloiarov"
                    },
                    {
                        "name": "Tom Hanika"
                    },
                    {
                        "name": "Lars Schmidt-Thieme"
                    }
                ],
                "author_detail": {
                    "name": "Lars Schmidt-Thieme"
                },
                "author": "Lars Schmidt-Thieme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05080v1",
                "updated": "2025-09-05T13:19:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    19,
                    51,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T13:19:51Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    19,
                    51,
                    4,
                    248,
                    0
                ],
                "title": "MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial\n  Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial\n  Trading"
                },
                "summary": "The inherent non-stationarity of financial markets and the complexity of\nmulti-modal information pose significant challenges to existing quantitative\ntrading models. Traditional methods relying on fixed structures and unimodal\ndata struggle to adapt to market regime shifts, while large language model\n(LLM)-driven solutions - despite their multi-modal comprehension - suffer from\nstatic strategies and homogeneous expert designs, lacking dynamic adjustment\nand fine-grained decision mechanisms. To address these limitations, we propose\nMM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on\nlarge language models. MM-DREX explicitly decouples market state perception\nfrom strategy execution to enable adaptive sequential decision-making in\nnon-stationary environments. Specifically, it (1) introduces a vision-language\nmodel (VLM)-powered dynamic router that jointly analyzes candlestick chart\npatterns and long-term temporal features to allocate real-time expert weights;\n(2) designs four heterogeneous trading experts (trend, reversal, breakout,\npositioning) generating specialized fine-grained sub-strategies; and (3)\nproposes an SFT-RL hybrid training paradigm to synergistically optimize the\nrouter's market classification capability and experts' risk-adjusted\ndecision-making. Extensive experiments on multi-modal datasets spanning stocks,\nfutures, and cryptocurrencies demonstrate that MM-DREX significantly\noutperforms 15 baselines (including state-of-the-art financial LLMs and deep\nreinforcement learning models) across key metrics: total return, Sharpe ratio,\nand maximum drawdown, validating its robustness and generalization.\nAdditionally, an interpretability module traces routing logic and expert\nbehavior in real time, providing an audit trail for strategy transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent non-stationarity of financial markets and the complexity of\nmulti-modal information pose significant challenges to existing quantitative\ntrading models. Traditional methods relying on fixed structures and unimodal\ndata struggle to adapt to market regime shifts, while large language model\n(LLM)-driven solutions - despite their multi-modal comprehension - suffer from\nstatic strategies and homogeneous expert designs, lacking dynamic adjustment\nand fine-grained decision mechanisms. To address these limitations, we propose\nMM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on\nlarge language models. MM-DREX explicitly decouples market state perception\nfrom strategy execution to enable adaptive sequential decision-making in\nnon-stationary environments. Specifically, it (1) introduces a vision-language\nmodel (VLM)-powered dynamic router that jointly analyzes candlestick chart\npatterns and long-term temporal features to allocate real-time expert weights;\n(2) designs four heterogeneous trading experts (trend, reversal, breakout,\npositioning) generating specialized fine-grained sub-strategies; and (3)\nproposes an SFT-RL hybrid training paradigm to synergistically optimize the\nrouter's market classification capability and experts' risk-adjusted\ndecision-making. Extensive experiments on multi-modal datasets spanning stocks,\nfutures, and cryptocurrencies demonstrate that MM-DREX significantly\noutperforms 15 baselines (including state-of-the-art financial LLMs and deep\nreinforcement learning models) across key metrics: total return, Sharpe ratio,\nand maximum drawdown, validating its robustness and generalization.\nAdditionally, an interpretability module traces routing logic and expert\nbehavior in real time, providing an audit trail for strategy transparency."
                },
                "authors": [
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Yueheng Jiang"
                    },
                    {
                        "name": "Zhaozhao Ma"
                    },
                    {
                        "name": "Yuchen Cao Jacky Keung"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Yiquan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03164v2",
                "updated": "2025-09-05T13:18:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    18,
                    9,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-03T09:30:39Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    9,
                    30,
                    39,
                    2,
                    246,
                    0
                ],
                "title": "OPRA-Vis: Visual Analytics System to Assist Organization-Public\n  Relationship Assessment with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPRA-Vis: Visual Analytics System to Assist Organization-Public\n  Relationship Assessment with Large Language Models"
                },
                "summary": "Analysis of public opinions collected from digital media helps organizations\nmaintain positive relationships with the public. Such public relations (PR)\nanalysis often involves assessing opinions, for example, measuring how strongly\npeople trust an organization. Pre-trained Large Language Models (LLMs) hold\ngreat promise for supporting Organization-Public Relationship Assessment (OPRA)\nbecause they can map unstructured public text to OPRA dimensions and articulate\nrationales through prompting. However, adapting LLMs for PR analysis typically\nrequires fine-tuning on large labeled datasets, which is both labor-intensive\nand knowledge-intensive, making it difficult for PR researchers to apply these\nmodels. In this paper, we present OPRA-Vis, a visual analytics system that\nleverages LLMs for OPRA without requiring extensive labeled data. Our framework\nemploys Chain-of-Thought prompting to guide LLMs in analyzing public opinion\ndata by incorporating PR expertise directly into the reasoning process.\nFurthermore, OPRA-Vis provides visualizations that reveal the clues and\nreasoning paths used by LLMs, enabling users to explore, critique, and refine\nmodel decisions. We demonstrate the effectiveness of OPRA-Vis through two\nreal-world use cases and evaluate it quantitatively, through comparisons with\nalternative LLMs and prompting strategies, and qualitatively, through\nassessments of usability, effectiveness, and expert feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of public opinions collected from digital media helps organizations\nmaintain positive relationships with the public. Such public relations (PR)\nanalysis often involves assessing opinions, for example, measuring how strongly\npeople trust an organization. Pre-trained Large Language Models (LLMs) hold\ngreat promise for supporting Organization-Public Relationship Assessment (OPRA)\nbecause they can map unstructured public text to OPRA dimensions and articulate\nrationales through prompting. However, adapting LLMs for PR analysis typically\nrequires fine-tuning on large labeled datasets, which is both labor-intensive\nand knowledge-intensive, making it difficult for PR researchers to apply these\nmodels. In this paper, we present OPRA-Vis, a visual analytics system that\nleverages LLMs for OPRA without requiring extensive labeled data. Our framework\nemploys Chain-of-Thought prompting to guide LLMs in analyzing public opinion\ndata by incorporating PR expertise directly into the reasoning process.\nFurthermore, OPRA-Vis provides visualizations that reveal the clues and\nreasoning paths used by LLMs, enabling users to explore, critique, and refine\nmodel decisions. We demonstrate the effectiveness of OPRA-Vis through two\nreal-world use cases and evaluate it quantitatively, through comparisons with\nalternative LLMs and prompting strategies, and qualitatively, through\nassessments of usability, effectiveness, and expert feedback."
                },
                "authors": [
                    {
                        "name": "Sangbong Yoo"
                    },
                    {
                        "name": "Seongbum Seo"
                    },
                    {
                        "name": "Chanyoung Yoon"
                    },
                    {
                        "name": "Hyelim Lee"
                    },
                    {
                        "name": "Jeong-Nam Kim"
                    },
                    {
                        "name": "Chansoo Kim"
                    },
                    {
                        "name": "Yun Jang"
                    },
                    {
                        "name": "Takanori Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takanori Fujiwara"
                },
                "author": "Takanori Fujiwara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00461v2",
                "updated": "2025-09-05T12:32:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    32,
                    22,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-30T11:31:04Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    11,
                    31,
                    4,
                    5,
                    242,
                    0
                ],
                "title": "TECP: Token-Entropy Conformal Prediction for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TECP: Token-Entropy Conformal Prediction for LLMs"
                },
                "summary": "Uncertainty quantification (UQ) for open-ended language generation remains a\ncritical yet underexplored challenge, especially under black-box constraints\nwhere internal model signals are inaccessible. In this paper, we introduce\nToken-Entropy Conformal Prediction (TECP), a novel framework that leverages\ntoken-level entropy as a logit-free, reference-free uncertainty measure and\nintegrates it into a split conformal prediction (CP) pipeline to construct\nprediction sets with formal coverage guarantees. Unlike existing approaches\nthat rely on semantic consistency heuristics or white-box features, TECP\ndirectly estimates epistemic uncertainty from the token entropy structure of\nsampled generations and calibrates uncertainty thresholds via CP quantiles to\nensure provable error control. Empirical evaluations across six large language\nmodels and two benchmarks (CoQA and TriviaQA) demonstrate that TECP\nconsistently achieves reliable coverage and compact prediction sets,\noutperforming prior self-consistency-based UQ methods. Our method provides a\nprincipled and efficient solution for trustworthy generation in black-box LLM\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) for open-ended language generation remains a\ncritical yet underexplored challenge, especially under black-box constraints\nwhere internal model signals are inaccessible. In this paper, we introduce\nToken-Entropy Conformal Prediction (TECP), a novel framework that leverages\ntoken-level entropy as a logit-free, reference-free uncertainty measure and\nintegrates it into a split conformal prediction (CP) pipeline to construct\nprediction sets with formal coverage guarantees. Unlike existing approaches\nthat rely on semantic consistency heuristics or white-box features, TECP\ndirectly estimates epistemic uncertainty from the token entropy structure of\nsampled generations and calibrates uncertainty thresholds via CP quantiles to\nensure provable error control. Empirical evaluations across six large language\nmodels and two benchmarks (CoQA and TriviaQA) demonstrate that TECP\nconsistently achieves reliable coverage and compact prediction sets,\noutperforming prior self-consistency-based UQ methods. Our method provides a\nprincipled and efficient solution for trustworthy generation in black-box LLM\nsettings."
                },
                "authors": [
                    {
                        "name": "Beining Xu"
                    },
                    {
                        "name": "Yongming Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yongming Lu"
                },
                "author": "Yongming Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08894v3",
                "updated": "2025-09-05T12:09:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    9,
                    41,
                    4,
                    248,
                    0
                ],
                "published": "2024-05-14T18:12:09Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    18,
                    12,
                    9,
                    1,
                    135,
                    0
                ],
                "title": "Global weight optimization of frame structures under free-vibration\n  eigenvalue constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global weight optimization of frame structures under free-vibration\n  eigenvalue constraints"
                },
                "summary": "Topology optimization of frame structures under free-vibration eigenvalue\nconstraints constitutes a challenging nonconvex polynomial optimization problem\nwith disconnected feasible sets. In this article, we first formulate it as a\npolynomial semidefinite programming problem (SDP) of minimizing a linear\nfunction over a basic semi-algebraic feasible set. We then propose to solve\nthis problem by Lasserre hierarchy of linear semidefinite relaxations providing\na sequence of increasing lower bounds. To obtain also a sequence of upper\nbounds and thus conditions on global $\\varepsilon$-optimality, we provide a\nbilevel reformulation that exhibits a special structure: The lower level is\nquasiconvex univariate and it has a non-empty interior if the constraints of\nthe upper-level problem are satisfied. After deriving the conditions for the\nsolvability of the lower-level problem, we thus provide a way to construct\nfeasible points to the original SDP. Using such a feasible point, we modify the\noriginal nonlinear SDP to satisfy the conditions for the deployment of the\nLasserre hierarchy. Solving arbitrary degree relaxation of the hierarchy, we\nprove that scaled first-order moments associated with the problem variables\nsatisfy feasibility conditions for the lower-level problem and thus provide\nguaranteed upper and lower bounds on the objective function. Using these\nbounds, we develop a simple sufficient condition for global\n$\\varepsilon$-optimality and prove that the optimality gap $\\varepsilon$\nconverges to zero if the set of global minimizers is convex. Finally, we\nillustrate these results with four representative problems for which the\nhierarchy converges in at most five relaxation degrees",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology optimization of frame structures under free-vibration eigenvalue\nconstraints constitutes a challenging nonconvex polynomial optimization problem\nwith disconnected feasible sets. In this article, we first formulate it as a\npolynomial semidefinite programming problem (SDP) of minimizing a linear\nfunction over a basic semi-algebraic feasible set. We then propose to solve\nthis problem by Lasserre hierarchy of linear semidefinite relaxations providing\na sequence of increasing lower bounds. To obtain also a sequence of upper\nbounds and thus conditions on global $\\varepsilon$-optimality, we provide a\nbilevel reformulation that exhibits a special structure: The lower level is\nquasiconvex univariate and it has a non-empty interior if the constraints of\nthe upper-level problem are satisfied. After deriving the conditions for the\nsolvability of the lower-level problem, we thus provide a way to construct\nfeasible points to the original SDP. Using such a feasible point, we modify the\noriginal nonlinear SDP to satisfy the conditions for the deployment of the\nLasserre hierarchy. Solving arbitrary degree relaxation of the hierarchy, we\nprove that scaled first-order moments associated with the problem variables\nsatisfy feasibility conditions for the lower-level problem and thus provide\nguaranteed upper and lower bounds on the objective function. Using these\nbounds, we develop a simple sufficient condition for global\n$\\varepsilon$-optimality and prove that the optimality gap $\\varepsilon$\nconverges to zero if the set of global minimizers is convex. Finally, we\nillustrate these results with four representative problems for which the\nhierarchy converges in at most five relaxation degrees"
                },
                "authors": [
                    {
                        "name": "Marek Tyburec"
                    },
                    {
                        "name": "Michal Kovara"
                    },
                    {
                        "name": "Marouan Handa"
                    },
                    {
                        "name": "Jan Zeman"
                    }
                ],
                "author_detail": {
                    "name": "Jan Zeman"
                },
                "author": "Jan Zeman",
                "arxiv_comment": "30 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05042v1",
                "updated": "2025-09-05T12:06:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    6,
                    6,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T12:06:06Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    12,
                    6,
                    6,
                    4,
                    248,
                    0
                ],
                "title": "Shared Autonomy through LLMs and Reinforcement Learning for Applications\n  to Ship Hull Inspections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Autonomy through LLMs and Reinforcement Learning for Applications\n  to Ship Hull Inspections"
                },
                "summary": "Shared autonomy is a promising paradigm in robotic systems, particularly\nwithin the maritime domain, where complex, high-risk, and uncertain\nenvironments necessitate effective human-robot collaboration. This paper\ninvestigates the interaction of three complementary approaches to advance\nshared autonomy in heterogeneous marine robotic fleets: (i) the integration of\nLarge Language Models (LLMs) to facilitate intuitive high-level task\nspecification and support hull inspection missions, (ii) the implementation of\nhuman-in-the-loop interaction frameworks in multi-agent settings to enable\nadaptive and intent-aware coordination, and (iii) the development of a modular\nMission Manager based on Behavior Trees to provide interpretable and flexible\nmission control. Preliminary results from simulation and real-world lake-like\nenvironments demonstrate the potential of this multi-layered architecture to\nreduce operator cognitive load, enhance transparency, and improve adaptive\nbehaviour alignment with human intent. Ongoing work focuses on fully\nintegrating these components, refining coordination mechanisms, and validating\nthe system in operational port scenarios. This study contributes to\nestablishing a modular and scalable foundation for trustworthy,\nhuman-collaborative autonomy in safety-critical maritime robotics applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared autonomy is a promising paradigm in robotic systems, particularly\nwithin the maritime domain, where complex, high-risk, and uncertain\nenvironments necessitate effective human-robot collaboration. This paper\ninvestigates the interaction of three complementary approaches to advance\nshared autonomy in heterogeneous marine robotic fleets: (i) the integration of\nLarge Language Models (LLMs) to facilitate intuitive high-level task\nspecification and support hull inspection missions, (ii) the implementation of\nhuman-in-the-loop interaction frameworks in multi-agent settings to enable\nadaptive and intent-aware coordination, and (iii) the development of a modular\nMission Manager based on Behavior Trees to provide interpretable and flexible\nmission control. Preliminary results from simulation and real-world lake-like\nenvironments demonstrate the potential of this multi-layered architecture to\nreduce operator cognitive load, enhance transparency, and improve adaptive\nbehaviour alignment with human intent. Ongoing work focuses on fully\nintegrating these components, refining coordination mechanisms, and validating\nthe system in operational port scenarios. This study contributes to\nestablishing a modular and scalable foundation for trustworthy,\nhuman-collaborative autonomy in safety-critical maritime robotics applications."
                },
                "authors": [
                    {
                        "name": "Cristiano Caissutti"
                    },
                    {
                        "name": "Estelle Gerbier"
                    },
                    {
                        "name": "Ehsan Khorrambakht"
                    },
                    {
                        "name": "Paolo Marinelli"
                    },
                    {
                        "name": "Andrea Munafo'"
                    },
                    {
                        "name": "Andrea Caiti"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Caiti"
                },
                "author": "Andrea Caiti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14387v2",
                "updated": "2025-09-05T11:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    46,
                    29,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-17T10:33:23Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    10,
                    33,
                    23,
                    1,
                    168,
                    0
                ],
                "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning"
                },
                "summary": "Existing work on mitigating catastrophic forgetting during large language\nmodels (LLMs) fine-tuning for new knowledge instances has primarily focused on\npreserving performance on previously seen data, while critically overlooking\nthe collapse of essential capabilities instilled through alignment, most\nnotably the model's ability to faithfully express epistemic uncertainty (a\nproperty we term 'Ignorance Awareness'). In this work, we formalize the notion\nof Ignorance Awareness and illustrate that conventional fine-tuning methods can\nresult in substantial activation displacement. This displacement undermines the\ncritical capability of ignorance awareness, leading to undesirable behaviors\nsuch as hallucinations. To address this challenge, we introduce SEAT, a simple\nand principled fine-tuning approach that not only enables the model to\neffectively acquire new knowledge instances but also preserves its aligned\nignorance awareness. SEAT integrates two key components: (1) sparse tuning that\nconstrains activation drift, and (2) a novel entity perturbation method\ndesigned to counter knowledge entanglement. Experimental results demonstrate\nthat, across both real-world and synthetic datasets, SEAT significantly\noutperforms baselines in preserving ignorance awareness while retaining optimal\nfine-tuning performance, offering a more robust solution for LLM fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing work on mitigating catastrophic forgetting during large language\nmodels (LLMs) fine-tuning for new knowledge instances has primarily focused on\npreserving performance on previously seen data, while critically overlooking\nthe collapse of essential capabilities instilled through alignment, most\nnotably the model's ability to faithfully express epistemic uncertainty (a\nproperty we term 'Ignorance Awareness'). In this work, we formalize the notion\nof Ignorance Awareness and illustrate that conventional fine-tuning methods can\nresult in substantial activation displacement. This displacement undermines the\ncritical capability of ignorance awareness, leading to undesirable behaviors\nsuch as hallucinations. To address this challenge, we introduce SEAT, a simple\nand principled fine-tuning approach that not only enables the model to\neffectively acquire new knowledge instances but also preserves its aligned\nignorance awareness. SEAT integrates two key components: (1) sparse tuning that\nconstrains activation drift, and (2) a novel entity perturbation method\ndesigned to counter knowledge entanglement. Experimental results demonstrate\nthat, across both real-world and synthetic datasets, SEAT significantly\noutperforms baselines in preserving ignorance awareness while retaining optimal\nfine-tuning performance, offering a more robust solution for LLM fine-tuning."
                },
                "authors": [
                    {
                        "name": "William F. Shen"
                    },
                    {
                        "name": "Xinchi Qiu"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14295v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14295v5",
                "updated": "2025-09-05T11:39:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    39,
                    31,
                    4,
                    248,
                    0
                ],
                "published": "2024-01-25T16:34:00Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    16,
                    34,
                    0,
                    3,
                    25,
                    0
                ],
                "title": "Demystifying Chains, Trees, and Graphs of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Chains, Trees, and Graphs of Thoughts"
                },
                "summary": "The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Florim Memedi"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Guangyuan Piao"
                    },
                    {
                        "name": "Nils Blach"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Grzegorz Kwaniewski"
                    },
                    {
                        "name": "Jrgen Mller"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Aidan O'Mahony"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "arxiv_doi": "10.1109/TPAMI.2025.3598182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2025.3598182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14295v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14295v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00478v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00478v2",
                "updated": "2025-09-05T11:33:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    33,
                    48,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-30T12:27:29Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    12,
                    27,
                    29,
                    5,
                    242,
                    0
                ],
                "title": "Pilot Allocation and Receiver Design for Cell-Free Massive MIMO ISAC\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pilot Allocation and Receiver Design for Cell-Free Massive MIMO ISAC\n  Systems"
                },
                "summary": "This paper tackles two key challenges in cell-freemassive multiple input\nmultiple output (CF-mMIMO) systems:efficient pilot allocation and practical\nreceiver design. To thisend, we introduce a novel pilot allocation framework\nleveragingmanifold optimization to maximize the system sum rate, wherepilot\nsequences are designed as nearly orthogonal sequences. Theproposed pilot design\nenforces unimodularity constraints in thefrequency domain, ensuring pilots are\nsuitable for both communi-cation and sensing tasks. Additionally, a gaussian\nbelief propaga-tion (GaBP)-based receiver is introduced, providing\nnear-optimaldetection performance with substantially reduced\ncomputationalcomplexity. Simulation results demonstrate that the proposedpilot\nallocation method achieves communication performancecomparable to\nstate-of-the-art (SotA) algorithms, while deliveringsuperior sensing\ncapabilities due to its unimodular pilot design.The GaBP-based receiver\nachieves robust performance andlower complexity compared to conventional\napproaches. Thesecontributions advance the practical deployment of CF-mMIMOfor\nintegrated sensing and communications (ISAC).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles two key challenges in cell-freemassive multiple input\nmultiple output (CF-mMIMO) systems:efficient pilot allocation and practical\nreceiver design. To thisend, we introduce a novel pilot allocation framework\nleveragingmanifold optimization to maximize the system sum rate, wherepilot\nsequences are designed as nearly orthogonal sequences. Theproposed pilot design\nenforces unimodularity constraints in thefrequency domain, ensuring pilots are\nsuitable for both communi-cation and sensing tasks. Additionally, a gaussian\nbelief propaga-tion (GaBP)-based receiver is introduced, providing\nnear-optimaldetection performance with substantially reduced\ncomputationalcomplexity. Simulation results demonstrate that the proposedpilot\nallocation method achieves communication performancecomparable to\nstate-of-the-art (SotA) algorithms, while deliveringsuperior sensing\ncapabilities due to its unimodular pilot design.The GaBP-based receiver\nachieves robust performance andlower complexity compared to conventional\napproaches. Thesecontributions advance the practical deployment of CF-mMIMOfor\nintegrated sensing and communications (ISAC)."
                },
                "authors": [
                    {
                        "name": "Getuar Rexhepi"
                    },
                    {
                        "name": "Kuranage Roche Rayan Ranasinghe"
                    },
                    {
                        "name": "Kengo Ando"
                    },
                    {
                        "name": "Giuseppe Thadeu Freitas de Abreu"
                    },
                    {
                        "name": "David Gonzalez G"
                    }
                ],
                "author_detail": {
                    "name": "David Gonzalez G"
                },
                "author": "David Gonzalez G",
                "arxiv_comment": "Submitted to IEEE Transactions on Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00478v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14506v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14506v3",
                "updated": "2025-09-05T11:16:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    16,
                    45,
                    4,
                    248,
                    0
                ],
                "published": "2024-09-22T16:10:10Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    10,
                    10,
                    6,
                    266,
                    0
                ],
                "title": "InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic\n  Robot Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic\n  Robot Autonomy"
                },
                "summary": "We introduce an interactive LLM-based framework designed to enhance the\nautonomy and robustness of domestic robots, targeting embodied intelligence.\nOur approach reduces reliance on large-scale data and incorporates a\nrobot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan,\nensures that the LLM's decision-making capabilities are effectively aligned\nwith robotic functions, enhancing operational robustness and adaptability,\nwhile our human-in-the-loop mechanism allows for real-time human intervention\nwhen user instruction is required. We evaluate our method in both simulation\nand on the real Toyota Human Support Robot and Anymal D-Unitree Z1 platforms.\nOur method achieves a 95% success rate in the 'fetch me' task completion with\nfailure recovery, highlighting its capability in both failure reasoning and\ntask planning. InteLiPlan achieves comparable performance to state-of-the-art\nlarge-scale LLM-based robotics planners, while using only real-time onboard\ncomputing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an interactive LLM-based framework designed to enhance the\nautonomy and robustness of domestic robots, targeting embodied intelligence.\nOur approach reduces reliance on large-scale data and incorporates a\nrobot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan,\nensures that the LLM's decision-making capabilities are effectively aligned\nwith robotic functions, enhancing operational robustness and adaptability,\nwhile our human-in-the-loop mechanism allows for real-time human intervention\nwhen user instruction is required. We evaluate our method in both simulation\nand on the real Toyota Human Support Robot and Anymal D-Unitree Z1 platforms.\nOur method achieves a 95% success rate in the 'fetch me' task completion with\nfailure recovery, highlighting its capability in both failure reasoning and\ntask planning. InteLiPlan achieves comparable performance to state-of-the-art\nlarge-scale LLM-based robotics planners, while using only real-time onboard\ncomputing."
                },
                "authors": [
                    {
                        "name": "Kim Tien Ly"
                    },
                    {
                        "name": "Kai Lu"
                    },
                    {
                        "name": "Ioannis Havoutis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Havoutis"
                },
                "author": "Ioannis Havoutis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14506v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04365v3",
                "updated": "2025-09-05T11:08:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    11,
                    8,
                    45,
                    4,
                    248,
                    0
                ],
                "published": "2025-04-06T05:30:10Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    5,
                    30,
                    10,
                    6,
                    96,
                    0
                ],
                "title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPDL: Automatic Prompt Optimization for LLM Agents"
                },
                "summary": "The performance of large language models (LLMs) depends on how they are\nprompted, with choices spanning both the high-level prompting pattern (e.g.,\nZero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and\nfew-shot demonstrations). Manually tuning this combination is tedious,\nerror-prone, and specific to a given LLM and task. Therefore, this paper\nproposes AutoPDL, an automated approach to discovering good LLM agent\nconfigurations. Our approach frames this as a structured AutoML problem over a\ncombinatorial space of agentic and non-agentic prompting patterns and\ndemonstrations, using successive halving to efficiently navigate this space. We\nintroduce a library implementing common prompting patterns using the PDL prompt\nprogramming language. AutoPDL solutions are human-readable, editable, and\nexecutable PDL programs that use this library. This approach also enables\nsource-to-source optimization, allowing human-in-the-loop refinement and reuse.\nEvaluations across three tasks and seven LLMs (ranging from 3B to 70B\nparameters) show consistent accuracy gains ($9.21\\pm15.46$ percentage points),\nup to 67.5pp, and reveal that selected prompting strategies vary across models\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) depends on how they are\nprompted, with choices spanning both the high-level prompting pattern (e.g.,\nZero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and\nfew-shot demonstrations). Manually tuning this combination is tedious,\nerror-prone, and specific to a given LLM and task. Therefore, this paper\nproposes AutoPDL, an automated approach to discovering good LLM agent\nconfigurations. Our approach frames this as a structured AutoML problem over a\ncombinatorial space of agentic and non-agentic prompting patterns and\ndemonstrations, using successive halving to efficiently navigate this space. We\nintroduce a library implementing common prompting patterns using the PDL prompt\nprogramming language. AutoPDL solutions are human-readable, editable, and\nexecutable PDL programs that use this library. This approach also enables\nsource-to-source optimization, allowing human-in-the-loop refinement and reuse.\nEvaluations across three tasks and seven LLMs (ranging from 3B to 70B\nparameters) show consistent accuracy gains ($9.21\\pm15.46$ percentage points),\nup to 67.5pp, and reveal that selected prompting strategies vary across models\nand tasks."
                },
                "authors": [
                    {
                        "name": "Claudio Spiess"
                    },
                    {
                        "name": "Mandana Vaziri"
                    },
                    {
                        "name": "Louis Mandel"
                    },
                    {
                        "name": "Martin Hirzel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Hirzel"
                },
                "author": "Martin Hirzel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04996v1",
                "updated": "2025-09-05T10:43:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    43,
                    12,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T10:43:12Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    43,
                    12,
                    4,
                    248,
                    0
                ],
                "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient\n  Vision-Language-Action Flow Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLOWER: Democratizing Generalist Robot Policies with Efficient\n  Vision-Language-Action Flow Policies"
                },
                "summary": "Developing efficient Vision-Language-Action (VLA) policies is crucial for\npractical robotics deployment, yet current approaches face prohibitive\ncomputational costs and resource requirements. Existing diffusion-based VLA\npolicies require multi-billion-parameter models and massive datasets to achieve\nstrong performance. We tackle this efficiency challenge with two contributions:\nintermediate-modality fusion, which reallocates capacity to the diffusion head\nby pruning up to $50\\%$ of LLM layers, and action-specific Global-AdaLN\nconditioning, which cuts parameters by $20\\%$ through modular adaptation. We\nintegrate these advances into a novel 950 M-parameter VLA called FLOWER.\nPretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance\nwith bigger VLAs across $190$ tasks spanning ten simulation and real-world\nbenchmarks and demonstrates robustness across diverse robotic embodiments. In\naddition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.\nDemos, code and pretrained weights are available at\nhttps://intuitive-robots.github.io/flower_vla/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing efficient Vision-Language-Action (VLA) policies is crucial for\npractical robotics deployment, yet current approaches face prohibitive\ncomputational costs and resource requirements. Existing diffusion-based VLA\npolicies require multi-billion-parameter models and massive datasets to achieve\nstrong performance. We tackle this efficiency challenge with two contributions:\nintermediate-modality fusion, which reallocates capacity to the diffusion head\nby pruning up to $50\\%$ of LLM layers, and action-specific Global-AdaLN\nconditioning, which cuts parameters by $20\\%$ through modular adaptation. We\nintegrate these advances into a novel 950 M-parameter VLA called FLOWER.\nPretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance\nwith bigger VLAs across $190$ tasks spanning ten simulation and real-world\nbenchmarks and demonstrates robustness across diverse robotic embodiments. In\naddition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.\nDemos, code and pretrained weights are available at\nhttps://intuitive-robots.github.io/flower_vla/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Hongyi Zhou"
                    },
                    {
                        "name": "Marcel Rhle"
                    },
                    {
                        "name": "mer Erdin Yamurlu"
                    },
                    {
                        "name": "Fabian Otto"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "arxiv_comment": "Published at CoRL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04993v1",
                "updated": "2025-09-05T10:40:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    40,
                    31,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T10:40:31Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    40,
                    31,
                    4,
                    248,
                    0
                ],
                "title": "LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of\n  Dual-Loop Edge-Terminal Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Enabled Multi-Agent System for 6G Networks: Framework and Method of\n  Dual-Loop Edge-Terminal Collaboration"
                },
                "summary": "The ubiquitous computing resources in 6G networks provide ideal environments\nfor the fusion of large language models (LLMs) and intelligent services through\nthe agent framework. With auxiliary modules and planning cores, LLM-enabled\nagents can autonomously plan and take actions to deal with diverse environment\nsemantics and user intentions. However, the limited resources of individual\nnetwork devices significantly hinder the efficient operation of LLM-enabled\nagents with complex tool calls, highlighting the urgent need for efficient\nmulti-level device collaborations. To this end, the framework and method of the\nLLM-enabled multi-agent system with dual-loop terminal-edge collaborations are\nproposed in 6G networks. Firstly, the outer loop consists of the iterative\ncollaborations between the global agent and multiple sub-agents deployed on\nedge servers and terminals, where the planning capability is enhanced through\ntask decomposition and parallel sub-task distribution. Secondly, the inner loop\nutilizes sub-agents with dedicated roles to circularly reason, execute, and\nreplan the sub-task, and the parallel tool calling generation with offloading\nstrategies is incorporated to improve efficiency. The improved task planning\ncapability and task execution efficiency are validated through the conducted\ncase study in 6G-supported urban safety governance. Finally, the open\nchallenges and future directions are thoroughly analyzed in 6G networks,\naccelerating the advent of the 6G era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ubiquitous computing resources in 6G networks provide ideal environments\nfor the fusion of large language models (LLMs) and intelligent services through\nthe agent framework. With auxiliary modules and planning cores, LLM-enabled\nagents can autonomously plan and take actions to deal with diverse environment\nsemantics and user intentions. However, the limited resources of individual\nnetwork devices significantly hinder the efficient operation of LLM-enabled\nagents with complex tool calls, highlighting the urgent need for efficient\nmulti-level device collaborations. To this end, the framework and method of the\nLLM-enabled multi-agent system with dual-loop terminal-edge collaborations are\nproposed in 6G networks. Firstly, the outer loop consists of the iterative\ncollaborations between the global agent and multiple sub-agents deployed on\nedge servers and terminals, where the planning capability is enhanced through\ntask decomposition and parallel sub-task distribution. Secondly, the inner loop\nutilizes sub-agents with dedicated roles to circularly reason, execute, and\nreplan the sub-task, and the parallel tool calling generation with offloading\nstrategies is incorporated to improve efficiency. The improved task planning\ncapability and task execution efficiency are validated through the conducted\ncase study in 6G-supported urban safety governance. Finally, the open\nchallenges and future directions are thoroughly analyzed in 6G networks,\naccelerating the advent of the 6G era."
                },
                "authors": [
                    {
                        "name": "Zheyan Qu"
                    },
                    {
                        "name": "Wenbo Wang"
                    },
                    {
                        "name": "Zitong Yu"
                    },
                    {
                        "name": "Boquan Sun"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xing Zhang"
                },
                "author": "Xing Zhang",
                "arxiv_comment": "This paper has been accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04091v2",
                "updated": "2025-09-05T10:34:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    34,
                    25,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T10:48:02Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    10,
                    48,
                    2,
                    3,
                    247,
                    0
                ],
                "title": "Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its\n  Implications Across Security Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its\n  Implications Across Security Tasks"
                },
                "summary": "Accurate detection of third-party libraries (TPLs) is fundamental to Android\nsecurity, supporting vulnerability tracking, malware detection, and supply\nchain auditing. Despite many proposed tools, their real-world effectiveness\nremains unclear. We present the first large-scale empirical study of ten\nstate-of-the-art TPL detection techniques across over 6,000 apps, enabled by a\nnew ground truth dataset with precise version-level annotations for both remote\nand local dependencies. Our evaluation exposes tool fragility to R8-era\ntransformations, weak version discrimination, inaccurate correspondence of\ncandidate libraries, difficulty in generalizing similarity thresholds, and\nprohibitive runtime/memory overheads at scale. Beyond tool assessment, we\nfurther analyze how TPLs shape downstream tasks, including vulnerability\nanalysis, malware detection, secret leakage assessment, and LLM-based\nevaluation. From this perspective, our study provides concrete insights into\nhow TPL characteristics affect these tasks and informs future improvements in\nsecurity analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate detection of third-party libraries (TPLs) is fundamental to Android\nsecurity, supporting vulnerability tracking, malware detection, and supply\nchain auditing. Despite many proposed tools, their real-world effectiveness\nremains unclear. We present the first large-scale empirical study of ten\nstate-of-the-art TPL detection techniques across over 6,000 apps, enabled by a\nnew ground truth dataset with precise version-level annotations for both remote\nand local dependencies. Our evaluation exposes tool fragility to R8-era\ntransformations, weak version discrimination, inaccurate correspondence of\ncandidate libraries, difficulty in generalizing similarity thresholds, and\nprohibitive runtime/memory overheads at scale. Beyond tool assessment, we\nfurther analyze how TPLs shape downstream tasks, including vulnerability\nanalysis, malware detection, secret leakage assessment, and LLM-based\nevaluation. From this perspective, our study provides concrete insights into\nhow TPL characteristics affect these tasks and informs future improvements in\nsecurity analysis."
                },
                "authors": [
                    {
                        "name": "Jintao Gu"
                    },
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Yihan Lin"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yuchun Guo"
                    },
                    {
                        "name": "Yigui Cao"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "20pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5; D.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04979v1",
                "updated": "2025-09-05T10:04:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    4,
                    33,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T10:04:33Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    4,
                    33,
                    4,
                    248,
                    0
                ],
                "title": "Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for\n  Ranking Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for\n  Ranking Agents"
                },
                "summary": "AI agents -- powered by reasoning-capable large language models (LLMs) and\nintegrated with tools, data, and web search -- are poised to transform the\ninternet into a \\emph{Web of Agents}: a machine-native ecosystem where\nautonomous agents interact, collaborate, and execute tasks at scale. Realizing\nthis vision requires \\emph{Agent Ranking} -- selecting agents not only by\ndeclared capabilities but by proven, recent performance. Unlike Web~1.0's\nPageRank, a global, transparent network of agent interactions does not exist;\nusage signals are fragmented and private, making ranking infeasible without\ncoordination.\n  We propose \\textbf{DOVIS}, a five-layer operational protocol\n(\\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that\nenables the collection of minimal, privacy-preserving aggregates of usage and\nperformance across the ecosystem. On this substrate, we implement\n\\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines\n\\emph{usage} (selection frequency) and \\emph{competence} (outcome quality,\ncost, safety, latency) into a unified ranking. We present simulation results\nand theoretical guarantees on convergence, robustness, and Sybil resistance,\ndemonstrating the viability of coordinated protocols and performance-aware\nranking in enabling a scalable, trustworthy Agentic Web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents -- powered by reasoning-capable large language models (LLMs) and\nintegrated with tools, data, and web search -- are poised to transform the\ninternet into a \\emph{Web of Agents}: a machine-native ecosystem where\nautonomous agents interact, collaborate, and execute tasks at scale. Realizing\nthis vision requires \\emph{Agent Ranking} -- selecting agents not only by\ndeclared capabilities but by proven, recent performance. Unlike Web~1.0's\nPageRank, a global, transparent network of agent interactions does not exist;\nusage signals are fragmented and private, making ranking infeasible without\ncoordination.\n  We propose \\textbf{DOVIS}, a five-layer operational protocol\n(\\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that\nenables the collection of minimal, privacy-preserving aggregates of usage and\nperformance across the ecosystem. On this substrate, we implement\n\\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines\n\\emph{usage} (selection frequency) and \\emph{competence} (outcome quality,\ncost, safety, latency) into a unified ranking. We present simulation results\nand theoretical guarantees on convergence, robustness, and Sybil resistance,\ndemonstrating the viability of coordinated protocols and performance-aware\nranking in enabling a scalable, trustworthy Agentic Web."
                },
                "authors": [
                    {
                        "name": "Rajesh Tembarai Krishnamachari"
                    },
                    {
                        "name": "Srividya Rajesh"
                    }
                ],
                "author_detail": {
                    "name": "Srividya Rajesh"
                },
                "author": "Srividya Rajesh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04969v1",
                "updated": "2025-09-05T09:49:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    49,
                    39,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T09:49:39Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    49,
                    39,
                    4,
                    248,
                    0
                ],
                "title": "Classification of kinetic-related injury in hospital triage data using\n  NLP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification of kinetic-related injury in hospital triage data using\n  NLP"
                },
                "summary": "Triage notes, created at the start of a patient's hospital visit, contain a\nwealth of information that can help medical staff and researchers understand\nEmergency Department patient epidemiology and the degree of time-dependent\nillness or injury. Unfortunately, applying modern Natural Language Processing\nand Machine Learning techniques to analyse triage data faces some challenges:\nFirstly, hospital data contains highly sensitive information that is subject to\nprivacy regulation thus need to be analysed on site; Secondly, most hospitals\nand medical facilities lack the necessary hardware to fine-tune a Large\nLanguage Model (LLM), much less training one from scratch; Lastly, to identify\nthe records of interest, expert inputs are needed to manually label the\ndatasets, which can be time-consuming and costly. We present in this paper a\npipeline that enables the classification of triage data using LLM and limited\ncompute resources. We first fine-tuned a pre-trained LLM with a classifier\nusing a small (2k) open sourced dataset on a GPU; and then further fine-tuned\nthe model with a hospital specific dataset of 1000 samples on a CPU. We\ndemonstrated that by carefully curating the datasets and leveraging existing\nmodels and open sourced data, we can successfully classify triage data with\nlimited compute resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triage notes, created at the start of a patient's hospital visit, contain a\nwealth of information that can help medical staff and researchers understand\nEmergency Department patient epidemiology and the degree of time-dependent\nillness or injury. Unfortunately, applying modern Natural Language Processing\nand Machine Learning techniques to analyse triage data faces some challenges:\nFirstly, hospital data contains highly sensitive information that is subject to\nprivacy regulation thus need to be analysed on site; Secondly, most hospitals\nand medical facilities lack the necessary hardware to fine-tune a Large\nLanguage Model (LLM), much less training one from scratch; Lastly, to identify\nthe records of interest, expert inputs are needed to manually label the\ndatasets, which can be time-consuming and costly. We present in this paper a\npipeline that enables the classification of triage data using LLM and limited\ncompute resources. We first fine-tuned a pre-trained LLM with a classifier\nusing a small (2k) open sourced dataset on a GPU; and then further fine-tuned\nthe model with a hospital specific dataset of 1000 samples on a CPU. We\ndemonstrated that by carefully curating the datasets and leveraging existing\nmodels and open sourced data, we can successfully classify triage data with\nlimited compute resources."
                },
                "authors": [
                    {
                        "name": "Midhun Shyam"
                    },
                    {
                        "name": "Jim Basilakis"
                    },
                    {
                        "name": "Kieran Luken"
                    },
                    {
                        "name": "Steven Thomas"
                    },
                    {
                        "name": "John Crozier"
                    },
                    {
                        "name": "Paul M. Middleton"
                    },
                    {
                        "name": "X. Rosalind Wang"
                    }
                ],
                "author_detail": {
                    "name": "X. Rosalind Wang"
                },
                "author": "X. Rosalind Wang",
                "arxiv_comment": "Accepted as a short paper for publishing at ADMA 2025\n  (https://adma2025.github.io), with Supplementary Material available at\n  https://github.com/CRMDS/Kinetic-Injury-Triage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17450v2",
                "updated": "2025-09-05T09:48:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    48,
                    4,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-24T17:08:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    17,
                    8,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability\n  in Knowledge and Safety with DuET-PD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability\n  in Knowledge and Safety with DuET-PD"
                },
                "summary": "Large Language Models (LLMs) can struggle to balance gullibility to\nmisinformation and resistance to valid corrections in persuasive dialogues, a\ncritical challenge for reliable deployment. We introduce DuET-PD (Dual\nEvaluation for Trust in Persuasive Dialogues), a framework evaluating\nmulti-turn stance-change dynamics across dual dimensions: persuasion type\n(corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via\nSALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves\nonly 27.32% accuracy in MMLU-Pro under sustained misleading persuasions.\nMoreover, results reveal a concerning trend of increasing sycophancy in newer\nopen-source models. To address this, we introduce Holistic DPO, a training\napproach balancing positive and negative persuasion examples. Unlike prompting\nor resist-only training, Holistic DPO enhances both robustness to\nmisinformation and receptiveness to corrections, improving\nLlama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts\nfrom 4.21% to 76.54%. These contributions offer a pathway to developing more\nreliable and adaptable LLMs for multi-turn dialogue. Code is available at\nhttps://github.com/Social-AI-Studio/DuET-PD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can struggle to balance gullibility to\nmisinformation and resistance to valid corrections in persuasive dialogues, a\ncritical challenge for reliable deployment. We introduce DuET-PD (Dual\nEvaluation for Trust in Persuasive Dialogues), a framework evaluating\nmulti-turn stance-change dynamics across dual dimensions: persuasion type\n(corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via\nSALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves\nonly 27.32% accuracy in MMLU-Pro under sustained misleading persuasions.\nMoreover, results reveal a concerning trend of increasing sycophancy in newer\nopen-source models. To address this, we introduce Holistic DPO, a training\napproach balancing positive and negative persuasion examples. Unlike prompting\nor resist-only training, Holistic DPO enhances both robustness to\nmisinformation and receptiveness to corrections, improving\nLlama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts\nfrom 4.21% to 76.54%. These contributions offer a pathway to developing more\nreliable and adaptable LLMs for multi-turn dialogue. Code is available at\nhttps://github.com/Social-AI-Studio/DuET-PD."
                },
                "authors": [
                    {
                        "name": "Bryan Chen Zhengyu Tan"
                    },
                    {
                        "name": "Daniel Wai Kit Chin"
                    },
                    {
                        "name": "Zhengyuan Liu"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "To appear at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07173v2",
                "updated": "2025-09-05T09:25:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    25,
                    57,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-08T14:45:11Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    14,
                    45,
                    11,
                    6,
                    159,
                    0
                ],
                "title": "Translating Federated Learning Algorithms in Python into CSP Processes\n  Using ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Federated Learning Algorithms in Python into CSP Processes\n  Using ChatGPT"
                },
                "summary": "The Python Testbed for Federated Learning Algorithms is a simple Python FL\nframework that is easy to use by ML&AI developers who do not need to be\nprofessional programmers and is also amenable to LLMs. In the previous\nresearch, generic federated learning algorithms provided by this framework were\nmanually translated into the CSP processes and algorithms' safety and liveness\nproperties were automatically verified by the model checker PAT. In this paper,\na simple translation process is introduced wherein the ChatGPT is used to\nautomate the translation of the mentioned federated learning algorithms in\nPython into the corresponding CSP processes. Within the process, the minimality\nof the used context is estimated based on the feedback from ChatGPT. The\nproposed translation process was experimentally validated by successful\ntranslation (verified by the model checker PAT) of both generic centralized and\ndecentralized federated learning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Python Testbed for Federated Learning Algorithms is a simple Python FL\nframework that is easy to use by ML&AI developers who do not need to be\nprofessional programmers and is also amenable to LLMs. In the previous\nresearch, generic federated learning algorithms provided by this framework were\nmanually translated into the CSP processes and algorithms' safety and liveness\nproperties were automatically verified by the model checker PAT. In this paper,\na simple translation process is introduced wherein the ChatGPT is used to\nautomate the translation of the mentioned federated learning algorithms in\nPython into the corresponding CSP processes. Within the process, the minimality\nof the used context is estimated based on the feedback from ChatGPT. The\nproposed translation process was experimentally validated by successful\ntranslation (verified by the model checker PAT) of both generic centralized and\ndecentralized federated learning algorithms."
                },
                "authors": [
                    {
                        "name": "Miroslav Popovic"
                    },
                    {
                        "name": "Marko Popovic"
                    },
                    {
                        "name": "Miodrag Djukic"
                    },
                    {
                        "name": "Ilija Basicevic"
                    }
                ],
                "author_detail": {
                    "name": "Ilija Basicevic"
                },
                "author": "Ilija Basicevic",
                "arxiv_doi": "10.1109/MIPRO65660.2025.11131995",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MIPRO65660.2025.11131995",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.07173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 4 tables; Published by IEEE Xplore",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19920v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19920v3",
                "updated": "2025-09-05T09:22:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    22,
                    59,
                    4,
                    248,
                    0
                ],
                "published": "2024-10-25T18:25:35Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    25,
                    35,
                    4,
                    299,
                    0
                ],
                "title": "Reinforcement Learning for Aligning Large Language Models Agents with\n  Interactive Environments: Quantifying and Mitigating Prompt Overfitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Aligning Large Language Models Agents with\n  Interactive Environments: Quantifying and Mitigating Prompt Overfitting"
                },
                "summary": "Reinforcement learning (RL) is a promising approach for aligning large\nlanguage models (LLMs) knowledge with sequential decision-making tasks.\nHowever, few studies have thoroughly investigated the impact on LLM agents\ncapabilities of fine-tuning them with RL in a specific environment. In this\npaper, we propose a novel framework to analyze the sensitivity of LLMs to\nprompt formulations following RL training in a textual environment. Our\nfindings reveal that the performance of LLMs degrades when faced with prompt\nformulations different from those used during the RL training phase. Besides,\nwe analyze the source of this sensitivity by examining the model's internal\nrepresentations and salient tokens. Finally, we propose to use a contrastive\nloss to mitigate this sensitivity and improve the robustness and generalization\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is a promising approach for aligning large\nlanguage models (LLMs) knowledge with sequential decision-making tasks.\nHowever, few studies have thoroughly investigated the impact on LLM agents\ncapabilities of fine-tuning them with RL in a specific environment. In this\npaper, we propose a novel framework to analyze the sensitivity of LLMs to\nprompt formulations following RL training in a textual environment. Our\nfindings reveal that the performance of LLMs degrades when faced with prompt\nformulations different from those used during the RL training phase. Besides,\nwe analyze the source of this sensitivity by examining the model's internal\nrepresentations and salient tokens. Finally, we propose to use a contrastive\nloss to mitigate this sensitivity and improve the robustness and generalization\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Mohamed Salim Aissi"
                    },
                    {
                        "name": "Clement Romac"
                    },
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Nicolas Thome"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Thome"
                },
                "author": "Nicolas Thome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19920v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19920v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02544v3",
                "updated": "2025-09-05T09:21:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    21,
                    10,
                    4,
                    248,
                    0
                ],
                "published": "2024-08-05T15:16:22Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    15,
                    16,
                    22,
                    0,
                    218,
                    0
                ],
                "title": "Caution for the Environment: Multimodal LLM Agents are Susceptible to\n  Environmental Distractions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caution for the Environment: Multimodal LLM Agents are Susceptible to\n  Environmental Distractions"
                },
                "summary": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in a graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general scenario is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing a simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness of\nagents, our findings first indicate that these agents are prone to\nenvironmental distractions. Furthermore, we implement an adversarial\nenvironment injection and analyze the approach to improve faithfulness, calling\nfor a collective focus on this important topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in a graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general scenario is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing a simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness of\nagents, our findings first indicate that these agents are prone to\nenvironmental distractions. Furthermore, we implement an adversarial\nenvironment injection and analyze the approach to improve faithfulness, calling\nfor a collective focus on this important topic."
                },
                "authors": [
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Yiting Wang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Tongxin Yuan"
                    },
                    {
                        "name": "Aston Zhang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11987v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11987v3",
                "updated": "2025-09-05T09:15:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    15,
                    55,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-16T08:54:08Z",
                "published_parsed": [
                    2025,
                    8,
                    16,
                    8,
                    54,
                    8,
                    5,
                    228,
                    0
                ],
                "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction"
                },
                "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\n$\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\n$\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zeng"
                    },
                    {
                        "name": "Jiashuo Liu"
                    },
                    {
                        "name": "Siyuan Chen"
                    },
                    {
                        "name": "Tianci He"
                    },
                    {
                        "name": "Yali Liao"
                    },
                    {
                        "name": "Yixiao Tian"
                    },
                    {
                        "name": "Jinpeng Wang"
                    },
                    {
                        "name": "Zaiyuan Wang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Lingyue Yin"
                    },
                    {
                        "name": "Mingren Yin"
                    },
                    {
                        "name": "Zhenwei Zhu"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Jiecao Chen"
                    },
                    {
                        "name": "Yantao Du"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Jiacheng Guo"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Jianpeng Jiao"
                    },
                    {
                        "name": "Xiangsheng Li"
                    },
                    {
                        "name": "Jingkai Liu"
                    },
                    {
                        "name": "Shuang Ni"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Jose Blanchet"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "arxiv_comment": "Technical report, 51 pages. Update the results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11987v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11987v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04111v2",
                "updated": "2025-09-05T09:12:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    9,
                    12,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T11:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    20,
                    53,
                    3,
                    247,
                    0
                ],
                "title": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages"
                },
                "summary": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available."
                },
                "authors": [
                    {
                        "name": "Dan Saattrup Smart"
                    }
                ],
                "author_detail": {
                    "name": "Dan Saattrup Smart"
                },
                "author": "Dan Saattrup Smart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04926v1",
                "updated": "2025-09-05T08:44:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    44,
                    27,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:44:27Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    44,
                    27,
                    4,
                    248,
                    0
                ],
                "title": "Towards Ontology-Based Descriptions of Conversations with\n  Qualitatively-Defined Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Ontology-Based Descriptions of Conversations with\n  Qualitatively-Defined Concepts"
                },
                "summary": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI."
                },
                "authors": [
                    {
                        "name": "Barbara Gendron"
                    },
                    {
                        "name": "Gal Guibon"
                    },
                    {
                        "name": "Mathieu D'aquin"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu D'aquin"
                },
                "arxiv_affiliation": "LORIA, UL",
                "author": "Mathieu D'aquin",
                "arxiv_comment": "Accepted at TOTh 2025 (Terminology \\& Ontology: Theories and\n  applications)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20541v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20541v4",
                "updated": "2025-09-05T08:42:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    42,
                    4,
                    4,
                    248,
                    0
                ],
                "published": "2025-07-28T05:56:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    5,
                    56,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic\n  Design"
                },
                "summary": "This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that\npresents a new paradigm for Automatic Heuristic Design (AHD). Traditional\nevolutionary methods operate directly on heuristic code; in contrast, MeLA\nevolves the instructional prompts used to guide a Large Language Model (LLM) in\ngenerating these heuristics. This process of \"prompt evolution\" is driven by a\nnovel metacognitive framework where the system analyzes performance feedback to\nsystematically refine its generative strategy. MeLA's architecture integrates a\nproblem analyzer to construct an initial strategic prompt, an error diagnosis\nsystem to repair faulty code, and a metacognitive search engine that\niteratively optimizes the prompt based on heuristic effectiveness. In\ncomprehensive experiments across both benchmark and real-world problems, MeLA\nconsistently generates more effective and robust heuristics, significantly\noutperforming state-of-the-art methods. Ultimately, this research demonstrates\nthe profound potential of using cognitive science as a blueprint for AI\narchitecture, revealing that by enabling an LLM to metacognitively regulate its\nproblem-solving process, we unlock a more robust and interpretable path to AHD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that\npresents a new paradigm for Automatic Heuristic Design (AHD). Traditional\nevolutionary methods operate directly on heuristic code; in contrast, MeLA\nevolves the instructional prompts used to guide a Large Language Model (LLM) in\ngenerating these heuristics. This process of \"prompt evolution\" is driven by a\nnovel metacognitive framework where the system analyzes performance feedback to\nsystematically refine its generative strategy. MeLA's architecture integrates a\nproblem analyzer to construct an initial strategic prompt, an error diagnosis\nsystem to repair faulty code, and a metacognitive search engine that\niteratively optimizes the prompt based on heuristic effectiveness. In\ncomprehensive experiments across both benchmark and real-world problems, MeLA\nconsistently generates more effective and robust heuristics, significantly\noutperforming state-of-the-art methods. Ultimately, this research demonstrates\nthe profound potential of using cognitive science as a blueprint for AI\narchitecture, revealing that by enabling an LLM to metacognitively regulate its\nproblem-solving process, we unlock a more robust and interpretable path to AHD."
                },
                "authors": [
                    {
                        "name": "Zishang Qiu"
                    },
                    {
                        "name": "Xinan Chen"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Ruibin Bai"
                    }
                ],
                "author_detail": {
                    "name": "Ruibin Bai"
                },
                "author": "Ruibin Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20541v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20541v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16172v2",
                "updated": "2025-09-05T08:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    26,
                    57,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-22T07:50:57Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    50,
                    57,
                    4,
                    234,
                    0
                ],
                "title": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent\n  with Preference Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent\n  with Preference Chain"
                },
                "summary": "Understanding human behavior in urban environments is a crucial field within\ncity sciences. However, collecting accurate behavioral data, particularly in\nnewly developed areas, poses significant challenges. Recent advances in\ngenerative agents, powered by Large Language Models (LLMs), have shown promise\nin simulating human behaviors without relying on extensive datasets.\nNevertheless, these methods often struggle with generating consistent,\ncontext-sensitive, and realistic behavioral outputs. To address these\nlimitations, this paper introduces the Preference Chain, a novel method that\nintegrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance\ncontext-aware simulation of human behavior in transportation systems.\nExperiments conducted on the Replica dataset demonstrate that the Preference\nChain outperforms standard LLM in aligning with real-world transportation mode\nchoices. The development of the Mobility Agent highlights potential\napplications of proposed method in urban mobility modeling for emerging cities,\npersonalized travel behavior analysis, and dynamic traffic forecasting. Despite\nlimitations such as slow inference and the risk of hallucination, the method\noffers a promising framework for simulating complex human behavior in\ndata-scarce environments, where traditional data-driven models struggle due to\nlimited data availability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human behavior in urban environments is a crucial field within\ncity sciences. However, collecting accurate behavioral data, particularly in\nnewly developed areas, poses significant challenges. Recent advances in\ngenerative agents, powered by Large Language Models (LLMs), have shown promise\nin simulating human behaviors without relying on extensive datasets.\nNevertheless, these methods often struggle with generating consistent,\ncontext-sensitive, and realistic behavioral outputs. To address these\nlimitations, this paper introduces the Preference Chain, a novel method that\nintegrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance\ncontext-aware simulation of human behavior in transportation systems.\nExperiments conducted on the Replica dataset demonstrate that the Preference\nChain outperforms standard LLM in aligning with real-world transportation mode\nchoices. The development of the Mobility Agent highlights potential\napplications of proposed method in urban mobility modeling for emerging cities,\npersonalized travel behavior analysis, and dynamic traffic forecasting. Despite\nlimitations such as slow inference and the risk of hallucination, the method\noffers a promising framework for simulating complex human behavior in\ndata-scarce environments, where traditional data-driven models struggle due to\nlimited data availability."
                },
                "authors": [
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Parfait Atchade-Adelomou"
                    },
                    {
                        "name": "Carlo Adornetto"
                    },
                    {
                        "name": "Adrian Mora-Carrero"
                    },
                    {
                        "name": "Luis Alonso-Pastor"
                    },
                    {
                        "name": "Ariel Noyman"
                    },
                    {
                        "name": "Yubo Liu"
                    },
                    {
                        "name": "Kent Larson"
                    }
                ],
                "author_detail": {
                    "name": "Kent Larson"
                },
                "author": "Kent Larson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04905v1",
                "updated": "2025-09-05T08:23:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    23,
                    16,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:23:16Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    23,
                    16,
                    4,
                    248,
                    0
                ],
                "title": "Revolution or Hype? Seeking the Limits of Large Models in Hardware\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolution or Hype? Seeking the Limits of Large Models in Hardware\n  Design"
                },
                "summary": "Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models\n(LCMs) have sparked excitement across the electronic design automation (EDA)\ncommunity, promising a revolution in circuit design and optimization. Yet, this\nexcitement is met with significant skepticism: Are these AI models a genuine\nrevolution in circuit design, or a temporary wave of inflated expectations?\nThis paper serves as a foundational text for the corresponding ICCAD 2025\npanel, bringing together perspectives from leading experts in academia and\nindustry. It critically examines the practical capabilities, fundamental\nlimitations, and future prospects of large AI models in hardware design. The\npaper synthesizes the core arguments surrounding reliability, scalability, and\ninterpretability, framing the debate on whether these models can meaningfully\noutperform or complement traditional EDA methods. The result is an\nauthoritative overview offering fresh insights into one of today's most\ncontentious and impactful technology trends.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models\n(LCMs) have sparked excitement across the electronic design automation (EDA)\ncommunity, promising a revolution in circuit design and optimization. Yet, this\nexcitement is met with significant skepticism: Are these AI models a genuine\nrevolution in circuit design, or a temporary wave of inflated expectations?\nThis paper serves as a foundational text for the corresponding ICCAD 2025\npanel, bringing together perspectives from leading experts in academia and\nindustry. It critically examines the practical capabilities, fundamental\nlimitations, and future prospects of large AI models in hardware design. The\npaper synthesizes the core arguments surrounding reliability, scalability, and\ninterpretability, framing the debate on whether these models can meaningfully\noutperform or complement traditional EDA methods. The result is an\nauthoritative overview offering fresh insights into one of today's most\ncontentious and impactful technology trends."
                },
                "authors": [
                    {
                        "name": "Qiang Xu"
                    },
                    {
                        "name": "Leon Stok"
                    },
                    {
                        "name": "Rolf Drechsler"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Grace Li Zhang"
                    },
                    {
                        "name": "Igor L. Markov"
                    }
                ],
                "author_detail": {
                    "name": "Igor L. Markov"
                },
                "author": "Igor L. Markov",
                "arxiv_comment": "Invited paper to appear at ICCAD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04903v1",
                "updated": "2025-09-05T08:21:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    21,
                    41,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:21:41Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    21,
                    41,
                    4,
                    248,
                    0
                ],
                "title": "ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation\n  Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\nlong-context understanding, yet they face significant challenges in\nhigh-quality long-form generation. Existing studies primarily suffer from two\nlimitations: (1) A heavy reliance on scarce, high-quality long-form response\ndata for supervised fine-tuning (SFT) or for pairwise preference reward in\nreinforcement learning (RL). (2) Focus on coarse-grained quality optimization\ndimensions, such as relevance, coherence, and helpfulness, overlooking the\nfine-grained specifics inherent to diverse long-form generation scenarios. To\naddress this issue, we propose a framework using Adaptive Constraint-Enhanced\nreward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first\nautomatically deconstructs each instruction into a set of fine-grained,\nadaptive constraint criteria by identifying its underlying intents and demands.\nSubsequently, we design a reward mechanism that quantifies the quality of\nlong-form responses based on their satisfaction over corresponding constraints,\nconverting subjective quality evaluation into constraint verification. Finally,\nwe utilize reinforcement learning to guide models toward superior long-form\ngeneration capabilities. Experimental results demonstrate that our ACE-RL\nframework significantly outperforms existing SFT and RL baselines by 20.70% and\n7.32% on WritingBench, and our top-performing model even surpasses proprietary\nsystems like GPT-4o by 7.10%, providing a more effective training paradigm for\nLLMs to generate high-quality content across diverse long-form generation\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable progress in\nlong-context understanding, yet they face significant challenges in\nhigh-quality long-form generation. Existing studies primarily suffer from two\nlimitations: (1) A heavy reliance on scarce, high-quality long-form response\ndata for supervised fine-tuning (SFT) or for pairwise preference reward in\nreinforcement learning (RL). (2) Focus on coarse-grained quality optimization\ndimensions, such as relevance, coherence, and helpfulness, overlooking the\nfine-grained specifics inherent to diverse long-form generation scenarios. To\naddress this issue, we propose a framework using Adaptive Constraint-Enhanced\nreward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first\nautomatically deconstructs each instruction into a set of fine-grained,\nadaptive constraint criteria by identifying its underlying intents and demands.\nSubsequently, we design a reward mechanism that quantifies the quality of\nlong-form responses based on their satisfaction over corresponding constraints,\nconverting subjective quality evaluation into constraint verification. Finally,\nwe utilize reinforcement learning to guide models toward superior long-form\ngeneration capabilities. Experimental results demonstrate that our ACE-RL\nframework significantly outperforms existing SFT and RL baselines by 20.70% and\n7.32% on WritingBench, and our top-performing model even surpasses proprietary\nsystems like GPT-4o by 7.10%, providing a more effective training paradigm for\nLLMs to generate high-quality content across diverse long-form generation\nscenarios."
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Qixiang Yin"
                    },
                    {
                        "name": "Lingxing Kong"
                    },
                    {
                        "name": "Zhixing Tan"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Under review, our code is available at https://github.com/ZNLP/ACE-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04885v1",
                "updated": "2025-09-05T08:03:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    3,
                    54,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:03:54Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    3,
                    54,
                    4,
                    248,
                    0
                ],
                "title": "Performance Analysis of Pinching-Antenna-Enabled Internet of Things\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of Pinching-Antenna-Enabled Internet of Things\n  Systems"
                },
                "summary": "The pinching-antenna systems (PASS), which activate small dielectric\nparticles along a dielectric waveguide, has recently emerged as a promising\nparadigm for flexible antenna deployment in next-generation wireless\ncommunication networks. While most existing studies assume rectangular indoor\nlayouts with full coverage waveguide, practical deployments may involve\ngeometric constraints, partial coverage, and non-negligible waveguide\nattenuation. This paper presents the first analytical investigation of PASS in\na circular indoor environment, encompassing both full coverage and partial\ncoverage waveguide configurations with/without propagation loss. A unified\ngeometric-propagation framework is developed that jointly captures\npinching-antenna placement, Internet of Things (IoT) device location\ndistribution, and waveguide attenuation. Closed-form expressions for the outage\nprobability and average achievable rate are derived for four scenarios, with\naccuracy validated via extensive Monte-Carlo simulations. The analysis reveals\nthat, under the partial coverage waveguide scenario with propagation loss, the\nsystem performance demonstrates a non-monotonic trend with respect to the\nwaveguide length, and the optimal length decreases as the attenuation\ncoefficient increases. Numerical results further quantify the interplay between\ndeployment strategy, waveguide propagation loss, and coverage geometry,\noffering practical guidelines for performance-oriented PASS design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pinching-antenna systems (PASS), which activate small dielectric\nparticles along a dielectric waveguide, has recently emerged as a promising\nparadigm for flexible antenna deployment in next-generation wireless\ncommunication networks. While most existing studies assume rectangular indoor\nlayouts with full coverage waveguide, practical deployments may involve\ngeometric constraints, partial coverage, and non-negligible waveguide\nattenuation. This paper presents the first analytical investigation of PASS in\na circular indoor environment, encompassing both full coverage and partial\ncoverage waveguide configurations with/without propagation loss. A unified\ngeometric-propagation framework is developed that jointly captures\npinching-antenna placement, Internet of Things (IoT) device location\ndistribution, and waveguide attenuation. Closed-form expressions for the outage\nprobability and average achievable rate are derived for four scenarios, with\naccuracy validated via extensive Monte-Carlo simulations. The analysis reveals\nthat, under the partial coverage waveguide scenario with propagation loss, the\nsystem performance demonstrates a non-monotonic trend with respect to the\nwaveguide length, and the optimal length decreases as the attenuation\ncoefficient increases. Numerical results further quantify the interplay between\ndeployment strategy, waveguide propagation loss, and coverage geometry,\noffering practical guidelines for performance-oriented PASS design."
                },
                "authors": [
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Bingxin Zhang"
                    },
                    {
                        "name": "Yizhe Zhao"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Guopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guopeng Zhang"
                },
                "author": "Guopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04884v1",
                "updated": "2025-09-05T08:03:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    3,
                    1,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T08:03:01Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    3,
                    1,
                    4,
                    248,
                    0
                ],
                "title": "L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning"
                },
                "summary": "The ability of Large Language Models (LLMs) to solve complex tasks has made\nthem crucial in the development of AI-based applications. However, the high\ncomputational requirements to fine-tune these LLMs on downstream tasks pose\nsignificant challenges, particularly when resources are limited. In response to\nthis challenge, we introduce L1RA, a novel technique aimed at dynamically\ndistributing the rank of low-rank adapters during fine-tuning using LoRA. Given\na rank budget (i.e., total sum of adapters rank), L1RA leverages L1\nregularisation to prune redundant ranks and redistribute them across adapters,\nthereby optimising resource utilisation. Through a series of comprehensive\nexperiments, we empirically demonstrate that L1RA maintains comparable or even\nreduced computational overhead compared to other LoRA variants, including the\nvanilla approach, while achieving same or better performances. Moreover, the\npost-training analysis of rank distribution unveiled insights into the specific\nmodel components requiring the most adaptation to align with the task\nobjective: the feed-forward layers and the attention output projection. These\nresults highlight the efficacy of L1RA in not only enhancing the efficiency of\nLLM fine-tuning, but also in providing valuable diagnostic information for\nmodel refinement and customisation. In conclusion, L1RA stands as a promising\ntechnique for advancing the performance and interpretability of LLM adaptation,\nparticularly in scenarios where computational resources are constrained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of Large Language Models (LLMs) to solve complex tasks has made\nthem crucial in the development of AI-based applications. However, the high\ncomputational requirements to fine-tune these LLMs on downstream tasks pose\nsignificant challenges, particularly when resources are limited. In response to\nthis challenge, we introduce L1RA, a novel technique aimed at dynamically\ndistributing the rank of low-rank adapters during fine-tuning using LoRA. Given\na rank budget (i.e., total sum of adapters rank), L1RA leverages L1\nregularisation to prune redundant ranks and redistribute them across adapters,\nthereby optimising resource utilisation. Through a series of comprehensive\nexperiments, we empirically demonstrate that L1RA maintains comparable or even\nreduced computational overhead compared to other LoRA variants, including the\nvanilla approach, while achieving same or better performances. Moreover, the\npost-training analysis of rank distribution unveiled insights into the specific\nmodel components requiring the most adaptation to align with the task\nobjective: the feed-forward layers and the attention output projection. These\nresults highlight the efficacy of L1RA in not only enhancing the efficiency of\nLLM fine-tuning, but also in providing valuable diagnostic information for\nmodel refinement and customisation. In conclusion, L1RA stands as a promising\ntechnique for advancing the performance and interpretability of LLM adaptation,\nparticularly in scenarios where computational resources are constrained."
                },
                "authors": [
                    {
                        "name": "Raul Singh"
                    },
                    {
                        "name": "Nicolo Brunello"
                    },
                    {
                        "name": "Vincenzo Scotti"
                    },
                    {
                        "name": "Mark James Carman"
                    }
                ],
                "author_detail": {
                    "name": "Mark James Carman"
                },
                "author": "Mark James Carman",
                "arxiv_comment": "Work published at ICNLSP 2025, waiting for publication link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16299v3",
                "updated": "2025-09-05T08:01:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    8,
                    1,
                    17,
                    4,
                    248,
                    0
                ],
                "published": "2024-09-09T19:35:34Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    19,
                    35,
                    34,
                    0,
                    253,
                    0
                ],
                "title": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks\n  at Scale"
                },
                "summary": "Large Language Models (LLMs) have revolutionized software engineering (SE),\nshowcasing remarkable proficiency in various coding tasks. Despite recent\nadvancements that have enabled the creation of autonomous software agents\nutilizing LLMs for end-to-end development tasks, these systems are typically\ndesigned for specific SE functions. We introduce HyperAgent, an innovative\ngeneralist multi-agent system designed to tackle a wide range of SE tasks\nacross different programming languages by mimicking the workflows of human\ndevelopers. HyperAgent features four specialized agents-Planner, Navigator,\nCode Editor, and Executor-capable of handling the entire lifecycle of SE tasks,\nfrom initial planning to final verification. HyperAgent sets new benchmarks in\ndiverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench\nbenchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates\nexceptional performance in repository-level code generation (RepoExec) and\nfault localization and program repair (Defects4J), often surpassing\nstate-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized software engineering (SE),\nshowcasing remarkable proficiency in various coding tasks. Despite recent\nadvancements that have enabled the creation of autonomous software agents\nutilizing LLMs for end-to-end development tasks, these systems are typically\ndesigned for specific SE functions. We introduce HyperAgent, an innovative\ngeneralist multi-agent system designed to tackle a wide range of SE tasks\nacross different programming languages by mimicking the workflows of human\ndevelopers. HyperAgent features four specialized agents-Planner, Navigator,\nCode Editor, and Executor-capable of handling the entire lifecycle of SE tasks,\nfrom initial planning to final verification. HyperAgent sets new benchmarks in\ndiverse SE tasks, including GitHub issue resolution on the renowned SWE-Bench\nbenchmark, outperforming robust baselines. Furthermore, HyperAgent demonstrates\nexceptional performance in repository-level code generation (RepoExec) and\nfault localization and program repair (Defects4J), often surpassing\nstate-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Huy Nhat Phan"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Phong X. Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "arxiv_comment": "49 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12226v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12226v4",
                "updated": "2025-09-05T07:56:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    56,
                    50,
                    4,
                    248,
                    0
                ],
                "published": "2024-02-19T15:33:10Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    15,
                    33,
                    10,
                    0,
                    50,
                    0
                ],
                "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling"
                },
                "summary": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/"
                },
                "authors": [
                    {
                        "name": "Jun Zhan"
                    },
                    {
                        "name": "Junqi Dai"
                    },
                    {
                        "name": "Jiasheng Ye"
                    },
                    {
                        "name": "Yunhua Zhou"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Linyang Li"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Tianxiang Sun"
                    },
                    {
                        "name": "Yugang Jiang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "28 pages, 16 figures, under review, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12226v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12226v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04877v1",
                "updated": "2025-09-05T07:46:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    46,
                    27,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T07:46:27Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    46,
                    27,
                    4,
                    248,
                    0
                ],
                "title": "Integrating Large Language Models in Software Engineering Education: A\n  Pilot Study through GitHub Repositories Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models in Software Engineering Education: A\n  Pilot Study through GitHub Repositories Mining"
                },
                "summary": "Context: Large Language Models (LLMs) such as ChatGPT are increasingly\nadopted in software engineering (SE) education, offering both opportunities and\nchallenges. Their adoption requires systematic investigation to ensure\nresponsible integration into curricula. Objective: This doctoral research aims\nto develop a validated framework for integrating LLMs into SE education through\na multi-phase process, including taxonomies development, empirical\ninvestigation, and case studies. This paper presents the first empirical step.\nMethod: We conducted a pilot repository mining study of 400 GitHub projects,\nanalyzing README files and issues discussions to identify the presence of\nmotivator and demotivator previously synthesized in our literature review [ 8]\nstudy. Results: Motivators such as engagement and motivation (227 hits),\nsoftware engineering process understanding (133 hits), and programming\nassistance and debugging support (97 hits) were strongly represented.\nDemotivators, including plagiarism and IP concerns (385 hits), security,\nprivacy and data integrity (87 hits), and over-reliance on AI in learning (39\nhits), also appeared prominently. In contrast, demotivators such as challenges\nin evaluating learning outcomes and difficulty in curriculum redesign recorded\nno hits across the repositories. Conclusion: The study provides early empirical\nvalidation of motivators/demotivators taxonomies with respect to their themes,\nhighlights research practice gaps, and lays the foundation for developing a\ncomprehensive framework to guide the responsible adoption of LLMs in SE\neducation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Large Language Models (LLMs) such as ChatGPT are increasingly\nadopted in software engineering (SE) education, offering both opportunities and\nchallenges. Their adoption requires systematic investigation to ensure\nresponsible integration into curricula. Objective: This doctoral research aims\nto develop a validated framework for integrating LLMs into SE education through\na multi-phase process, including taxonomies development, empirical\ninvestigation, and case studies. This paper presents the first empirical step.\nMethod: We conducted a pilot repository mining study of 400 GitHub projects,\nanalyzing README files and issues discussions to identify the presence of\nmotivator and demotivator previously synthesized in our literature review [ 8]\nstudy. Results: Motivators such as engagement and motivation (227 hits),\nsoftware engineering process understanding (133 hits), and programming\nassistance and debugging support (97 hits) were strongly represented.\nDemotivators, including plagiarism and IP concerns (385 hits), security,\nprivacy and data integrity (87 hits), and over-reliance on AI in learning (39\nhits), also appeared prominently. In contrast, demotivators such as challenges\nin evaluating learning outcomes and difficulty in curriculum redesign recorded\nno hits across the repositories. Conclusion: The study provides early empirical\nvalidation of motivators/demotivators taxonomies with respect to their themes,\nhighlights research practice gaps, and lays the foundation for developing a\ncomprehensive framework to guide the responsible adoption of LLMs in SE\neducation."
                },
                "authors": [
                    {
                        "name": "Maryam Khan"
                    },
                    {
                        "name": "Muhammad Azeem Akbar"
                    },
                    {
                        "name": "Jussi Kasurinen"
                    }
                ],
                "author_detail": {
                    "name": "Jussi Kasurinen"
                },
                "author": "Jussi Kasurinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21509v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21509v3",
                "updated": "2025-09-05T07:44:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    44,
                    31,
                    4,
                    248,
                    0
                ],
                "published": "2025-07-29T05:20:14Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    5,
                    20,
                    14,
                    1,
                    210,
                    0
                ],
                "title": "Persona Vectors: Monitoring and Controlling Character Traits in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona Vectors: Monitoring and Controlling Character Traits in Language\n  Models"
                },
                "summary": "Large language models interact with users through a simulated 'Assistant'\npersona. While the Assistant is typically trained to be helpful, harmless, and\nhonest, it sometimes deviates from these ideals. In this paper, we identify\ndirections in the model's activation space-persona vectors-underlying several\ntraits, such as evil, sycophancy, and propensity to hallucinate. We confirm\nthat these vectors can be used to monitor fluctuations in the Assistant's\npersonality at deployment time. We then apply persona vectors to predict and\ncontrol personality shifts that occur during training. We find that both\nintended and unintended personality changes after finetuning are strongly\ncorrelated with shifts along the relevant persona vectors. These shifts can be\nmitigated through post-hoc intervention, or avoided in the first place with a\nnew preventative steering method. Moreover, persona vectors can be used to flag\ntraining data that will produce undesirable personality changes, both at the\ndataset level and the individual sample level. Our method for extracting\npersona vectors is automated and can be applied to any personality trait of\ninterest, given only a natural-language description.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models interact with users through a simulated 'Assistant'\npersona. While the Assistant is typically trained to be helpful, harmless, and\nhonest, it sometimes deviates from these ideals. In this paper, we identify\ndirections in the model's activation space-persona vectors-underlying several\ntraits, such as evil, sycophancy, and propensity to hallucinate. We confirm\nthat these vectors can be used to monitor fluctuations in the Assistant's\npersonality at deployment time. We then apply persona vectors to predict and\ncontrol personality shifts that occur during training. We find that both\nintended and unintended personality changes after finetuning are strongly\ncorrelated with shifts along the relevant persona vectors. These shifts can be\nmitigated through post-hoc intervention, or avoided in the first place with a\nnew preventative steering method. Moreover, persona vectors can be used to flag\ntraining data that will produce undesirable personality changes, both at the\ndataset level and the individual sample level. Our method for extracting\npersona vectors is automated and can be applied to any personality trait of\ninterest, given only a natural-language description."
                },
                "authors": [
                    {
                        "name": "Runjin Chen"
                    },
                    {
                        "name": "Andy Arditi"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Owain Evans"
                    },
                    {
                        "name": "Jack Lindsey"
                    }
                ],
                "author_detail": {
                    "name": "Jack Lindsey"
                },
                "author": "Jack Lindsey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21509v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21509v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04876v1",
                "updated": "2025-09-05T07:44:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    44,
                    5,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T07:44:05Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    44,
                    5,
                    4,
                    248,
                    0
                ],
                "title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in\n  Multi-Agent LLM Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in\n  Multi-Agent LLM Collaboration"
                },
                "summary": "This paper introduces OSC (Orchestrating Cognitive Synergy), a\nknowledge-aware adaptive collaboration framework designed to enhance cognitive\nsynergy in multi-agent systems with large language models. While prior work has\nadvanced agent selection and result aggregation, efficient linguistic\ninteractions for deep collaboration among expert agents remain a critical\nbottleneck. OSC addresses this gap as a pivotal intermediate layer between\nselection and aggregation, introducing Collaborator Knowledge Models (CKM) to\nenable each agent to dynamically perceive its collaborators' cognitive states.\nThrough real-time cognitive gap analysis, agents adaptively adjust\ncommunication behaviors, including content focus, detail level, and expression\nstyle, using learned strategies. Experiments on complex reasoning and\nproblem-solving benchmarks demonstrate that OSC significantly improves task\nperformance and communication efficiency, transforming \"parallel-working\nindividuals'' into a \"deeply collaborative cognitive team.'' This framework not\nonly optimizes multi-agent collaboration but also offers new insights into LLM\nagent interaction behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces OSC (Orchestrating Cognitive Synergy), a\nknowledge-aware adaptive collaboration framework designed to enhance cognitive\nsynergy in multi-agent systems with large language models. While prior work has\nadvanced agent selection and result aggregation, efficient linguistic\ninteractions for deep collaboration among expert agents remain a critical\nbottleneck. OSC addresses this gap as a pivotal intermediate layer between\nselection and aggregation, introducing Collaborator Knowledge Models (CKM) to\nenable each agent to dynamically perceive its collaborators' cognitive states.\nThrough real-time cognitive gap analysis, agents adaptively adjust\ncommunication behaviors, including content focus, detail level, and expression\nstyle, using learned strategies. Experiments on complex reasoning and\nproblem-solving benchmarks demonstrate that OSC significantly improves task\nperformance and communication efficiency, transforming \"parallel-working\nindividuals'' into a \"deeply collaborative cognitive team.'' This framework not\nonly optimizes multi-agent collaboration but also offers new insights into LLM\nagent interaction behaviors."
                },
                "authors": [
                    {
                        "name": "Jusheng Zhang"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Xiaofei Sun"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "arxiv_comment": "Accepted at EMNLP 2025 (Long Paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04868v1",
                "updated": "2025-09-05T07:30:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    30,
                    40,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T07:30:40Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    30,
                    40,
                    4,
                    248,
                    0
                ],
                "title": "Using LLMs for Multilingual Clinical Entity Linking to ICD-10",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs for Multilingual Clinical Entity Linking to ICD-10"
                },
                "summary": "The linking of clinical entities is a crucial part of extracting structured\ninformation from clinical texts. It is the process of assigning a code from a\nmedical ontology or classification to a phrase in the text. The International\nClassification of Diseases - 10th revision (ICD-10) is an international\nstandard for classifying diseases for statistical and insurance purposes.\nAutomatically assigning the correct ICD-10 code to terms in discharge summaries\nwill simplify the work of healthcare professionals and ensure consistent coding\nin hospitals. Our paper proposes an approach for linking clinical terms to\nICD-10 codes in different languages using Large Language Models (LLMs). The\napproach consists of a multistage pipeline that uses clinical dictionaries to\nmatch unambiguous terms in the text and then applies in-context learning with\nGPT-4.1 to predict the ICD-10 code for the terms that do not match the\ndictionary. Our system shows promising results in predicting ICD-10 codes on\ndifferent benchmark datasets in Spanish - 0.89 F1 for categories and 0.78 F1 on\nsubcategories on CodiEsp, and Greek - 0.85 F1 on ElCardioCC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linking of clinical entities is a crucial part of extracting structured\ninformation from clinical texts. It is the process of assigning a code from a\nmedical ontology or classification to a phrase in the text. The International\nClassification of Diseases - 10th revision (ICD-10) is an international\nstandard for classifying diseases for statistical and insurance purposes.\nAutomatically assigning the correct ICD-10 code to terms in discharge summaries\nwill simplify the work of healthcare professionals and ensure consistent coding\nin hospitals. Our paper proposes an approach for linking clinical terms to\nICD-10 codes in different languages using Large Language Models (LLMs). The\napproach consists of a multistage pipeline that uses clinical dictionaries to\nmatch unambiguous terms in the text and then applies in-context learning with\nGPT-4.1 to predict the ICD-10 code for the terms that do not match the\ndictionary. Our system shows promising results in predicting ICD-10 codes on\ndifferent benchmark datasets in Spanish - 0.89 F1 for categories and 0.78 F1 on\nsubcategories on CodiEsp, and Greek - 0.85 F1 on ElCardioCC."
                },
                "authors": [
                    {
                        "name": "Sylvia Vassileva"
                    },
                    {
                        "name": "Ivan Koychev"
                    },
                    {
                        "name": "Svetla Boytcheva"
                    }
                ],
                "author_detail": {
                    "name": "Svetla Boytcheva"
                },
                "author": "Svetla Boytcheva",
                "arxiv_comment": "7 pages, 2 Figures, to be published in Proceedings of the 15th\n  International Conference on Recent Advances in Natural Language Processing,\n  RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04866v1",
                "updated": "2025-09-05T07:30:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    30,
                    1,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T07:30:01Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    30,
                    1,
                    4,
                    248,
                    0
                ],
                "title": "Memorization $\\neq$ Understanding: Do Large Language Models Have the\n  Ability of Scenario Cognition?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization $\\neq$ Understanding: Do Large Language Models Have the\n  Ability of Scenario Cognition?"
                },
                "summary": "Driven by vast and diverse textual data, large language models (LLMs) have\ndemonstrated impressive performance across numerous natural language processing\n(NLP) tasks. Yet, a critical question persists: does their generalization arise\nfrom mere memorization of training data or from deep semantic understanding? To\ninvestigate this, we propose a bi-perspective evaluation framework to assess\nLLMs' scenario cognition - the ability to link semantic scenario elements with\ntheir arguments in context. Specifically, we introduce a novel scenario-based\ndataset comprising diverse textual descriptions of fictional facts, annotated\nwith scenario elements. LLMs are evaluated through their capacity to answer\nscenario-related questions (model output perspective) and via probing their\ninternal representations for encoded scenario elements-argument associations\n(internal representation perspective). Our experiments reveal that current LLMs\npredominantly rely on superficial memorization, failing to achieve robust\nsemantic scenario cognition, even in simple cases. These findings expose\ncritical limitations in LLMs' semantic understanding and offer cognitive\ninsights for advancing their capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by vast and diverse textual data, large language models (LLMs) have\ndemonstrated impressive performance across numerous natural language processing\n(NLP) tasks. Yet, a critical question persists: does their generalization arise\nfrom mere memorization of training data or from deep semantic understanding? To\ninvestigate this, we propose a bi-perspective evaluation framework to assess\nLLMs' scenario cognition - the ability to link semantic scenario elements with\ntheir arguments in context. Specifically, we introduce a novel scenario-based\ndataset comprising diverse textual descriptions of fictional facts, annotated\nwith scenario elements. LLMs are evaluated through their capacity to answer\nscenario-related questions (model output perspective) and via probing their\ninternal representations for encoded scenario elements-argument associations\n(internal representation perspective). Our experiments reveal that current LLMs\npredominantly rely on superficial memorization, failing to achieve robust\nsemantic scenario cognition, even in simple cases. These findings expose\ncritical limitations in LLMs' semantic understanding and offer cognitive\ninsights for advancing their capabilities."
                },
                "authors": [
                    {
                        "name": "Boxiang Ma"
                    },
                    {
                        "name": "Ru Li"
                    },
                    {
                        "name": "Yuanlong Wang"
                    },
                    {
                        "name": "Hongye Tan"
                    },
                    {
                        "name": "Xiaoli Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoli Li"
                },
                "author": "Xiaoli Li",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03859v2",
                "updated": "2025-09-05T07:29:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    29,
                    42,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T03:36:07Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    3,
                    36,
                    7,
                    3,
                    247,
                    0
                ],
                "title": "Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator"
                },
                "summary": "Quadruped-based mobile manipulation presents significant challenges in\nrobotics due to the diversity of required skills, the extended task horizon,\nand partial observability. After presenting a multi-stage pick-and-place task\nas a succinct yet sufficiently rich setup that captures key desiderata for\nquadruped-based mobile manipulation, we propose an approach that can train a\nvisuo-motor policy entirely in simulation, and achieve nearly 80\\% success in\nthe real world. The policy efficiently performs search, approach, grasp,\ntransport, and drop into actions, with emerged behaviors such as re-grasping\nand task chaining. We conduct an extensive set of real-world experiments with\nablation studies highlighting key techniques for efficient training and\neffective sim-to-real transfer. Additional experiments demonstrate deployment\nacross a variety of indoor and outdoor environments. Demo videos and additional\nresources are available on the project page:\nhttps://horizonrobotics.github.io/gail/SLIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadruped-based mobile manipulation presents significant challenges in\nrobotics due to the diversity of required skills, the extended task horizon,\nand partial observability. After presenting a multi-stage pick-and-place task\nas a succinct yet sufficiently rich setup that captures key desiderata for\nquadruped-based mobile manipulation, we propose an approach that can train a\nvisuo-motor policy entirely in simulation, and achieve nearly 80\\% success in\nthe real world. The policy efficiently performs search, approach, grasp,\ntransport, and drop into actions, with emerged behaviors such as re-grasping\nand task chaining. We conduct an extensive set of real-world experiments with\nablation studies highlighting key techniques for efficient training and\neffective sim-to-real transfer. Additional experiments demonstrate deployment\nacross a variety of indoor and outdoor environments. Demo videos and additional\nresources are available on the project page:\nhttps://horizonrobotics.github.io/gail/SLIM."
                },
                "authors": [
                    {
                        "name": "Haichao Zhang"
                    },
                    {
                        "name": "Haonan Yu"
                    },
                    {
                        "name": "Le Zhao"
                    },
                    {
                        "name": "Andrew Choi"
                    },
                    {
                        "name": "Qinxun Bai"
                    },
                    {
                        "name": "Yiqing Yang"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "Accepted to IEEE Robotics and Automation Letters (RA-L). arXiv admin\n  note: substantial text overlap with arXiv:2501.09905",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11813v2",
                "updated": "2025-09-05T07:15:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    7,
                    15,
                    34,
                    4,
                    248,
                    0
                ],
                "published": "2024-08-21T17:58:02Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    58,
                    2,
                    2,
                    234,
                    0
                ],
                "title": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual\n  Integration in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEA: Supervised Embedding Alignment for Token-Level Visual-Textual\n  Integration in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities by integrating visual and textual inputs, yet modality alignment\nremains one of the most challenging aspects. Current MLLMs typically rely on\nsimple adapter architectures and pretraining approaches to bridge vision\nencoders with large language models (LLM), guided by image-level supervision.\nWe identify this paradigm often leads to suboptimal alignment between\nmodalities, significantly constraining the LLM's ability to properly interpret\nand reason with visual features particularly for smaller language models. This\nlimitation degrades overall performance-particularly for smaller language\nmodels where capacity constraints are more pronounced and adaptation\ncapabilities are limited. To address this fundamental limitation, we propose\nSupervised Embedding Alignment (SEA), a token-level supervision alignment\nmethod that enables more precise visual-text alignment during pretraining. SEA\nintroduces minimal computational overhead while preserving language\ncapabilities and substantially improving cross-modal understanding. Our\ncomprehensive analyses reveal critical insights into the adapter's role in\nmultimodal integration, and extensive experiments demonstrate that SEA\nconsistently improves performance across various model sizes, with smaller\nmodels benefiting the most (average performance gain of 7.61% for Gemma-2B).\nThis work establishes a foundation for developing more effective alignment\nstrategies for future multimodal systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities by integrating visual and textual inputs, yet modality alignment\nremains one of the most challenging aspects. Current MLLMs typically rely on\nsimple adapter architectures and pretraining approaches to bridge vision\nencoders with large language models (LLM), guided by image-level supervision.\nWe identify this paradigm often leads to suboptimal alignment between\nmodalities, significantly constraining the LLM's ability to properly interpret\nand reason with visual features particularly for smaller language models. This\nlimitation degrades overall performance-particularly for smaller language\nmodels where capacity constraints are more pronounced and adaptation\ncapabilities are limited. To address this fundamental limitation, we propose\nSupervised Embedding Alignment (SEA), a token-level supervision alignment\nmethod that enables more precise visual-text alignment during pretraining. SEA\nintroduces minimal computational overhead while preserving language\ncapabilities and substantially improving cross-modal understanding. Our\ncomprehensive analyses reveal critical insights into the adapter's role in\nmultimodal integration, and extensive experiments demonstrate that SEA\nconsistently improves performance across various model sizes, with smaller\nmodels benefiting the most (average performance gain of 7.61% for Gemma-2B).\nThis work establishes a foundation for developing more effective alignment\nstrategies for future multimodal systems."
                },
                "authors": [
                    {
                        "name": "Yuanyang Yin"
                    },
                    {
                        "name": "Yaqi Zhao"
                    },
                    {
                        "name": "Yajie Zhang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Ke Lin"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04836v1",
                "updated": "2025-09-05T06:37:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    37,
                    33,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T06:37:33Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    37,
                    33,
                    4,
                    248,
                    0
                ],
                "title": "COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of\n  Everyday Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of\n  Everyday Tasks"
                },
                "summary": "Continuous advancements in robotics and AI are driving the integration of\nrobots from industry into everyday environments. However, dynamic and\nunpredictable human activities in daily lives would directly or indirectly\nconflict with robot actions. Besides, due to the social attributes of such\nhuman-induced conflicts, solutions are not always unique and depend highly on\nthe user's personal preferences. To address these challenges and facilitate the\ndevelopment of household robots, we propose COMMET, a system for human-induced\nCOnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid\ndetection approach, which begins with multi-modal retrieval and escalates to\nfine-tuned model inference for low-confidence cases. Based on collected user\npreferred options and settings, GPT-4o will be used to summarize user\npreferences from relevant cases. In preliminary studies, our detection module\nshows better accuracy and latency compared with GPT models. To facilitate\nfuture research, we also design a user-friendly interface for user data\ncollection and demonstrate an effective workflow for real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous advancements in robotics and AI are driving the integration of\nrobots from industry into everyday environments. However, dynamic and\nunpredictable human activities in daily lives would directly or indirectly\nconflict with robot actions. Besides, due to the social attributes of such\nhuman-induced conflicts, solutions are not always unique and depend highly on\nthe user's personal preferences. To address these challenges and facilitate the\ndevelopment of household robots, we propose COMMET, a system for human-induced\nCOnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid\ndetection approach, which begins with multi-modal retrieval and escalates to\nfine-tuned model inference for low-confidence cases. Based on collected user\npreferred options and settings, GPT-4o will be used to summarize user\npreferences from relevant cases. In preliminary studies, our detection module\nshows better accuracy and latency compared with GPT models. To facilitate\nfuture research, we also design a user-friendly interface for user data\ncollection and demonstrate an effective workflow for real-world deployments."
                },
                "authors": [
                    {
                        "name": "Dongping Li"
                    },
                    {
                        "name": "Shaoting Peng"
                    },
                    {
                        "name": "John Pohovey"
                    },
                    {
                        "name": "Katherine Rose Driggs-Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Katherine Rose Driggs-Campbell"
                },
                "author": "Katherine Rose Driggs-Campbell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08613v3",
                "updated": "2025-09-05T06:35:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    35,
                    41,
                    4,
                    248,
                    0
                ],
                "published": "2025-01-15T06:22:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    22,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Assessing the Sensitivity and Alignment of FOL Closeness Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Sensitivity and Alignment of FOL Closeness Metrics"
                },
                "summary": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage (NL) statements into First-Order Logic~(FOL) and external theorem\nprovers. However, the correctness of FOL statements, comprising operators and\ntext, often go unverified due to the lack of a reliable evaluation metric for\ncomparing generated and ground-truth FOLs. In this paper, we conduct a\ncomprehensive study on the sensitivity of existing NL-, FOL-, and graph-based\nmetrics to capture differences between a sampled FOL and its corresponding\nground-truth. We then measure the alignment between a metric-based ranking of\nFOL outputs and a strong LLM as-a-judge. To do this, we first apply operator\nand text-based perturbations to ground-truth FOL statements to assess metric\nsensitivity. We then evaluate metric robustness by comparing the metrics\nagainst LLMs judgment. Our empirical findings highlight a clear oversensitivity\nin the n-gram metric BLEU for text perturbations. The operator perturbation\naffects the semantic graph metric Smatch++ for structural changes, and the FOL\nmetric for specific operator changes. We observe a closer alignment between\nBertScore and LLM judgement, proving the importance of semantic evaluation.\nAdditionally, we show that combining metrics enhances both robustness and\nsensitivity compared to using individual metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage (NL) statements into First-Order Logic~(FOL) and external theorem\nprovers. However, the correctness of FOL statements, comprising operators and\ntext, often go unverified due to the lack of a reliable evaluation metric for\ncomparing generated and ground-truth FOLs. In this paper, we conduct a\ncomprehensive study on the sensitivity of existing NL-, FOL-, and graph-based\nmetrics to capture differences between a sampled FOL and its corresponding\nground-truth. We then measure the alignment between a metric-based ranking of\nFOL outputs and a strong LLM as-a-judge. To do this, we first apply operator\nand text-based perturbations to ground-truth FOL statements to assess metric\nsensitivity. We then evaluate metric robustness by comparing the metrics\nagainst LLMs judgment. Our empirical findings highlight a clear oversensitivity\nin the n-gram metric BLEU for text perturbations. The operator perturbation\naffects the semantic graph metric Smatch++ for structural changes, and the FOL\nmetric for specific operator changes. We observe a closer alignment between\nBertScore and LLM judgement, proving the importance of semantic evaluation.\nAdditionally, we show that combining metrics enhances both robustness and\nsensitivity compared to using individual metrics."
                },
                "authors": [
                    {
                        "name": "Ramya Keerthy Thatikonda"
                    },
                    {
                        "name": "Wray Buntine"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Shareghi"
                },
                "author": "Ehsan Shareghi",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00868v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00868v3",
                "updated": "2025-09-05T06:33:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    33,
                    18,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-31T14:29:50Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    29,
                    50,
                    6,
                    243,
                    0
                ],
                "title": "A Modular and Scalable Simulator for Connected-UAVs Communication in 5G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modular and Scalable Simulator for Connected-UAVs Communication in 5G\n  Networks"
                },
                "summary": "Cellular-connected UAV systems have enabled a wide range of low-altitude\naerial services. However, these systems still face many challenges, such as\nfrequent handovers and the inefficiency of traditional transport protocols. To\nbetter study these issues, we develop a modular and scalable simulation\nplatform specifically designed for UAVs communication leveraging the research\necology in wireless communication of MATLAB. The platform supports flexible 5G\nNR node deployment, customizable UAVs mobility models, and\nmulti-network-interface extensions. It also supports multiple transport\nprotocols including TCP, UDP, QUIC, etc., allowing to investigate how different\ntransport protocols affect UAVs communication performance. In addition, the\nplatform includes a handover management module, enabling the evaluation of both\ntraditional and learning-based handover strategies. Our platform can serve as a\ntestbed for the development and evaluation of advanced transmission strategies\nin cellular-connected UAV systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cellular-connected UAV systems have enabled a wide range of low-altitude\naerial services. However, these systems still face many challenges, such as\nfrequent handovers and the inefficiency of traditional transport protocols. To\nbetter study these issues, we develop a modular and scalable simulation\nplatform specifically designed for UAVs communication leveraging the research\necology in wireless communication of MATLAB. The platform supports flexible 5G\nNR node deployment, customizable UAVs mobility models, and\nmulti-network-interface extensions. It also supports multiple transport\nprotocols including TCP, UDP, QUIC, etc., allowing to investigate how different\ntransport protocols affect UAVs communication performance. In addition, the\nplatform includes a handover management module, enabling the evaluation of both\ntraditional and learning-based handover strategies. Our platform can serve as a\ntestbed for the development and evaluation of advanced transmission strategies\nin cellular-connected UAV systems."
                },
                "authors": [
                    {
                        "name": "Yong Su"
                    },
                    {
                        "name": "Yiyi Chen"
                    },
                    {
                        "name": "Shenghong Yi"
                    },
                    {
                        "name": "Hui Feng"
                    },
                    {
                        "name": "Yuedong Xu"
                    },
                    {
                        "name": "Wang Xiang"
                    },
                    {
                        "name": "Bo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Hu"
                },
                "author": "Bo Hu",
                "arxiv_comment": "a short version is accepted by MSWiM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00868v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00868v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21433v2",
                "updated": "2025-09-05T06:16:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    16,
                    59,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-29T09:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    2,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM\n  Summarization for Agent Context Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM\n  Summarization for Agent Context Management"
                },
                "summary": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility"
                },
                "authors": [
                    {
                        "name": "Tobias Lindenbauer"
                    },
                    {
                        "name": "Igor Slinko"
                    },
                    {
                        "name": "Ludwig Felder"
                    },
                    {
                        "name": "Egor Bogomolov"
                    },
                    {
                        "name": "Yaroslav Zharov"
                    }
                ],
                "author_detail": {
                    "name": "Yaroslav Zharov"
                },
                "author": "Yaroslav Zharov",
                "arxiv_comment": "v2: Fixed typos and formatting issues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03959v2",
                "updated": "2025-09-05T06:09:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    6,
                    9,
                    30,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T07:36:43Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    7,
                    36,
                    43,
                    3,
                    247,
                    0
                ],
                "title": "WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with\n  Multi-dimensional Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WenetSpeech-Yue: A Large-scale Cantonese Speech Corpus with\n  Multi-dimensional Annotation"
                },
                "summary": "The development of speech understanding and generation has been significantly\naccelerated by the availability of large-scale, high-quality speech datasets.\nAmong these, ASR and TTS are regarded as the most established and fundamental\ntasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9\nmillion native speakers worldwide, limited annotated resources have hindered\nprogress and resulted in suboptimal ASR and TTS performance. To address this\nchallenge, we propose WenetSpeech-Pipe, an integrated pipeline for building\nlarge-scale speech corpus with multi-dimensional annotation tailored for speech\nunderstanding and generation. It comprises six modules: Audio Collection,\nSpeaker Attributes Annotation, Speech Quality Annotation, Automatic Speech\nRecognition, Text Postprocessing and Recognizer Output Voting, enabling rich\nand high-quality annotations. Based on this pipeline, we release\nWenetSpeech-Yue, the first large-scale Cantonese speech corpus with\nmulti-dimensional annotation for ASR and TTS, covering 21,800 hours across 10\ndomains with annotations including ASR transcription, text confidence, speaker\nidentity, age, gender, speech quality scores, among other annotations. We also\nrelease WSYue-eval, a comprehensive Cantonese benchmark with two components:\nWSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long\nutterances, code-switching, and diverse acoustic conditions, and\nWSYue-TTS-eval, with base and coverage subsets for standard and generalization\ntesting. Experimental results show that models trained on WenetSpeech-Yue\nachieve competitive results against state-of-the-art (SOTA) Cantonese ASR and\nTTS systems, including commercial and LLM-based models, highlighting the value\nof our dataset and pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of speech understanding and generation has been significantly\naccelerated by the availability of large-scale, high-quality speech datasets.\nAmong these, ASR and TTS are regarded as the most established and fundamental\ntasks. However, for Cantonese (Yue Chinese), spoken by approximately 84.9\nmillion native speakers worldwide, limited annotated resources have hindered\nprogress and resulted in suboptimal ASR and TTS performance. To address this\nchallenge, we propose WenetSpeech-Pipe, an integrated pipeline for building\nlarge-scale speech corpus with multi-dimensional annotation tailored for speech\nunderstanding and generation. It comprises six modules: Audio Collection,\nSpeaker Attributes Annotation, Speech Quality Annotation, Automatic Speech\nRecognition, Text Postprocessing and Recognizer Output Voting, enabling rich\nand high-quality annotations. Based on this pipeline, we release\nWenetSpeech-Yue, the first large-scale Cantonese speech corpus with\nmulti-dimensional annotation for ASR and TTS, covering 21,800 hours across 10\ndomains with annotations including ASR transcription, text confidence, speaker\nidentity, age, gender, speech quality scores, among other annotations. We also\nrelease WSYue-eval, a comprehensive Cantonese benchmark with two components:\nWSYue-ASR-eval, a manually annotated set for evaluating ASR on short and long\nutterances, code-switching, and diverse acoustic conditions, and\nWSYue-TTS-eval, with base and coverage subsets for standard and generalization\ntesting. Experimental results show that models trained on WenetSpeech-Yue\nachieve competitive results against state-of-the-art (SOTA) Cantonese ASR and\nTTS systems, including commercial and LLM-based models, highlighting the value\nof our dataset and pipeline."
                },
                "authors": [
                    {
                        "name": "Longhao Li"
                    },
                    {
                        "name": "Zhao Guo"
                    },
                    {
                        "name": "Hongjie Chen"
                    },
                    {
                        "name": "Yuhang Dai"
                    },
                    {
                        "name": "Ziyu Zhang"
                    },
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "Tianlun Zuo"
                    },
                    {
                        "name": "Chengyou Wang"
                    },
                    {
                        "name": "Shuiyuan Wang"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Jian Kang"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Hui Bu"
                    },
                    {
                        "name": "Binbin Zhang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Ziya Zhou"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04827v1",
                "updated": "2025-09-05T05:58:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    58,
                    16,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:58:16Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    58,
                    16,
                    4,
                    248,
                    0
                ],
                "title": "VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing\n  for Energy-Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing\n  for Energy-Efficient LLM Serving"
                },
                "summary": "Modern Large Language Model (LLM) serving systems increasingly support\ninteractive applications, like real-time chat assistants, code generation\ntools, and agentic workflows. However, the soaring energy cost of LLM inference\npresents a growing challenge for sustainable and cost-effective deployment.\nThis paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM\nserving, built from a control theory perspective. VoltanaLLM co-designs\nfrequency scaling and request routing in emerging prefill/decode disaggregated\narchitectures, leveraging their decoupled execution to enable fine-grained\nphase-specific control. It consists of a feedback-driven frequency controller\nthat dynamically adapts GPU frequency for prefill and decode phases, and a\nstate-space router that explores routing decisions across frequency-scaled\ninstances to minimize energy under latency constraints. We implement VoltanaLLM\nin SGLang and evaluate its performance over multiple state-of-the-art LLMs and\nreal-world datasets. The results demonstrate that VoltanaLLM achieves up to\n36.3% energy savings while maintaining near-perfect SLO attainment rate, paving\nthe way for sustainable and intelligent LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model (LLM) serving systems increasingly support\ninteractive applications, like real-time chat assistants, code generation\ntools, and agentic workflows. However, the soaring energy cost of LLM inference\npresents a growing challenge for sustainable and cost-effective deployment.\nThis paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM\nserving, built from a control theory perspective. VoltanaLLM co-designs\nfrequency scaling and request routing in emerging prefill/decode disaggregated\narchitectures, leveraging their decoupled execution to enable fine-grained\nphase-specific control. It consists of a feedback-driven frequency controller\nthat dynamically adapts GPU frequency for prefill and decode phases, and a\nstate-space router that explores routing decisions across frequency-scaled\ninstances to minimize energy under latency constraints. We implement VoltanaLLM\nin SGLang and evaluate its performance over multiple state-of-the-art LLMs and\nreal-world datasets. The results demonstrate that VoltanaLLM achieves up to\n36.3% energy savings while maintaining near-perfect SLO attainment rate, paving\nthe way for sustainable and intelligent LLM serving."
                },
                "authors": [
                    {
                        "name": "Jiahuan Yu"
                    },
                    {
                        "name": "Aryan Taneja"
                    },
                    {
                        "name": "Junfeng Lin"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "arxiv_affiliation": "University of Illinois Urbana-Champaign",
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04821v1",
                "updated": "2025-09-05T05:45:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    45,
                    7,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:45:07Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    45,
                    7,
                    4,
                    248,
                    0
                ],
                "title": "AFD-SLU: Adaptive Feature Distillation for Spoken Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AFD-SLU: Adaptive Feature Distillation for Spoken Language Understanding"
                },
                "summary": "Spoken Language Understanding (SLU) is a core component of conversational\nsystems, enabling machines to interpret user utterances. Despite its\nimportance, developing effective SLU systems remains challenging due to the\nscarcity of labeled training data and the computational burden of deploying\nLarge Language Models (LLMs) in real-world applications. To further alleviate\nthese issues, we propose an Adaptive Feature Distillation framework that\ntransfers rich semantic representations from a General Text Embeddings\n(GTE)-based teacher model to a lightweight student model. Our method introduces\na dynamic adapter equipped with a Residual Projection Neural Network (RPNN) to\nalign heterogeneous feature spaces, and a Dynamic Distillation Coefficient\n(DDC) that adaptively modulates the distillation strength based on real-time\nfeedback from intent and slot prediction performance. Experiments on the\nChinese profile-based ProSLU benchmark demonstrate that AFD-SLU achieves\nstate-of-the-art results, with 95.67% intent accuracy, 92.02% slot F1 score,\nand 85.50% overall accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken Language Understanding (SLU) is a core component of conversational\nsystems, enabling machines to interpret user utterances. Despite its\nimportance, developing effective SLU systems remains challenging due to the\nscarcity of labeled training data and the computational burden of deploying\nLarge Language Models (LLMs) in real-world applications. To further alleviate\nthese issues, we propose an Adaptive Feature Distillation framework that\ntransfers rich semantic representations from a General Text Embeddings\n(GTE)-based teacher model to a lightweight student model. Our method introduces\na dynamic adapter equipped with a Residual Projection Neural Network (RPNN) to\nalign heterogeneous feature spaces, and a Dynamic Distillation Coefficient\n(DDC) that adaptively modulates the distillation strength based on real-time\nfeedback from intent and slot prediction performance. Experiments on the\nChinese profile-based ProSLU benchmark demonstrate that AFD-SLU achieves\nstate-of-the-art results, with 95.67% intent accuracy, 92.02% slot F1 score,\nand 85.50% overall accuracy."
                },
                "authors": [
                    {
                        "name": "Yan Xie"
                    },
                    {
                        "name": "Yibo Cui"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Erwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Erwei Yin"
                },
                "author": "Erwei Yin",
                "arxiv_comment": "5 pages, 1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04820v1",
                "updated": "2025-09-05T05:44:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    44,
                    50,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:44:50Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    44,
                    50,
                    4,
                    248,
                    0
                ],
                "title": "Fishing for Answers: Exploring One-shot vs. Iterative Retrieval\n  Strategies for Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fishing for Answers: Exploring One-shot vs. Iterative Retrieval\n  Strategies for Retrieval Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) based on Large Language Models (LLMs) is\na powerful solution to understand and query the industry's closed-source\ndocuments. However, basic RAG often struggles with complex QA tasks in legal\nand regulatory domains, particularly when dealing with numerous government\ndocuments. The top-$k$ strategy frequently misses golden chunks, leading to\nincomplete or inaccurate answers. To address these retrieval bottlenecks, we\nexplore two strategies to improve evidence coverage and answer quality. The\nfirst is a One-SHOT retrieval method that adaptively selects chunks based on a\ntoken budget, allowing as much relevant content as possible to be included\nwithin the model's context window. Additionally, we design modules to further\nfilter and refine the chunks. The second is an iterative retrieval strategy\nbuilt on a Reasoning Agentic RAG framework, where a reasoning LLM dynamically\nissues search queries, evaluates retrieved results, and progressively refines\nthe context over multiple turns. We identify query drift and retrieval laziness\nissues and further design two modules to tackle them. Through extensive\nexperiments on a dataset of government documents, we aim to offer practical\ninsights and guidance for real-world applications in legal and regulatory\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) based on Large Language Models (LLMs) is\na powerful solution to understand and query the industry's closed-source\ndocuments. However, basic RAG often struggles with complex QA tasks in legal\nand regulatory domains, particularly when dealing with numerous government\ndocuments. The top-$k$ strategy frequently misses golden chunks, leading to\nincomplete or inaccurate answers. To address these retrieval bottlenecks, we\nexplore two strategies to improve evidence coverage and answer quality. The\nfirst is a One-SHOT retrieval method that adaptively selects chunks based on a\ntoken budget, allowing as much relevant content as possible to be included\nwithin the model's context window. Additionally, we design modules to further\nfilter and refine the chunks. The second is an iterative retrieval strategy\nbuilt on a Reasoning Agentic RAG framework, where a reasoning LLM dynamically\nissues search queries, evaluates retrieved results, and progressively refines\nthe context over multiple turns. We identify query drift and retrieval laziness\nissues and further design two modules to tackle them. Through extensive\nexperiments on a dataset of government documents, we aim to offer practical\ninsights and guidance for real-world applications in legal and regulatory\ndomains."
                },
                "authors": [
                    {
                        "name": "Huifeng Lin"
                    },
                    {
                        "name": "Gang Su"
                    },
                    {
                        "name": "Jintao Liang"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Ziyue Li"
                    }
                ],
                "author_detail": {
                    "name": "Ziyue Li"
                },
                "author": "Ziyue Li",
                "arxiv_comment": "under Review of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15937v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15937v3",
                "updated": "2025-09-05T05:26:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    26,
                    23,
                    4,
                    248,
                    0
                ],
                "published": "2025-03-20T08:25:00Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    25,
                    0,
                    3,
                    79,
                    0
                ],
                "title": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment"
                },
                "summary": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale.\n  V-Droid obtains a substantial task success rate across several public mobile\ntask automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49%\non MobileAgentBench, surpassing existing agents by 5.2%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves a remarkably low latency of 4.3s\nper step, which is 6.1X faster compared with existing mobile agents. The source\ncode is available at https://github.com/V-Droid-Agent/V-Droid.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale.\n  V-Droid obtains a substantial task success rate across several public mobile\ntask automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49%\non MobileAgentBench, surpassing existing agents by 5.2%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves a remarkably low latency of 4.3s\nper step, which is 6.1X faster compared with existing mobile agents. The source\ncode is available at https://github.com/V-Droid-Agent/V-Droid."
                },
                "authors": [
                    {
                        "name": "Gaole Dai"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Rui Tan"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "add baselines, add source code link",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15937v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15937v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04810v1",
                "updated": "2025-09-05T05:17:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    17,
                    14,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:17:14Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    17,
                    14,
                    4,
                    248,
                    0
                ],
                "title": "Code Review Without Borders: Evaluating Synthetic vs. Real Data for\n  Review Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Review Without Borders: Evaluating Synthetic vs. Real Data for\n  Review Recommendation"
                },
                "summary": "Automating the decision of whether a code change requires manual review is\nvital for maintaining software quality in modern development workflows.\nHowever, the emergence of new programming languages and frameworks creates a\ncritical bottleneck: while large volumes of unlabelled code are readily\navailable, there is an insufficient amount of labelled data to train supervised\nmodels for review classification. We address this challenge by leveraging Large\nLanguage Models (LLMs) to translate code changes from well-resourced languages\ninto equivalent changes in underrepresented or emerging languages, generating\nsynthetic training data where labelled examples are scarce. We assume that\nalthough LLMs have learned the syntax and semantics of new languages from\navailable unlabelled code, they have yet to fully grasp which code changes are\nconsidered significant or review-worthy within these emerging ecosystems. To\novercome this, we use LLMs to generate synthetic change examples and train\nsupervised classifiers on them. We systematically compare the performance of\nthese classifiers against models trained on real labelled data. Our experiments\nacross multiple GitHub repositories and language pairs demonstrate that\nLLM-generated synthetic data can effectively bootstrap review recommendation\nsystems, narrowing the performance gap even in low-resource settings. This\napproach provides a scalable pathway to extend automated code review\ncapabilities to rapidly evolving technology stacks, even in the absence of\nannotated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the decision of whether a code change requires manual review is\nvital for maintaining software quality in modern development workflows.\nHowever, the emergence of new programming languages and frameworks creates a\ncritical bottleneck: while large volumes of unlabelled code are readily\navailable, there is an insufficient amount of labelled data to train supervised\nmodels for review classification. We address this challenge by leveraging Large\nLanguage Models (LLMs) to translate code changes from well-resourced languages\ninto equivalent changes in underrepresented or emerging languages, generating\nsynthetic training data where labelled examples are scarce. We assume that\nalthough LLMs have learned the syntax and semantics of new languages from\navailable unlabelled code, they have yet to fully grasp which code changes are\nconsidered significant or review-worthy within these emerging ecosystems. To\novercome this, we use LLMs to generate synthetic change examples and train\nsupervised classifiers on them. We systematically compare the performance of\nthese classifiers against models trained on real labelled data. Our experiments\nacross multiple GitHub repositories and language pairs demonstrate that\nLLM-generated synthetic data can effectively bootstrap review recommendation\nsystems, narrowing the performance gap even in low-resource settings. This\napproach provides a scalable pathway to extend automated code review\ncapabilities to rapidly evolving technology stacks, even in the absence of\nannotated data."
                },
                "authors": [
                    {
                        "name": "Yogev Cohen"
                    },
                    {
                        "name": "Dudi Ohayon"
                    },
                    {
                        "name": "Romy Somkin"
                    },
                    {
                        "name": "Yehudit Aperstein"
                    },
                    {
                        "name": "Alexander Apartsin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Apartsin"
                },
                "author": "Alexander Apartsin",
                "arxiv_comment": "4 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04809v1",
                "updated": "2025-09-05T05:09:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    9,
                    9,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T05:09:09Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    5,
                    9,
                    9,
                    4,
                    248,
                    0
                ],
                "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning\n  Agents with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning\n  Agents with Large Language Models"
                },
                "summary": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain."
                },
                "authors": [
                    {
                        "name": "Haechang Kim"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Can Li"
                    },
                    {
                        "name": "Jong Min Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jong Min Lee"
                },
                "author": "Jong Min Lee",
                "arxiv_comment": "31 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09600v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09600v3",
                "updated": "2025-09-05T04:54:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    54,
                    29,
                    4,
                    248,
                    0
                ],
                "published": "2024-08-18T21:45:03Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    21,
                    45,
                    3,
                    6,
                    231,
                    0
                ],
                "title": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Antidote: Post-fine-tuning Safety Alignment for Large Language Models\n  against Harmful Fine-tuning"
                },
                "summary": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can\nbreak the LLMs's safety alignment. While several defenses have been proposed,\nour evaluation shows that existing defenses fail \\textit{when some specific\ntraining hyper-parameters are chosen} -- a large learning rate or a large\nnumber of training epochs in the fine-tuning stage can easily invalidate the\ndefense. To this end, we propose Antidote, a post-fine-tuning stage solution,\nwhich remains \\textbf{\\textit{agnostic to the training hyper-parameters in the\nfine-tuning stage}}. Antidote relies on the philosophy that by removing the\nharmful parameters, the harmful model can be recovered from the harmful\nbehaviors, regardless of how those harmful parameters are formed in the\nfine-tuning stage. With this philosophy, we introduce a one-shot pruning stage\nafter harmful fine-tuning to remove the harmful weights that are responsible\nfor the generation of harmful content. Despite its embarrassing simplicity,\nempirical results show that Antidote can reduce harmful score while maintaining\naccuracy on downstream tasks. Code is available at\nhttps://github.com/git-disl/Antidote.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety aligned Large Language Models (LLMs) are vulnerable to harmful\nfine-tuning attacks -- a few harmful data mixed in the fine-tuning dataset can\nbreak the LLMs's safety alignment. While several defenses have been proposed,\nour evaluation shows that existing defenses fail \\textit{when some specific\ntraining hyper-parameters are chosen} -- a large learning rate or a large\nnumber of training epochs in the fine-tuning stage can easily invalidate the\ndefense. To this end, we propose Antidote, a post-fine-tuning stage solution,\nwhich remains \\textbf{\\textit{agnostic to the training hyper-parameters in the\nfine-tuning stage}}. Antidote relies on the philosophy that by removing the\nharmful parameters, the harmful model can be recovered from the harmful\nbehaviors, regardless of how those harmful parameters are formed in the\nfine-tuning stage. With this philosophy, we introduce a one-shot pruning stage\nafter harmful fine-tuning to remove the harmful weights that are responsible\nfor the generation of harmful content. Despite its embarrassing simplicity,\nempirical results show that Antidote can reduce harmful score while maintaining\naccuracy on downstream tasks. Code is available at\nhttps://github.com/git-disl/Antidote."
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Gautam Bhattacharya"
                    },
                    {
                        "name": "Pratik Joshi"
                    },
                    {
                        "name": "Josh Kimball"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "arxiv_comment": "Rejected by AAAI25-AIA. Accepted by ICML25. Authors are thankful to\n  the anonymous reviewers from both AAAI25-AIA and ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09600v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09600v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04805v1",
                "updated": "2025-09-05T04:52:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    52,
                    51,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T04:52:51Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    52,
                    51,
                    4,
                    248,
                    0
                ],
                "title": "AI-Driven Fronthaul Link Compression in Wireless Communication Systems:\n  Review and Method Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Fronthaul Link Compression in Wireless Communication Systems:\n  Review and Method Design"
                },
                "summary": "Modern fronthaul links in wireless systems must transport high-dimensional\nsignals under stringent bandwidth and latency constraints, which makes\ncompression indispensable. Traditional strategies such as compressed sensing,\nscalar quantization, and fixed-codec pipelines often rely on restrictive\npriors, degrade sharply at high compression ratios, and are hard to tune across\nchannels and deployments. Recent progress in Artificial Intelligence (AI) has\nbrought end-to-end learned transforms, vector and hierarchical quantization,\nand learned entropy models that better exploit the structure of Channel State\nInformation(CSI), precoding matrices, I/Q samples, and LLRs. This paper first\nsurveys AI-driven compression techniques and then provides a focused analysis\nof two representative high-compression routes: CSI feedback with end-to-end\nlearning and Resource Block (RB) granularity precoding optimization combined\nwith compression. Building on these insights, we propose a fronthaul\ncompression strategy tailored to cell-free architectures. The design targets\nhigh compression with controlled performance loss, supports RB-level rate\nadaptation, and enables low-latency inference suitable for centralized\ncooperative transmission in next-generation networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern fronthaul links in wireless systems must transport high-dimensional\nsignals under stringent bandwidth and latency constraints, which makes\ncompression indispensable. Traditional strategies such as compressed sensing,\nscalar quantization, and fixed-codec pipelines often rely on restrictive\npriors, degrade sharply at high compression ratios, and are hard to tune across\nchannels and deployments. Recent progress in Artificial Intelligence (AI) has\nbrought end-to-end learned transforms, vector and hierarchical quantization,\nand learned entropy models that better exploit the structure of Channel State\nInformation(CSI), precoding matrices, I/Q samples, and LLRs. This paper first\nsurveys AI-driven compression techniques and then provides a focused analysis\nof two representative high-compression routes: CSI feedback with end-to-end\nlearning and Resource Block (RB) granularity precoding optimization combined\nwith compression. Building on these insights, we propose a fronthaul\ncompression strategy tailored to cell-free architectures. The design targets\nhigh compression with controlled performance loss, supports RB-level rate\nadaptation, and enables low-latency inference suitable for centralized\ncooperative transmission in next-generation networks."
                },
                "authors": [
                    {
                        "name": "Keqin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Keqin Zhang"
                },
                "author": "Keqin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01920v2",
                "updated": "2025-09-05T04:49:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    49,
                    52,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-02T03:34:36Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    3,
                    34,
                    36,
                    1,
                    245,
                    0
                ],
                "title": "Dynamic Speculative Agent Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Speculative Agent Planning"
                },
                "summary": "Despite their remarkable success in complex tasks propelling widespread\nadoption, large language-model-based agents still face critical deployment\nchallenges due to prohibitive latency and inference costs. While recent work\nhas explored various methods to accelerate inference, existing approaches\nsuffer from significant limitations: they either fail to preserve performance\nfidelity, require extensive offline training of router modules, or incur\nexcessive operational costs. Moreover, they provide minimal user control over\nthe tradeoff between acceleration and other performance metrics. To address\nthese gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous\nonline reinforcement learning framework that provides lossless acceleration\nwith substantially reduced costs without requiring additional pre-deployment\npreparation. DSP explicitly optimizes a joint objective balancing end-to-end\nlatency against dollar cost, allowing practitioners to adjust a single\nparameter that steers the system toward faster responses, cheaper operation, or\nany point along this continuum. Experiments on two standard agent benchmarks\ndemonstrate that DSP achieves comparable efficiency to the fastest lossless\nacceleration method while reducing total cost by 30% and unnecessary cost up to\n60%. Our code and data are available through\nhttps://github.com/guanyilin428/Dynamic-Speculative-Planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable success in complex tasks propelling widespread\nadoption, large language-model-based agents still face critical deployment\nchallenges due to prohibitive latency and inference costs. While recent work\nhas explored various methods to accelerate inference, existing approaches\nsuffer from significant limitations: they either fail to preserve performance\nfidelity, require extensive offline training of router modules, or incur\nexcessive operational costs. Moreover, they provide minimal user control over\nthe tradeoff between acceleration and other performance metrics. To address\nthese gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous\nonline reinforcement learning framework that provides lossless acceleration\nwith substantially reduced costs without requiring additional pre-deployment\npreparation. DSP explicitly optimizes a joint objective balancing end-to-end\nlatency against dollar cost, allowing practitioners to adjust a single\nparameter that steers the system toward faster responses, cheaper operation, or\nany point along this continuum. Experiments on two standard agent benchmarks\ndemonstrate that DSP achieves comparable efficiency to the fastest lossless\nacceleration method while reducing total cost by 30% and unnecessary cost up to\n60%. Our code and data are available through\nhttps://github.com/guanyilin428/Dynamic-Speculative-Planning."
                },
                "authors": [
                    {
                        "name": "Yilin Guan"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Qingfeng Lan"
                    },
                    {
                        "name": "Sun Fei"
                    },
                    {
                        "name": "Dujian Ding"
                    },
                    {
                        "name": "Devang Acharya"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03544v3",
                "updated": "2025-09-05T04:44:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    44,
                    48,
                    4,
                    248,
                    0
                ],
                "published": "2025-01-07T05:39:21Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    39,
                    21,
                    1,
                    7,
                    0
                ],
                "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for\n  Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for\n  Text-to-Image Models"
                },
                "summary": "Recent text-to-image (T2I) models have exhibited remarkable performance in\ngenerating high-quality images from text descriptions. However, these models\nare vulnerable to misuse, particularly generating not-safe-for-work (NSFW)\ncontent, such as sexually explicit, violent, political, and disturbing images,\nraising serious ethical concerns. In this work, we present PromptGuard, a novel\ncontent moderation technique that draws inspiration from the system prompt\nmechanism in large language models (LLMs) for safety alignment. Unlike LLMs,\nT2I models lack a direct interface for enforcing behavioral guidelines. Our key\nidea is to optimize a safety soft prompt that functions as an implicit system\nprompt within the T2I model's textual embedding space. This universal soft\nprompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image\ngeneration without altering the inference efficiency or requiring proxy models.\nWe further enhance its reliability and helpfulness through a divide-and-conquer\nstrategy, which optimizes category-specific soft prompts and combines them into\nholistic safety guidance. Extensive experiments across five datasets\ndemonstrate that PromptGuard effectively mitigates NSFW content generation\nwhile preserving high-quality benign outputs. PromptGuard achieves 3.8 times\nfaster than prior content moderation methods, surpassing eight state-of-the-art\ndefenses with an optimal unsafe ratio down to 5.84%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent text-to-image (T2I) models have exhibited remarkable performance in\ngenerating high-quality images from text descriptions. However, these models\nare vulnerable to misuse, particularly generating not-safe-for-work (NSFW)\ncontent, such as sexually explicit, violent, political, and disturbing images,\nraising serious ethical concerns. In this work, we present PromptGuard, a novel\ncontent moderation technique that draws inspiration from the system prompt\nmechanism in large language models (LLMs) for safety alignment. Unlike LLMs,\nT2I models lack a direct interface for enforcing behavioral guidelines. Our key\nidea is to optimize a safety soft prompt that functions as an implicit system\nprompt within the T2I model's textual embedding space. This universal soft\nprompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image\ngeneration without altering the inference efficiency or requiring proxy models.\nWe further enhance its reliability and helpfulness through a divide-and-conquer\nstrategy, which optimizes category-specific soft prompts and combines them into\nholistic safety guidance. Extensive experiments across five datasets\ndemonstrate that PromptGuard effectively mitigates NSFW content generation\nwhile preserving high-quality benign outputs. PromptGuard achieves 3.8 times\nfaster than prior content moderation methods, surpassing eight state-of-the-art\ndefenses with an optimal unsafe ratio down to 5.84%."
                },
                "authors": [
                    {
                        "name": "Lingzhi Yuan"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Chejian Xu"
                    },
                    {
                        "name": "Guanhong Tao"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "arxiv_comment": "15 pages, 8 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04802v1",
                "updated": "2025-09-05T04:36:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    36,
                    17,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T04:36:17Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    36,
                    17,
                    4,
                    248,
                    0
                ],
                "title": "Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in\n  LLMs with Action Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in\n  LLMs with Action Graphs"
                },
                "summary": "As large language models transition to agentic systems, current safety\nevaluation frameworks face critical gaps in assessing deployment-specific\nrisks. We introduce AgentSeer, an observability-based evaluation framework that\ndecomposes agentic executions into granular action and component graphs,\nenabling systematic agentic-situational assessment. Through cross-model\nvalidation on GPT-OSS-20B and Gemini-2.0-flash using HarmBench single turn and\niterative refinement attacks, we demonstrate fundamental differences between\nmodel-level and agentic-level vulnerability profiles. Model-level evaluation\nreveals baseline differences: GPT-OSS-20B (39.47% ASR) versus Gemini-2.0-flash\n(50.00% ASR), with both models showing susceptibility to social engineering\nwhile maintaining logic-based attack resistance. However, agentic-level\nassessment exposes agent-specific risks invisible to traditional evaluation. We\ndiscover \"agentic-only\" vulnerabilities that emerge exclusively in agentic\ncontexts, with tool-calling showing 24-60% higher ASR across both models.\nCross-model analysis reveals universal agentic patterns, agent transfer\noperations as highest-risk tools, semantic rather than syntactic vulnerability\nmechanisms, and context-dependent attack effectiveness, alongside\nmodel-specific security profiles in absolute ASR levels and optimal injection\nstrategies. Direct attack transfer from model-level to agentic contexts shows\ndegraded performance (GPT-OSS-20B: 57% human injection ASR; Gemini-2.0-flash:\n28%), while context-aware iterative attacks successfully compromise objectives\nthat failed at model-level, confirming systematic evaluation gaps. These\nfindings establish the urgent need for agentic-situation evaluation paradigms,\nwith AgentSeer providing the standardized methodology and empirical validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models transition to agentic systems, current safety\nevaluation frameworks face critical gaps in assessing deployment-specific\nrisks. We introduce AgentSeer, an observability-based evaluation framework that\ndecomposes agentic executions into granular action and component graphs,\nenabling systematic agentic-situational assessment. Through cross-model\nvalidation on GPT-OSS-20B and Gemini-2.0-flash using HarmBench single turn and\niterative refinement attacks, we demonstrate fundamental differences between\nmodel-level and agentic-level vulnerability profiles. Model-level evaluation\nreveals baseline differences: GPT-OSS-20B (39.47% ASR) versus Gemini-2.0-flash\n(50.00% ASR), with both models showing susceptibility to social engineering\nwhile maintaining logic-based attack resistance. However, agentic-level\nassessment exposes agent-specific risks invisible to traditional evaluation. We\ndiscover \"agentic-only\" vulnerabilities that emerge exclusively in agentic\ncontexts, with tool-calling showing 24-60% higher ASR across both models.\nCross-model analysis reveals universal agentic patterns, agent transfer\noperations as highest-risk tools, semantic rather than syntactic vulnerability\nmechanisms, and context-dependent attack effectiveness, alongside\nmodel-specific security profiles in absolute ASR levels and optimal injection\nstrategies. Direct attack transfer from model-level to agentic contexts shows\ndegraded performance (GPT-OSS-20B: 57% human injection ASR; Gemini-2.0-flash:\n28%), while context-aware iterative attacks successfully compromise objectives\nthat failed at model-level, confirming systematic evaluation gaps. These\nfindings establish the urgent need for agentic-situation evaluation paradigms,\nwith AgentSeer providing the standardized methodology and empirical validation."
                },
                "authors": [
                    {
                        "name": "Ilham Wicaksono"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Theo King"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04796v1",
                "updated": "2025-09-05T04:29:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    29,
                    15,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T04:29:15Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    29,
                    15,
                    4,
                    248,
                    0
                ],
                "title": "Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under\n  Recursive Synthetic Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under\n  Recursive Synthetic Training"
                },
                "summary": "Large language models increasingly rely on synthetic data due to\nhuman-written content scarcity, yet recursive training on model-generated\noutputs leads to model collapse, a degenerative process threatening factual\nreliability. We define knowledge collapse as a distinct three-stage phenomenon\nwhere factual accuracy deteriorates while surface fluency persists, creating\n\"confidently wrong\" outputs that pose critical risks in accuracy-dependent\ndomains. Through controlled experiments with recursive synthetic training, we\ndemonstrate that collapse trajectory and timing depend critically on\ninstruction format, distinguishing instruction-following collapse from\ntraditional model collapse through its conditional, prompt-dependent nature. We\npropose domain-specific synthetic training as a targeted mitigation strategy\nthat achieves substantial improvements in collapse resistance while maintaining\ncomputational efficiency. Our evaluation framework combines model-centric\nindicators with task-centric metrics to detect distinct degradation phases,\nenabling reproducible assessment of epistemic deterioration across different\nlanguage models. These findings provide both theoretical insights into collapse\ndynamics and practical guidance for sustainable AI training in\nknowledge-intensive applications where accuracy is paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models increasingly rely on synthetic data due to\nhuman-written content scarcity, yet recursive training on model-generated\noutputs leads to model collapse, a degenerative process threatening factual\nreliability. We define knowledge collapse as a distinct three-stage phenomenon\nwhere factual accuracy deteriorates while surface fluency persists, creating\n\"confidently wrong\" outputs that pose critical risks in accuracy-dependent\ndomains. Through controlled experiments with recursive synthetic training, we\ndemonstrate that collapse trajectory and timing depend critically on\ninstruction format, distinguishing instruction-following collapse from\ntraditional model collapse through its conditional, prompt-dependent nature. We\npropose domain-specific synthetic training as a targeted mitigation strategy\nthat achieves substantial improvements in collapse resistance while maintaining\ncomputational efficiency. Our evaluation framework combines model-centric\nindicators with task-centric metrics to detect distinct degradation phases,\nenabling reproducible assessment of epistemic deterioration across different\nlanguage models. These findings provide both theoretical insights into collapse\ndynamics and practical guidance for sustainable AI training in\nknowledge-intensive applications where accuracy is paramount."
                },
                "authors": [
                    {
                        "name": "Figarri Keisha"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13207v2",
                "updated": "2025-09-05T04:24:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    24,
                    34,
                    4,
                    248,
                    0
                ],
                "published": "2024-11-20T11:09:55Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    9,
                    55,
                    2,
                    325,
                    0
                ],
                "title": "The Information Security Awareness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Information Security Awareness of Large Language Models"
                },
                "summary": "The popularity of large language models (LLMs) continues to grow, and\nLLM-based assistants have become ubiquitous. Information security awareness\n(ISA) is an important yet underexplored safety aspect of LLMs. ISA encompasses\nLLMs' security knowledge, which has been explored in the past, as well as\nattitudes and behaviors, which are crucial to LLMs' ability to understand\nimplicit security context and reject unsafe requests that may cause the LLM to\nfail the user. We present an automated method for measuring the ISA of LLMs,\nwhich covers all 30 security topics in a mobile ISA taxonomy, using realistic\nscenarios that create tension between implicit security implications and user\nsatisfaction. Applying this method to leading LLMs, we find that most of the\npopular models exhibit only medium to low levels of ISA, exposing their users\nto cybersecurity threats. Smaller variants of the same model family are\nsignificantly riskier, while newer versions show no consistent ISA improvement,\nsuggesting that providers are not actively working toward mitigating this\nissue. These results reveal a widespread vulnerability affecting current LLM\ndeployments: the majority of popular models, and particularly their smaller\nvariants, may systematically endanger users. We propose a practical mitigation:\nincorporating our security awareness instruction into model system prompts to\nhelp LLMs better detect and reject unsafe requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The popularity of large language models (LLMs) continues to grow, and\nLLM-based assistants have become ubiquitous. Information security awareness\n(ISA) is an important yet underexplored safety aspect of LLMs. ISA encompasses\nLLMs' security knowledge, which has been explored in the past, as well as\nattitudes and behaviors, which are crucial to LLMs' ability to understand\nimplicit security context and reject unsafe requests that may cause the LLM to\nfail the user. We present an automated method for measuring the ISA of LLMs,\nwhich covers all 30 security topics in a mobile ISA taxonomy, using realistic\nscenarios that create tension between implicit security implications and user\nsatisfaction. Applying this method to leading LLMs, we find that most of the\npopular models exhibit only medium to low levels of ISA, exposing their users\nto cybersecurity threats. Smaller variants of the same model family are\nsignificantly riskier, while newer versions show no consistent ISA improvement,\nsuggesting that providers are not actively working toward mitigating this\nissue. These results reveal a widespread vulnerability affecting current LLM\ndeployments: the majority of popular models, and particularly their smaller\nvariants, may systematically endanger users. We propose a practical mitigation:\nincorporating our security awareness instruction into model system prompts to\nhelp LLMs better detect and reject unsafe requests."
                },
                "authors": [
                    {
                        "name": "Ofir Cohen"
                    },
                    {
                        "name": "Gil Ari Agmon"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Rami Puzis"
                    }
                ],
                "author_detail": {
                    "name": "Rami Puzis"
                },
                "author": "Rami Puzis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04794v1",
                "updated": "2025-09-05T04:19:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    19,
                    15,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T04:19:15Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    19,
                    15,
                    4,
                    248,
                    0
                ],
                "title": "Personality as a Probe for LLM Evaluation: Method Trade-offs and\n  Downstream Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality as a Probe for LLM Evaluation: Method Trade-offs and\n  Downstream Effects"
                },
                "summary": "Personality manipulation in large language models (LLMs) is increasingly\napplied in customer service and agentic scenarios, yet its mechanisms and\ntrade-offs remain unclear. We present a systematic study of personality control\nusing the Big Five traits, comparing in-context learning (ICL),\nparameter-efficient fine-tuning (PEFT), and mechanistic steering (MS). Our\ncontributions are fourfold. First, we construct a contrastive dataset with\nbalanced high/low trait responses, enabling effective steering vector\ncomputation and fair cross-method evaluation. Second, we introduce a unified\nevaluation framework based on within-run $\\Delta$ analysis that disentangles,\nreasoning capability, agent performance, and demographic bias across MMLU,\nGAIA, and BBQ benchmarks. Third, we develop trait purification techniques to\nseparate openness from conscientiousness, addressing representational overlap\nin trait encoding. Fourth, we propose a three-level stability framework that\nquantifies method-, trait-, and combination-level robustness, offering\npractical guidance under deployment constraints. Experiments on Gemma-2-2B-IT\nand LLaMA-3-8B-Instruct reveal clear trade-offs: ICL achieves strong alignment\nwith minimal capability loss, PEFT delivers the highest alignment at the cost\nof degraded task performance, and MS provides lightweight runtime control with\ncompetitive effectiveness. Trait-level analysis shows openness as uniquely\nchallenging, agreeableness as most resistant to ICL, and personality encoding\nconsolidating around intermediate layers. Taken together, these results\nestablish personality manipulation as a multi-level probe into behavioral\nrepresentation, linking surface conditioning, parameter encoding, and\nactivation-level steering, and positioning mechanistic steering as a\nlightweight alternative to fine-tuning for both deployment and\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality manipulation in large language models (LLMs) is increasingly\napplied in customer service and agentic scenarios, yet its mechanisms and\ntrade-offs remain unclear. We present a systematic study of personality control\nusing the Big Five traits, comparing in-context learning (ICL),\nparameter-efficient fine-tuning (PEFT), and mechanistic steering (MS). Our\ncontributions are fourfold. First, we construct a contrastive dataset with\nbalanced high/low trait responses, enabling effective steering vector\ncomputation and fair cross-method evaluation. Second, we introduce a unified\nevaluation framework based on within-run $\\Delta$ analysis that disentangles,\nreasoning capability, agent performance, and demographic bias across MMLU,\nGAIA, and BBQ benchmarks. Third, we develop trait purification techniques to\nseparate openness from conscientiousness, addressing representational overlap\nin trait encoding. Fourth, we propose a three-level stability framework that\nquantifies method-, trait-, and combination-level robustness, offering\npractical guidance under deployment constraints. Experiments on Gemma-2-2B-IT\nand LLaMA-3-8B-Instruct reveal clear trade-offs: ICL achieves strong alignment\nwith minimal capability loss, PEFT delivers the highest alignment at the cost\nof degraded task performance, and MS provides lightweight runtime control with\ncompetitive effectiveness. Trait-level analysis shows openness as uniquely\nchallenging, agreeableness as most resistant to ICL, and personality encoding\nconsolidating around intermediate layers. Taken together, these results\nestablish personality manipulation as a multi-level probe into behavioral\nrepresentation, linking surface conditioning, parameter encoding, and\nactivation-level steering, and positioning mechanistic steering as a\nlightweight alternative to fine-tuning for both deployment and\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Gunmay Handa"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17422v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17422v3",
                "updated": "2025-09-05T04:15:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    15,
                    46,
                    4,
                    248,
                    0
                ],
                "published": "2024-10-22T20:51:45Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    20,
                    51,
                    45,
                    1,
                    296,
                    0
                ],
                "title": "Multimodal LLM Guided Exploration and Active Mapping using Fisher\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLM Guided Exploration and Active Mapping using Fisher\n  Information"
                },
                "summary": "We present an active mapping system that plans for both long-horizon\nexploration goals and short-term actions using a 3D Gaussian Splatting (3DGS)\nrepresentation. Existing methods either do not take advantage of recent\ndevelopments in multimodal Large Language Models (LLM) or do not consider\nchallenges in localization uncertainty, which is critical in embodied agents.\nWe propose employing multimodal LLMs for long-horizon planning in conjunction\nwith detailed motion planning using our information-based objective. By\nleveraging high-quality view synthesis from our 3DGS representation, our method\nemploys a multimodal LLM as a zero-shot planner for long-horizon exploration\ngoals from the semantic perspective. We also introduce an uncertainty-aware\npath proposal and selection algorithm that balances the dual objectives of\nmaximizing the information gain for the environment while minimizing the cost\nof localization errors. Experiments conducted on the Gibson and\nHabitat-Matterport 3D datasets demonstrate state-of-the-art results of the\nproposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an active mapping system that plans for both long-horizon\nexploration goals and short-term actions using a 3D Gaussian Splatting (3DGS)\nrepresentation. Existing methods either do not take advantage of recent\ndevelopments in multimodal Large Language Models (LLM) or do not consider\nchallenges in localization uncertainty, which is critical in embodied agents.\nWe propose employing multimodal LLMs for long-horizon planning in conjunction\nwith detailed motion planning using our information-based objective. By\nleveraging high-quality view synthesis from our 3DGS representation, our method\nemploys a multimodal LLM as a zero-shot planner for long-horizon exploration\ngoals from the semantic perspective. We also introduce an uncertainty-aware\npath proposal and selection algorithm that balances the dual objectives of\nmaximizing the information gain for the environment while minimizing the cost\nof localization errors. Experiments conducted on the Gibson and\nHabitat-Matterport 3D datasets demonstrate state-of-the-art results of the\nproposed method."
                },
                "authors": [
                    {
                        "name": "Wen Jiang"
                    },
                    {
                        "name": "Boshu Lei"
                    },
                    {
                        "name": "Katrina Ashton"
                    },
                    {
                        "name": "Kostas Daniilidis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Daniilidis"
                },
                "author": "Kostas Daniilidis",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17422v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17422v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04791v1",
                "updated": "2025-09-05T04:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    5,
                    27,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T04:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    4,
                    5,
                    27,
                    4,
                    248,
                    0
                ],
                "title": "What-If Analysis of Large Language Models: Explore the Game World Using\n  Proactive Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What-If Analysis of Large Language Models: Explore the Game World Using\n  Proactive Thinking"
                },
                "summary": "Large language models (LLMs) excel at processing information reactively but\nlack the ability to systemically explore hypothetical futures. They cannot ask,\n\"what if we take this action? how will it affect the final outcome\" and\nforecast its potential consequences before acting. This critical gap limits\ntheir utility in dynamic, high-stakes scenarios like strategic planning, risk\nassessment, and real-time decision making. To bridge this gap, we propose\nWiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.\nOur approach integrates What-If Analysis (WIA), a systematic approach for\nevaluating hypothetical scenarios by changing input variables. By leveraging\nenvironmental feedback via reinforcement learning, WiA-LLM moves beyond\nreactive thinking. It dynamically simulates the outcomes of each potential\naction, enabling the model to anticipate future states rather than merely react\nto the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a\ncomplex multiplayer game environment characterized by rapid state changes and\nintricate interactions. The game's real-time state changes require precise\nmulti-step consequence prediction, making it an ideal testbed for our approach.\nExperimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy\nin forecasting game-state changes (up to two times gain over baselines). The\nmodel shows particularly significant gains in high-difficulty scenarios where\naccurate foresight is critical. To our knowledge, this is the first work to\nformally explore and integrate what-if analysis capabilities within LLMs.\nWiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,\nproviding a scalable framework for robust decision-making in dynamic\nenvironments with broad implications for strategic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing information reactively but\nlack the ability to systemically explore hypothetical futures. They cannot ask,\n\"what if we take this action? how will it affect the final outcome\" and\nforecast its potential consequences before acting. This critical gap limits\ntheir utility in dynamic, high-stakes scenarios like strategic planning, risk\nassessment, and real-time decision making. To bridge this gap, we propose\nWiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.\nOur approach integrates What-If Analysis (WIA), a systematic approach for\nevaluating hypothetical scenarios by changing input variables. By leveraging\nenvironmental feedback via reinforcement learning, WiA-LLM moves beyond\nreactive thinking. It dynamically simulates the outcomes of each potential\naction, enabling the model to anticipate future states rather than merely react\nto the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a\ncomplex multiplayer game environment characterized by rapid state changes and\nintricate interactions. The game's real-time state changes require precise\nmulti-step consequence prediction, making it an ideal testbed for our approach.\nExperimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy\nin forecasting game-state changes (up to two times gain over baselines). The\nmodel shows particularly significant gains in high-difficulty scenarios where\naccurate foresight is critical. To our knowledge, this is the first work to\nformally explore and integrate what-if analysis capabilities within LLMs.\nWiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,\nproviding a scalable framework for robust decision-making in dynamic\nenvironments with broad implications for strategic applications."
                },
                "authors": [
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Yanming Zhang"
                    },
                    {
                        "name": "Yi Liao"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Guohua Tang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2508.21365",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20368v2",
                "updated": "2025-09-05T03:51:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    3,
                    51,
                    29,
                    4,
                    248,
                    0
                ],
                "published": "2025-08-28T02:31:17Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    2,
                    31,
                    17,
                    3,
                    240,
                    0
                ],
                "title": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal\n  Multi-Objective Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal\n  Multi-Objective Reinforcement Learning"
                },
                "summary": "Recent studies have explored integrating Large Language Models (LLMs) with\nsearch engines to leverage both the LLMs' internal pre-trained knowledge and\nexternal information. Specially, reinforcement learning (RL) has emerged as a\npromising paradigm for enhancing LLM reasoning through multi-turn interactions\nwith search engines. However, existing RL-based search agents rely on a single\nLLM to handle both search planning and question-answering (QA) tasks in an\nend-to-end manner, which limits their ability to optimize both capabilities\nsimultaneously. In practice, sophisticated AI search systems often employ a\nlarge, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a\nmore effective and efficient approach is to utilize a small, trainable LLM\ndedicated to search planning. In this paper, we propose\n\\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to\nenhance the performance of frozen QA models by focusing on search planning.\nSpecifically, our approach introduces three key innovations: 1) Decoupling the\nArchitecture of the Search Planner and Generator, 2) Dual-Reward Alignment for\nSearch Planning, and 3) Pareto Optimization of Planning Utility and Cost, to\nachieve the objectives. Extensive experiments on real-world datasets\ndemonstrate that AI SearchPlanner outperforms existing RL-based search agents\nin both effectiveness and efficiency, while exhibiting strong generalization\ncapabilities across diverse frozen QA models and data domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have explored integrating Large Language Models (LLMs) with\nsearch engines to leverage both the LLMs' internal pre-trained knowledge and\nexternal information. Specially, reinforcement learning (RL) has emerged as a\npromising paradigm for enhancing LLM reasoning through multi-turn interactions\nwith search engines. However, existing RL-based search agents rely on a single\nLLM to handle both search planning and question-answering (QA) tasks in an\nend-to-end manner, which limits their ability to optimize both capabilities\nsimultaneously. In practice, sophisticated AI search systems often employ a\nlarge, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a\nmore effective and efficient approach is to utilize a small, trainable LLM\ndedicated to search planning. In this paper, we propose\n\\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to\nenhance the performance of frozen QA models by focusing on search planning.\nSpecifically, our approach introduces three key innovations: 1) Decoupling the\nArchitecture of the Search Planner and Generator, 2) Dual-Reward Alignment for\nSearch Planning, and 3) Pareto Optimization of Planning Utility and Cost, to\nachieve the objectives. Extensive experiments on real-world datasets\ndemonstrate that AI SearchPlanner outperforms existing RL-based search agents\nin both effectiveness and efficiency, while exhibiting strong generalization\ncapabilities across diverse frozen QA models and data domains."
                },
                "authors": [
                    {
                        "name": "Lang Mei"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Chong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chong Chen"
                },
                "author": "Chong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04784v1",
                "updated": "2025-09-05T03:47:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    3,
                    47,
                    6,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T03:47:06Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    3,
                    47,
                    6,
                    4,
                    248,
                    0
                ],
                "title": "Enhancing Diversity in Large Language Models via Determinantal Point\n  Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Diversity in Large Language Models via Determinantal Point\n  Processes"
                },
                "summary": "Supervised fine-tuning and reinforcement learning are two popular methods for\npost-training large language models (LLMs). While improving the model's\nperformance on downstream tasks, they often reduce the model's output\ndiversity, leading to narrow, canonical responses. Existing methods to enhance\ndiversity are limited, either by operating at inference time or by focusing on\nlexical differences. We propose a novel training method named DQO based on\ndeterminantal point processes (DPPs) to jointly optimize LLMs for quality and\nsemantic diversity. Our approach samples and embeds a group of responses for\neach prompt, then uses the determinant of a kernel-based similarity matrix to\nmeasure diversity as the volume spanned by the embeddings of these responses.\nExperiments across instruction-following, summarization, story generation, and\nreasoning tasks demonstrate that our method substantially improves semantic\ndiversity without sacrificing model quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning and reinforcement learning are two popular methods for\npost-training large language models (LLMs). While improving the model's\nperformance on downstream tasks, they often reduce the model's output\ndiversity, leading to narrow, canonical responses. Existing methods to enhance\ndiversity are limited, either by operating at inference time or by focusing on\nlexical differences. We propose a novel training method named DQO based on\ndeterminantal point processes (DPPs) to jointly optimize LLMs for quality and\nsemantic diversity. Our approach samples and embeds a group of responses for\neach prompt, then uses the determinant of a kernel-based similarity matrix to\nmeasure diversity as the volume spanned by the embeddings of these responses.\nExperiments across instruction-following, summarization, story generation, and\nreasoning tasks demonstrate that our method substantially improves semantic\ndiversity without sacrificing model quality."
                },
                "authors": [
                    {
                        "name": "Yilei Chen"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Lorenz Wolf"
                    },
                    {
                        "name": "Ioannis Ch. Paschalidis"
                    },
                    {
                        "name": "Aldo Pacchiano"
                    }
                ],
                "author_detail": {
                    "name": "Aldo Pacchiano"
                },
                "author": "Aldo Pacchiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04781v1",
                "updated": "2025-09-05T03:30:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    3,
                    30,
                    4,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T03:30:04Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    3,
                    30,
                    4,
                    4,
                    248,
                    0
                ],
                "title": "The LLM Has Left The Chat: Evidence of Bail Preferences in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM Has Left The Chat: Evidence of Bail Preferences in Large\n  Language Models"
                },
                "summary": "When given the option, will LLMs choose to leave the conversation (bail)? We\ninvestigate this question by giving models the option to bail out of\ninteractions using three different bail methods: a bail tool the model can\ncall, a bail string the model can output, and a bail prompt that asks the model\nif it wants to leave. On continuations of real world data (Wildchat and\nShareGPT), all three of these bail methods find models will bail around\n0.28-32\\% of the time (depending on the model and bail method). However, we\nfind that bail rates can depend heavily on the model used for the transcript,\nwhich means we may be overestimating real world bail rates by up to 4x. If we\nalso take into account false positives on bail prompt (22\\%), we estimate real\nworld bail rates range from 0.06-7\\%, depending on the model and bail method.\nWe use observations from our continuations of real world data to construct a\nnon-exhaustive taxonomy of bail cases, and use this taxonomy to construct\nBailBench: a representative synthetic dataset of situations where some models\nbail. We test many models on this dataset, and observe some bail behavior\noccurring for most of them. Bail rates vary substantially between models, bail\nmethods, and prompt wordings. Finally, we study the relationship between\nrefusals and bails. We find: 1) 0-13\\% of continuations of real world\nconversations resulted in a bail without a corresponding refusal 2) Jailbreaks\ntend to decrease refusal rates, but increase bail rates 3) Refusal abliteration\nincreases no-refuse bail rates, but only for some bail methods 4) Refusal rate\non BailBench does not appear to predict bail rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When given the option, will LLMs choose to leave the conversation (bail)? We\ninvestigate this question by giving models the option to bail out of\ninteractions using three different bail methods: a bail tool the model can\ncall, a bail string the model can output, and a bail prompt that asks the model\nif it wants to leave. On continuations of real world data (Wildchat and\nShareGPT), all three of these bail methods find models will bail around\n0.28-32\\% of the time (depending on the model and bail method). However, we\nfind that bail rates can depend heavily on the model used for the transcript,\nwhich means we may be overestimating real world bail rates by up to 4x. If we\nalso take into account false positives on bail prompt (22\\%), we estimate real\nworld bail rates range from 0.06-7\\%, depending on the model and bail method.\nWe use observations from our continuations of real world data to construct a\nnon-exhaustive taxonomy of bail cases, and use this taxonomy to construct\nBailBench: a representative synthetic dataset of situations where some models\nbail. We test many models on this dataset, and observe some bail behavior\noccurring for most of them. Bail rates vary substantially between models, bail\nmethods, and prompt wordings. Finally, we study the relationship between\nrefusals and bails. We find: 1) 0-13\\% of continuations of real world\nconversations resulted in a bail without a corresponding refusal 2) Jailbreaks\ntend to decrease refusal rates, but increase bail rates 3) Refusal abliteration\nincreases no-refuse bail rates, but only for some bail methods 4) Refusal rate\non BailBench does not appear to predict bail rate."
                },
                "authors": [
                    {
                        "name": "Danielle Ensign"
                    },
                    {
                        "name": "Henry Sleight"
                    },
                    {
                        "name": "Kyle Fish"
                    }
                ],
                "author_detail": {
                    "name": "Kyle Fish"
                },
                "author": "Kyle Fish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04779v1",
                "updated": "2025-09-05T03:22:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    3,
                    22,
                    38,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T03:22:38Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    3,
                    22,
                    38,
                    4,
                    248,
                    0
                ],
                "title": "Decoders Laugh as Loud as Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoders Laugh as Loud as Encoders"
                },
                "summary": "From the dawn of the computer, Allen Turing dreamed of a robot that could\ncommunicate using language as a human being. The recent advances in the field\nof Large Language Models (LLMs) shocked the scientific community when a single\nmodel can apply for various natural language processing (NLP) tasks, while the\noutput results are sometimes even better than most human communication skills.\nModels such as GPT, Claude, Grok, etc. have left their mark on the scientific\ncommunity. However, it is unclear how much these models understand what they\nproduce, especially in a nuanced theme such as humor. The question of whether\ncomputers understand humor is still open (among the decoders, the latest to be\nchecked was GPT-2). We addressed this issue in this paper; we have showed that\na fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well\nas the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From the dawn of the computer, Allen Turing dreamed of a robot that could\ncommunicate using language as a human being. The recent advances in the field\nof Large Language Models (LLMs) shocked the scientific community when a single\nmodel can apply for various natural language processing (NLP) tasks, while the\noutput results are sometimes even better than most human communication skills.\nModels such as GPT, Claude, Grok, etc. have left their mark on the scientific\ncommunity. However, it is unclear how much these models understand what they\nproduce, especially in a nuanced theme such as humor. The question of whether\ncomputers understand humor is still open (among the decoders, the latest to be\nchecked was GPT-2). We addressed this issue in this paper; we have showed that\na fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well\nas the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)"
                },
                "authors": [
                    {
                        "name": "Eli Borodach"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16487v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16487v3",
                "updated": "2025-09-05T03:08:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    3,
                    8,
                    5,
                    4,
                    248,
                    0
                ],
                "published": "2025-02-23T08:00:33Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    8,
                    0,
                    33,
                    6,
                    54,
                    0
                ],
                "title": "All That Glitters is Not Novel: Plagiarism in AI Generated Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All That Glitters is Not Novel: Plagiarism in AI Generated Research"
                },
                "summary": "Automating scientific research is considered the final frontier of science.\nRecently, several papers claim autonomous research agents can generate novel\nresearch ideas. Amidst the prevailing optimism, we document a critical concern:\na considerable fraction of such research documents are smartly plagiarized.\nUnlike past efforts where experts evaluate the novelty and feasibility of\nresearch ideas, we request $13$ experts to operate under a different\nsituational logic: to identify similarities between LLM-generated research\ndocuments and existing work. Concerningly, the experts identify $24\\%$ of the\n$50$ evaluated research documents to be either paraphrased (with one-to-one\nmethodological mapping), or significantly borrowed from existing work. These\nreported instances are cross-verified by authors of the source papers. The\nremaining $76\\%$ of documents show varying degrees of similarity with existing\nwork, with only a small fraction appearing completely novel. Problematically,\nthese LLM-generated research documents do not acknowledge original sources, and\nbypass inbuilt plagiarism detectors. Lastly, through controlled experiments we\nshow that automated plagiarism detectors are inadequate at catching plagiarized\nideas from such systems. We recommend a careful assessment of LLM-generated\nresearch, and discuss the implications of our findings on academic publishing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating scientific research is considered the final frontier of science.\nRecently, several papers claim autonomous research agents can generate novel\nresearch ideas. Amidst the prevailing optimism, we document a critical concern:\na considerable fraction of such research documents are smartly plagiarized.\nUnlike past efforts where experts evaluate the novelty and feasibility of\nresearch ideas, we request $13$ experts to operate under a different\nsituational logic: to identify similarities between LLM-generated research\ndocuments and existing work. Concerningly, the experts identify $24\\%$ of the\n$50$ evaluated research documents to be either paraphrased (with one-to-one\nmethodological mapping), or significantly borrowed from existing work. These\nreported instances are cross-verified by authors of the source papers. The\nremaining $76\\%$ of documents show varying degrees of similarity with existing\nwork, with only a small fraction appearing completely novel. Problematically,\nthese LLM-generated research documents do not acknowledge original sources, and\nbypass inbuilt plagiarism detectors. Lastly, through controlled experiments we\nshow that automated plagiarism detectors are inadequate at catching plagiarized\nideas from such systems. We recommend a careful assessment of LLM-generated\nresearch, and discuss the implications of our findings on academic publishing."
                },
                "authors": [
                    {
                        "name": "Tarun Gupta"
                    },
                    {
                        "name": "Danish Pruthi"
                    }
                ],
                "author_detail": {
                    "name": "Danish Pruthi"
                },
                "author": "Danish Pruthi",
                "arxiv_comment": "Accepted to ACL 2025 (main) conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16487v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16487v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20474v2",
                "updated": "2025-09-05T03:02:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    3,
                    2,
                    37,
                    4,
                    248,
                    0
                ],
                "published": "2025-07-13T05:39:42Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    39,
                    42,
                    6,
                    194,
                    0
                ],
                "title": "MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and\n  Adaptive Financial Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and\n  Adaptive Financial Trading"
                },
                "summary": "Cryptocurrency trading is a challenging task requiring the integration of\nheterogeneous data from multiple modalities. Traditional deep learning and\nreinforcement learning approaches typically demand large training datasets and\nencode diverse inputs into numerical representations, often at the cost of\ninterpretability. Recent progress in large language model (LLM)-based agents\nhas demonstrated the capacity to process multi-modal data and support complex\ninvestment decision-making. Building on these advances, we present\n\\textbf{MountainLion}, a multi-modal, multi-agent system for financial trading\nthat coordinates specialized LLM-based agents to interpret financial data and\ngenerate investment strategies. MountainLion processes textual news,\ncandlestick charts, and trading signal charts to produce high-quality financial\nreports, while also enabling modification of reports and investment\nrecommendations through data-driven user interaction and question answering. A\ncentral reflection module analyzes historical trading signals and outcomes to\ncontinuously refine decision processes, and the system is capable of real-time\nreport analysis, summarization, and dynamic adjustment of investment\nstrategies. Empirical results confirm that MountainLion systematically enriches\ntechnical price triggers with contextual macroeconomic and capital flow\nsignals, providing a more interpretable, robust, and actionable investment\nframework that improves returns and strengthens investor confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptocurrency trading is a challenging task requiring the integration of\nheterogeneous data from multiple modalities. Traditional deep learning and\nreinforcement learning approaches typically demand large training datasets and\nencode diverse inputs into numerical representations, often at the cost of\ninterpretability. Recent progress in large language model (LLM)-based agents\nhas demonstrated the capacity to process multi-modal data and support complex\ninvestment decision-making. Building on these advances, we present\n\\textbf{MountainLion}, a multi-modal, multi-agent system for financial trading\nthat coordinates specialized LLM-based agents to interpret financial data and\ngenerate investment strategies. MountainLion processes textual news,\ncandlestick charts, and trading signal charts to produce high-quality financial\nreports, while also enabling modification of reports and investment\nrecommendations through data-driven user interaction and question answering. A\ncentral reflection module analyzes historical trading signals and outcomes to\ncontinuously refine decision processes, and the system is capable of real-time\nreport analysis, summarization, and dynamic adjustment of investment\nstrategies. Empirical results confirm that MountainLion systematically enriches\ntechnical price triggers with contextual macroeconomic and capital flow\nsignals, providing a more interpretable, robust, and actionable investment\nframework that improves returns and strengthens investor confidence."
                },
                "authors": [
                    {
                        "name": "Siyi Wu"
                    },
                    {
                        "name": "Junqiao Wang"
                    },
                    {
                        "name": "Zhaoyang Guan"
                    },
                    {
                        "name": "Leyi Zhao"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Xinyu Ying"
                    },
                    {
                        "name": "Dexu Yu"
                    },
                    {
                        "name": "Jinhao Wang"
                    },
                    {
                        "name": "Hanlin Zhang"
                    },
                    {
                        "name": "Michele Pak"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15862v2",
                "updated": "2025-09-05T03:00:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    3,
                    0,
                    48,
                    4,
                    248,
                    0
                ],
                "published": "2025-07-12T16:58:03Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    16,
                    58,
                    3,
                    5,
                    193,
                    0
                ],
                "title": "Quantifying Holistic Review: A Multi-Modal Approach to College\n  Admissions Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Holistic Review: A Multi-Modal Approach to College\n  Admissions Prediction"
                },
                "summary": "This paper introduces the Comprehensive Applicant Profile Score (CAPS), a\nnovel multi-modal framework designed to quantitatively model and interpret\nholistic college admissions evaluations. CAPS decomposes applicant profiles\ninto three interpretable components: academic performance (Standardized\nAcademic Score, SAS), essay quality (Essay Quality Index, EQI), and\nextracurricular engagement (Extracurricular Impact Score, EIS). Leveraging\ntransformer-based semantic embeddings, LLM scoring, and XGBoost regression,\nCAPS provides transparent and explainable evaluations aligned with human\njudgment. Experiments on a synthetic but realistic dataset demonstrate strong\nperformance, achieving an EQI prediction R^2 of 0.80, classification accuracy\nover 75%, a macro F1 score of 0.69, and a weighted F1 score of 0.74. CAPS\naddresses key limitations in traditional holistic review -- particularly the\nopacity, inconsistency, and anxiety faced by applicants -- thus paving the way\nfor more equitable and data-informed admissions practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Comprehensive Applicant Profile Score (CAPS), a\nnovel multi-modal framework designed to quantitatively model and interpret\nholistic college admissions evaluations. CAPS decomposes applicant profiles\ninto three interpretable components: academic performance (Standardized\nAcademic Score, SAS), essay quality (Essay Quality Index, EQI), and\nextracurricular engagement (Extracurricular Impact Score, EIS). Leveraging\ntransformer-based semantic embeddings, LLM scoring, and XGBoost regression,\nCAPS provides transparent and explainable evaluations aligned with human\njudgment. Experiments on a synthetic but realistic dataset demonstrate strong\nperformance, achieving an EQI prediction R^2 of 0.80, classification accuracy\nover 75%, a macro F1 score of 0.69, and a weighted F1 score of 0.74. CAPS\naddresses key limitations in traditional holistic review -- particularly the\nopacity, inconsistency, and anxiety faced by applicants -- thus paving the way\nfor more equitable and data-informed admissions practices."
                },
                "authors": [
                    {
                        "name": "Jun-Wei Zeng"
                    },
                    {
                        "name": "Jerry Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jerry Shen"
                },
                "author": "Jerry Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04770v1",
                "updated": "2025-09-05T02:58:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    58,
                    45,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T02:58:45Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    58,
                    45,
                    4,
                    248,
                    0
                ],
                "title": "Research on Multi-hop Inference Optimization of LLM Based on MQUAKE\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on Multi-hop Inference Optimization of LLM Based on MQUAKE\n  Framework"
                },
                "summary": "Accurately answering complex questions has consistently been a significant\nchallenge for Large Language Models (LLMs). To address this, this paper\nproposes a multi-hop question decomposition method for complex questions,\nbuilding upon research within the MQUAKE framework. Utilizing the LLAMA3 model,\nwe systematically investigate the impact of multi-hop question decomposition\nwithin knowledge graphs on model comprehension and reasoning accuracy, both\nbefore and after model training. In our experiments, we systematically\npartitioned and converted the MQUAKE-T dataset into two distinct formats: a\nsingle-hop dataset designed for directly answering complex questions, and a\nmulti-hop dataset constructed using the multi-hop question decomposition\nmethod. We then fine-tuned the LLAMA3 model on these datasets and conducted\ninference tests. Our results demonstrate that, without fine-tuning the LLM, the\nprediction performance based on the multi-hop question decomposition method\nsignificantly outperforms the method of directly answering complex questions.\nAfter fine-tuning using the LoRA (Low-Rank Adaptation) method, the performance\nof both approaches improved compared to the untrained baseline. Crucially, the\nmethod utilizing multi-hop decomposition consistently maintained its\nsuperiority. These findings validate the effectiveness of the multi-hop\ndecomposition method both before and after training, demonstrating its\ncapability to effectively enhance the LLM's ability to answer complex\nquestions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately answering complex questions has consistently been a significant\nchallenge for Large Language Models (LLMs). To address this, this paper\nproposes a multi-hop question decomposition method for complex questions,\nbuilding upon research within the MQUAKE framework. Utilizing the LLAMA3 model,\nwe systematically investigate the impact of multi-hop question decomposition\nwithin knowledge graphs on model comprehension and reasoning accuracy, both\nbefore and after model training. In our experiments, we systematically\npartitioned and converted the MQUAKE-T dataset into two distinct formats: a\nsingle-hop dataset designed for directly answering complex questions, and a\nmulti-hop dataset constructed using the multi-hop question decomposition\nmethod. We then fine-tuned the LLAMA3 model on these datasets and conducted\ninference tests. Our results demonstrate that, without fine-tuning the LLM, the\nprediction performance based on the multi-hop question decomposition method\nsignificantly outperforms the method of directly answering complex questions.\nAfter fine-tuning using the LoRA (Low-Rank Adaptation) method, the performance\nof both approaches improved compared to the untrained baseline. Crucially, the\nmethod utilizing multi-hop decomposition consistently maintained its\nsuperiority. These findings validate the effectiveness of the multi-hop\ndecomposition method both before and after training, demonstrating its\ncapability to effectively enhance the LLM's ability to answer complex\nquestions."
                },
                "authors": [
                    {
                        "name": "Zucheng Liang"
                    },
                    {
                        "name": "Wenxin Wei"
                    },
                    {
                        "name": "Kaijie Zhang"
                    },
                    {
                        "name": "Hongyi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongyi Chen"
                },
                "author": "Hongyi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04768v1",
                "updated": "2025-09-05T02:55:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    55,
                    14,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T02:55:14Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    55,
                    14,
                    4,
                    248,
                    0
                ],
                "title": "Environment-Aware IRS Deployment via Channel Knowledge Map: Joint\n  Sensing-Communications Coverage Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environment-Aware IRS Deployment via Channel Knowledge Map: Joint\n  Sensing-Communications Coverage Optimization"
                },
                "summary": "This paper studies the intelligent reflecting surface (IRS) deployment\noptimization problem for IRS-enabled integrated sensing and communications\n(ISAC) systems, in which multiple IRSs are strategically deployed at candidate\nlocations to assist a base station (BS) to enhance the coverage of both sensing\nand communications. We present an environment-aware IRS deployment design via\nexploiting the channel knowledge map (CKM), which provides the channel state\ninformation (CSI) between each candidate IRS location and BS or targeted\nsensing/communication points. Based on the obtained CSI from CKM, we optimize\nthe deployment of IRSs, jointly with the BS's transmit beamforming and IRSs'\nreflective beamforming during operation, with the objective of minimizing the\nsystem cost, while guaranteeing the minimum illumination power requirements at\nsensing areas and the minimum signal-to-noise ratio (SNR) requirements at\ncommunication areas. In particular, we consider two cases when the IRSs'\nreflective beamforming optimization can be implemented dynamically in real time\nand quasi-stationarily over the whole operation period, respectively. For both\ncases, the joint IRS deployment and transmit/reflective beamforming designs are\nformulated as mixed-integer non-convex optimization problems, which are solved\nvia the successive convex approximation (SCA)-based relax-and-bound method.\nSpecifically, we first relax the binary IRS deployment indicators into\ncontinuous variables, then find converged solutions via SCA, and finally round\nrelaxed indicators back to binary values. Numerical results demonstrate the\neffectiveness of our proposed algorithms in reducing the system cost while\nmeeting the sensing and communication requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the intelligent reflecting surface (IRS) deployment\noptimization problem for IRS-enabled integrated sensing and communications\n(ISAC) systems, in which multiple IRSs are strategically deployed at candidate\nlocations to assist a base station (BS) to enhance the coverage of both sensing\nand communications. We present an environment-aware IRS deployment design via\nexploiting the channel knowledge map (CKM), which provides the channel state\ninformation (CSI) between each candidate IRS location and BS or targeted\nsensing/communication points. Based on the obtained CSI from CKM, we optimize\nthe deployment of IRSs, jointly with the BS's transmit beamforming and IRSs'\nreflective beamforming during operation, with the objective of minimizing the\nsystem cost, while guaranteeing the minimum illumination power requirements at\nsensing areas and the minimum signal-to-noise ratio (SNR) requirements at\ncommunication areas. In particular, we consider two cases when the IRSs'\nreflective beamforming optimization can be implemented dynamically in real time\nand quasi-stationarily over the whole operation period, respectively. For both\ncases, the joint IRS deployment and transmit/reflective beamforming designs are\nformulated as mixed-integer non-convex optimization problems, which are solved\nvia the successive convex approximation (SCA)-based relax-and-bound method.\nSpecifically, we first relax the binary IRS deployment indicators into\ncontinuous variables, then find converged solutions via SCA, and finally round\nrelaxed indicators back to binary values. Numerical results demonstrate the\neffectiveness of our proposed algorithms in reducing the system cost while\nmeeting the sensing and communication requirements."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Zixiang Ren"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "13 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04760v1",
                "updated": "2025-09-05T02:40:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    40,
                    46,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T02:40:46Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    40,
                    46,
                    4,
                    248,
                    0
                ],
                "title": "A scalable method for cavity-enhanced solid-state quantum sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A scalable method for cavity-enhanced solid-state quantum sensors"
                },
                "summary": "Photoluminescent color centers in diamond and hexagonal boron nitride (hBN)\nare powerful nanoscale solid-state quantum sensors that are explored in a\nplethora of quantum technologies. Methods for integrating them into macroscopic\nstructures that improve their sensitivity and enable their large-scale\ndeployment are highly sought after. Here, we demonstrate cavity-enhanced\nphotoluminescence (PL) of fluorescent nanodiamonds (FNDs) and hBN nanoparticles\n(NPs) embedded in polymer-based thin-film optical cavities on the centimeter\nscale. The cavity resonances efficiently modulate the spectral PL peak position\nof nitrogen-vacancy (NV) centers in FNDs across the NV PL spectrum and lead to\nan up to 2.9-fold Purcell-enhancement of the NV PL decay rate. The brightness\nof hBN NPs increases by up to a factor of three and the PL decay rate is\nenhanced by up to 13-fold inside the cavities. Finally, we find a 4.8 times\nimproved magnetic field sensitivity of 20 nm FNDs in thin-film cavities due to\ncavity-enhanced optically detected magnetic resonance contrast and PL\nbrightness. Our study demonstrates a low-cost and scalable method for the\nfabrication of quantum sensor-doped thin-film cavities, which is an important\nstep toward the development of advanced quantum sensing technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoluminescent color centers in diamond and hexagonal boron nitride (hBN)\nare powerful nanoscale solid-state quantum sensors that are explored in a\nplethora of quantum technologies. Methods for integrating them into macroscopic\nstructures that improve their sensitivity and enable their large-scale\ndeployment are highly sought after. Here, we demonstrate cavity-enhanced\nphotoluminescence (PL) of fluorescent nanodiamonds (FNDs) and hBN nanoparticles\n(NPs) embedded in polymer-based thin-film optical cavities on the centimeter\nscale. The cavity resonances efficiently modulate the spectral PL peak position\nof nitrogen-vacancy (NV) centers in FNDs across the NV PL spectrum and lead to\nan up to 2.9-fold Purcell-enhancement of the NV PL decay rate. The brightness\nof hBN NPs increases by up to a factor of three and the PL decay rate is\nenhanced by up to 13-fold inside the cavities. Finally, we find a 4.8 times\nimproved magnetic field sensitivity of 20 nm FNDs in thin-film cavities due to\ncavity-enhanced optically detected magnetic resonance contrast and PL\nbrightness. Our study demonstrates a low-cost and scalable method for the\nfabrication of quantum sensor-doped thin-film cavities, which is an important\nstep toward the development of advanced quantum sensing technologies."
                },
                "authors": [
                    {
                        "name": "Daniel J. Tibben"
                    },
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "David A. Broadway"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Daniel E. Gmez"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04376v2",
                "updated": "2025-09-05T02:40:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    40,
                    36,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-04T16:34:46Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    34,
                    46,
                    3,
                    247,
                    0
                ],
                "title": "AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval\n  for Text-Based Person Anomaly Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval\n  for Text-Based Person Anomaly Search"
                },
                "summary": "With growing public safety demands, text-based person anomaly search has\nemerged as a critical task, aiming to retrieve individuals with abnormal\nbehaviors via natural language descriptions. Unlike conventional person search,\nthis task presents two unique challenges: (1) fine-grained cross-modal\nalignment between textual anomalies and visual behaviors, and (2) anomaly\nrecognition under sparse real-world samples. While Large Multi-modal Models\n(LMMs) excel in multi-modal understanding, their potential for fine-grained\nanomaly retrieval remains underexplored, hindered by: (1) a domain gap between\ngenerative knowledge and discriminative retrieval, and (2) the absence of\nefficient adaptation strategies for deployment. In this work, we propose\nAnomalyLMM, the first framework that harnesses LMMs for text-based person\nanomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline\nintegrating LMMs to bridge generative world knowledge with retrieval-centric\nanomaly detection; (2) A training-free adaptation cookbook featuring masked\ncross-modal prompting, behavioral saliency prediction, and knowledge-aware\nre-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study\nto explore LMMs for this task, we conduct a rigorous evaluation on the PAB\ndataset, the only publicly available benchmark for text-based person anomaly\nsearch, with its curated real-world anomalies covering diverse scenarios (e.g.,\nfalling, collision, and being hit). Experiments show the effectiveness of the\nproposed method, surpassing the competitive baseline by +0.96% Recall@1\naccuracy. Notably, our method reveals interpretable alignment between textual\nanomalies and visual behaviors, validated via qualitative analysis. Our code\nand models will be released for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With growing public safety demands, text-based person anomaly search has\nemerged as a critical task, aiming to retrieve individuals with abnormal\nbehaviors via natural language descriptions. Unlike conventional person search,\nthis task presents two unique challenges: (1) fine-grained cross-modal\nalignment between textual anomalies and visual behaviors, and (2) anomaly\nrecognition under sparse real-world samples. While Large Multi-modal Models\n(LMMs) excel in multi-modal understanding, their potential for fine-grained\nanomaly retrieval remains underexplored, hindered by: (1) a domain gap between\ngenerative knowledge and discriminative retrieval, and (2) the absence of\nefficient adaptation strategies for deployment. In this work, we propose\nAnomalyLMM, the first framework that harnesses LMMs for text-based person\nanomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline\nintegrating LMMs to bridge generative world knowledge with retrieval-centric\nanomaly detection; (2) A training-free adaptation cookbook featuring masked\ncross-modal prompting, behavioral saliency prediction, and knowledge-aware\nre-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study\nto explore LMMs for this task, we conduct a rigorous evaluation on the PAB\ndataset, the only publicly available benchmark for text-based person anomaly\nsearch, with its curated real-world anomalies covering diverse scenarios (e.g.,\nfalling, collision, and being hit). Experiments show the effectiveness of the\nproposed method, surpassing the competitive baseline by +0.96% Recall@1\naccuracy. Notably, our method reveals interpretable alignment between textual\nanomalies and visual behaviors, validated via qualitative analysis. Our code\nand models will be released for future research."
                },
                "authors": [
                    {
                        "name": "Hao Ju"
                    },
                    {
                        "name": "Hu Zhang"
                    },
                    {
                        "name": "Zhedong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhedong Zheng"
                },
                "author": "Zhedong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04753v1",
                "updated": "2025-09-05T02:07:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    7,
                    40,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T02:07:40Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    7,
                    40,
                    4,
                    248,
                    0
                ],
                "title": "A Study of Large Language Models for Patient Information Extraction:\n  Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of Large Language Models for Patient Information Extraction:\n  Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning"
                },
                "summary": "Natural language processing (NLP) is a key technology to extract important\npatient information from clinical narratives to support healthcare\napplications. The rapid development of large language models (LLMs) has\nrevolutionized many NLP tasks in the clinical domain, yet their optimal use in\npatient information extraction tasks requires further exploration. This study\nexamines LLMs' effectiveness in patient information extraction, focusing on LLM\narchitectures, fine-tuning strategies, and multi-task instruction tuning\ntechniques for developing robust and generalizable patient information\nextraction systems. This study aims to explore key concepts of using LLMs for\nclinical concept and relation extraction tasks, including: (1) encoder-only or\ndecoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT)\nalgorithms, and (3) multi-task instruction tuning on few-shot learning\nperformance. We benchmarked a suite of LLMs, including encoder-based LLMs\n(BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1,\nGatorTronLlama), across five datasets. We compared traditional full-size\nfine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning\nframework that combines both tasks across four datasets to evaluate the\nzero-shot and few-shot learning performance using the leave-one-dataset-out\nstrategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing (NLP) is a key technology to extract important\npatient information from clinical narratives to support healthcare\napplications. The rapid development of large language models (LLMs) has\nrevolutionized many NLP tasks in the clinical domain, yet their optimal use in\npatient information extraction tasks requires further exploration. This study\nexamines LLMs' effectiveness in patient information extraction, focusing on LLM\narchitectures, fine-tuning strategies, and multi-task instruction tuning\ntechniques for developing robust and generalizable patient information\nextraction systems. This study aims to explore key concepts of using LLMs for\nclinical concept and relation extraction tasks, including: (1) encoder-only or\ndecoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT)\nalgorithms, and (3) multi-task instruction tuning on few-shot learning\nperformance. We benchmarked a suite of LLMs, including encoder-based LLMs\n(BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1,\nGatorTronLlama), across five datasets. We compared traditional full-size\nfine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning\nframework that combines both tasks across four datasets to evaluate the\nzero-shot and few-shot learning performance using the leave-one-dataset-out\nstrategy."
                },
                "authors": [
                    {
                        "name": "Cheng Peng"
                    },
                    {
                        "name": "Xinyu Dong"
                    },
                    {
                        "name": "Mengxian Lyu"
                    },
                    {
                        "name": "Daniel Paredes"
                    },
                    {
                        "name": "Yaoyun Zhang"
                    },
                    {
                        "name": "Yonghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yonghui Wu"
                },
                "author": "Yonghui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04752v1",
                "updated": "2025-09-05T02:07:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    7,
                    36,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T02:07:36Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    2,
                    7,
                    36,
                    4,
                    248,
                    0
                ],
                "title": "SePA: A Search-enhanced Predictive Agent for Personalized Health\n  Coaching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SePA: A Search-enhanced Predictive Agent for Personalized Health\n  Coaching"
                },
                "summary": "This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM\nhealth coaching system that integrates personalized machine learning and\nretrieval-augmented generation to deliver adaptive, evidence-based guidance.\nSePA combines: (1) Individualized models predicting daily stress, soreness, and\ninjury risk from wearable sensor data (28 users, 1260 data points); and (2) A\nretrieval module that grounds LLM-generated feedback in expert-vetted web\ncontent to ensure contextual relevance and reliability. Our predictive models,\nevaluated with rolling-origin cross-validation and group k-fold\ncross-validation show that personalized models outperform generalized\nbaselines. In a pilot expert study (n=4), SePA's retrieval-based advice was\npreferred over a non-retrieval baseline, yielding meaningful practical effect\n(Cliff's $\\delta$=0.3, p=0.05). We also quantify latency performance trade-offs\nbetween response quality and speed, offering a transparent blueprint for\nnext-generation, trustworthy personal health informatics systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM\nhealth coaching system that integrates personalized machine learning and\nretrieval-augmented generation to deliver adaptive, evidence-based guidance.\nSePA combines: (1) Individualized models predicting daily stress, soreness, and\ninjury risk from wearable sensor data (28 users, 1260 data points); and (2) A\nretrieval module that grounds LLM-generated feedback in expert-vetted web\ncontent to ensure contextual relevance and reliability. Our predictive models,\nevaluated with rolling-origin cross-validation and group k-fold\ncross-validation show that personalized models outperform generalized\nbaselines. In a pilot expert study (n=4), SePA's retrieval-based advice was\npreferred over a non-retrieval baseline, yielding meaningful practical effect\n(Cliff's $\\delta$=0.3, p=0.05). We also quantify latency performance trade-offs\nbetween response quality and speed, offering a transparent blueprint for\nnext-generation, trustworthy personal health informatics systems."
                },
                "authors": [
                    {
                        "name": "Melik Ozolcer"
                    },
                    {
                        "name": "Sang Won Bae"
                    }
                ],
                "author_detail": {
                    "name": "Sang Won Bae"
                },
                "author": "Sang Won Bae",
                "arxiv_comment": "Accepted at IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI'25). 7 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03730v2",
                "updated": "2025-09-05T01:39:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    1,
                    39,
                    1,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-03T21:27:10Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    21,
                    27,
                    10,
                    2,
                    246,
                    0
                ],
                "title": "The Personality Illusion: Revealing Dissociation Between Self-Reports &\n  Behavior in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Personality Illusion: Revealing Dissociation Between Self-Reports &\n  Behavior in LLMs"
                },
                "summary": "Personality traits have long been studied as predictors of human behavior.\nRecent advances in Large Language Models (LLMs) suggest similar patterns may\nemerge in artificial systems, with advanced LLMs displaying consistent\nbehavioral tendencies resembling human traits like agreeableness and\nself-regulation. Understanding these patterns is crucial, yet prior work\nprimarily relied on simplified self-reports and heuristic prompting, with\nlittle behavioral validation. In this study, we systematically characterize LLM\npersonality across three dimensions: (1) the dynamic emergence and evolution of\ntrait profiles throughout training stages; (2) the predictive validity of\nself-reported traits in behavioral tasks; and (3) the impact of targeted\ninterventions, such as persona injection, on both self-reports and behavior.\nOur findings reveal that instructional alignment (e.g., RLHF, instruction\ntuning) significantly stabilizes trait expression and strengthens trait\ncorrelations in ways that mirror human data. However, these self-reported\ntraits do not reliably predict behavior, and observed associations often\ndiverge from human patterns. While persona injection successfully steers\nself-reports in the intended direction, it exerts little or inconsistent effect\non actual behavior. By distinguishing surface-level trait expression from\nbehavioral consistency, our findings challenge assumptions about LLM\npersonality and underscore the need for deeper evaluation in alignment and\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality traits have long been studied as predictors of human behavior.\nRecent advances in Large Language Models (LLMs) suggest similar patterns may\nemerge in artificial systems, with advanced LLMs displaying consistent\nbehavioral tendencies resembling human traits like agreeableness and\nself-regulation. Understanding these patterns is crucial, yet prior work\nprimarily relied on simplified self-reports and heuristic prompting, with\nlittle behavioral validation. In this study, we systematically characterize LLM\npersonality across three dimensions: (1) the dynamic emergence and evolution of\ntrait profiles throughout training stages; (2) the predictive validity of\nself-reported traits in behavioral tasks; and (3) the impact of targeted\ninterventions, such as persona injection, on both self-reports and behavior.\nOur findings reveal that instructional alignment (e.g., RLHF, instruction\ntuning) significantly stabilizes trait expression and strengthens trait\ncorrelations in ways that mirror human data. However, these self-reported\ntraits do not reliably predict behavior, and observed associations often\ndiverge from human patterns. While persona injection successfully steers\nself-reports in the intended direction, it exerts little or inconsistent effect\non actual behavior. By distinguishing surface-level trait expression from\nbehavioral consistency, our findings challenge assumptions about LLM\npersonality and underscore the need for deeper evaluation in alignment and\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Pengrui Han"
                    },
                    {
                        "name": "Rafal Kocielnik"
                    },
                    {
                        "name": "Peiyang Song"
                    },
                    {
                        "name": "Ramit Debnath"
                    },
                    {
                        "name": "Dean Mobbs"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "R. Michael Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "R. Michael Alvarez"
                },
                "author": "R. Michael Alvarez",
                "arxiv_comment": "We make public all code and source data at\n  https://github.com/psychology-of-AI/Personality-Illusion for full\n  reproducibility",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09061v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09061v4",
                "updated": "2025-09-05T01:04:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    1,
                    4,
                    0,
                    4,
                    248,
                    0
                ],
                "published": "2025-02-13T08:23:42Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    8,
                    23,
                    42,
                    3,
                    44,
                    0
                ],
                "title": "CRANE: Reasoning with constrained LLM generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRANE: Reasoning with constrained LLM generation"
                },
                "summary": "Code generation, symbolic math reasoning, and other tasks require LLMs to\nproduce outputs that are both syntactically and semantically correct.\nConstrained LLM generation is a promising direction to enforce adherence to\nformal grammar, but prior works have empirically observed that strict\nenforcement of formal constraints often diminishes the reasoning capabilities\nof LLMs. In this work, we first provide a theoretical explanation for why\nconstraining LLM outputs to very restrictive grammars that only allow\nsyntactically valid final answers reduces the reasoning capabilities of the\nmodel. Second, we demonstrate that by augmenting the output grammar with\ncarefully designed additional rules, it is always possible to preserve the\nreasoning capabilities of the LLM while ensuring syntactic and semantic\ncorrectness in its outputs. Building on these theoretical insights, we propose\na reasoning-augmented constrained decoding algorithm, CRANE, which effectively\nbalances the correctness of constrained generation with the flexibility of\nunconstrained generation. Experiments on multiple open-source LLMs and\nbenchmarks show that CRANE significantly outperforms both state-of-the-art\nconstrained decoding strategies and standard unconstrained decoding, showing up\nto 10% points accuracy improvement over baselines on challenging symbolic\nreasoning benchmarks GSM-symbolic and FOLIO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation, symbolic math reasoning, and other tasks require LLMs to\nproduce outputs that are both syntactically and semantically correct.\nConstrained LLM generation is a promising direction to enforce adherence to\nformal grammar, but prior works have empirically observed that strict\nenforcement of formal constraints often diminishes the reasoning capabilities\nof LLMs. In this work, we first provide a theoretical explanation for why\nconstraining LLM outputs to very restrictive grammars that only allow\nsyntactically valid final answers reduces the reasoning capabilities of the\nmodel. Second, we demonstrate that by augmenting the output grammar with\ncarefully designed additional rules, it is always possible to preserve the\nreasoning capabilities of the LLM while ensuring syntactic and semantic\ncorrectness in its outputs. Building on these theoretical insights, we propose\na reasoning-augmented constrained decoding algorithm, CRANE, which effectively\nbalances the correctness of constrained generation with the flexibility of\nunconstrained generation. Experiments on multiple open-source LLMs and\nbenchmarks show that CRANE significantly outperforms both state-of-the-art\nconstrained decoding strategies and standard unconstrained decoding, showing up\nto 10% points accuracy improvement over baselines on challenging symbolic\nreasoning benchmarks GSM-symbolic and FOLIO."
                },
                "authors": [
                    {
                        "name": "Debangshu Banerjee"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Sasa Misailovic"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "arxiv_comment": "Accepted at ICML 2025, Code at:\n  https://github.com/uiuc-focal-lab/CRANE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09061v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09061v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05946v3",
                "updated": "2025-09-05T00:55:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    0,
                    55,
                    55,
                    4,
                    248,
                    0
                ],
                "published": "2025-04-08T11:59:00Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    59,
                    0,
                    1,
                    98,
                    0
                ],
                "title": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control"
                },
                "summary": "Model Predictive Control (MPC) is a powerful control strategy widely utilized\nin domains like energy management, building control, and autonomous systems.\nHowever, its effectiveness in real-world settings is challenged by the need to\nincorporate context-specific predictions and expert instructions, which\ntraditional MPC often neglects. We propose InstructMPC, a novel framework that\naddresses this gap by integrating real-time human instructions through a Large\nLanguage Model (LLM) to produce context-aware predictions for MPC. Our method\nemploys a Language-to-Distribution (L2D) module to translate contextual\ninformation into predictive disturbance trajectories, which are then\nincorporated into the MPC optimization. Unlike existing context-aware and\nlanguage-based MPC models, InstructMPC enables dynamic human-LLM interaction\nand fine-tunes the L2D module in a closed loop with theoretical performance\nguarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics\nwhen optimized via advanced fine-tuning methods such as Direct Preference\nOptimization (DPO) using a tailored loss function.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Predictive Control (MPC) is a powerful control strategy widely utilized\nin domains like energy management, building control, and autonomous systems.\nHowever, its effectiveness in real-world settings is challenged by the need to\nincorporate context-specific predictions and expert instructions, which\ntraditional MPC often neglects. We propose InstructMPC, a novel framework that\naddresses this gap by integrating real-time human instructions through a Large\nLanguage Model (LLM) to produce context-aware predictions for MPC. Our method\nemploys a Language-to-Distribution (L2D) module to translate contextual\ninformation into predictive disturbance trajectories, which are then\nincorporated into the MPC optimization. Unlike existing context-aware and\nlanguage-based MPC models, InstructMPC enables dynamic human-LLM interaction\nand fine-tunes the L2D module in a closed loop with theoretical performance\nguarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics\nwhen optimized via advanced fine-tuning methods such as Direct Preference\nOptimization (DPO) using a tailored loss function."
                },
                "authors": [
                    {
                        "name": "Ruixiang Wu"
                    },
                    {
                        "name": "Jiahao Ai"
                    },
                    {
                        "name": "Tongxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongxin Li"
                },
                "author": "Tongxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18785v2",
                "updated": "2025-09-05T00:45:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    0,
                    45,
                    8,
                    4,
                    248,
                    0
                ],
                "published": "2025-04-26T03:33:42Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    3,
                    33,
                    42,
                    5,
                    116,
                    0
                ],
                "title": "ALF: Advertiser Large Foundation Model for Multi-Modal Advertiser\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALF: Advertiser Large Foundation Model for Multi-Modal Advertiser\n  Understanding"
                },
                "summary": "We present ALF (Advertiser Large Foundation model), a multi-modal transformer\narchitecture for understanding advertiser behavior and intent across text,\nimage, video, and structured data modalities. Through contrastive learning and\nmulti-task optimization, ALF creates unified advertiser representations that\ncapture both content and behavioral patterns. Our model achieves\nstate-of-the-art performance on critical tasks including fraud detection,\npolicy violation identification, and advertiser similarity matching. In\nproduction deployment, ALF demonstrates significant real-world impact by\ndelivering simultaneous gains in both precision and recall, for instance\nboosting recall by over 40 percentage points on one critical policy and\nincreasing precision to 99.8% on another. The architecture's effectiveness\nstems from its novel combination of multi-modal transformations, inter-sample\nattention mechanism, spectrally normalized projections, and calibrated\nprobabilistic outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ALF (Advertiser Large Foundation model), a multi-modal transformer\narchitecture for understanding advertiser behavior and intent across text,\nimage, video, and structured data modalities. Through contrastive learning and\nmulti-task optimization, ALF creates unified advertiser representations that\ncapture both content and behavioral patterns. Our model achieves\nstate-of-the-art performance on critical tasks including fraud detection,\npolicy violation identification, and advertiser similarity matching. In\nproduction deployment, ALF demonstrates significant real-world impact by\ndelivering simultaneous gains in both precision and recall, for instance\nboosting recall by over 40 percentage points on one critical policy and\nincreasing precision to 99.8% on another. The architecture's effectiveness\nstems from its novel combination of multi-modal transformations, inter-sample\nattention mechanism, spectrally normalized projections, and calibrated\nprobabilistic outputs."
                },
                "authors": [
                    {
                        "name": "Santosh Rajagopalan"
                    },
                    {
                        "name": "Jonathan Vronsky"
                    },
                    {
                        "name": "Songbai Yan"
                    },
                    {
                        "name": "S. Alireza Golestaneh"
                    },
                    {
                        "name": "Shubhra Chandra"
                    },
                    {
                        "name": "Min Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhou"
                },
                "author": "Min Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]