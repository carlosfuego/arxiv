[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.17333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v1",
                "updated": "2025-03-21T17:33:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v1",
                "updated": "2025-03-21T10:48:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v3",
                "updated": "2025-03-20T21:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v1",
                "updated": "2025-03-20T19:10:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v1",
                "updated": "2025-03-20T17:37:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v1",
                "updated": "2025-03-20T16:23:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qinxiang Lin"
                    },
                    {
                        "name": "Jinwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v1",
                "updated": "2025-03-20T13:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Zongming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zongming Guo"
                },
                "author": "Zongming Guo",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15927v1",
                "updated": "2025-03-20T08:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers"
                },
                "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."
                },
                "authors": [
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tingwei Gao"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18921v2",
                "updated": "2025-03-20T05:23:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    5,
                    23,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-09T13:47:05Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    47,
                    5,
                    1,
                    191,
                    0
                ],
                "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey"
                },
                "summary": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14881v1",
                "updated": "2025-03-19T04:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T04:18:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers"
                },
                "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."
                },
                "authors": [
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yekun Ke"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Song"
                },
                "author": "Zhao Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14805v1",
                "updated": "2025-03-19T00:30:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T00:30:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C"
                },
                "summary": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C."
                },
                "authors": [
                    {
                        "name": "Hunter Ellis"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Imteaz Rahaman"
                    },
                    {
                        "name": "Apostoli Hillas"
                    },
                    {
                        "name": "Botong Li"
                    },
                    {
                        "name": "Michael A. Scarpulla"
                    },
                    {
                        "name": "Berardi Sensale Rodriguez"
                    },
                    {
                        "name": "Kai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fu"
                },
                "author": "Kai Fu",
                "arxiv_comment": "7 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14708v1",
                "updated": "2025-03-18T20:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T20:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16"
                },
                "summary": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama."
                },
                "authors": [
                    {
                        "name": "Viansa Schmulbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Ethan Gao"
                    },
                    {
                        "name": "Lucy Revina"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Ethan Wu"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    }
                ],
                "author_detail": {
                    "name": "Borivoje Nikolic"
                },
                "author": "Borivoje Nikolic",
                "arxiv_doi": "10.1109/HCS61935.2024.10665203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HCS61935.2024.10665203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14647v1",
                "updated": "2025-03-18T18:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T18:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache"
                },
                "summary": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v1",
                "updated": "2025-03-17T23:38:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13679v1",
                "updated": "2025-03-17T19:32:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T19:32:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning"
                },
                "summary": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis."
                },
                "authors": [
                    {
                        "name": "Risheng Xu"
                    },
                    {
                        "name": "Philipp Sieweck"
                    },
                    {
                        "name": "Hermann von Hasseln"
                    },
                    {
                        "name": "Dirk Nowotka"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Nowotka"
                },
                "author": "Dirk Nowotka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16525v1",
                "updated": "2025-03-17T16:43:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference"
                },
                "summary": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Renji Zhang"
                    },
                    {
                        "name": "Deyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhang"
                },
                "author": "Deyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v1",
                "updated": "2025-03-17T11:10:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12991v1",
                "updated": "2025-03-17T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge"
                },
                "summary": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Sam Albin"
                    },
                    {
                        "name": "Garhan Attebury"
                    },
                    {
                        "name": "Kenneth Bloom"
                    },
                    {
                        "name": "Brian Paul Bockelman"
                    },
                    {
                        "name": "Benjamin Tovar Lopez"
                    },
                    {
                        "name": "Carl Lundstedt"
                    },
                    {
                        "name": "Oksana Shadura"
                    },
                    {
                        "name": "John Thiltges"
                    },
                    {
                        "name": "Derek Weitzel"
                    },
                    {
                        "name": "Andrew Wightman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Wightman"
                },
                "arxiv_affiliation": "University of Nebraska-Lincoln",
                "author": "Andrew Wightman",
                "arxiv_comment": "Draft submitted to EPJ journal (CHEP 2024 conference proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v2",
                "updated": "2025-03-17T03:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    3,
                    30,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v4",
                "updated": "2025-03-16T16:25:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    16,
                    25,
                    31,
                    6,
                    75,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12491v1",
                "updated": "2025-03-16T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12450v1",
                "updated": "2025-03-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T10:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching"
                },
                "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."
                },
                "authors": [
                    {
                        "name": "Feihong Yan"
                    },
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v1",
                "updated": "2025-03-15T14:13:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v1",
                "updated": "2025-03-15T02:48:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11946v1",
                "updated": "2025-03-15T01:35:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T01:35:53Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks"
                },
                "summary": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zhishu Shen"
                    },
                    {
                        "name": "Dawen Jiang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qiushi Zheng"
                    },
                    {
                        "name": "Jiong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Jiong Jin"
                },
                "author": "Jiong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06348v2",
                "updated": "2025-03-15T00:49:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    0,
                    49,
                    55,
                    5,
                    74,
                    0
                ],
                "published": "2024-03-11T00:30:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    30,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation"
                },
                "summary": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs."
                },
                "authors": [
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Ahmed E. Helal"
                    },
                    {
                        "name": "S. Isaac Geronimo Anderson"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Yongseok Soh"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Teresa Ranadive"
                    },
                    {
                        "name": "Brian J Gravelle"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Jee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jee Choi"
                },
                "author": "Jee Choi",
                "arxiv_comment": "Accepted to TPDS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v1",
                "updated": "2025-03-14T19:02:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11750v1",
                "updated": "2025-03-14T17:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization"
                },
                "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."
                },
                "authors": [
                    {
                        "name": "Shuyang Hao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v1",
                "updated": "2025-03-14T06:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v1",
                "updated": "2025-03-14T06:01:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers"
                },
                "summary": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v1",
                "updated": "2025-03-13T03:36:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v3",
                "updated": "2025-03-12T18:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    14,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache"
                },
                "summary": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09218v1",
                "updated": "2025-03-12T10:05:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:05:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Víctor Gutiérrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v2",
                "updated": "2025-03-12T03:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    40,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08966v1",
                "updated": "2025-03-12T00:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T00:12:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Performance Models for a Two-tiered Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Models for a Two-tiered Storage System"
                },
                "summary": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models."
                },
                "authors": [
                    {
                        "name": "Aparna Sasidharan"
                    },
                    {
                        "name": "Xian-He"
                    },
                    {
                        "name": "Jay Lofstead"
                    },
                    {
                        "name": "Scott Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Scott Klasky"
                },
                "author": "Scott Klasky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08941v1",
                "updated": "2025-03-11T22:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T22:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors"
                },
                "summary": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors."
                },
                "authors": [
                    {
                        "name": "Afaak Lakouader"
                    },
                    {
                        "name": "Abdelilah Lahmar"
                    },
                    {
                        "name": "Spela Kunej"
                    },
                    {
                        "name": "Daoud Mezzane"
                    },
                    {
                        "name": "Jamal Belhadi"
                    },
                    {
                        "name": "El Hassan Choukri"
                    },
                    {
                        "name": "Lahoucine Hajji"
                    },
                    {
                        "name": "Mbarek Amjoud"
                    },
                    {
                        "name": "Zdravko Kutnjak"
                    },
                    {
                        "name": "Igor A. Lukyanchuk"
                    },
                    {
                        "name": "Mimoun El Marssi"
                    }
                ],
                "author_detail": {
                    "name": "Mimoun El Marssi"
                },
                "author": "Mimoun El Marssi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08879v1",
                "updated": "2025-03-11T20:45:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T20:45:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference"
                },
                "summary": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest."
                },
                "authors": [
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Darshan Gandhi"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Changran Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v3",
                "updated": "2025-03-11T13:13:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    },
                    {
                        "name": "Qihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qihua Zhou"
                },
                "author": "Qihua Zhou",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v5",
                "updated": "2025-03-11T09:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07545v1",
                "updated": "2025-03-10T17:12:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T17:12:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing, Predictions, and LLMs: Challenges and Open Problems"
                },
                "summary": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Rana Shahout"
                    }
                ],
                "author_detail": {
                    "name": "Rana Shahout"
                },
                "author": "Rana Shahout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07518v1",
                "updated": "2025-03-10T16:41:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:41:14Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "title": "TokenButler: Token Importance is Predictable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenButler: Token Importance is Predictable"
                },
                "summary": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler"
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Ahmed F AbouElhamayed"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07474v1",
                "updated": "2025-03-10T15:49:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v2",
                "updated": "2025-03-10T12:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    10,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v1",
                "updated": "2025-03-10T09:49:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07027v1",
                "updated": "2025-03-10T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer"
                },
                "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yirui Yuan"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v1",
                "updated": "2025-03-10T05:09:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v2",
                "updated": "2025-03-10T02:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    41,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v3",
                "updated": "2025-03-09T17:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    28,
                    6,
                    68,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v2",
                "updated": "2025-03-09T16:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    16,
                    14,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v1",
                "updated": "2025-03-09T12:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06545v1",
                "updated": "2025-03-09T10:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T10:31:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation"
                },
                "summary": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/JunyiWuCode/QuantCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06433v1",
                "updated": "2025-03-09T04:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T04:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seesaw: High-throughput LLM Inference via Model Re-sharding"
                },
                "summary": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine."
                },
                "authors": [
                    {
                        "name": "Qidong Su"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Muralidhar Andoorveedu"
                    },
                    {
                        "name": "Chenhao Jiang"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v3",
                "updated": "2025-03-09T02:19:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    19,
                    22,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v2",
                "updated": "2025-03-08T21:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    21,
                    55,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06302v1",
                "updated": "2025-03-08T18:30:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T18:30:54Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "title": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security"
                },
                "summary": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Dianwei Chen"
                    },
                    {
                        "name": "Xianfeng Yang"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v1",
                "updated": "2025-03-08T02:35:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v1",
                "updated": "2025-03-07T21:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.17363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17363v1",
                "updated": "2025-03-21T17:59:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    59,
                    55,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:59:55Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    59,
                    55,
                    4,
                    80,
                    0
                ],
                "title": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural\n  Language Self-Critique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural\n  Language Self-Critique"
                },
                "summary": "Enhancing the reasoning capabilities of large language models (LLMs),\nparticularly for complex tasks requiring multi-step logical deductions, remains\na significant challenge. Traditional inference time scaling methods utilize\nscalar reward signals from process reward models to evaluate candidate\nreasoning steps, but these scalar rewards lack the nuanced qualitative\ninformation essential for understanding and justifying each step. In this\npaper, we propose a novel inference-time scaling approach -- stepwise natural\nlanguage self-critique (PANEL), which employs self-generated natural language\ncritiques as feedback to guide the step-level search process. By generating\nrich, human-readable critiques for each candidate reasoning step, PANEL retains\nessential qualitative information, facilitating better-informed decision-making\nduring inference. This approach bypasses the need for task-specific verifiers\nand the associated training overhead, making it broadly applicable across\ndiverse tasks. Experimental results on challenging reasoning benchmarks,\nincluding AIME and GPQA, demonstrate that PANEL significantly enhances\nreasoning performance, outperforming traditional scalar reward-based methods.\nOur code is available at https://github.com/puddingyeah/PANEL to support and\nencourage future research in this promising field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the reasoning capabilities of large language models (LLMs),\nparticularly for complex tasks requiring multi-step logical deductions, remains\na significant challenge. Traditional inference time scaling methods utilize\nscalar reward signals from process reward models to evaluate candidate\nreasoning steps, but these scalar rewards lack the nuanced qualitative\ninformation essential for understanding and justifying each step. In this\npaper, we propose a novel inference-time scaling approach -- stepwise natural\nlanguage self-critique (PANEL), which employs self-generated natural language\ncritiques as feedback to guide the step-level search process. By generating\nrich, human-readable critiques for each candidate reasoning step, PANEL retains\nessential qualitative information, facilitating better-informed decision-making\nduring inference. This approach bypasses the need for task-specific verifiers\nand the associated training overhead, making it broadly applicable across\ndiverse tasks. Experimental results on challenging reasoning benchmarks,\nincluding AIME and GPQA, demonstrate that PANEL significantly enhances\nreasoning performance, outperforming traditional scalar reward-based methods.\nOur code is available at https://github.com/puddingyeah/PANEL to support and\nencourage future research in this promising field."
                },
                "authors": [
                    {
                        "name": "Yansi Li"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_doi": "10.13140/RG.2.2.27912.33289",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.27912.33289",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17361v1",
                "updated": "2025-03-21T17:59:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    59,
                    43,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:59:43Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    59,
                    43,
                    4,
                    80,
                    0
                ],
                "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation"
                },
                "summary": "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment."
                },
                "authors": [
                    {
                        "name": "Sophia Tang"
                    },
                    {
                        "name": "Yinuo Zhang"
                    },
                    {
                        "name": "Alexander Tong"
                    },
                    {
                        "name": "Pranam Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Pranam Chatterjee"
                },
                "author": "Pranam Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17352v1",
                "updated": "2025-03-21T17:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    52,
                    43,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    52,
                    43,
                    4,
                    80,
                    0
                ],
                "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement"
                },
                "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker."
                },
                "authors": [
                    {
                        "name": "Yihe Deng"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Fan Yin"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "arxiv_comment": "23 pages, 11 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14649v2",
                "updated": "2025-03-21T17:51:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    51,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-18T18:58:13Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    58,
                    13,
                    1,
                    77,
                    0
                ],
                "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented\n  Generation Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGO: Systematic Performance Optimization for Retrieval-Augmented\n  Generation Serving"
                },
                "summary": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions."
                },
                "authors": [
                    {
                        "name": "Wenqi Jiang"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Cat Graves"
                    },
                    {
                        "name": "Gustavo Alonso"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Vidushi Dadu"
                    }
                ],
                "author_detail": {
                    "name": "Vidushi Dadu"
                },
                "author": "Vidushi Dadu",
                "arxiv_comment": "16 pages, 19 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; C.4; H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17347v1",
                "updated": "2025-03-21T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    48,
                    14,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:48:14Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    48,
                    14,
                    4,
                    80,
                    0
                ],
                "title": "Dereflection Any Image with Diffusion Priors and Diversified Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dereflection Any Image with Diffusion Priors and Diversified Data"
                },
                "summary": "Reflection removal of a single image remains a highly challenging task due to\nthe complex entanglement between target scenes and unwanted reflections.\nDespite significant progress, existing methods are hindered by the scarcity of\nhigh-quality, diverse data and insufficient restoration priors, resulting in\nlimited generalization across various real-world scenarios. In this paper, we\npropose Dereflection Any Image, a comprehensive solution with an efficient data\npreparation pipeline and a generalizable model for robust reflection removal.\nFirst, we introduce a dataset named Diverse Reflection Removal (DRR) created by\nrandomly rotating reflective mediums in target scenes, enabling variation of\nreflection angles and intensities, and setting a new benchmark in scale,\nquality, and diversity. Second, we propose a diffusion-based framework with\none-step diffusion for deterministic outputs and fast inference. To ensure\nstable learning, we design a three-stage progressive training strategy,\nincluding reflection-invariant finetuning to encourage consistent outputs\nacross varying reflection patterns that characterize our dataset. Extensive\nexperiments show that our method achieves SOTA performance on both common\nbenchmarks and challenging in-the-wild images, showing superior generalization\nacross diverse real-world scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflection removal of a single image remains a highly challenging task due to\nthe complex entanglement between target scenes and unwanted reflections.\nDespite significant progress, existing methods are hindered by the scarcity of\nhigh-quality, diverse data and insufficient restoration priors, resulting in\nlimited generalization across various real-world scenarios. In this paper, we\npropose Dereflection Any Image, a comprehensive solution with an efficient data\npreparation pipeline and a generalizable model for robust reflection removal.\nFirst, we introduce a dataset named Diverse Reflection Removal (DRR) created by\nrandomly rotating reflective mediums in target scenes, enabling variation of\nreflection angles and intensities, and setting a new benchmark in scale,\nquality, and diversity. Second, we propose a diffusion-based framework with\none-step diffusion for deterministic outputs and fast inference. To ensure\nstable learning, we design a three-stage progressive training strategy,\nincluding reflection-invariant finetuning to encourage consistent outputs\nacross varying reflection patterns that characterize our dataset. Extensive\nexperiments show that our method achieves SOTA performance on both common\nbenchmarks and challenging in-the-wild images, showing superior generalization\nacross diverse real-world scenes."
                },
                "authors": [
                    {
                        "name": "Jichen Hu"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Zanwei Zhou"
                    },
                    {
                        "name": "Jiemin Fang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    },
                    {
                        "name": "Qi Tian"
                    },
                    {
                        "name": "Wei Shen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Shen"
                },
                "author": "Wei Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17336v1",
                "updated": "2025-03-21T17:34:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    34,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:34:37Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    34,
                    37,
                    4,
                    80,
                    0
                ],
                "title": "Efficient Intent-Based Filtering for Multi-Party Conversations Using\n  Knowledge Distillation from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Intent-Based Filtering for Multi-Party Conversations Using\n  Knowledge Distillation from LLMs"
                },
                "summary": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments."
                },
                "authors": [
                    {
                        "name": "Reem Gody"
                    },
                    {
                        "name": "Mohamed Abdelghaffar"
                    },
                    {
                        "name": "Mohammed Jabreel"
                    },
                    {
                        "name": "Ahmed Tawfik"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Tawfik"
                },
                "author": "Ahmed Tawfik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16041v2",
                "updated": "2025-03-21T17:33:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T11:19:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    19,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation"
                },
                "summary": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes"
                },
                "authors": [
                    {
                        "name": "Oluwole Fagbohun"
                    },
                    {
                        "name": "Sai Yashwanth"
                    },
                    {
                        "name": "Akinyemi Sadeeq Akintola"
                    },
                    {
                        "name": "Ifeoluwa Wurola"
                    },
                    {
                        "name": "Lanre Shittu"
                    },
                    {
                        "name": "Aniema Inyang"
                    },
                    {
                        "name": "Oluwatimilehin Odubola"
                    },
                    {
                        "name": "Udodirim Offia"
                    },
                    {
                        "name": "Said Olanrewaju"
                    },
                    {
                        "name": "Ogidan Toluwaleke"
                    },
                    {
                        "name": "Ilemona Abutu"
                    },
                    {
                        "name": "Taiwo Akinbolaji"
                    }
                ],
                "author_detail": {
                    "name": "Taiwo Akinbolaji"
                },
                "author": "Taiwo Akinbolaji",
                "arxiv_comment": "12 Pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17332v1",
                "updated": "2025-03-21T17:32:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    32,
                    32,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:32:32Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    32,
                    32,
                    4,
                    80,
                    0
                ],
                "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web\n  Application Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web\n  Application Vulnerabilities"
                },
                "summary": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Antony Kellermann"
                    },
                    {
                        "name": "Dylan Bowman"
                    },
                    {
                        "name": "Philip Li"
                    },
                    {
                        "name": "Akul Gupta"
                    },
                    {
                        "name": "Adarsh Danda"
                    },
                    {
                        "name": "Richard Fang"
                    },
                    {
                        "name": "Conner Jensen"
                    },
                    {
                        "name": "Eric Ihli"
                    },
                    {
                        "name": "Jason Benn"
                    },
                    {
                        "name": "Jet Geronimo"
                    },
                    {
                        "name": "Avi Dhir"
                    },
                    {
                        "name": "Sudhit Rao"
                    },
                    {
                        "name": "Kaicheng Yu"
                    },
                    {
                        "name": "Twm Stone"
                    },
                    {
                        "name": "Daniel Kang"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kang"
                },
                "author": "Daniel Kang",
                "arxiv_comment": "15 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17316v1",
                "updated": "2025-03-21T17:12:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    12,
                    30,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:12:30Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    12,
                    30,
                    4,
                    80,
                    0
                ],
                "title": "Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene\n  Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene\n  Priors"
                },
                "summary": "We present Pow3r, a novel large 3D vision regression model that is highly\nversatile in the input modalities it accepts. Unlike previous feed-forward\nmodels that lack any mechanism to exploit known camera or scene priors at test\ntime, Pow3r incorporates any combination of auxiliary information such as\nintrinsics, relative pose, dense or sparse depth, alongside input images,\nwithin a single network. Building upon the recent DUSt3R paradigm, a\ntransformer-based architecture that leverages powerful pre-training, our\nlightweight and versatile conditioning acts as additional guidance for the\nnetwork to predict more accurate estimates when auxiliary information is\navailable. During training we feed the model with random subsets of modalities\nat each iteration, which enables the model to operate under different levels of\nknown priors at test time. This in turn opens up new capabilities, such as\nperforming inference in native image resolution, or point-cloud completion. Our\nexperiments on 3D reconstruction, depth completion, multi-view depth\nprediction, multi-view stereo, and multi-view pose estimation tasks yield\nstate-of-the-art results and confirm the effectiveness of Pow3r at exploiting\nall available information. The project webpage is\nhttps://europe.naverlabs.com/pow3r.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Pow3r, a novel large 3D vision regression model that is highly\nversatile in the input modalities it accepts. Unlike previous feed-forward\nmodels that lack any mechanism to exploit known camera or scene priors at test\ntime, Pow3r incorporates any combination of auxiliary information such as\nintrinsics, relative pose, dense or sparse depth, alongside input images,\nwithin a single network. Building upon the recent DUSt3R paradigm, a\ntransformer-based architecture that leverages powerful pre-training, our\nlightweight and versatile conditioning acts as additional guidance for the\nnetwork to predict more accurate estimates when auxiliary information is\navailable. During training we feed the model with random subsets of modalities\nat each iteration, which enables the model to operate under different levels of\nknown priors at test time. This in turn opens up new capabilities, such as\nperforming inference in native image resolution, or point-cloud completion. Our\nexperiments on 3D reconstruction, depth completion, multi-view depth\nprediction, multi-view stereo, and multi-view pose estimation tasks yield\nstate-of-the-art results and confirm the effectiveness of Pow3r at exploiting\nall available information. The project webpage is\nhttps://europe.naverlabs.com/pow3r."
                },
                "authors": [
                    {
                        "name": "Wonbong Jang"
                    },
                    {
                        "name": "Philippe Weinzaepfel"
                    },
                    {
                        "name": "Vincent Leroy"
                    },
                    {
                        "name": "Lourdes Agapito"
                    },
                    {
                        "name": "Jerome Revaud"
                    }
                ],
                "author_detail": {
                    "name": "Jerome Revaud"
                },
                "author": "Jerome Revaud",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17314v1",
                "updated": "2025-03-21T17:10:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    10,
                    32,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:10:32Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    10,
                    32,
                    4,
                    80,
                    0
                ],
                "title": "Numerical Investigation of Preferential Flow Paths in Enzymatically\n  Induced Calcite Precipitation supported by Bayesian Model Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical Investigation of Preferential Flow Paths in Enzymatically\n  Induced Calcite Precipitation supported by Bayesian Model Analysis"
                },
                "summary": "The usability of enzymatically induced calcium carbonate precipitation (EICP)\nas a method for altering porous-media properties, soil stabilization, or\nbiocementation depends on our ability to predict the spatial distribution of\nthe precipitated calcium carbonate in porous media. While current REV-scale\nmodels are able to reproduce the main features of laboratory experiments, they\nneglect effects like the formation of preferential flow paths and the\nappearance of multiple polymorphs of calcium carbonate with differing\nproperties. We show that extending an existing EICP model by the conceptual\nassumption of a mobile precipitate, amorphous calcium carbonate (ACC), allows\nfor the formation of preferential flow paths when the initial porosity is\nheterogeneous. We apply sensitivity analysis and Bayesian inference to gain an\nunderstanding of the influence of characteristic parameters of ACC that are\nuncertain or unknown and compare two variations of the model based on different\nformulations of the ACC detachment term to analyse the plausibility of our\nhypothesis. An arbitrary Polynomial Chaos (aPC) surrogate model is trained\nbased on the full model and used to reduce the computational cost of this\nstudy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The usability of enzymatically induced calcium carbonate precipitation (EICP)\nas a method for altering porous-media properties, soil stabilization, or\nbiocementation depends on our ability to predict the spatial distribution of\nthe precipitated calcium carbonate in porous media. While current REV-scale\nmodels are able to reproduce the main features of laboratory experiments, they\nneglect effects like the formation of preferential flow paths and the\nappearance of multiple polymorphs of calcium carbonate with differing\nproperties. We show that extending an existing EICP model by the conceptual\nassumption of a mobile precipitate, amorphous calcium carbonate (ACC), allows\nfor the formation of preferential flow paths when the initial porosity is\nheterogeneous. We apply sensitivity analysis and Bayesian inference to gain an\nunderstanding of the influence of characteristic parameters of ACC that are\nuncertain or unknown and compare two variations of the model based on different\nformulations of the ACC detachment term to analyse the plausibility of our\nhypothesis. An arbitrary Polynomial Chaos (aPC) surrogate model is trained\nbased on the full model and used to reduce the computational cost of this\nstudy."
                },
                "authors": [
                    {
                        "name": "Rebecca Kohlhaas"
                    },
                    {
                        "name": "Johannes Hommel"
                    },
                    {
                        "name": "Felix Weinhardt"
                    },
                    {
                        "name": "Holger Class"
                    },
                    {
                        "name": "Sergey Oladyshkin"
                    },
                    {
                        "name": "Bernd Flemisch"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Flemisch"
                },
                "author": "Bernd Flemisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17309v1",
                "updated": "2025-03-21T17:04:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    4,
                    1,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:04:01Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    4,
                    1,
                    4,
                    80,
                    0
                ],
                "title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and\n  Planning Domain Definition Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and\n  Planning Domain Definition Language"
                },
                "summary": "Bimanual robotic manipulation provides significant versatility, but also\npresents an inherent challenge due to the complexity involved in the spatial\nand temporal coordination between two hands. Existing works predominantly focus\non attaining human-level manipulation skills for robotic hands, yet little\nattention has been paid to task planning on long-horizon timescales. With their\noutstanding in-context learning and zero-shot generation abilities, Large\nLanguage Models (LLMs) have been applied and grounded in diverse robotic\nembodiments to facilitate task planning. However, LLMs still suffer from errors\nin long-horizon reasoning and from hallucinations in complex robotic tasks,\nlacking a guarantee of logical correctness when generating the plan. Previous\nworks, such as LLM+P, extended LLMs with symbolic planners. However, none have\nbeen successfully applied to bimanual robots. New challenges inevitably arise\nin bimanual manipulation, necessitating not only effective task decomposition\nbut also efficient task allocation. To address these challenges, this paper\nintroduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning\nand multi-agent planning, automating effective and efficient bimanual task\nplanning. We conduct simulated experiments on various long-horizon manipulation\ntasks of differing complexity. Our method is built using GPT-4o as the backend,\nand we compare its performance against plans generated directly by LLMs,\nincluding GPT-4o, V3 and also recent strong reasoning models o1 and R1. By\nanalyzing metrics such as planning time, success rate, group debits, and\nplanning-step reduction rate, we demonstrate the superior performance of\nLLM+MAP, while also providing insights into robotic reasoning. Code is\navailable at https://github.com/Kchu/LLM-MAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bimanual robotic manipulation provides significant versatility, but also\npresents an inherent challenge due to the complexity involved in the spatial\nand temporal coordination between two hands. Existing works predominantly focus\non attaining human-level manipulation skills for robotic hands, yet little\nattention has been paid to task planning on long-horizon timescales. With their\noutstanding in-context learning and zero-shot generation abilities, Large\nLanguage Models (LLMs) have been applied and grounded in diverse robotic\nembodiments to facilitate task planning. However, LLMs still suffer from errors\nin long-horizon reasoning and from hallucinations in complex robotic tasks,\nlacking a guarantee of logical correctness when generating the plan. Previous\nworks, such as LLM+P, extended LLMs with symbolic planners. However, none have\nbeen successfully applied to bimanual robots. New challenges inevitably arise\nin bimanual manipulation, necessitating not only effective task decomposition\nbut also efficient task allocation. To address these challenges, this paper\nintroduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning\nand multi-agent planning, automating effective and efficient bimanual task\nplanning. We conduct simulated experiments on various long-horizon manipulation\ntasks of differing complexity. Our method is built using GPT-4o as the backend,\nand we compare its performance against plans generated directly by LLMs,\nincluding GPT-4o, V3 and also recent strong reasoning models o1 and R1. By\nanalyzing metrics such as planning time, success rate, group debits, and\nplanning-step reduction rate, we demonstrate the superior performance of\nLLM+MAP, while also providing insights into robotic reasoning. Code is\navailable at https://github.com/Kchu/LLM-MAP."
                },
                "authors": [
                    {
                        "name": "Kun Chu"
                    },
                    {
                        "name": "Xufeng Zhao"
                    },
                    {
                        "name": "Cornelius Weber"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Code and video are available at https://github.com/Kchu/LLM-MAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00937v2",
                "updated": "2025-03-21T16:53:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    53,
                    47,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-02T22:10:40Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    22,
                    10,
                    40,
                    6,
                    33,
                    0
                ],
                "title": "ModServe: Scalable and Resource-Efficient Large Multimodal Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ModServe: Scalable and Resource-Efficient Large Multimodal Model Serving"
                },
                "summary": "Large multimodal models (LMMs) demonstrate impressive capabilities in\nunderstanding images, videos, and audio beyond text. However, efficiently\nserving LMMs in production environments poses significant challenges due to\ntheir complex architectures and heterogeneous characteristics across their\nmulti-stage inference pipelines. We present the first comprehensive systems\nanalysis of two prominent LMM architectures, decoder-only and cross-attention,\nacross six representative open-source models, revealing key systems design\nimplications. We also present an in-depth analysis of production LMM inference\ntraces, uncovering unique workload characteristics, including variable,\nheavy-tailed request distributions and bursty traffic patterns. Based on these\ninsights, we propose ModServe, a modular LMM serving system that decouples\nstages for independent optimization and adaptive scaling. ModServe dynamically\nreconfigures stages and handles bursty traffic with modality-aware scheduling\nand autoscaling to meet tail latency SLOs while minimizing costs. ModServe\nachieves 3.3-5.5x higher throughput (leading to 25-41.3% cost saving) while\nmeeting SLOs on a 128-GPU cluster with production traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) demonstrate impressive capabilities in\nunderstanding images, videos, and audio beyond text. However, efficiently\nserving LMMs in production environments poses significant challenges due to\ntheir complex architectures and heterogeneous characteristics across their\nmulti-stage inference pipelines. We present the first comprehensive systems\nanalysis of two prominent LMM architectures, decoder-only and cross-attention,\nacross six representative open-source models, revealing key systems design\nimplications. We also present an in-depth analysis of production LMM inference\ntraces, uncovering unique workload characteristics, including variable,\nheavy-tailed request distributions and bursty traffic patterns. Based on these\ninsights, we propose ModServe, a modular LMM serving system that decouples\nstages for independent optimization and adaptive scaling. ModServe dynamically\nreconfigures stages and handles bursty traffic with modality-aware scheduling\nand autoscaling to meet tail latency SLOs while minimizing costs. ModServe\nachieves 3.3-5.5x higher throughput (leading to 25-41.3% cost saving) while\nmeeting SLOs on a 128-GPU cluster with production traces."
                },
                "authors": [
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Anish Biswas"
                    },
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Alind Khare"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Rodrigo Fonseca"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Fonseca"
                },
                "author": "Rodrigo Fonseca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17302v1",
                "updated": "2025-03-21T16:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    52,
                    3,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T16:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    52,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests"
                },
                "summary": "As software systems grow increasingly complex, ensuring security during\ndevelopment poses significant challenges. Traditional manual code audits are\noften expensive, time-intensive, and ill-suited for fast-paced workflows, while\nautomated tools frequently suffer from high false-positive rates, limiting\ntheir reliability. To address these issues, we introduce Bugdar, an\nAI-augmented code review system that integrates seamlessly into GitHub pull\nrequests, providing near real-time, context-aware vulnerability analysis.\nBugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAGs) to deliver project-specific, actionable feedback\nthat aligns with each codebase's unique requirements and developer practices.\nSupporting multiple programming languages, including Solidity, Move, Rust, and\nPython, Bugdar demonstrates exceptional efficiency, processing an average of\n56.4 seconds per pull request or 30 lines of code per second. This is\nsignificantly faster than manual reviews, which could take hours per pull\nrequest. By facilitating a proactive approach to secure coding, Bugdar reduces\nthe reliance on manual reviews, accelerates development cycles, and enhances\nthe security posture of software systems without compromising productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software systems grow increasingly complex, ensuring security during\ndevelopment poses significant challenges. Traditional manual code audits are\noften expensive, time-intensive, and ill-suited for fast-paced workflows, while\nautomated tools frequently suffer from high false-positive rates, limiting\ntheir reliability. To address these issues, we introduce Bugdar, an\nAI-augmented code review system that integrates seamlessly into GitHub pull\nrequests, providing near real-time, context-aware vulnerability analysis.\nBugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAGs) to deliver project-specific, actionable feedback\nthat aligns with each codebase's unique requirements and developer practices.\nSupporting multiple programming languages, including Solidity, Move, Rust, and\nPython, Bugdar demonstrates exceptional efficiency, processing an average of\n56.4 seconds per pull request or 30 lines of code per second. This is\nsignificantly faster than manual reviews, which could take hours per pull\nrequest. By facilitating a proactive approach to secure coding, Bugdar reduces\nthe reliance on manual reviews, accelerates development cycles, and enhances\nthe security posture of software systems without compromising productivity."
                },
                "authors": [
                    {
                        "name": "John Naulty"
                    },
                    {
                        "name": "Eason Chen"
                    },
                    {
                        "name": "Joy Wang"
                    },
                    {
                        "name": "George Digkas"
                    },
                    {
                        "name": "Kostas Chalkias"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Chalkias"
                },
                "author": "Kostas Chalkias",
                "arxiv_comment": "4 pages, 1 figure, accepted at IEEE Conference on Artificial\n  Intelligence (CAI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17290v1",
                "updated": "2025-03-21T16:41:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    41,
                    10,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T16:41:10Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    41,
                    10,
                    4,
                    80,
                    0
                ],
                "title": "Calibration Strategies for Robust Causal Estimation: Theoretical and\n  Empirical Insights on Propensity Score Based Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibration Strategies for Robust Causal Estimation: Theoretical and\n  Empirical Insights on Propensity Score Based Estimators"
                },
                "summary": "The partitioning of data for estimation and calibration critically impacts\nthe performance of propensity score based estimators like inverse probability\nweighting (IPW) and double/debiased machine learning (DML) frameworks. We\nextend recent advances in calibration techniques for propensity score\nestimation, improving the robustness of propensity scores in challenging\nsettings such as limited overlap, small sample sizes, or unbalanced data. Our\ncontributions are twofold: First, we provide a theoretical analysis of the\nproperties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a\nparticular emphasis on the role of sample-splitting schemes in ensuring valid\ncausal inference. Second, through extensive simulations, we show that\ncalibration reduces variance of inverse-based propensity score estimators while\nalso mitigating bias in IPW, even in small-sample regimes. Notably, calibration\nimproves stability for flexible learners (e.g., gradient boosting) while\npreserving the doubly robust properties of DML. A key insight is that, even\nwhen methods perform well without calibration, incorporating a calibration step\ndoes not degrade performance, provided that an appropriate sample-splitting\napproach is chosen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The partitioning of data for estimation and calibration critically impacts\nthe performance of propensity score based estimators like inverse probability\nweighting (IPW) and double/debiased machine learning (DML) frameworks. We\nextend recent advances in calibration techniques for propensity score\nestimation, improving the robustness of propensity scores in challenging\nsettings such as limited overlap, small sample sizes, or unbalanced data. Our\ncontributions are twofold: First, we provide a theoretical analysis of the\nproperties of calibrated estimators in the context of DML. To this end, we\nrefine existing calibration frameworks for propensity score models, with a\nparticular emphasis on the role of sample-splitting schemes in ensuring valid\ncausal inference. Second, through extensive simulations, we show that\ncalibration reduces variance of inverse-based propensity score estimators while\nalso mitigating bias in IPW, even in small-sample regimes. Notably, calibration\nimproves stability for flexible learners (e.g., gradient boosting) while\npreserving the doubly robust properties of DML. A key insight is that, even\nwhen methods perform well without calibration, incorporating a calibration step\ndoes not degrade performance, provided that an appropriate sample-splitting\napproach is chosen."
                },
                "authors": [
                    {
                        "name": "Jan Rabenseifner"
                    },
                    {
                        "name": "Sven Klaassen"
                    },
                    {
                        "name": "Jannis Kueck"
                    },
                    {
                        "name": "Philipp Bach"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Bach"
                },
                "author": "Philipp Bach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16304v2",
                "updated": "2025-03-21T16:34:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    34,
                    40,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T16:25:24Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    25,
                    24,
                    3,
                    79,
                    0
                ],
                "title": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1"
                },
                "summary": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications."
                },
                "authors": [
                    {
                        "name": "Peiran Gu"
                    },
                    {
                        "name": "Fuhao Duan"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Bochen Xu"
                    },
                    {
                        "name": "Ying Cai"
                    },
                    {
                        "name": "Teng Yao"
                    },
                    {
                        "name": "Chenxun Zhuo"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Bao Ge"
                    }
                ],
                "author_detail": {
                    "name": "Bao Ge"
                },
                "author": "Bao Ge",
                "arxiv_comment": "52 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17279v1",
                "updated": "2025-03-21T16:27:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    27,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T16:27:12Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    27,
                    12,
                    4,
                    80,
                    0
                ],
                "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic\n  Textual Similarity Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic\n  Textual Similarity Measurement"
                },
                "summary": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance."
                },
                "authors": [
                    {
                        "name": "Gaifan Zhang"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Danushka Bollegala"
                    }
                ],
                "author_detail": {
                    "name": "Danushka Bollegala"
                },
                "author": "Danushka Bollegala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11702v2",
                "updated": "2025-03-21T16:17:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    17,
                    59,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-12T09:32:43Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    32,
                    43,
                    2,
                    71,
                    0
                ],
                "title": "Toward a method for LLM-enabled Indoor Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a method for LLM-enabled Indoor Navigation"
                },
                "summary": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 52% correct indications and a maximum of 62%. The results do not\nappear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 52% correct indications and a maximum of 62%. The results do not\nappear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance."
                },
                "authors": [
                    {
                        "name": "Alberto Coffrini"
                    },
                    {
                        "name": "Mohammad Amin Zadenoori"
                    },
                    {
                        "name": "Paolo Barsocchi"
                    },
                    {
                        "name": "Francesco Furfari"
                    },
                    {
                        "name": "Antonino Crivello"
                    },
                    {
                        "name": "Alessio Ferrari"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Ferrari"
                },
                "author": "Alessio Ferrari",
                "arxiv_comment": "7 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17269v1",
                "updated": "2025-03-21T16:11:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    11,
                    21,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T16:11:21Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    11,
                    21,
                    4,
                    80,
                    0
                ],
                "title": "Recovering Pulse Waves from Video Using Deep Unrolling and Deep\n  Equilibrium Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering Pulse Waves from Video Using Deep Unrolling and Deep\n  Equilibrium Models"
                },
                "summary": "Camera-based monitoring of vital signs, also known as imaging\nphotoplethysmography (iPPG), has seen applications in driver-monitoring,\nperfusion assessment in surgical settings, affective computing, and more. iPPG\ninvolves sensing the underlying cardiac pulse from video of the skin and\nestimating vital signs such as the heart rate or a full pulse waveform. Some\nprevious iPPG methods impose model-based sparse priors on the pulse signals and\nuse iterative optimization for pulse wave recovery, while others use end-to-end\nblack-box deep learning methods. In contrast, we introduce methods that combine\nsignal processing and deep learning methods in an inverse problem framework.\nOur methods estimate the underlying pulse signal and heart rate from facial\nvideo by learning deep-network-based denoising operators that leverage deep\nalgorithm unfolding and deep equilibrium models. Experiments show that our\nmethods can denoise an acquired signal from the face and infer the correct\nunderlying pulse rate, achieving state-of-the-art heart rate estimation\nperformance on well-known benchmarks, all with less than one-fifth the number\nof learnable parameters as the closest competing method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera-based monitoring of vital signs, also known as imaging\nphotoplethysmography (iPPG), has seen applications in driver-monitoring,\nperfusion assessment in surgical settings, affective computing, and more. iPPG\ninvolves sensing the underlying cardiac pulse from video of the skin and\nestimating vital signs such as the heart rate or a full pulse waveform. Some\nprevious iPPG methods impose model-based sparse priors on the pulse signals and\nuse iterative optimization for pulse wave recovery, while others use end-to-end\nblack-box deep learning methods. In contrast, we introduce methods that combine\nsignal processing and deep learning methods in an inverse problem framework.\nOur methods estimate the underlying pulse signal and heart rate from facial\nvideo by learning deep-network-based denoising operators that leverage deep\nalgorithm unfolding and deep equilibrium models. Experiments show that our\nmethods can denoise an acquired signal from the face and infer the correct\nunderlying pulse rate, achieving state-of-the-art heart rate estimation\nperformance on well-known benchmarks, all with less than one-fifth the number\nof learnable parameters as the closest competing method."
                },
                "authors": [
                    {
                        "name": "Vineet R Shenoy"
                    },
                    {
                        "name": "Suhas Lohit"
                    },
                    {
                        "name": "Hassan Mansour"
                    },
                    {
                        "name": "Rama Chellappa"
                    },
                    {
                        "name": "Tim K. Marks"
                    }
                ],
                "author_detail": {
                    "name": "Tim K. Marks"
                },
                "author": "Tim K. Marks",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17267v1",
                "updated": "2025-03-21T16:08:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    8,
                    25,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T16:08:25Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    8,
                    25,
                    4,
                    80,
                    0
                ],
                "title": "Physical Plausibility-aware Trajectory Prediction via Locomotion\n  Embodiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Plausibility-aware Trajectory Prediction via Locomotion\n  Embodiment"
                },
                "summary": "Humans can predict future human trajectories even from momentary observations\nby using human pose-related cues. However, previous Human Trajectory Prediction\n(HTP) methods leverage the pose cues implicitly, resulting in implausible\npredictions. To address this, we propose Locomotion Embodiment, a framework\nthat explicitly evaluates the physical plausibility of the predicted trajectory\nby locomotion generation under the laws of physics. While the plausibility of\nlocomotion is learned with an indifferentiable physics simulator, it is\nreplaced by our differentiable Locomotion Value function to train an HTP\nnetwork in a data-driven manner. In particular, our proposed Embodied\nLocomotion loss is beneficial for efficiently training a stochastic HTP network\nusing multiple heads. Furthermore, the Locomotion Value filter is proposed to\nfilter out implausible trajectories at inference. Experiments demonstrate that\nour method enhances even the state-of-the-art HTP methods across diverse\ndatasets and problem settings. Our code is available at:\nhttps://github.com/ImIntheMiddle/EmLoco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can predict future human trajectories even from momentary observations\nby using human pose-related cues. However, previous Human Trajectory Prediction\n(HTP) methods leverage the pose cues implicitly, resulting in implausible\npredictions. To address this, we propose Locomotion Embodiment, a framework\nthat explicitly evaluates the physical plausibility of the predicted trajectory\nby locomotion generation under the laws of physics. While the plausibility of\nlocomotion is learned with an indifferentiable physics simulator, it is\nreplaced by our differentiable Locomotion Value function to train an HTP\nnetwork in a data-driven manner. In particular, our proposed Embodied\nLocomotion loss is beneficial for efficiently training a stochastic HTP network\nusing multiple heads. Furthermore, the Locomotion Value filter is proposed to\nfilter out implausible trajectories at inference. Experiments demonstrate that\nour method enhances even the state-of-the-art HTP methods across diverse\ndatasets and problem settings. Our code is available at:\nhttps://github.com/ImIntheMiddle/EmLoco."
                },
                "authors": [
                    {
                        "name": "Hiromu Taketsugu"
                    },
                    {
                        "name": "Takeru Oba"
                    },
                    {
                        "name": "Takahiro Maeda"
                    },
                    {
                        "name": "Shohei Nobuhara"
                    },
                    {
                        "name": "Norimichi Ukita"
                    }
                ],
                "author_detail": {
                    "name": "Norimichi Ukita"
                },
                "author": "Norimichi Ukita",
                "arxiv_comment": "CVPR2025. Project page: https://iminthemiddle.github.io/EmLoco-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17262v1",
                "updated": "2025-03-21T16:04:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    4,
                    13,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T16:04:13Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    4,
                    13,
                    4,
                    80,
                    0
                ],
                "title": "Unsupervised Joint Learning of Optical Flow and Intensity with Event\n  Cameras",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Joint Learning of Optical Flow and Intensity with Event\n  Cameras"
                },
                "summary": "Event cameras rely on motion to obtain information about scene appearance. In\nother words, for event cameras, motion and appearance are seen both or neither,\nwhich are encoded in the output event stream. Previous works consider\nrecovering these two visual quantities as separate tasks, which does not fit\nwith the nature of event cameras and neglects the inherent relations between\nboth tasks. In this paper, we propose an unsupervised learning framework that\njointly estimates optical flow (motion) and image intensity (appearance), with\na single network. Starting from the event generation model, we newly derive the\nevent-based photometric error as a function of optical flow and image\nintensity, which is further combined with the contrast maximization framework,\nyielding a comprehensive loss function that provides proper constraints for\nboth flow and intensity estimation. Exhaustive experiments show that our model\nachieves state-of-the-art performance for both optical flow (achieves 20% and\n25% improvement in EPE and AE respectively in the unsupervised learning\ncategory) and intensity estimation (produces competitive results with other\nbaselines, particularly in high dynamic range scenarios). Last but not least,\nour model achieves shorter inference time than all the other optical flow\nmodels and many of the image reconstruction models, while they output only one\nquantity. Project page: https://github.com/tub-rip/e2fai",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras rely on motion to obtain information about scene appearance. In\nother words, for event cameras, motion and appearance are seen both or neither,\nwhich are encoded in the output event stream. Previous works consider\nrecovering these two visual quantities as separate tasks, which does not fit\nwith the nature of event cameras and neglects the inherent relations between\nboth tasks. In this paper, we propose an unsupervised learning framework that\njointly estimates optical flow (motion) and image intensity (appearance), with\na single network. Starting from the event generation model, we newly derive the\nevent-based photometric error as a function of optical flow and image\nintensity, which is further combined with the contrast maximization framework,\nyielding a comprehensive loss function that provides proper constraints for\nboth flow and intensity estimation. Exhaustive experiments show that our model\nachieves state-of-the-art performance for both optical flow (achieves 20% and\n25% improvement in EPE and AE respectively in the unsupervised learning\ncategory) and intensity estimation (produces competitive results with other\nbaselines, particularly in high dynamic range scenarios). Last but not least,\nour model achieves shorter inference time than all the other optical flow\nmodels and many of the image reconstruction models, while they output only one\nquantity. Project page: https://github.com/tub-rip/e2fai"
                },
                "authors": [
                    {
                        "name": "Shuang Guo"
                    },
                    {
                        "name": "Friedhelm Hamann"
                    },
                    {
                        "name": "Guillermo Gallego"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Gallego"
                },
                "author": "Guillermo Gallego",
                "arxiv_comment": "14 page, 8 figures, 9 tables. Project page:\n  https://github.com/tub-rip/e2fai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10602v2",
                "updated": "2025-03-21T15:58:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    58,
                    26,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-13T17:46:06Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    46,
                    6,
                    3,
                    72,
                    0
                ],
                "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention"
                },
                "summary": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt."
                },
                "authors": [
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Fei Kong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "James Diffenderfer"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Xiaoshuang Shi"
                    },
                    {
                        "name": "Kaidi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Xu"
                },
                "author": "Kaidi Xu",
                "arxiv_comment": "15 pages, 9 figures, the first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17239v1",
                "updated": "2025-03-21T15:44:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    44,
                    9,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T15:44:09Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    44,
                    9,
                    4,
                    80,
                    0
                ],
                "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language\n  Models via Selective Layer-Wise Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language\n  Models via Selective Layer-Wise Model Merging"
                },
                "summary": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses."
                },
                "authors": [
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Swanand Ravindra Kadhe"
                    },
                    {
                        "name": "Farhan Ahmed"
                    },
                    {
                        "name": "Syed Zawad"
                    },
                    {
                        "name": "Holger Boche"
                    }
                ],
                "author_detail": {
                    "name": "Holger Boche"
                },
                "author": "Holger Boche",
                "arxiv_journal_ref": "ICLR 2025 Workshop on Building Trust in Language Models and\n  Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06796v2",
                "updated": "2025-03-21T15:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    40,
                    38,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-11T08:50:24Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    50,
                    24,
                    0,
                    316,
                    0
                ],
                "title": "Write Your Own CodeChecker: An Automated Test-Driven Checker Development\n  Approach with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write Your Own CodeChecker: An Automated Test-Driven Checker Development\n  Approach with LLMs"
                },
                "summary": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic and the complex API usage of large-scale\nchecker frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we propose AutoChecker, an innovative LLM-powered approach that can\nwrite code checkers automatically based on only a rule description and a test\nsuite. To achieve comprehensive checking logic, AutoChecker incrementally\nupdates the checker's logic by focusing on solving one selected case each time.\nTo obtain precise API knowledge, during each iteration, it leverages\nfine-grained logic-guided API-context retrieval, where it first decomposes the\nchecking logic into a series of sub-operations and then retrieves\nchecker-related API-contexts for each sub-operation. For evaluation, we apply\nAutoChecker, five baselines, and three ablation methods using multiple LLMs to\ngenerate checkers for 20 randomly selected PMD rules. Experimental results show\nthat AutoChecker significantly outperforms others across all effectiveness\nmetrics, with an average test pass rate of 82.28%. Additionally, the checkers\ngenerated by AutoChecker can be successfully applied to real-world projects,\nmatching the performance of official checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic and the complex API usage of large-scale\nchecker frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we propose AutoChecker, an innovative LLM-powered approach that can\nwrite code checkers automatically based on only a rule description and a test\nsuite. To achieve comprehensive checking logic, AutoChecker incrementally\nupdates the checker's logic by focusing on solving one selected case each time.\nTo obtain precise API knowledge, during each iteration, it leverages\nfine-grained logic-guided API-context retrieval, where it first decomposes the\nchecking logic into a series of sub-operations and then retrieves\nchecker-related API-contexts for each sub-operation. For evaluation, we apply\nAutoChecker, five baselines, and three ablation methods using multiple LLMs to\ngenerate checkers for 20 randomly selected PMD rules. Experimental results show\nthat AutoChecker significantly outperforms others across all effectiveness\nmetrics, with an average test pass rate of 82.28%. Additionally, the checkers\ngenerated by AutoChecker can be successfully applied to real-world projects,\nmatching the performance of official checkers."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Xie"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Jiwei Yan"
                    },
                    {
                        "name": "Jinhao Huang"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17237v1",
                "updated": "2025-03-21T15:40:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    40,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T15:40:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    40,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID"
                },
                "summary": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal\ninfrared video is inherently challenging due to low contrast, environmental\nnoise, and small target sizes. This paper provides a straightforward approach\nto address multi-UAV tracking in thermal infrared video, leveraging recent\nadvances in detection and tracking. Instead of relying on the YOLOv5 with the\nDeepSORT pipeline, we present a tracking framework built on YOLOv12 and\nBoT-SORT, enhanced with tailored training and inference strategies. We evaluate\nour approach following the metrics from the 4th Anti-UAV Challenge and\ndemonstrate competitive performance. Notably, we achieve strong results without\nusing contrast enhancement or temporal information fusion to enrich UAV\nfeatures, highlighting our approach as a \"Strong Baseline\" for the multi-UAV\ntracking task. We provide implementation details, in-depth experimental\nanalysis, and a discussion of potential improvements. The code is available at\nhttps://github.com/wish44165/YOLOv12-BoT-SORT-ReID ."
                },
                "authors": [
                    {
                        "name": "Yu-Hsi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Hsi Chen"
                },
                "author": "Yu-Hsi Chen",
                "arxiv_comment": "10 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17229v1",
                "updated": "2025-03-21T15:32:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    32,
                    24,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T15:32:24Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    32,
                    24,
                    4,
                    80,
                    0
                ],
                "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs"
                },
                "summary": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content."
                },
                "authors": [
                    {
                        "name": "Albert Sawczyn"
                    },
                    {
                        "name": "Jakub Binkowski"
                    },
                    {
                        "name": "Denis Janiak"
                    },
                    {
                        "name": "Bogdan Gabrys"
                    },
                    {
                        "name": "Tomasz Kajdanowicz"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kajdanowicz"
                },
                "author": "Tomasz Kajdanowicz",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12262v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12262v4",
                "updated": "2025-03-21T15:32:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    32,
                    7,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-18T18:47:58Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    18,
                    47,
                    58,
                    2,
                    262,
                    0
                ],
                "title": "Bootstrapping Object-level Planning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Object-level Planning with Large Language Models"
                },
                "summary": "We introduce a new method that extracts knowledge from a large language model\n(LLM) to produce object-level plans, which describe high-level changes to\nobject state, and uses them to bootstrap task and motion planning (TAMP).\nExisting work uses LLMs to directly output task plans or generate goals in\nrepresentations like PDDL. However, these methods fall short because they rely\non the LLM to do the actual planning or output a hard-to-satisfy goal. Our\napproach instead extracts knowledge from an LLM in the form of plan schemas as\nan object-level representation called functional object-oriented networks\n(FOON), from which we automatically generate PDDL subgoals. Our method markedly\noutperforms alternative planning strategies in completing several\npick-and-place tasks in simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new method that extracts knowledge from a large language model\n(LLM) to produce object-level plans, which describe high-level changes to\nobject state, and uses them to bootstrap task and motion planning (TAMP).\nExisting work uses LLMs to directly output task plans or generate goals in\nrepresentations like PDDL. However, these methods fall short because they rely\non the LLM to do the actual planning or output a hard-to-satisfy goal. Our\napproach instead extracts knowledge from an LLM in the form of plan schemas as\nan object-level representation called functional object-oriented networks\n(FOON), from which we automatically generate PDDL subgoals. Our method markedly\noutperforms alternative planning strategies in completing several\npick-and-place tasks in simulation."
                },
                "authors": [
                    {
                        "name": "David Paulius"
                    },
                    {
                        "name": "Alejandro Agostini"
                    },
                    {
                        "name": "Benedict Quartey"
                    },
                    {
                        "name": "George Konidaris"
                    }
                ],
                "author_detail": {
                    "name": "George Konidaris"
                },
                "author": "George Konidaris",
                "arxiv_comment": "Accepted to ICRA 2025; 11 pages (6 pages + 1 page references + 4\n  pages appendix); for demo videos, please see\n  https://davidpaulius.github.io/olp_llm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12262v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12262v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04170v2",
                "updated": "2025-03-21T15:26:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    26,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-01-07T22:40:37Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    22,
                    40,
                    37,
                    1,
                    7,
                    0
                ],
                "title": "A Bayesian Modeling Framework for Estimation and Ground Segmentation of\n  Cluttered Staircases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Modeling Framework for Estimation and Ground Segmentation of\n  Cluttered Staircases"
                },
                "summary": "Autonomous robot navigation in complex environments requires robust\nperception as well as high-level scene understanding due to perceptual\nchallenges, such as occlusions, and uncertainty introduced by robot movement.\nFor example, a robot climbing a cluttered staircase can misinterpret clutter as\na step, misrepresenting the state and compromising safety. This requires robust\nstate estimation methods capable of inferring the underlying structure of the\nenvironment even from incomplete sensor data. In this paper, we introduce a\nnovel method for robust state estimation of staircases. To address the\nchallenge of perceiving occluded staircases extending beyond the robot's\nfield-of-view, our approach combines an infinite-width staircase representation\nwith a finite endpoint state to capture the overall staircase structure. This\nrepresentation is integrated into a Bayesian inference framework to fuse noisy\nmeasurements enabling accurate estimation of staircase location even with\npartial observations and occlusions. Additionally, we present a segmentation\nalgorithm that works in conjunction with the staircase estimation pipeline to\naccurately identify clutter-free regions on a staircase. Our method is\nextensively evaluated on real robot across diverse staircases, demonstrating\nsignificant improvements in estimation accuracy and segmentation performance\ncompared to baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous robot navigation in complex environments requires robust\nperception as well as high-level scene understanding due to perceptual\nchallenges, such as occlusions, and uncertainty introduced by robot movement.\nFor example, a robot climbing a cluttered staircase can misinterpret clutter as\na step, misrepresenting the state and compromising safety. This requires robust\nstate estimation methods capable of inferring the underlying structure of the\nenvironment even from incomplete sensor data. In this paper, we introduce a\nnovel method for robust state estimation of staircases. To address the\nchallenge of perceiving occluded staircases extending beyond the robot's\nfield-of-view, our approach combines an infinite-width staircase representation\nwith a finite endpoint state to capture the overall staircase structure. This\nrepresentation is integrated into a Bayesian inference framework to fuse noisy\nmeasurements enabling accurate estimation of staircase location even with\npartial observations and occlusions. Additionally, we present a segmentation\nalgorithm that works in conjunction with the staircase estimation pipeline to\naccurately identify clutter-free regions on a staircase. Our method is\nextensively evaluated on real robot across diverse staircases, demonstrating\nsignificant improvements in estimation accuracy and segmentation performance\ncompared to baseline approaches."
                },
                "authors": [
                    {
                        "name": "Prasanna Sriganesh"
                    },
                    {
                        "name": "Burhanuddin Shirose"
                    },
                    {
                        "name": "Matthew Travers"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Travers"
                },
                "author": "Matthew Travers",
                "arxiv_doi": "10.1109/LRA.2025.3549662",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3549662",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.04170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "In IEEE Robotics and Automation Letters (RA-L), Volume 10, Issue\n  5, 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17222v1",
                "updated": "2025-03-21T15:25:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    25,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T15:25:53Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    25,
                    53,
                    4,
                    80,
                    0
                ],
                "title": "Automating Adjudication of Cardiovascular Events Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Adjudication of Cardiovascular Events Using Large Language\n  Models"
                },
                "summary": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies."
                },
                "authors": [
                    {
                        "name": "Sonish Sivarajkumar"
                    },
                    {
                        "name": "Kimia Ameri"
                    },
                    {
                        "name": "Chuqin Li"
                    },
                    {
                        "name": "Yanshan Wang"
                    },
                    {
                        "name": "Min Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Min Jiang"
                },
                "author": "Min Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17220v1",
                "updated": "2025-03-21T15:24:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    24,
                    54,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T15:24:54Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    24,
                    54,
                    4,
                    80,
                    0
                ],
                "title": "InfraFix: Technology-Agnostic Repair of Infrastructure as Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfraFix: Technology-Agnostic Repair of Infrastructure as Code"
                },
                "summary": "Infrastructure as Code (IaC) enables scalable and automated IT infrastructure\nmanagement but is prone to errors that can lead to security vulnerabilities,\noutages, and data loss. While prior research has focused on detecting IaC\nissues, Automated Program Repair (APR) remains underexplored, largely due to\nthe lack of suitable specifications. In this work, we propose InfraFix, the\nfirst technology-agnostic framework for repairing IaC scripts. Unlike prior\napproaches, InfraFix allows APR techniques to be guided by diverse information\nsources.\n  Additionally, we introduce a novel approach for generating repair scenarios,\nenabling large-scale evaluation of APR techniques for IaC. We implement and\nevaluate InfraFix using an SMT-based repair module and a state inference module\nthat uses system calls, demonstrating its effectiveness across 254,755 repair\nscenarios with a success rate of 95.5%. Our work provides a foundation for\nadvancing APR in IaC by enabling researchers to experiment with new state\ninference and repair techniques using InfraFix and to evaluate their approaches\nat scale with our repair scenario generation method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infrastructure as Code (IaC) enables scalable and automated IT infrastructure\nmanagement but is prone to errors that can lead to security vulnerabilities,\noutages, and data loss. While prior research has focused on detecting IaC\nissues, Automated Program Repair (APR) remains underexplored, largely due to\nthe lack of suitable specifications. In this work, we propose InfraFix, the\nfirst technology-agnostic framework for repairing IaC scripts. Unlike prior\napproaches, InfraFix allows APR techniques to be guided by diverse information\nsources.\n  Additionally, we introduce a novel approach for generating repair scenarios,\nenabling large-scale evaluation of APR techniques for IaC. We implement and\nevaluate InfraFix using an SMT-based repair module and a state inference module\nthat uses system calls, demonstrating its effectiveness across 254,755 repair\nscenarios with a success rate of 95.5%. Our work provides a foundation for\nadvancing APR in IaC by enabling researchers to experiment with new state\ninference and repair techniques using InfraFix and to evaluate their approaches\nat scale with our repair scenario generation method."
                },
                "authors": [
                    {
                        "name": "Nuno Saavedra"
                    },
                    {
                        "name": "João F. Ferreira"
                    },
                    {
                        "name": "Alexandra Mendes"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Mendes"
                },
                "author": "Alexandra Mendes",
                "arxiv_comment": "Submitted to ISSTA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17213v1",
                "updated": "2025-03-21T15:20:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    20,
                    47,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T15:20:47Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    20,
                    47,
                    4,
                    80,
                    0
                ],
                "title": "PP-DocLayout: A Unified Document Layout Detection Model to Accelerate\n  Large-Scale Data Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PP-DocLayout: A Unified Document Layout Detection Model to Accelerate\n  Large-Scale Data Construction"
                },
                "summary": "Document layout analysis is a critical preprocessing step in document\nintelligence, enabling the detection and localization of structural elements\nsuch as titles, text blocks, tables, and formulas. Despite its importance,\nexisting layout detection models face significant challenges in generalizing\nacross diverse document types, handling complex layouts, and achieving\nreal-time performance for large-scale data processing. To address these\nlimitations, we present PP-DocLayout, which achieves high precision and\nefficiency in recognizing 23 types of layout regions across diverse document\nformats. To meet different needs, we offer three models of varying scales.\nPP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,\nachieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on\na T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an\ninference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a\nhigh-efficiency model designed for resource-constrained environments and\nreal-time applications, with an inference time of 8.1 ms per page on a T4 GPU\nand 14.5 ms on a CPU. This work not only advances the state of the art in\ndocument layout analysis but also provides a robust solution for constructing\nhigh-quality training data, enabling advancements in document intelligence and\nmultimodal AI systems. Code and models are available at\nhttps://github.com/PaddlePaddle/PaddleX .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document layout analysis is a critical preprocessing step in document\nintelligence, enabling the detection and localization of structural elements\nsuch as titles, text blocks, tables, and formulas. Despite its importance,\nexisting layout detection models face significant challenges in generalizing\nacross diverse document types, handling complex layouts, and achieving\nreal-time performance for large-scale data processing. To address these\nlimitations, we present PP-DocLayout, which achieves high precision and\nefficiency in recognizing 23 types of layout regions across diverse document\nformats. To meet different needs, we offer three models of varying scales.\nPP-DocLayout-L is a high-precision model based on the RT-DETR-L detector,\nachieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on\na T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an\ninference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a\nhigh-efficiency model designed for resource-constrained environments and\nreal-time applications, with an inference time of 8.1 ms per page on a T4 GPU\nand 14.5 ms on a CPU. This work not only advances the state of the art in\ndocument layout analysis but also provides a robust solution for constructing\nhigh-quality training data, enabling advancements in document intelligence and\nmultimodal AI systems. Code and models are available at\nhttps://github.com/PaddlePaddle/PaddleX ."
                },
                "authors": [
                    {
                        "name": "Ting Sun"
                    },
                    {
                        "name": "Cheng Cui"
                    },
                    {
                        "name": "Yuning Du"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "arxiv_comment": "Github Repo: https://github.com/PaddlePaddle/PaddleX",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11006v2",
                "updated": "2025-03-21T15:07:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    7,
                    55,
                    4,
                    80,
                    0
                ],
                "published": "2025-01-19T10:44:03Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    10,
                    44,
                    3,
                    6,
                    19,
                    0
                ],
                "title": "GREEN-CODE: Learning to Optimize Energy Efficiency in LLM-based Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GREEN-CODE: Learning to Optimize Energy Efficiency in LLM-based Code\n  Generation"
                },
                "summary": "Large Language Models (LLMs) are becoming integral to daily life, showcasing\ntheir vast potential across various Natural Language Processing (NLP) tasks.\nBeyond NLP, LLMs are increasingly used in software development tasks, such as\ncode completion, modification, bug fixing, and code translation. Software\nengineers widely use tools like GitHub Copilot and Amazon Q, streamlining\nworkflows and automating tasks with high accuracy. While the resource and\nenergy intensity of LLM training is often highlighted, inference can be even\nmore resource-intensive over time, as it's a continuous process with a high\nnumber of invocations. Therefore, developing resource-efficient alternatives\nfor LLM inference is crucial for sustainability. This work proposes GREEN-CODE,\na framework for energy-aware code generation in LLMs. GREEN-CODE performs\ndynamic early exit during LLM inference. We train a Reinforcement Learning (RL)\nagent that learns to balance the trade-offs between accuracy, latency, and\nenergy consumption. Our approach is evaluated on two open-source LLMs, Llama\n3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that\nour method reduces the energy consumption between 23-50 % on average for code\ngeneration tasks without significantly affecting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming integral to daily life, showcasing\ntheir vast potential across various Natural Language Processing (NLP) tasks.\nBeyond NLP, LLMs are increasingly used in software development tasks, such as\ncode completion, modification, bug fixing, and code translation. Software\nengineers widely use tools like GitHub Copilot and Amazon Q, streamlining\nworkflows and automating tasks with high accuracy. While the resource and\nenergy intensity of LLM training is often highlighted, inference can be even\nmore resource-intensive over time, as it's a continuous process with a high\nnumber of invocations. Therefore, developing resource-efficient alternatives\nfor LLM inference is crucial for sustainability. This work proposes GREEN-CODE,\na framework for energy-aware code generation in LLMs. GREEN-CODE performs\ndynamic early exit during LLM inference. We train a Reinforcement Learning (RL)\nagent that learns to balance the trade-offs between accuracy, latency, and\nenergy consumption. Our approach is evaluated on two open-source LLMs, Llama\n3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that\nour method reduces the energy consumption between 23-50 % on average for code\ngeneration tasks without significantly affecting accuracy."
                },
                "authors": [
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Lukas Florian Briem"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "arxiv_comment": "Under submission in ACM/IEEE conference, 11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; D.0; E.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12082v2",
                "updated": "2025-03-21T15:06:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    6,
                    41,
                    4,
                    80,
                    0
                ],
                "published": "2024-06-17T20:46:18Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    20,
                    46,
                    18,
                    0,
                    169,
                    0
                ],
                "title": "Uncertainty modeling for fine-tuned implicit functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty modeling for fine-tuned implicit functions"
                },
                "summary": "Implicit functions such as Neural Radiance Fields (NeRFs), occupancy\nnetworks, and signed distance functions (SDFs) have become pivotal in computer\nvision for reconstructing detailed object shapes from sparse views. Achieving\noptimal performance with these models can be challenging due to the extreme\nsparsity of inputs and distribution shifts induced by data corruptions. To this\nend, large, noise-free synthetic datasets can serve as shape priors to help\nmodels fill in gaps, but the resulting reconstructions must be approached with\ncaution. Uncertainty estimation is crucial for assessing the quality of these\nreconstructions, particularly in identifying areas where the model is uncertain\nabout the parts it has inferred from the prior. In this paper, we introduce\nDropsembles, a novel method for uncertainty estimation in tuned implicit\nfunctions. We demonstrate the efficacy of our approach through a series of\nexperiments, starting with toy examples and progressing to a real-world\nscenario. Specifically, we train a Convolutional Occupancy Network on synthetic\nanatomical data and test it on low-resolution MRI segmentations of the lumbar\nspine. Our results show that Dropsembles achieve the accuracy and calibration\nlevels of deep ensembles but with significantly less computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit functions such as Neural Radiance Fields (NeRFs), occupancy\nnetworks, and signed distance functions (SDFs) have become pivotal in computer\nvision for reconstructing detailed object shapes from sparse views. Achieving\noptimal performance with these models can be challenging due to the extreme\nsparsity of inputs and distribution shifts induced by data corruptions. To this\nend, large, noise-free synthetic datasets can serve as shape priors to help\nmodels fill in gaps, but the resulting reconstructions must be approached with\ncaution. Uncertainty estimation is crucial for assessing the quality of these\nreconstructions, particularly in identifying areas where the model is uncertain\nabout the parts it has inferred from the prior. In this paper, we introduce\nDropsembles, a novel method for uncertainty estimation in tuned implicit\nfunctions. We demonstrate the efficacy of our approach through a series of\nexperiments, starting with toy examples and progressing to a real-world\nscenario. Specifically, we train a Convolutional Occupancy Network on synthetic\nanatomical data and test it on low-resolution MRI segmentations of the lumbar\nspine. Our results show that Dropsembles achieve the accuracy and calibration\nlevels of deep ensembles but with significantly less computational cost."
                },
                "authors": [
                    {
                        "name": "Anna Susmelj"
                    },
                    {
                        "name": "Mael Macuglia"
                    },
                    {
                        "name": "Nataša Tagasovska"
                    },
                    {
                        "name": "Reto Sutter"
                    },
                    {
                        "name": "Sebastiano Caprara"
                    },
                    {
                        "name": "Jean-Philippe Thiran"
                    },
                    {
                        "name": "Ender Konukoglu"
                    }
                ],
                "author_detail": {
                    "name": "Ender Konukoglu"
                },
                "author": "Ender Konukoglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15249v2",
                "updated": "2025-03-21T14:56:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    56,
                    58,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-15T01:12:26Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    1,
                    12,
                    26,
                    6,
                    350,
                    0
                ],
                "title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LitLLMs, LLMs for Literature Review: Are we there yet?"
                },
                "summary": "Literature reviews are an essential component of scientific research, but\nthey remain time-intensive and challenging to write, especially due to the\nrecent influx of research papers. This paper explores the zero-shot abilities\nof recent Large Language Models (LLMs) in assisting with the writing of\nliterature reviews based on an abstract. We decompose the task into two\ncomponents: 1. Retrieving related works given a query abstract, and 2. Writing\na literature review based on the retrieved results. We analyze how effective\nLLMs are for both components. For retrieval, we introduce a novel two-step\nsearch strategy that first uses an LLM to extract meaningful keywords from the\nabstract of a paper and then retrieves potentially relevant papers by querying\nan external knowledge base. Additionally, we study a prompting-based re-ranking\nmechanism with attribution and show that re-ranking doubles the normalized\nrecall compared to naive search methods, while providing insights into the\nLLM's decision-making process. In the generation phase, we propose a two-step\napproach that first outlines a plan for the review and then executes steps in\nthe plan to generate the actual review. To evaluate different LLM-based\nliterature review methods, we create test sets from arXiv papers using a\nprotocol designed for rolling use with newly released LLMs to avoid test set\ncontamination in zero-shot evaluations. We release this evaluation protocol to\npromote additional research and development in this regard. Our empirical\nresults suggest that LLMs show promising potential for writing literature\nreviews when the task is decomposed into smaller components of retrieval and\nplanning. Our project page including a demonstration system and toolkit can be\naccessed here: https://litllm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature reviews are an essential component of scientific research, but\nthey remain time-intensive and challenging to write, especially due to the\nrecent influx of research papers. This paper explores the zero-shot abilities\nof recent Large Language Models (LLMs) in assisting with the writing of\nliterature reviews based on an abstract. We decompose the task into two\ncomponents: 1. Retrieving related works given a query abstract, and 2. Writing\na literature review based on the retrieved results. We analyze how effective\nLLMs are for both components. For retrieval, we introduce a novel two-step\nsearch strategy that first uses an LLM to extract meaningful keywords from the\nabstract of a paper and then retrieves potentially relevant papers by querying\nan external knowledge base. Additionally, we study a prompting-based re-ranking\nmechanism with attribution and show that re-ranking doubles the normalized\nrecall compared to naive search methods, while providing insights into the\nLLM's decision-making process. In the generation phase, we propose a two-step\napproach that first outlines a plan for the review and then executes steps in\nthe plan to generate the actual review. To evaluate different LLM-based\nliterature review methods, we create test sets from arXiv papers using a\nprotocol designed for rolling use with newly released LLMs to avoid test set\ncontamination in zero-shot evaluations. We release this evaluation protocol to\npromote additional research and development in this regard. Our empirical\nresults suggest that LLMs show promising potential for writing literature\nreviews when the task is decomposed into smaller components of retrieval and\nplanning. Our project page including a demonstration system and toolkit can be\naccessed here: https://litllm.github.io."
                },
                "authors": [
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Gaurav Sahu"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Issam H. Laradji"
                    },
                    {
                        "name": "Krishnamurthy DJ Dvijotham"
                    },
                    {
                        "name": "Jason Stanley"
                    },
                    {
                        "name": "Laurent Charlin"
                    },
                    {
                        "name": "Christopher Pal"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Pal"
                },
                "author": "Christopher Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01788v2",
                "updated": "2025-03-21T14:49:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    49,
                    10,
                    4,
                    80,
                    0
                ],
                "published": "2024-02-02T02:41:28Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    2,
                    41,
                    28,
                    4,
                    33,
                    0
                ],
                "title": "LitLLM: A Toolkit for Scientific Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LitLLM: A Toolkit for Scientific Literature Review"
                },
                "summary": "Conducting literature reviews for scientific papers is essential for\nunderstanding research, its limitations, and building on existing work. It is a\ntedious task which makes an automatic literature review generator appealing.\nUnfortunately, many existing works that generate such reviews using Large\nLanguage Models (LLMs) have significant limitations. They tend to\nhallucinate-generate non-factual information-and ignore the latest research\nthey have not been trained on. To address these limitations, we propose a\ntoolkit that operates on Retrieval Augmented Generation (RAG) principles,\nspecialized prompting and instructing techniques with the help of LLMs. Our\nsystem first initiates a web search to retrieve relevant papers by summarizing\nuser-provided abstracts into keywords using an off-the-shelf LLM. Authors can\nenhance the search by supplementing it with relevant papers or keywords,\ncontributing to a tailored retrieval process. Second, the system re-ranks the\nretrieved papers based on the user-provided abstract. Finally, the related work\nsection is generated based on the re-ranked results and the abstract. There is\na substantial reduction in time and effort for literature review compared to\ntraditional methods, establishing our toolkit as an efficient alternative. Our\nproject page including the demo and toolkit can be accessed here:\nhttps://litllm.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conducting literature reviews for scientific papers is essential for\nunderstanding research, its limitations, and building on existing work. It is a\ntedious task which makes an automatic literature review generator appealing.\nUnfortunately, many existing works that generate such reviews using Large\nLanguage Models (LLMs) have significant limitations. They tend to\nhallucinate-generate non-factual information-and ignore the latest research\nthey have not been trained on. To address these limitations, we propose a\ntoolkit that operates on Retrieval Augmented Generation (RAG) principles,\nspecialized prompting and instructing techniques with the help of LLMs. Our\nsystem first initiates a web search to retrieve relevant papers by summarizing\nuser-provided abstracts into keywords using an off-the-shelf LLM. Authors can\nenhance the search by supplementing it with relevant papers or keywords,\ncontributing to a tailored retrieval process. Second, the system re-ranks the\nretrieved papers based on the user-provided abstract. Finally, the related work\nsection is generated based on the re-ranked results and the abstract. There is\na substantial reduction in time and effort for literature review compared to\ntraditional methods, establishing our toolkit as an efficient alternative. Our\nproject page including the demo and toolkit can be accessed here:\nhttps://litllm.github.io"
                },
                "authors": [
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Gaurav Sahu"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Issam H. Laradji"
                    },
                    {
                        "name": "Krishnamurthy DJ Dvijotham"
                    },
                    {
                        "name": "Jason Stanley"
                    },
                    {
                        "name": "Laurent Charlin"
                    },
                    {
                        "name": "Christopher Pal"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Pal"
                },
                "author": "Christopher Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17197v1",
                "updated": "2025-03-21T14:44:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    44,
                    22,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T14:44:22Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    44,
                    22,
                    4,
                    80,
                    0
                ],
                "title": "FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via\n  Cross-Assembly Inference Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via\n  Cross-Assembly Inference Strategy"
                },
                "summary": "Recovering high-quality 3D facial textures from single-view 2D images is a\nchallenging task, especially under constraints of limited data and complex\nfacial details such as makeup, wrinkles, and occlusions. In this paper, we\nintroduce FreeUV, a novel ground-truth-free UV texture recovery framework that\neliminates the need for annotated or synthetic UV data. FreeUV leverages\npre-trained stable diffusion model alongside a Cross-Assembly inference\nstrategy to fulfill this objective. In FreeUV, separate networks are trained\nindependently to focus on realistic appearance and structural consistency, and\nthese networks are combined during inference to generate coherent textures. Our\napproach accurately captures intricate facial features and demonstrates robust\nperformance across diverse poses and occlusions. Extensive experiments validate\nFreeUV's effectiveness, with results surpassing state-of-the-art methods in\nboth quantitative and qualitative metrics. Additionally, FreeUV enables new\napplications, including local editing, facial feature interpolation, and\nmulti-view texture recovery. By reducing data requirements, FreeUV offers a\nscalable solution for generating high-fidelity 3D facial textures suitable for\nreal-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering high-quality 3D facial textures from single-view 2D images is a\nchallenging task, especially under constraints of limited data and complex\nfacial details such as makeup, wrinkles, and occlusions. In this paper, we\nintroduce FreeUV, a novel ground-truth-free UV texture recovery framework that\neliminates the need for annotated or synthetic UV data. FreeUV leverages\npre-trained stable diffusion model alongside a Cross-Assembly inference\nstrategy to fulfill this objective. In FreeUV, separate networks are trained\nindependently to focus on realistic appearance and structural consistency, and\nthese networks are combined during inference to generate coherent textures. Our\napproach accurately captures intricate facial features and demonstrates robust\nperformance across diverse poses and occlusions. Extensive experiments validate\nFreeUV's effectiveness, with results surpassing state-of-the-art methods in\nboth quantitative and qualitative metrics. Additionally, FreeUV enables new\napplications, including local editing, facial feature interpolation, and\nmulti-view texture recovery. By reducing data requirements, FreeUV offers a\nscalable solution for generating high-fidelity 3D facial textures suitable for\nreal-world scenarios."
                },
                "authors": [
                    {
                        "name": "Xingchao Yang"
                    },
                    {
                        "name": "Takafumi Taketomi"
                    },
                    {
                        "name": "Yuki Endo"
                    },
                    {
                        "name": "Yoshihiro Kanamori"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihiro Kanamori"
                },
                "author": "Yoshihiro Kanamori",
                "arxiv_comment": "CVPR 2025. Project: https://yangxingchao.github.io/FreeUV-page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06553v2",
                "updated": "2025-03-21T14:43:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    43,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2025-01-11T14:09:34Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    14,
                    9,
                    34,
                    5,
                    11,
                    0
                ],
                "title": "VASparse: Towards Efficient Visual Hallucination Mitigation via\n  Visual-Aware Token Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VASparse: Towards Efficient Visual Hallucination Mitigation via\n  Visual-Aware Token Sparsification"
                },
                "summary": "Large Vision-Language Models (LVLMs) may produce outputs that are unfaithful\nto reality, also known as visual hallucinations (VH), which significantly\nimpedes their real-world usage. To alleviate VH, various decoding strategies\nhave been proposed to enhance visual information. However, many of these\nmethods may require secondary decoding and rollback, which significantly\nreduces inference speed. In this work, we propose an efficient plug-and-play\ndecoding algorithm via Visual-Aware Sparsification (VASparse) from the\nperspective of token sparsity for mitigating VH. VASparse is inspired by\nempirical observations: (1) the sparse activation of attention in LVLMs, and\n(2) visual-agnostic tokens sparsification exacerbates VH. Based on these\ninsights, we propose a novel token sparsification strategy that balances\nefficiency and trustworthiness. Specifically, VASparse implements a\nvisual-aware token selection strategy during decoding to reduce redundant\ntokens while preserving visual context effectively. Additionally, we\ninnovatively introduce a sparse-based visual contrastive decoding method to\nrecalibrate the distribution of hallucinated outputs without the time overhead\nassociated with secondary decoding. Subsequently, VASparse recalibrates\nattention scores to penalize attention sinking of LVLMs towards text tokens.\nExtensive experiments across four popular benchmarks confirm the effectiveness\nof VASparse in mitigating VH across different LVLM families without requiring\nadditional training or post-processing. Impressively, VASparse achieves\nstate-of-the-art performance for mitigating VH while maintaining competitive\ndecoding speed. Code is available at\nhttps://github.com/mengchuang123/VASparse-github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) may produce outputs that are unfaithful\nto reality, also known as visual hallucinations (VH), which significantly\nimpedes their real-world usage. To alleviate VH, various decoding strategies\nhave been proposed to enhance visual information. However, many of these\nmethods may require secondary decoding and rollback, which significantly\nreduces inference speed. In this work, we propose an efficient plug-and-play\ndecoding algorithm via Visual-Aware Sparsification (VASparse) from the\nperspective of token sparsity for mitigating VH. VASparse is inspired by\nempirical observations: (1) the sparse activation of attention in LVLMs, and\n(2) visual-agnostic tokens sparsification exacerbates VH. Based on these\ninsights, we propose a novel token sparsification strategy that balances\nefficiency and trustworthiness. Specifically, VASparse implements a\nvisual-aware token selection strategy during decoding to reduce redundant\ntokens while preserving visual context effectively. Additionally, we\ninnovatively introduce a sparse-based visual contrastive decoding method to\nrecalibrate the distribution of hallucinated outputs without the time overhead\nassociated with secondary decoding. Subsequently, VASparse recalibrates\nattention scores to penalize attention sinking of LVLMs towards text tokens.\nExtensive experiments across four popular benchmarks confirm the effectiveness\nof VASparse in mitigating VH across different LVLM families without requiring\nadditional training or post-processing. Impressively, VASparse achieves\nstate-of-the-art performance for mitigating VH while maintaining competitive\ndecoding speed. Code is available at\nhttps://github.com/mengchuang123/VASparse-github."
                },
                "authors": [
                    {
                        "name": "Xianwei Zhuang"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Yuxin Xie"
                    },
                    {
                        "name": "Liming Liang"
                    },
                    {
                        "name": "Yuexian Zou"
                    }
                ],
                "author_detail": {
                    "name": "Yuexian Zou"
                },
                "author": "Yuexian Zou",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17195v1",
                "updated": "2025-03-21T14:43:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    43,
                    23,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T14:43:23Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    43,
                    23,
                    4,
                    80,
                    0
                ],
                "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided\n  Subspace Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided\n  Subspace Partitioning"
                },
                "summary": "Model customization requires high-quality and diverse datasets, but acquiring\nsuch data remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited\nseed data, model bias and insufficient control over the generation process,\nresulting in limited diversity and biased distribution with the increase of\ndata scales. To tackle this challenge, we present TreeSynth, a tree-guided\nsubspace-based data synthesis framework that recursively partitions the entire\ndata space into hierar-chical subspaces, enabling comprehensive and diverse\nscaling of data synthesis. Briefly, given a task-specific description, we\nconstruct a data space partitioning tree by iteratively executing criteria\ndetermination and subspace coverage steps. This hierarchically divides the\nwhole space (i.e., root node) into mutually exclusive and complementary atomic\nsubspaces (i.e., leaf nodes). By collecting synthesized data according to the\nattributes of each leaf node, we obtain a diverse dataset that fully covers the\ndata space. Empirically, our extensive experiments demonstrate that TreeSynth\nsurpasses both human-designed datasets and the state-of-the-art data synthesis\nbaselines, achieving maximum improvements of 45.2% in data diversity and 17.6%\nin downstream task performance across various models and tasks. Hopefully,\nTreeSynth provides a scalable solution to synthesize diverse and comprehensive\ndatasets from scratch without human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model customization requires high-quality and diverse datasets, but acquiring\nsuch data remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited\nseed data, model bias and insufficient control over the generation process,\nresulting in limited diversity and biased distribution with the increase of\ndata scales. To tackle this challenge, we present TreeSynth, a tree-guided\nsubspace-based data synthesis framework that recursively partitions the entire\ndata space into hierar-chical subspaces, enabling comprehensive and diverse\nscaling of data synthesis. Briefly, given a task-specific description, we\nconstruct a data space partitioning tree by iteratively executing criteria\ndetermination and subspace coverage steps. This hierarchically divides the\nwhole space (i.e., root node) into mutually exclusive and complementary atomic\nsubspaces (i.e., leaf nodes). By collecting synthesized data according to the\nattributes of each leaf node, we obtain a diverse dataset that fully covers the\ndata space. Empirically, our extensive experiments demonstrate that TreeSynth\nsurpasses both human-designed datasets and the state-of-the-art data synthesis\nbaselines, achieving maximum improvements of 45.2% in data diversity and 17.6%\nin downstream task performance across various models and tasks. Hopefully,\nTreeSynth provides a scalable solution to synthesize diverse and comprehensive\ndatasets from scratch without human intervention."
                },
                "authors": [
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Jingqi Zhou"
                    },
                    {
                        "name": "Qintong Li"
                    },
                    {
                        "name": "Jingwei Dong"
                    },
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17194v1",
                "updated": "2025-03-21T14:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    43,
                    11,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T14:43:11Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    43,
                    11,
                    4,
                    80,
                    0
                ],
                "title": "Curriculum RL meets Monte Carlo Planning: Optimization of a Real World\n  Container Management Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curriculum RL meets Monte Carlo Planning: Optimization of a Real World\n  Container Management Problem"
                },
                "summary": "In this work, we augment reinforcement learning with an inference-time\ncollision model to ensure safe and efficient container management in a\nwaste-sorting facility with limited processing capacity. Each container has two\noptimal emptying volumes that trade off higher throughput against overflow\nrisk. Conventional reinforcement learning (RL) approaches struggle under\ndelayed rewards, sparse critical events, and high-dimensional uncertainty --\nfailing to consistently balance higher-volume empties with the risk of\nsafety-limit violations. To address these challenges, we propose a hybrid\nmethod comprising: (1) a curriculum-learning pipeline that incrementally trains\na PPO agent to handle delayed rewards and class imbalance, and (2) an offline\npairwise collision model used at inference time to proactively avert collisions\nwith minimal online cost. Experimental results show that our targeted\ninference-time collision checks significantly improve collision avoidance,\nreduce safety-limit violations, maintain high throughput, and scale effectively\nacross varying container-to-PU ratios. These findings offer actionable\nguidelines for designing safe and efficient container-management systems in\nreal-world facilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we augment reinforcement learning with an inference-time\ncollision model to ensure safe and efficient container management in a\nwaste-sorting facility with limited processing capacity. Each container has two\noptimal emptying volumes that trade off higher throughput against overflow\nrisk. Conventional reinforcement learning (RL) approaches struggle under\ndelayed rewards, sparse critical events, and high-dimensional uncertainty --\nfailing to consistently balance higher-volume empties with the risk of\nsafety-limit violations. To address these challenges, we propose a hybrid\nmethod comprising: (1) a curriculum-learning pipeline that incrementally trains\na PPO agent to handle delayed rewards and class imbalance, and (2) an offline\npairwise collision model used at inference time to proactively avert collisions\nwith minimal online cost. Experimental results show that our targeted\ninference-time collision checks significantly improve collision avoidance,\nreduce safety-limit violations, maintain high throughput, and scale effectively\nacross varying container-to-PU ratios. These findings offer actionable\nguidelines for designing safe and efficient container-management systems in\nreal-world facilities."
                },
                "authors": [
                    {
                        "name": "Abhijeet Pendyala"
                    },
                    {
                        "name": "Tobias Glasmachers"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Glasmachers"
                },
                "author": "Tobias Glasmachers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17183v1",
                "updated": "2025-03-21T14:31:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    31,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T14:31:12Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    31,
                    12,
                    4,
                    80,
                    0
                ],
                "title": "Halfway to the Peak: ice absorption bands at $z\\approx0.5$ with JWST\n  MIRI/MRS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Halfway to the Peak: ice absorption bands at $z\\approx0.5$ with JWST\n  MIRI/MRS"
                },
                "summary": "This paper presents the first combined detections of CO$_2$, CO, XCN and\nwater ices beyond the local Universe. We find gas-phase CO in addition to the\nsolid phase CO. Our source, SSTXFLS J172458.3+591545, is a $z=0.494$\nstar-forming galaxy which also hosts a deeply obscured AGN. The profiles of its\nice features are consistent with those of other Galactic and local galaxy\nsources and the implied ice mantle composition is similar to that of even more\nobscured sources. The ice features indicate the presence of a compact nucleus\nin our galaxy and allow us to place constraints on its density and temperature\n($n>10^5$cm$^{-3}$ and $T=20-90K$). We infer the visual extinction towards this\nnucleus to be $A_V\\approx6-7$. An observed plot of $\\tau_{Si}$ vs.\n$\\tau_{CO2}/\\tau_{Si}$ can be viewed as a probe for both the total dustiness of\na system as well as the clumpiness of the dust along the line of sight. This\npaper highlights the potential of using {\\sl JWST} MIRI spectra to study the\ndust composition and geometric distribution of sources beyond the local\nUniverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the first combined detections of CO$_2$, CO, XCN and\nwater ices beyond the local Universe. We find gas-phase CO in addition to the\nsolid phase CO. Our source, SSTXFLS J172458.3+591545, is a $z=0.494$\nstar-forming galaxy which also hosts a deeply obscured AGN. The profiles of its\nice features are consistent with those of other Galactic and local galaxy\nsources and the implied ice mantle composition is similar to that of even more\nobscured sources. The ice features indicate the presence of a compact nucleus\nin our galaxy and allow us to place constraints on its density and temperature\n($n>10^5$cm$^{-3}$ and $T=20-90K$). We infer the visual extinction towards this\nnucleus to be $A_V\\approx6-7$. An observed plot of $\\tau_{Si}$ vs.\n$\\tau_{CO2}/\\tau_{Si}$ can be viewed as a probe for both the total dustiness of\na system as well as the clumpiness of the dust along the line of sight. This\npaper highlights the potential of using {\\sl JWST} MIRI spectra to study the\ndust composition and geometric distribution of sources beyond the local\nUniverse."
                },
                "authors": [
                    {
                        "name": "Anna Sajina"
                    },
                    {
                        "name": "Alexandra Pope"
                    },
                    {
                        "name": "Henrik Spoon"
                    },
                    {
                        "name": "Lee Armus"
                    },
                    {
                        "name": "Miriam Eleazer"
                    },
                    {
                        "name": "Duncan Farrah"
                    },
                    {
                        "name": "Mark Lacy"
                    },
                    {
                        "name": "Thomas Lai"
                    },
                    {
                        "name": "Jed McKinney"
                    },
                    {
                        "name": "Sylvain Veilleux"
                    },
                    {
                        "name": "Lin Yan"
                    },
                    {
                        "name": "Jason Young"
                    }
                ],
                "author_detail": {
                    "name": "Jason Young"
                },
                "arxiv_affiliation": "Williams & SETI",
                "author": "Jason Young",
                "arxiv_comment": "19 pages, 10 figures, accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17182v1",
                "updated": "2025-03-21T14:29:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    29,
                    42,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T14:29:42Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    29,
                    42,
                    4,
                    80,
                    0
                ],
                "title": "Radar-Guided Polynomial Fitting for Metric Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radar-Guided Polynomial Fitting for Metric Depth Estimation"
                },
                "summary": "We propose PolyRad, a novel radar-guided depth estimation method that\nintroduces polynomial fitting to transform scaleless depth predictions from\npretrained monocular depth estimation (MDE) models into metric depth maps.\nUnlike existing approaches that rely on complex architectures or expensive\nsensors, our method is grounded in a simple yet fundamental insight: using\npolynomial coefficients predicted from cheap, ubiquitous radar data to\nadaptively adjust depth predictions non-uniformly across depth ranges. Although\nMDE models often infer reasonably accurate local depth structure within each\nobject or local region, they may misalign these regions relative to one\nanother, making a linear scale-and-shift transformation insufficient given\nthree or more of these regions. In contrast, PolyRad generalizes beyond linear\ntransformations and is able to correct such misalignments by introducing\ninflection points. Importantly, our polynomial fitting framework preserves\nstructural consistency through a novel training objective that enforces\nmonotonicity via first-derivative regularization. PolyRad achieves\nstate-of-the-art performance on the nuScenes, ZJU-4DRadarCam, and View-of-Delft\ndatasets, outperforming existing methods by 30.3% in MAE and 37.2% in RMSE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose PolyRad, a novel radar-guided depth estimation method that\nintroduces polynomial fitting to transform scaleless depth predictions from\npretrained monocular depth estimation (MDE) models into metric depth maps.\nUnlike existing approaches that rely on complex architectures or expensive\nsensors, our method is grounded in a simple yet fundamental insight: using\npolynomial coefficients predicted from cheap, ubiquitous radar data to\nadaptively adjust depth predictions non-uniformly across depth ranges. Although\nMDE models often infer reasonably accurate local depth structure within each\nobject or local region, they may misalign these regions relative to one\nanother, making a linear scale-and-shift transformation insufficient given\nthree or more of these regions. In contrast, PolyRad generalizes beyond linear\ntransformations and is able to correct such misalignments by introducing\ninflection points. Importantly, our polynomial fitting framework preserves\nstructural consistency through a novel training objective that enforces\nmonotonicity via first-derivative regularization. PolyRad achieves\nstate-of-the-art performance on the nuScenes, ZJU-4DRadarCam, and View-of-Delft\ndatasets, outperforming existing methods by 30.3% in MAE and 37.2% in RMSE."
                },
                "authors": [
                    {
                        "name": "Patrick Rim"
                    },
                    {
                        "name": "Hyoungseob Park"
                    },
                    {
                        "name": "Vadim Ezhov"
                    },
                    {
                        "name": "Jeffrey Moon"
                    },
                    {
                        "name": "Alex Wong"
                    }
                ],
                "author_detail": {
                    "name": "Alex Wong"
                },
                "author": "Alex Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17181v1",
                "updated": "2025-03-21T14:29:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    29,
                    35,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T14:29:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    29,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and\n  Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and\n  Libraries"
                },
                "summary": "Programming language and library choices are crucial to software reliability\nand security. Poor or inconsistent choices can lead to increased technical\ndebt, security vulnerabilities, and even catastrophic failures in\nsafety-critical systems. As Large Language Models (LLMs) play an increasing\nrole in code generation, it is essential to understand how they make these\ndecisions. However, little is known about their preferences when selecting\nprogramming languages and libraries for different coding tasks. To fill this\ngap, this study provides the first in-depth investigation into LLM preferences\nfor programming languages and libraries used when generating code. We assess\nthe preferences of eight diverse LLMs by prompting them to complete various\ncoding tasks, including widely-studied benchmarks and the more practical task\nof generating the initial structural code for new projects (a crucial step that\noften determines a project's language or library choices).\n  Our findings reveal that LLMs heavily favour Python when solving\nlanguage-agnostic problems, using it in 90%-97% of cases for benchmark tasks.\nEven when generating initial project code where Python is not a suitable\nlanguage, it remains the most-used language in 58% of instances. Moreover, LLMs\ncontradict their own language recommendations in 83% of project initialisation\ntasks, raising concerns about their reliability in guiding language selection.\nSimilar biases toward well-established libraries further create serious\ndiscoverability challenges for newer open-source projects. These results\nhighlight the need to improve LLMs' adaptability to diverse programming\ncontexts and to develop mechanisms for mitigating programming language and\nlibrary bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming language and library choices are crucial to software reliability\nand security. Poor or inconsistent choices can lead to increased technical\ndebt, security vulnerabilities, and even catastrophic failures in\nsafety-critical systems. As Large Language Models (LLMs) play an increasing\nrole in code generation, it is essential to understand how they make these\ndecisions. However, little is known about their preferences when selecting\nprogramming languages and libraries for different coding tasks. To fill this\ngap, this study provides the first in-depth investigation into LLM preferences\nfor programming languages and libraries used when generating code. We assess\nthe preferences of eight diverse LLMs by prompting them to complete various\ncoding tasks, including widely-studied benchmarks and the more practical task\nof generating the initial structural code for new projects (a crucial step that\noften determines a project's language or library choices).\n  Our findings reveal that LLMs heavily favour Python when solving\nlanguage-agnostic problems, using it in 90%-97% of cases for benchmark tasks.\nEven when generating initial project code where Python is not a suitable\nlanguage, it remains the most-used language in 58% of instances. Moreover, LLMs\ncontradict their own language recommendations in 83% of project initialisation\ntasks, raising concerns about their reliability in guiding language selection.\nSimilar biases toward well-established libraries further create serious\ndiscoverability challenges for newer open-source projects. These results\nhighlight the need to improve LLMs' adaptability to diverse programming\ncontexts and to develop mechanisms for mitigating programming language and\nlibrary bias."
                },
                "authors": [
                    {
                        "name": "Lukas Twist"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Don Syme"
                    },
                    {
                        "name": "Joost Noppen"
                    },
                    {
                        "name": "Detlef Nauck"
                    }
                ],
                "author_detail": {
                    "name": "Detlef Nauck"
                },
                "author": "Detlef Nauck",
                "arxiv_comment": "12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09352v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09352v2",
                "updated": "2025-03-21T14:29:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    29,
                    27,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-12T15:19:46Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    19,
                    46,
                    3,
                    347,
                    0
                ],
                "title": "Using nebular near-IR spectroscopy to measure asymmetric chemical\n  distributions in 2003fg-like thermonuclear supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using nebular near-IR spectroscopy to measure asymmetric chemical\n  distributions in 2003fg-like thermonuclear supernovae"
                },
                "summary": "We present an analysis of three near-infrared (NIR; 1.0-2.4 $\\mu$m) spectra\nof the SN 2003fg-like/\"super-Chandrasekhar\" type Ia supernovae (SNe Ia) SN\n2009dc, SN 2020hvf, and SN 2022pul at respective phases +372, +296, and +294~d\nrelative to the epoch of $B$-band maximum. We find that all objects in our\nsample have asymmetric, or \"tilted\", [Fe~II] 1.257 and 1.644 $\\mu$m profiles.\nWe quantify the asymmetry of these features using five methods: velocity at\npeak flux, profile tilts, residual testing, velocity fitting, and comparison to\ndeflagration-detonation transition models. Our results demonstrate that, while\nthe profiles of the [Fe II] 1.257 and 1.644 $\\mu$m features are widely varied\nbetween 2003fg-likes, these features are correlated in shape within the same\nSN. This implies that line blending is most likely not the dominant cause of\nthe asymmetries inferred from these profiles. Instead, it is more plausible\nthat 2003fg-like SNe have aspherical chemical distributions in their inner\nregions. These distributions may come from aspherical progenitor systems, such\nas double white dwarf mergers, or off-center delayed-detonation explosions of\nChandrasekhar-mass Carbon-Oxygen white dwarfs. Additional late-phase NIR\nobservation of 2003fg-like SNe and detailed 3-D NLTE modeling of these two\nexplosion scenarios are encouraged.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an analysis of three near-infrared (NIR; 1.0-2.4 $\\mu$m) spectra\nof the SN 2003fg-like/\"super-Chandrasekhar\" type Ia supernovae (SNe Ia) SN\n2009dc, SN 2020hvf, and SN 2022pul at respective phases +372, +296, and +294~d\nrelative to the epoch of $B$-band maximum. We find that all objects in our\nsample have asymmetric, or \"tilted\", [Fe~II] 1.257 and 1.644 $\\mu$m profiles.\nWe quantify the asymmetry of these features using five methods: velocity at\npeak flux, profile tilts, residual testing, velocity fitting, and comparison to\ndeflagration-detonation transition models. Our results demonstrate that, while\nthe profiles of the [Fe II] 1.257 and 1.644 $\\mu$m features are widely varied\nbetween 2003fg-likes, these features are correlated in shape within the same\nSN. This implies that line blending is most likely not the dominant cause of\nthe asymmetries inferred from these profiles. Instead, it is more plausible\nthat 2003fg-like SNe have aspherical chemical distributions in their inner\nregions. These distributions may come from aspherical progenitor systems, such\nas double white dwarf mergers, or off-center delayed-detonation explosions of\nChandrasekhar-mass Carbon-Oxygen white dwarfs. Additional late-phase NIR\nobservation of 2003fg-like SNe and detailed 3-D NLTE modeling of these two\nexplosion scenarios are encouraged."
                },
                "authors": [
                    {
                        "name": "J. O'Hora"
                    },
                    {
                        "name": "C. Ashall"
                    },
                    {
                        "name": "M. Shahbandeh"
                    },
                    {
                        "name": "E. Hsiao"
                    },
                    {
                        "name": "P. Hoeflich"
                    },
                    {
                        "name": "M. D. Stritzinger"
                    },
                    {
                        "name": "L. Galbany"
                    },
                    {
                        "name": "E. Baron"
                    },
                    {
                        "name": "J. DerKacy"
                    },
                    {
                        "name": "S. Kumar"
                    },
                    {
                        "name": "J. Lu"
                    },
                    {
                        "name": "K. Medler"
                    },
                    {
                        "name": "B. Shappee"
                    }
                ],
                "author_detail": {
                    "name": "B. Shappee"
                },
                "author": "B. Shappee",
                "arxiv_comment": "Accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09352v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09352v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12739v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12739v3",
                "updated": "2025-03-21T14:17:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    17,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-19T13:02:54Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    13,
                    2,
                    54,
                    3,
                    263,
                    0
                ],
                "title": "Edu-Values: Towards Evaluating the Chinese Education Values of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edu-Values: Towards Evaluating the Chinese Education Values of Large\n  Language Models"
                },
                "summary": "In this paper, we present Edu-Values, the first Chinese education values\nevaluation benchmark that includes seven core values: professional philosophy,\nteachers' professional ethics, education laws and regulations, cultural\nliteracy, educational knowledge and skills, basic competencies and subject\nknowledge. We meticulously design 1,418 questions, covering multiple-choice,\nmulti-modal question answering, subjective analysis, adversarial prompts, and\nChinese traditional culture (short answer) questions. We conduct human feedback\nbased automatic evaluation over 21 state-of-the-art (SoTA) LLMs, and highlight\nthree main findings: (1) due to differences in educational culture, Chinese\nLLMs outperform English LLMs, with Qwen 2 ranking the first with a score of\n81.37; (2) LLMs often struggle with teachers' professional ethics and\nprofessional philosophy; (3) leveraging Edu-Values to build an external\nknowledge repository for RAG significantly improves LLMs' alignment. This\ndemonstrates the effectiveness of the proposed benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Edu-Values, the first Chinese education values\nevaluation benchmark that includes seven core values: professional philosophy,\nteachers' professional ethics, education laws and regulations, cultural\nliteracy, educational knowledge and skills, basic competencies and subject\nknowledge. We meticulously design 1,418 questions, covering multiple-choice,\nmulti-modal question answering, subjective analysis, adversarial prompts, and\nChinese traditional culture (short answer) questions. We conduct human feedback\nbased automatic evaluation over 21 state-of-the-art (SoTA) LLMs, and highlight\nthree main findings: (1) due to differences in educational culture, Chinese\nLLMs outperform English LLMs, with Qwen 2 ranking the first with a score of\n81.37; (2) LLMs often struggle with teachers' professional ethics and\nprofessional philosophy; (3) leveraging Edu-Values to build an external\nknowledge repository for RAG significantly improves LLMs' alignment. This\ndemonstrates the effectiveness of the proposed benchmark."
                },
                "authors": [
                    {
                        "name": "Peiyi Zhang"
                    },
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Lu Rong"
                    },
                    {
                        "name": "Prayag Tiwari"
                    },
                    {
                        "name": "Jing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qin"
                },
                "author": "Jing Qin",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12739v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12739v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14897v2",
                "updated": "2025-03-21T14:15:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    15,
                    36,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-19T04:48:16Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    48,
                    16,
                    2,
                    78,
                    0
                ],
                "title": "When Domain Generalization meets Generalized Category Discovery: An\n  Adaptive Task-Arithmetic Driven Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Domain Generalization meets Generalized Category Discovery: An\n  Adaptive Task-Arithmetic Driven Approach"
                },
                "summary": "Generalized Class Discovery (GCD) clusters base and novel classes in a target\ndomain using supervision from a source domain with only base classes. Current\nmethods often falter with distribution shifts and typically require access to\ntarget data during training, which can sometimes be impractical. To address\nthis issue, we introduce the novel paradigm of Domain Generalization in GCD\n(DG-GCD), where only source data is available for training, while the target\ndomain, with a distinct data distribution, remains unseen until inference. To\nthis end, our solution, DG2CD-Net, aims to construct a domain-independent,\ndiscriminative embedding space for GCD. The core innovation is an episodic\ntraining strategy that enhances cross-domain generalization by adapting a base\nmodel on tasks derived from source and synthetic domains generated by a\nfoundation model. Each episode focuses on a cross-domain GCD task, diversifying\ntask setups over episodes and combining open-set domain adaptation with a novel\nmargin loss and representation learning for optimizing the feature space\nprogressively. To capture the effects of fine-tuning on the base model, we\nextend task arithmetic by adaptively weighting the local task vectors\nconcerning the fine-tuned models based on their GCD performance on a validation\ndistribution. This episodic update mechanism boosts the adaptability of the\nbase model to unseen targets. Experiments across three datasets confirm that\nDG2CD-Net outperforms existing GCD methods customized for DG-GCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Class Discovery (GCD) clusters base and novel classes in a target\ndomain using supervision from a source domain with only base classes. Current\nmethods often falter with distribution shifts and typically require access to\ntarget data during training, which can sometimes be impractical. To address\nthis issue, we introduce the novel paradigm of Domain Generalization in GCD\n(DG-GCD), where only source data is available for training, while the target\ndomain, with a distinct data distribution, remains unseen until inference. To\nthis end, our solution, DG2CD-Net, aims to construct a domain-independent,\ndiscriminative embedding space for GCD. The core innovation is an episodic\ntraining strategy that enhances cross-domain generalization by adapting a base\nmodel on tasks derived from source and synthetic domains generated by a\nfoundation model. Each episode focuses on a cross-domain GCD task, diversifying\ntask setups over episodes and combining open-set domain adaptation with a novel\nmargin loss and representation learning for optimizing the feature space\nprogressively. To capture the effects of fine-tuning on the base model, we\nextend task arithmetic by adaptively weighting the local task vectors\nconcerning the fine-tuned models based on their GCD performance on a validation\ndistribution. This episodic update mechanism boosts the adaptability of the\nbase model to unseen targets. Experiments across three datasets confirm that\nDG2CD-Net outperforms existing GCD methods customized for DG-GCD."
                },
                "authors": [
                    {
                        "name": "Vaibhav Rathore"
                    },
                    {
                        "name": "Shubhranil B"
                    },
                    {
                        "name": "Saikat Dutta"
                    },
                    {
                        "name": "Sarthak Mehrotra"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Biplab Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Biplab Banerjee"
                },
                "author": "Biplab Banerjee",
                "arxiv_comment": "Accepted at CVPR 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17064v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17064v3",
                "updated": "2025-03-21T13:58:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    58,
                    47,
                    4,
                    80,
                    0
                ],
                "published": "2024-08-30T07:49:35Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    49,
                    35,
                    4,
                    243,
                    0
                ],
                "title": "Instant Adversarial Purification with Adversarial Consistency\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instant Adversarial Purification with Adversarial Consistency\n  Distillation"
                },
                "summary": "Neural networks have revolutionized numerous fields with their exceptional\nperformance, yet they remain susceptible to adversarial attacks through subtle\nperturbations. While diffusion-based purification methods like DiffPure offer\npromising defense mechanisms, their computational overhead presents a\nsignificant practical limitation. In this paper, we introduce One Step Control\nPurification (OSCP), a novel defense framework that achieves robust adversarial\npurification in a single Neural Function Evaluation (NFE) within diffusion\nmodels. We propose Gaussian Adversarial Noise Distillation (GAND) as the\ndistillation objective and Controlled Adversarial Purification (CAP) as the\ninference pipeline, which makes OSCP demonstrate remarkable efficiency while\nmaintaining defense efficacy. Our proposed GAND addresses a fundamental tension\nbetween consistency distillation and adversarial perturbation, bridging the gap\nbetween natural and adversarial manifolds in the latent space, while remaining\ncomputationally efficient through Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA, eliminating the high computational budget request from\nfull parameter fine-tuning. The CAP guides the purification process through the\nunlearnable edge detection operator calculated by the input image as an extra\nprompt, effectively preventing the purified images from deviating from their\noriginal appearance when large purification steps are used. Our experimental\nresults on ImageNet showcase OSCP's superior performance, achieving a 74.19%\ndefense success rate with merely 0.1s per purification -- a 100-fold speedup\ncompared to conventional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have revolutionized numerous fields with their exceptional\nperformance, yet they remain susceptible to adversarial attacks through subtle\nperturbations. While diffusion-based purification methods like DiffPure offer\npromising defense mechanisms, their computational overhead presents a\nsignificant practical limitation. In this paper, we introduce One Step Control\nPurification (OSCP), a novel defense framework that achieves robust adversarial\npurification in a single Neural Function Evaluation (NFE) within diffusion\nmodels. We propose Gaussian Adversarial Noise Distillation (GAND) as the\ndistillation objective and Controlled Adversarial Purification (CAP) as the\ninference pipeline, which makes OSCP demonstrate remarkable efficiency while\nmaintaining defense efficacy. Our proposed GAND addresses a fundamental tension\nbetween consistency distillation and adversarial perturbation, bridging the gap\nbetween natural and adversarial manifolds in the latent space, while remaining\ncomputationally efficient through Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA, eliminating the high computational budget request from\nfull parameter fine-tuning. The CAP guides the purification process through the\nunlearnable edge detection operator calculated by the input image as an extra\nprompt, effectively preventing the purified images from deviating from their\noriginal appearance when large purification steps are used. Our experimental\nresults on ImageNet showcase OSCP's superior performance, achieving a 74.19%\ndefense success rate with merely 0.1s per purification -- a 100-fold speedup\ncompared to conventional approaches."
                },
                "authors": [
                    {
                        "name": "Chun Tong Lei"
                    },
                    {
                        "name": "Hon Ming Yam"
                    },
                    {
                        "name": "Zhongliang Guo"
                    },
                    {
                        "name": "Yifei Qian"
                    },
                    {
                        "name": "Chun Pong Lau"
                    }
                ],
                "author_detail": {
                    "name": "Chun Pong Lau"
                },
                "author": "Chun Pong Lau",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17064v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17064v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11637v2",
                "updated": "2025-03-21T13:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    55,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-10-15T14:28:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    28,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "Prediction-Centric Uncertainty Quantification via MMD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction-Centric Uncertainty Quantification via MMD"
                },
                "summary": "Deterministic mathematical models, such as those specified via differential\nequations, are a powerful tool to communicate scientific insight. However, such\nmodels are necessarily simplified descriptions of the real world. Generalised\nBayesian methodologies have been proposed for inference with misspecified\nmodels, but these are typically associated with vanishing parameter uncertainty\nas more data are observed. In the context of a misspecified deterministic\nmathematical model, this has the undesirable consequence that posterior\npredictions become deterministic and certain, while being incorrect. Taking\nthis observation as a starting point, we propose Prediction-Centric Uncertainty\nQuantification, where a mixture distribution based on the deterministic model\nconfers improved uncertainty quantification in the predictive context.\nComputation of the mixing distribution is cast as a (regularised) gradient flow\nof the maximum mean discrepancy (MMD), enabling consistent numerical\napproximations to be obtained. Results are reported on both a toy model from\npopulation ecology and a real model of protein signalling in cell biology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deterministic mathematical models, such as those specified via differential\nequations, are a powerful tool to communicate scientific insight. However, such\nmodels are necessarily simplified descriptions of the real world. Generalised\nBayesian methodologies have been proposed for inference with misspecified\nmodels, but these are typically associated with vanishing parameter uncertainty\nas more data are observed. In the context of a misspecified deterministic\nmathematical model, this has the undesirable consequence that posterior\npredictions become deterministic and certain, while being incorrect. Taking\nthis observation as a starting point, we propose Prediction-Centric Uncertainty\nQuantification, where a mixture distribution based on the deterministic model\nconfers improved uncertainty quantification in the predictive context.\nComputation of the mixing distribution is cast as a (regularised) gradient flow\nof the maximum mean discrepancy (MMD), enabling consistent numerical\napproximations to be obtained. Results are reported on both a toy model from\npopulation ecology and a real model of protein signalling in cell biology."
                },
                "authors": [
                    {
                        "name": "Zheyang Shen"
                    },
                    {
                        "name": "Jeremias Knoblauch"
                    },
                    {
                        "name": "Sam Power"
                    },
                    {
                        "name": "Chris. J. Oates"
                    }
                ],
                "author_detail": {
                    "name": "Chris. J. Oates"
                },
                "author": "Chris. J. Oates",
                "arxiv_comment": "Typos corrected. Accepted for AISTATS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16021v2",
                "updated": "2025-03-21T13:35:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    35,
                    52,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T10:37:29Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    37,
                    29,
                    3,
                    79,
                    0
                ],
                "title": "Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems"
                },
                "summary": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's impact on the\ndiversity and democratic value of information ecosystems. We introduce a\nlarge-scale simulation framework to examine AI-based imitation within news, a\ncontext crucial for public discourse. By systematically testing two distinct\nimitation strategies across a range of information environments varying in\ninitial diversity, we demonstrate that AI-generated articles do not uniformly\nhomogenize content. Instead, AI's influence is strongly context-dependent:\nAI-generated content can introduce valuable diversity in originally homogeneous\nnews environments but diminish diversity in initially heterogeneous contexts.\nThese results illustrate that the initial diversity of an information\nenvironment critically shapes AI's impact, challenging assumptions that\nAI-driven imitation uniformly threatens diversity. Instead, when information is\ninitially homogeneous, AI-driven imitation can expand perspectives, styles, and\ntopics. This is especially important in news contexts, where information\ndiversity fosters richer public debate by exposing citizens to alternative\nviewpoints, challenging biases, and preventing narrative monopolies, which is\nessential for a resilient democracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's impact on the\ndiversity and democratic value of information ecosystems. We introduce a\nlarge-scale simulation framework to examine AI-based imitation within news, a\ncontext crucial for public discourse. By systematically testing two distinct\nimitation strategies across a range of information environments varying in\ninitial diversity, we demonstrate that AI-generated articles do not uniformly\nhomogenize content. Instead, AI's influence is strongly context-dependent:\nAI-generated content can introduce valuable diversity in originally homogeneous\nnews environments but diminish diversity in initially heterogeneous contexts.\nThese results illustrate that the initial diversity of an information\nenvironment critically shapes AI's impact, challenging assumptions that\nAI-driven imitation uniformly threatens diversity. Instead, when information is\ninitially homogeneous, AI-driven imitation can expand perspectives, styles, and\ntopics. This is especially important in news contexts, where information\ndiversity fosters richer public debate by exposing citizens to alternative\nviewpoints, challenging biases, and preventing narrative monopolies, which is\nessential for a resilient democracy."
                },
                "authors": [
                    {
                        "name": "Emil Bakkensen Johansen"
                    },
                    {
                        "name": "Oliver Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Baumann"
                },
                "author": "Oliver Baumann",
                "arxiv_comment": "35 pages, 10 figures, 4 tables; v2: corrected typographical errors,\n  streamlined language, updated abstract, added supplementary information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16278v2",
                "updated": "2025-03-21T13:32:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    32,
                    47,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T16:07:04Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    7,
                    4,
                    3,
                    79,
                    0
                ],
                "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens"
                },
                "summary": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding (3D GU) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates 3D GU tasks via autoregressive prediction.\nAt its core, Uni-3DAR employs a novel hierarchical tokenization that compresses\n3D space using an octree, leveraging the inherent sparsity of 3D structures. It\nthen applies an additional tokenization for fine-grained structural details,\ncapturing key attributes such as atom types and precise spatial coordinates in\nmicroscopic 3D structures. We further propose two optimizations to enhance\nefficiency and effectiveness. The first is a two-level subtree compression\nstrategy, which reduces the octree token sequence by up to 8x. The second is a\nmasked next-token prediction mechanism tailored for dynamically varying token\npositions, significantly boosting model performance. By combining these\nstrategies, Uni-3DAR successfully unifies diverse 3D GU tasks within a single\nautoregressive framework. Extensive experiments across multiple microscopic 3D\nGU tasks, including molecules, proteins, polymers, and crystals, validate its\neffectiveness and versatility. Notably, Uni-3DAR surpasses previous\nstate-of-the-art diffusion models by a substantial margin, achieving up to\n256\\% relative improvement while delivering inference speeds up to 21.8x\nfaster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding (3D GU) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates 3D GU tasks via autoregressive prediction.\nAt its core, Uni-3DAR employs a novel hierarchical tokenization that compresses\n3D space using an octree, leveraging the inherent sparsity of 3D structures. It\nthen applies an additional tokenization for fine-grained structural details,\ncapturing key attributes such as atom types and precise spatial coordinates in\nmicroscopic 3D structures. We further propose two optimizations to enhance\nefficiency and effectiveness. The first is a two-level subtree compression\nstrategy, which reduces the octree token sequence by up to 8x. The second is a\nmasked next-token prediction mechanism tailored for dynamically varying token\npositions, significantly boosting model performance. By combining these\nstrategies, Uni-3DAR successfully unifies diverse 3D GU tasks within a single\nautoregressive framework. Extensive experiments across multiple microscopic 3D\nGU tasks, including molecules, proteins, polymers, and crystals, validate its\neffectiveness and versatility. Notably, Uni-3DAR surpasses previous\nstate-of-the-art diffusion models by a substantial margin, achieving up to\n256\\% relative improvement while delivering inference speeds up to 21.8x\nfaster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR."
                },
                "authors": [
                    {
                        "name": "Shuqi Lu"
                    },
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Lin Yao"
                    },
                    {
                        "name": "Zhifeng Gao"
                    },
                    {
                        "name": "Xiaohong Ji"
                    },
                    {
                        "name": "Weinan E"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17126v1",
                "updated": "2025-03-21T13:21:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    21,
                    45,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T13:21:45Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    21,
                    45,
                    4,
                    80,
                    0
                ],
                "title": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing"
                },
                "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO."
                },
                "authors": [
                    {
                        "name": "John Joon Young Chung"
                    },
                    {
                        "name": "Vishakh Padmakumar"
                    },
                    {
                        "name": "Melissa Roemmele"
                    },
                    {
                        "name": "Yuqian Sun"
                    },
                    {
                        "name": "Max Kreminski"
                    }
                ],
                "author_detail": {
                    "name": "Max Kreminski"
                },
                "author": "Max Kreminski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11302v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11302v3",
                "updated": "2025-03-21T13:15:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    15,
                    27,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-14T11:11:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "Are formal and functional linguistic mechanisms dissociated in language\n  models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are formal and functional linguistic mechanisms dissociated in language\n  models?"
                },
                "summary": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist."
                },
                "authors": [
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pezzelle"
                },
                "author": "Sandro Pezzelle",
                "arxiv_comment": "35 pages, 10 figures, 3 tables. Code available at\n  https://github.com/hannamw/formal-functional-dissociation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11302v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11302v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04077v2",
                "updated": "2025-03-21T13:03:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    3,
                    59,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-05T11:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    11,
                    17,
                    57,
                    3,
                    340,
                    0
                ],
                "title": "SoMA: Singular Value Decomposed Minor Components Adaptation for Domain\n  Generalizable Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoMA: Singular Value Decomposed Minor Components Adaptation for Domain\n  Generalizable Representation Learning"
                },
                "summary": "Domain generalization (DG) aims to adapt a model using one or multiple source\ndomains to ensure robust performance in unseen target domains. Recently,\nParameter-Efficient Fine-Tuning (PEFT) of foundation models has shown promising\nresults in the context of DG problem. Nevertheless, existing PEFT methods still\nstruggle to strike a balance between preserving generalizable components of the\npre-trained model and learning task-specific features. To gain insights into\nthe distribution of generalizable components, we begin by analyzing the\npre-trained weights through the lens of singular value decomposition. Building\non these insights, we introduce Singular Value Decomposed Minor Components\nAdaptation (SoMA), an approach that selectively tunes minor singular components\nwhile keeping the residual parts frozen. SoMA effectively retains the\ngeneralization ability of the pre-trained model while efficiently acquiring\ntask-specific skills. Moreover, we freeze domain-generalizable blocks and\nemploy an annealing weight decay strategy, thereby achieving an optimal balance\nin the delicate trade-off between generalizability and discriminability. SoMA\nattains state-of-the-art results on multiple benchmarks that span both domain\ngeneralized semantic segmentation to domain generalized object detection. In\naddition, our methods introduce no additional inference overhead or\nregularization loss, maintain compatibility with any backbone or head, and are\ndesigned to be versatile, allowing easy integration into a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain generalization (DG) aims to adapt a model using one or multiple source\ndomains to ensure robust performance in unseen target domains. Recently,\nParameter-Efficient Fine-Tuning (PEFT) of foundation models has shown promising\nresults in the context of DG problem. Nevertheless, existing PEFT methods still\nstruggle to strike a balance between preserving generalizable components of the\npre-trained model and learning task-specific features. To gain insights into\nthe distribution of generalizable components, we begin by analyzing the\npre-trained weights through the lens of singular value decomposition. Building\non these insights, we introduce Singular Value Decomposed Minor Components\nAdaptation (SoMA), an approach that selectively tunes minor singular components\nwhile keeping the residual parts frozen. SoMA effectively retains the\ngeneralization ability of the pre-trained model while efficiently acquiring\ntask-specific skills. Moreover, we freeze domain-generalizable blocks and\nemploy an annealing weight decay strategy, thereby achieving an optimal balance\nin the delicate trade-off between generalizability and discriminability. SoMA\nattains state-of-the-art results on multiple benchmarks that span both domain\ngeneralized semantic segmentation to domain generalized object detection. In\naddition, our methods introduce no additional inference overhead or\nregularization loss, maintain compatibility with any backbone or head, and are\ndesigned to be versatile, allowing easy integration into a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Seokju Yun"
                    },
                    {
                        "name": "Seunghye Chae"
                    },
                    {
                        "name": "Dongheon Lee"
                    },
                    {
                        "name": "Youngmin Ro"
                    }
                ],
                "author_detail": {
                    "name": "Youngmin Ro"
                },
                "author": "Youngmin Ro",
                "arxiv_comment": "CVPR 2025 Project page: https://ysj9909.github.io/SoRA.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15289v2",
                "updated": "2025-03-21T13:00:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    0,
                    44,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-19T05:57:37Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    5,
                    57,
                    37,
                    3,
                    354,
                    0
                ],
                "title": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage"
                },
                "summary": "Large language models (LLMs) have made significant advancements across\nvarious tasks, but their safety alignment remain a major concern. Exploring\njailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure\nthem. Existing methods primarily design sophisticated instructions for the LLM\nto follow, or rely on multiple iterations, which could hinder the performance\nand efficiency of jailbreaks. In this work, we propose a novel jailbreak\nparadigm, Simple Assistive Task Linkage (SATA), which can effectively\ncircumvent LLM safeguards and elicit harmful responses. Specifically, SATA\nfirst masks harmful keywords within a malicious query to generate a relatively\nbenign query containing one or multiple [MASK] special tokens. It then employs\na simple assistive task such as a masked language model task or an element\nlookup by position task to encode the semantics of the masked keywords.\nFinally, SATA links the assistive task with the masked query to jointly perform\nthe jailbreak. Extensive experiments show that SATA achieves state-of-the-art\nperformance and outperforms baselines by a large margin. Specifically, on\nAdvBench dataset, with mask language model (MLM) assistive task, SATA achieves\nan overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and\nwith element lookup by position (ELP) assistive task, SATA attains an overall\nASR of 76% and HS of 4.43.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advancements across\nvarious tasks, but their safety alignment remain a major concern. Exploring\njailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure\nthem. Existing methods primarily design sophisticated instructions for the LLM\nto follow, or rely on multiple iterations, which could hinder the performance\nand efficiency of jailbreaks. In this work, we propose a novel jailbreak\nparadigm, Simple Assistive Task Linkage (SATA), which can effectively\ncircumvent LLM safeguards and elicit harmful responses. Specifically, SATA\nfirst masks harmful keywords within a malicious query to generate a relatively\nbenign query containing one or multiple [MASK] special tokens. It then employs\na simple assistive task such as a masked language model task or an element\nlookup by position task to encode the semantics of the masked keywords.\nFinally, SATA links the assistive task with the masked query to jointly perform\nthe jailbreak. Extensive experiments show that SATA achieves state-of-the-art\nperformance and outperforms baselines by a large margin. Specifically, on\nAdvBench dataset, with mask language model (MLM) assistive task, SATA achieves\nan overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and\nwith element lookup by position (ELP) assistive task, SATA attains an overall\nASR of 76% and HS of 4.43."
                },
                "authors": [
                    {
                        "name": "Xiaoning Dong"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17935v2",
                "updated": "2025-03-21T12:56:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    56,
                    31,
                    4,
                    80,
                    0
                ],
                "published": "2024-10-23T15:00:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    0,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Semi-Implicit Functional Gradient Flow for Efficient Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Implicit Functional Gradient Flow for Efficient Sampling"
                },
                "summary": "Particle-based variational inference methods (ParVIs) use nonparametric\nvariational families represented by particles to approximate the target\ndistribution according to the kernelized Wasserstein gradient flow for the\nKullback-Leibler (KL) divergence. Although functional gradient flows have been\nintroduced to expand the kernel space for better flexibility, the deterministic\nupdating mechanism may limit exploration and require expensive repetitive runs\nfor new samples. In this paper, we propose Semi-Implicit Functional Gradient\nflow (SIFG), a functional gradient ParVI method that uses perturbed particles\nwith Gaussian noise as the approximation family. We show that the corresponding\nfunctional gradient flow, which can be estimated via denoising score matching\nwith neural networks, exhibits strong theoretical convergence guarantees due to\na higher-order smoothness brought to the approximation family via Gaussian\nperturbation. In addition, we present an adaptive version of our method that\nautomatically selects the appropriate noise magnitude during sampling, striking\na good balance between exploration efficiency and approximation accuracy.\nExtensive experiments on both simulated and real-world datasets demonstrate the\neffectiveness and efficiency of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle-based variational inference methods (ParVIs) use nonparametric\nvariational families represented by particles to approximate the target\ndistribution according to the kernelized Wasserstein gradient flow for the\nKullback-Leibler (KL) divergence. Although functional gradient flows have been\nintroduced to expand the kernel space for better flexibility, the deterministic\nupdating mechanism may limit exploration and require expensive repetitive runs\nfor new samples. In this paper, we propose Semi-Implicit Functional Gradient\nflow (SIFG), a functional gradient ParVI method that uses perturbed particles\nwith Gaussian noise as the approximation family. We show that the corresponding\nfunctional gradient flow, which can be estimated via denoising score matching\nwith neural networks, exhibits strong theoretical convergence guarantees due to\na higher-order smoothness brought to the approximation family via Gaussian\nperturbation. In addition, we present an adaptive version of our method that\nautomatically selects the appropriate noise magnitude during sampling, striking\na good balance between exploration efficiency and approximation accuracy.\nExtensive experiments on both simulated and real-world datasets demonstrate the\neffectiveness and efficiency of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Shiyue Zhang"
                    },
                    {
                        "name": "Ziheng Cheng"
                    },
                    {
                        "name": "Cheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Zhang"
                },
                "author": "Cheng Zhang",
                "arxiv_comment": "46 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18008v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18008v5",
                "updated": "2025-03-21T12:53:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    53,
                    4,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-25T09:12:07Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    12,
                    7,
                    1,
                    56,
                    0
                ],
                "title": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms"
                },
                "summary": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation."
                },
                "authors": [
                    {
                        "name": "Yashan Wang"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Jianhuai Hu"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Yueqi Peng"
                    },
                    {
                        "name": "Yongxin Huang"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Xiaobing Li"
                    },
                    {
                        "name": "Feng Yu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18008v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18008v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00363v2",
                "updated": "2025-03-21T12:52:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    52,
                    57,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-31T09:29:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    29,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "SPDZCoder: Combining Expert Knowledge with LLMs for Generating\n  Privacy-Computing Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPDZCoder: Combining Expert Knowledge with LLMs for Generating\n  Privacy-Computing Code"
                },
                "summary": "Privacy computing receives increasing attention but writing privacy computing\ncode remains challenging for developers due to limited library functions,\nnecessitating function implementation from scratch, and data-oblivious\nrequirement, contradicting intuitive thinking and usual practices of\nprogrammers. Automating the generation of privacy computing code with Large\nLanguage Models can streamline development effort and lower the barrier to\nusing privacy computing frameworks. However, existing LLMs still encounter\nchallenges in code translation for privacy-preserving computation, such as\ntranslating Python to MP-SPDZ, due to the scarcity of MP-SPDZ data required for\neffective pre-training or fine-tuning. Moreover, the lack of a benchmark\nfurther complicates the evaluation of translation quality. To address the\nlimitations, this work proposes SPDZCoder, a rule-based framework that combines\nLLMs with expert knowledge for generating privacy-computing code without\nrequiring additional training data. Specifically, SPDZCoder employ a rigorous\nprocedure for collecting high-quality expert knowledge to represent the\nsemantic-expressing differences between Python and MP-SPDZ, and to derive\ntransformation rules for translating Python to MP-SPDZ based on these\nknowledge. Then, SPDZCoder progressively converts Python code into MP-SPDZ code\nusing transformation rules in a three stage pipeline. To evaluate SPDZCoder, we\nmanually constructed a benchmark dataset, SPDZEval, which comprises six data\nsplits, each representing a distinct class of challenging tasks in MP-SPDZ\nimplementation. Extensive experiments show that SPDZCoder achieves superior\nperformance, significantly surpassing baselines in pass@1 and pass@2.\nSpecifically, SPDZCoder attains an overall correctness of 85.94% and 92.01% in\npass@1 and pass@2, respectively, whereas the best-performing baseline achieves\n63.58% and 76.36%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy computing receives increasing attention but writing privacy computing\ncode remains challenging for developers due to limited library functions,\nnecessitating function implementation from scratch, and data-oblivious\nrequirement, contradicting intuitive thinking and usual practices of\nprogrammers. Automating the generation of privacy computing code with Large\nLanguage Models can streamline development effort and lower the barrier to\nusing privacy computing frameworks. However, existing LLMs still encounter\nchallenges in code translation for privacy-preserving computation, such as\ntranslating Python to MP-SPDZ, due to the scarcity of MP-SPDZ data required for\neffective pre-training or fine-tuning. Moreover, the lack of a benchmark\nfurther complicates the evaluation of translation quality. To address the\nlimitations, this work proposes SPDZCoder, a rule-based framework that combines\nLLMs with expert knowledge for generating privacy-computing code without\nrequiring additional training data. Specifically, SPDZCoder employ a rigorous\nprocedure for collecting high-quality expert knowledge to represent the\nsemantic-expressing differences between Python and MP-SPDZ, and to derive\ntransformation rules for translating Python to MP-SPDZ based on these\nknowledge. Then, SPDZCoder progressively converts Python code into MP-SPDZ code\nusing transformation rules in a three stage pipeline. To evaluate SPDZCoder, we\nmanually constructed a benchmark dataset, SPDZEval, which comprises six data\nsplits, each representing a distinct class of challenging tasks in MP-SPDZ\nimplementation. Extensive experiments show that SPDZCoder achieves superior\nperformance, significantly surpassing baselines in pass@1 and pass@2.\nSpecifically, SPDZCoder attains an overall correctness of 85.94% and 92.01% in\npass@1 and pass@2, respectively, whereas the best-performing baseline achieves\n63.58% and 76.36%, respectively."
                },
                "authors": [
                    {
                        "name": "Xiaoning Dong"
                    },
                    {
                        "name": "Peilin Xin"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10332v3",
                "updated": "2025-03-21T12:40:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    40,
                    26,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-15T16:32:34Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    32,
                    34,
                    4,
                    320,
                    0
                ],
                "title": "Number it: Temporal Grounding Videos like Flipping Manga",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number it: Temporal Grounding Videos like Flipping Manga"
                },
                "summary": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro."
                },
                "authors": [
                    {
                        "name": "Yongliang Wu"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Yuyang Sun"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Wenbo Zhu"
                    },
                    {
                        "name": "Fengyun Rao"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16379v2",
                "updated": "2025-03-21T12:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    40,
                    7,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T17:42:11Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    42,
                    11,
                    3,
                    79,
                    0
                ],
                "title": "The impact of baryons on the sparsity of simulated galaxy clusters from\n  The Three Hundred Project",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of baryons on the sparsity of simulated galaxy clusters from\n  The Three Hundred Project"
                },
                "summary": "Measurements of the sparsity of galaxy clusters can be used to probe the\ncosmological information encoded in the host dark matter halo profile, and\ninfer constraints on the cosmological model parameters. Key to the success of\nthese analyses is the control of potential sources of systematic uncertainty.\nAs an example, the presence of baryons can alter the cluster sparsity with\nrespect to predictions from N-body simulations. Similarly, a radial dependent\nmass bias, as in the case of masses inferred under the hydrostatic equilibrium\n(HE) hypothesis, can affect sparsity estimates. We examine the imprint of\nbaryonic processes on the sparsity statistics. Then, we investigate the\nrelation between cluster sparsities and gas mass fraction. Finally, we perform\na study of the impact of HE mass bias on sparsity measurements and the\nimplication on cosmological parameter inference analyses. We use catalogues of\nsimulated galaxy clusters from The Three Hundred project and run a comparative\nanalysis of the sparsity of clusters from N-body/hydro simulations implementing\ndifferent feedback model scenarios. Sparsities which probe the mass profile\nacross a large radial range are affected by the presence of baryons in a way\nthat is particularly sensitive to astrophysical feedback, whereas those probing\nexclusively external cluster regions are less affected. In the former case, we\nfind the sparsities to be moderately correlated with measurements of the gas\nfraction in the inner cluster regions. We infer constraints on $S_8$ using\nsynthetic average sparsity measurements generated to evaluate the impact of\nbaryons, selection effects and HE bias. In the case of multiple sparsities\nthese lead to highly bias results. Hence, we calibrate linear bias models that\nenable us to correct for these effects and recover unbiased constraints that\nare significantly tighter than those inferred from single sparsity analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements of the sparsity of galaxy clusters can be used to probe the\ncosmological information encoded in the host dark matter halo profile, and\ninfer constraints on the cosmological model parameters. Key to the success of\nthese analyses is the control of potential sources of systematic uncertainty.\nAs an example, the presence of baryons can alter the cluster sparsity with\nrespect to predictions from N-body simulations. Similarly, a radial dependent\nmass bias, as in the case of masses inferred under the hydrostatic equilibrium\n(HE) hypothesis, can affect sparsity estimates. We examine the imprint of\nbaryonic processes on the sparsity statistics. Then, we investigate the\nrelation between cluster sparsities and gas mass fraction. Finally, we perform\na study of the impact of HE mass bias on sparsity measurements and the\nimplication on cosmological parameter inference analyses. We use catalogues of\nsimulated galaxy clusters from The Three Hundred project and run a comparative\nanalysis of the sparsity of clusters from N-body/hydro simulations implementing\ndifferent feedback model scenarios. Sparsities which probe the mass profile\nacross a large radial range are affected by the presence of baryons in a way\nthat is particularly sensitive to astrophysical feedback, whereas those probing\nexclusively external cluster regions are less affected. In the former case, we\nfind the sparsities to be moderately correlated with measurements of the gas\nfraction in the inner cluster regions. We infer constraints on $S_8$ using\nsynthetic average sparsity measurements generated to evaluate the impact of\nbaryons, selection effects and HE bias. In the case of multiple sparsities\nthese lead to highly bias results. Hence, we calibrate linear bias models that\nenable us to correct for these effects and recover unbiased constraints that\nare significantly tighter than those inferred from single sparsity analyses."
                },
                "authors": [
                    {
                        "name": "P. S. Corasaniti"
                    },
                    {
                        "name": "T. R. G. Richardson"
                    },
                    {
                        "name": "S. Ettori"
                    },
                    {
                        "name": "M. De Petris"
                    },
                    {
                        "name": "E. Rasia"
                    },
                    {
                        "name": "W. Cui"
                    },
                    {
                        "name": "G. Yepes"
                    },
                    {
                        "name": "G. Gianfagna"
                    },
                    {
                        "name": "A. M. C. Le Bun"
                    },
                    {
                        "name": "Y. Rasera"
                    }
                ],
                "author_detail": {
                    "name": "Y. Rasera"
                },
                "author": "Y. Rasera",
                "arxiv_comment": "23 pages, 29 figures, accepted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17101v1",
                "updated": "2025-03-21T12:39:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    39,
                    16,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T12:39:16Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    39,
                    16,
                    4,
                    80,
                    0
                ],
                "title": "Large Language Model Compression via the Nested Activation-Aware\n  Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Compression via the Nested Activation-Aware\n  Decomposition"
                },
                "summary": "In this paper, we tackle the critical challenge of compressing large language\nmodels (LLMs) to facilitate their practical deployment and broader adoption. We\nintroduce a novel post-training compression paradigm that focuses on low-rank\ndecomposition of LLM weights. Our analysis identifies two main challenges in\nthis task: the variability in LLM activation distributions and handling unseen\nactivations from different datasets and models.\n  To address these challenges, we propose a nested activation-aware framework\n(NSVD) for LLMs, a training-free approach designed to enhance the accuracy of\nlow-rank decompositions by managing activation outliers through transforming\nthe weight matrix based on activation distribution and the original weight\nmatrix. This method allows for the absorption of outliers into the transformed\nweight matrix, improving decomposition accuracy. Our comprehensive evaluation\nacross eight datasets and six models from three distinct LLM families\ndemonstrates the superiority of NSVD over current state-of-the-art methods,\nespecially at medium to large compression ratios or in multilingual and\nmultitask settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we tackle the critical challenge of compressing large language\nmodels (LLMs) to facilitate their practical deployment and broader adoption. We\nintroduce a novel post-training compression paradigm that focuses on low-rank\ndecomposition of LLM weights. Our analysis identifies two main challenges in\nthis task: the variability in LLM activation distributions and handling unseen\nactivations from different datasets and models.\n  To address these challenges, we propose a nested activation-aware framework\n(NSVD) for LLMs, a training-free approach designed to enhance the accuracy of\nlow-rank decompositions by managing activation outliers through transforming\nthe weight matrix based on activation distribution and the original weight\nmatrix. This method allows for the absorption of outliers into the transformed\nweight matrix, improving decomposition accuracy. Our comprehensive evaluation\nacross eight datasets and six models from three distinct LLM families\ndemonstrates the superiority of NSVD over current state-of-the-art methods,\nespecially at medium to large compression ratios or in multilingual and\nmultitask settings."
                },
                "authors": [
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Tianyi Xu"
                    },
                    {
                        "name": "Bill Ding"
                    },
                    {
                        "name": "David Li"
                    },
                    {
                        "name": "Yu Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Kang"
                },
                "author": "Yu Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16801v2",
                "updated": "2025-03-21T12:34:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    34,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-10-22T08:27:23Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    8,
                    27,
                    23,
                    1,
                    296,
                    0
                ],
                "title": "Controlled Low-Rank Adaptation with Subspace Regularization for\n  Continued Training on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlled Low-Rank Adaptation with Subspace Regularization for\n  Continued Training on Large Language Models"
                },
                "summary": "Large language models (LLMs) exhibit remarkable capabilities in natural\nlanguage processing but face catastrophic forgetting when learning new tasks,\nwhere adaptation to a new domain leads to a substantial decline in performance\non previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a\nsub-space regularization method on LoRA structure. Aiming to reduce the scale\nof output change while introduce minimal constraint on model capacity, CLoRA\nimposes constraint on the direction of updating matrix's null space.\nExperimental results on one-stage LLM finetuning tasks and continual learning\nsettings highlight the superority of CLoRA as a effective parameter efficient\nfinetuning method with catastrophic forgetting mitigating.Further investigation\nfor model parameters indicates that CLoRA effectively balances the trade-off\nbetween model capacity and degree of forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable capabilities in natural\nlanguage processing but face catastrophic forgetting when learning new tasks,\nwhere adaptation to a new domain leads to a substantial decline in performance\non previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a\nsub-space regularization method on LoRA structure. Aiming to reduce the scale\nof output change while introduce minimal constraint on model capacity, CLoRA\nimposes constraint on the direction of updating matrix's null space.\nExperimental results on one-stage LLM finetuning tasks and continual learning\nsettings highlight the superority of CLoRA as a effective parameter efficient\nfinetuning method with catastrophic forgetting mitigating.Further investigation\nfor model parameters indicates that CLoRA effectively balances the trade-off\nbetween model capacity and degree of forgetting."
                },
                "authors": [
                    {
                        "name": "Yuheng Lu"
                    },
                    {
                        "name": "Bingshuo Qian"
                    },
                    {
                        "name": "Caixia Yuan"
                    },
                    {
                        "name": "Huixing Jiang"
                    },
                    {
                        "name": "Xiaojie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Wang"
                },
                "author": "Xiaojie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17076v1",
                "updated": "2025-03-21T12:00:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    0,
                    59,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T12:00:59Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    0,
                    59,
                    4,
                    80,
                    0
                ],
                "title": "Halton Scheduler For Masked Generative Image Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Halton Scheduler For Masked Generative Image Transformer"
                },
                "summary": "Masked Generative Image Transformers (MaskGIT) have emerged as a scalable and\nefficient image generation framework, able to deliver high-quality visuals with\nlow inference costs. However, MaskGIT's token unmasking scheduler, an essential\ncomponent of the framework, has not received the attention it deserves. We\nanalyze the sampling objective in MaskGIT, based on the mutual information\nbetween tokens, and elucidate its shortcomings. We then propose a new sampling\nstrategy based on our Halton scheduler instead of the original Confidence\nscheduler. More precisely, our method selects the token's position according to\na quasi-random, low-discrepancy Halton sequence. Intuitively, that method\nspreads the tokens spatially, progressively covering the image uniformly at\neach step. Our analysis shows that it allows reducing non-recoverable sampling\nerrors, leading to simpler hyper-parameters tuning and better quality images.\nOur scheduler does not require retraining or noise injection and may serve as a\nsimple drop-in replacement for the original sampling strategy. Evaluation of\nboth class-to-image synthesis on ImageNet and text-to-image generation on the\nCOCO dataset demonstrates that the Halton scheduler outperforms the Confidence\nscheduler quantitatively by reducing the FID and qualitatively by generating\nmore diverse and more detailed images. Our code is at\nhttps://github.com/valeoai/Halton-MaskGIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Image Transformers (MaskGIT) have emerged as a scalable and\nefficient image generation framework, able to deliver high-quality visuals with\nlow inference costs. However, MaskGIT's token unmasking scheduler, an essential\ncomponent of the framework, has not received the attention it deserves. We\nanalyze the sampling objective in MaskGIT, based on the mutual information\nbetween tokens, and elucidate its shortcomings. We then propose a new sampling\nstrategy based on our Halton scheduler instead of the original Confidence\nscheduler. More precisely, our method selects the token's position according to\na quasi-random, low-discrepancy Halton sequence. Intuitively, that method\nspreads the tokens spatially, progressively covering the image uniformly at\neach step. Our analysis shows that it allows reducing non-recoverable sampling\nerrors, leading to simpler hyper-parameters tuning and better quality images.\nOur scheduler does not require retraining or noise injection and may serve as a\nsimple drop-in replacement for the original sampling strategy. Evaluation of\nboth class-to-image synthesis on ImageNet and text-to-image generation on the\nCOCO dataset demonstrates that the Halton scheduler outperforms the Confidence\nscheduler quantitatively by reducing the FID and qualitatively by generating\nmore diverse and more detailed images. Our code is at\nhttps://github.com/valeoai/Halton-MaskGIT."
                },
                "authors": [
                    {
                        "name": "Victor Besnier"
                    },
                    {
                        "name": "Mickael Chen"
                    },
                    {
                        "name": "David Hurych"
                    },
                    {
                        "name": "Eduardo Valle"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17073v1",
                "updated": "2025-03-21T11:56:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    11,
                    56,
                    17,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T11:56:17Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    11,
                    56,
                    17,
                    4,
                    80,
                    0
                ],
                "title": "A Study into Investigating Temporal Robustness of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study into Investigating Temporal Robustness of LLMs"
                },
                "summary": "Large Language Models (LLMs) encapsulate a surprising amount of factual world\nknowledge. However, their performance on temporal questions and historical\nknowledge is limited because they often cannot understand temporal scope and\norientation or neglect the temporal aspect altogether. In this study, we aim to\nmeasure precisely how robust LLMs are for question answering based on their\nability to process temporal information and perform tasks requiring temporal\nreasoning and temporal factual knowledge. Specifically, we design eight\ntime-sensitive robustness tests for factual information to check the\nsensitivity of six popular LLMs in the zero-shot setting. Overall, we find LLMs\nlacking temporal robustness, especially to temporal reformulations and the use\nof different granularities of temporal references. We show how a selection of\nthese eight tests can be used automatically to judge a model's temporal\nrobustness for user questions on the fly. Finally, we apply the findings of\nthis study to improve the temporal QA performance by up to 55 percent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encapsulate a surprising amount of factual world\nknowledge. However, their performance on temporal questions and historical\nknowledge is limited because they often cannot understand temporal scope and\norientation or neglect the temporal aspect altogether. In this study, we aim to\nmeasure precisely how robust LLMs are for question answering based on their\nability to process temporal information and perform tasks requiring temporal\nreasoning and temporal factual knowledge. Specifically, we design eight\ntime-sensitive robustness tests for factual information to check the\nsensitivity of six popular LLMs in the zero-shot setting. Overall, we find LLMs\nlacking temporal robustness, especially to temporal reformulations and the use\nof different granularities of temporal references. We show how a selection of\nthese eight tests can be used automatically to judge a model's temporal\nrobustness for user questions on the fly. Finally, we apply the findings of\nthis study to improve the temporal QA performance by up to 55 percent."
                },
                "authors": [
                    {
                        "name": "Jonas Wallat"
                    },
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Adam Jatowt"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12231v2",
                "updated": "2025-03-21T11:50:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    11,
                    50,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-17T18:59:42Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    59,
                    42,
                    0,
                    48,
                    0
                ],
                "title": "PUGS: Zero-shot Physical Understanding with Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUGS: Zero-shot Physical Understanding with Gaussian Splatting"
                },
                "summary": "Current robotic systems can understand the categories and poses of objects\nwell. But understanding physical properties like mass, friction, and hardness,\nin the wild, remains challenging. We propose a new method that reconstructs 3D\nobjects using the Gaussian splatting representation and predicts various\nphysical properties in a zero-shot manner. We propose two techniques during the\nreconstruction phase: a geometry-aware regularization loss function to improve\nthe shape quality and a region-aware feature contrastive loss function to\npromote region affinity. Two other new techniques are designed during\ninference: a feature-based property propagation module and a volume integration\nmodule tailored for the Gaussian representation. Our framework is named as\nzero-shot physical understanding with Gaussian splatting, or PUGS. PUGS\nachieves new state-of-the-art results on the standard benchmark of ABO-500 mass\nprediction. We provide extensive quantitative ablations and qualitative\nvisualization to demonstrate the mechanism of our designs. We show the proposed\nmethodology can help address challenging real-world grasping tasks. Our codes,\ndata, and models are available at https://github.com/EverNorif/PUGS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current robotic systems can understand the categories and poses of objects\nwell. But understanding physical properties like mass, friction, and hardness,\nin the wild, remains challenging. We propose a new method that reconstructs 3D\nobjects using the Gaussian splatting representation and predicts various\nphysical properties in a zero-shot manner. We propose two techniques during the\nreconstruction phase: a geometry-aware regularization loss function to improve\nthe shape quality and a region-aware feature contrastive loss function to\npromote region affinity. Two other new techniques are designed during\ninference: a feature-based property propagation module and a volume integration\nmodule tailored for the Gaussian representation. Our framework is named as\nzero-shot physical understanding with Gaussian splatting, or PUGS. PUGS\nachieves new state-of-the-art results on the standard benchmark of ABO-500 mass\nprediction. We provide extensive quantitative ablations and qualitative\nvisualization to demonstrate the mechanism of our designs. We show the proposed\nmethodology can help address challenging real-world grasping tasks. Our codes,\ndata, and models are available at https://github.com/EverNorif/PUGS"
                },
                "authors": [
                    {
                        "name": "Yinghao Shuai"
                    },
                    {
                        "name": "Ran Yu"
                    },
                    {
                        "name": "Yuantao Chen"
                    },
                    {
                        "name": "Zijian Jiang"
                    },
                    {
                        "name": "Xiaowei Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Jv Zheng"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "Meng Yang"
                    },
                    {
                        "name": "Zhicheng Wang"
                    },
                    {
                        "name": "Wenbo Ding"
                    },
                    {
                        "name": "Hao Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhao"
                },
                "author": "Hao Zhao",
                "arxiv_comment": "ICRA 2025, Project page: https://evernorif.github.io/PUGS/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16193v2",
                "updated": "2025-03-21T11:50:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    11,
                    50,
                    8,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T14:40:48Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    40,
                    48,
                    3,
                    79,
                    0
                ],
                "title": "Affective Polarization Amongst Swedish Politicians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affective Polarization Amongst Swedish Politicians"
                },
                "summary": "This study investigates affective polarization among Swedish politicians on\nTwitter from 2021 to 2023, including the September 2022 parliamentary election.\nAnalyzing over 25,000 tweets and employing large language models (LLMs) for\nsentiment and political classification, we distinguish between positive\npartisanship (support of allies) and negative partisanship (criticism of\nopponents).\n  Our findings are contingent on the definition of the in-group. When political\nin-groups are defined at the ideological bloc level, negative and positive\npartisanship occur at similar rates. However, when the in-group is defined at\nthe party level, negative partisanship becomes significantly more dominant and\nis 1.51 times more likely (1.45, 1.58). This effect is even stronger among\nextreme politicians, who engage in negativity more than their moderate\ncounterparts. Negative partisanship also proves to be a strategic choice for\nonline visibility, attracting 3.18 more likes and 1.69 more retweets on\naverage.\n  By adapting methods developed for two-party systems and leveraging LLMs for\nSwedish-language analysis, we provide novel insights into how multiparty\npolitics shapes polarizing discourse. Our results underscore both the strategic\nappeal of negativity in digital spaces and the growing potential of LLMs for\nlarge-scale, non-English political research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates affective polarization among Swedish politicians on\nTwitter from 2021 to 2023, including the September 2022 parliamentary election.\nAnalyzing over 25,000 tweets and employing large language models (LLMs) for\nsentiment and political classification, we distinguish between positive\npartisanship (support of allies) and negative partisanship (criticism of\nopponents).\n  Our findings are contingent on the definition of the in-group. When political\nin-groups are defined at the ideological bloc level, negative and positive\npartisanship occur at similar rates. However, when the in-group is defined at\nthe party level, negative partisanship becomes significantly more dominant and\nis 1.51 times more likely (1.45, 1.58). This effect is even stronger among\nextreme politicians, who engage in negativity more than their moderate\ncounterparts. Negative partisanship also proves to be a strategic choice for\nonline visibility, attracting 3.18 more likes and 1.69 more retweets on\naverage.\n  By adapting methods developed for two-party systems and leveraging LLMs for\nSwedish-language analysis, we provide novel insights into how multiparty\npolitics shapes polarizing discourse. Our results underscore both the strategic\nappeal of negativity in digital spaces and the growing potential of LLMs for\nlarge-scale, non-English political research."
                },
                "authors": [
                    {
                        "name": "François t'Serstevens"
                    },
                    {
                        "name": "Roberto Cerina"
                    },
                    {
                        "name": "Gustav Peper"
                    }
                ],
                "author_detail": {
                    "name": "Gustav Peper"
                },
                "author": "Gustav Peper",
                "arxiv_comment": "5 figures, 4 tables, 26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.04901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.04901v2",
                "updated": "2025-03-21T11:48:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    11,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "published": "2023-10-07T19:45:24Z",
                "published_parsed": [
                    2023,
                    10,
                    7,
                    19,
                    45,
                    24,
                    5,
                    280,
                    0
                ],
                "title": "WAIT: Feature Warping for Animation to Illustration video Translation\n  using GANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAIT: Feature Warping for Animation to Illustration video Translation\n  using GANs"
                },
                "summary": "In this paper, we explore a new domain for video-to-video translation.\nMotivated by the availability of animation movies that are adopted from\nillustrated books for children, we aim to stylize these videos with the style\nof the original illustrations. Current state-of-the-art video-to-video\ntranslation models rely on having a video sequence or a single style image to\nstylize an input video. We introduce a new problem for video stylizing where an\nunordered set of images are used. This is a challenging task for two reasons:\ni) we do not have the advantage of temporal consistency as in video sequences;\nii) it is more difficult to obtain consistent styles for video frames from a\nset of unordered images compared to using a single image. Most of the\nvideo-to-video translation methods are built on an image-to-image translation\nmodel, and integrate additional networks such as optical flow, or temporal\npredictors to capture temporal relations. These additional networks make the\nmodel training and inference complicated and slow down the process. To ensure\ntemporal coherency in video-to-video style transfer, we propose a new generator\nnetwork with feature warping layers which overcomes the limitations of the\nprevious methods. We show the effectiveness of our method on three datasets\nboth qualitatively and quantitatively. Code and pretrained models are available\nat https://github.com/giddyyupp/wait.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore a new domain for video-to-video translation.\nMotivated by the availability of animation movies that are adopted from\nillustrated books for children, we aim to stylize these videos with the style\nof the original illustrations. Current state-of-the-art video-to-video\ntranslation models rely on having a video sequence or a single style image to\nstylize an input video. We introduce a new problem for video stylizing where an\nunordered set of images are used. This is a challenging task for two reasons:\ni) we do not have the advantage of temporal consistency as in video sequences;\nii) it is more difficult to obtain consistent styles for video frames from a\nset of unordered images compared to using a single image. Most of the\nvideo-to-video translation methods are built on an image-to-image translation\nmodel, and integrate additional networks such as optical flow, or temporal\npredictors to capture temporal relations. These additional networks make the\nmodel training and inference complicated and slow down the process. To ensure\ntemporal coherency in video-to-video style transfer, we propose a new generator\nnetwork with feature warping layers which overcomes the limitations of the\nprevious methods. We show the effectiveness of our method on three datasets\nboth qualitatively and quantitatively. Code and pretrained models are available\nat https://github.com/giddyyupp/wait."
                },
                "authors": [
                    {
                        "name": "Samet Hicsonmez"
                    },
                    {
                        "name": "Nermin Samet"
                    },
                    {
                        "name": "Fidan Samet"
                    },
                    {
                        "name": "Oguz Bakir"
                    },
                    {
                        "name": "Emre Akbas"
                    },
                    {
                        "name": "Pinar Duygulu"
                    }
                ],
                "author_detail": {
                    "name": "Pinar Duygulu"
                },
                "author": "Pinar Duygulu",
                "arxiv_comment": "Accepted to Neurocomputing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.04901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.04901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01524v2",
                "updated": "2025-03-21T11:16:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    11,
                    16,
                    5,
                    4,
                    80,
                    0
                ],
                "published": "2024-07-01T17:59:28Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    28,
                    0,
                    183,
                    0
                ],
                "title": "Straightening the Ruler: Field-Level Inference of the BAO Scale with\n  LEFTfield",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Straightening the Ruler: Field-Level Inference of the BAO Scale with\n  LEFTfield"
                },
                "summary": "Current inferences of the BAO scale from galaxy clustering employ a\nreconstruction technique at fixed cosmology and bias parameters. Here, we\npresent the first consistent joint Bayesian inference of the isotropic BAO\nscale, jointly varying the initial conditions as well as all bias coefficients,\nbased on the EFT-based field-level forward model $\\texttt{LEFTfield}$. We apply\nthis analysis to mock data generated at a much higher cutoff, or resolution,\nresulting in a significant model mismatch between mock data and the model used\nin the inference. We demonstrate that the remaining systematic bias in the BAO\nscale is below 2% for all data considered and below 1% when Eulerian bias is\nused for inference. Furthermore, we find that the inferred error on the BAO\nscale is typically 30%, and up to 50%, smaller compared to that from a\nreplication of the standard post-reconstruction power-spectrum approach, using\nthe same scales as in the field-level inference. The improvement in BAO scale\nprecision grows towards smaller scales (higher $k$). As a validation test, we\nrepeat this comparison on a mock dataset that is linearly biased with respect\nto a 1LPT (Zel'dovich) density field, following the assumption made in standard\nreconstruction approaches. We find that field-level inference indeed yields the\nsame error bar as the post-reconstruction power spectrum, which is expectd to\nbe optimal in this case. In summary, a field-level approach to BAO not only\nallows for a consistent inference of the BAO scale, but promises to achieve\nmore precise measurements on realistic, nonlinearly biased tracers as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current inferences of the BAO scale from galaxy clustering employ a\nreconstruction technique at fixed cosmology and bias parameters. Here, we\npresent the first consistent joint Bayesian inference of the isotropic BAO\nscale, jointly varying the initial conditions as well as all bias coefficients,\nbased on the EFT-based field-level forward model $\\texttt{LEFTfield}$. We apply\nthis analysis to mock data generated at a much higher cutoff, or resolution,\nresulting in a significant model mismatch between mock data and the model used\nin the inference. We demonstrate that the remaining systematic bias in the BAO\nscale is below 2% for all data considered and below 1% when Eulerian bias is\nused for inference. Furthermore, we find that the inferred error on the BAO\nscale is typically 30%, and up to 50%, smaller compared to that from a\nreplication of the standard post-reconstruction power-spectrum approach, using\nthe same scales as in the field-level inference. The improvement in BAO scale\nprecision grows towards smaller scales (higher $k$). As a validation test, we\nrepeat this comparison on a mock dataset that is linearly biased with respect\nto a 1LPT (Zel'dovich) density field, following the assumption made in standard\nreconstruction approaches. We find that field-level inference indeed yields the\nsame error bar as the post-reconstruction power spectrum, which is expectd to\nbe optimal in this case. In summary, a field-level approach to BAO not only\nallows for a consistent inference of the BAO scale, but promises to achieve\nmore precise measurements on realistic, nonlinearly biased tracers as well."
                },
                "authors": [
                    {
                        "name": "Ivana Babić"
                    },
                    {
                        "name": "Fabian Schmidt"
                    },
                    {
                        "name": "Beatriz Tucci"
                    }
                ],
                "author_detail": {
                    "name": "Beatriz Tucci"
                },
                "author": "Beatriz Tucci",
                "arxiv_comment": "30 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15285v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15285v3",
                "updated": "2025-03-21T11:01:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    11,
                    1,
                    5,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-21T08:23:32Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    23,
                    32,
                    4,
                    52,
                    0
                ],
                "title": "Offload Rethinking by Cloud Assistance for Efficient Environmental Sound\n  Recognition on LPWANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offload Rethinking by Cloud Assistance for Efficient Environmental Sound\n  Recognition on LPWANs"
                },
                "summary": "Learning-based environmental sound recognition has emerged as a crucial\nmethod for ultra-low-power environmental monitoring in biological research and\ncity-scale sensing systems. These systems usually operate under limited\nresources and are often powered by harvested energy in remote areas. Recent\nefforts in on-device sound recognition suffer from low accuracy due to resource\nconstraints, whereas cloud offloading strategies are hindered by high\ncommunication costs. In this work, we introduce ORCA, a novel\nresource-efficient cloud-assisted environmental sound recognition system on\nbatteryless devices operating over the Low-Power Wide-Area Networks (LPWANs),\ntargeting wide-area audio sensing applications. We propose a cloud assistance\nstrategy that remedies the low accuracy of on-device inference while minimizing\nthe communication costs for cloud offloading. By leveraging a\nself-attention-based cloud sub-spectral feature selection method to facilitate\nefficient on-device inference, ORCA resolves three key challenges for\nresource-constrained cloud offloading over LPWANs: 1) high communication costs\nand low data rates, 2) dynamic wireless channel conditions, and 3) unreliable\noffloading. We implement ORCA on an energy-harvesting batteryless\nmicrocontroller and evaluate it in a real world urban sound testbed. Our\nresults show that ORCA outperforms state-of-the-art methods by up to $80\n\\times$ in energy savings and $220 \\times$ in latency reduction while\nmaintaining comparable accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based environmental sound recognition has emerged as a crucial\nmethod for ultra-low-power environmental monitoring in biological research and\ncity-scale sensing systems. These systems usually operate under limited\nresources and are often powered by harvested energy in remote areas. Recent\nefforts in on-device sound recognition suffer from low accuracy due to resource\nconstraints, whereas cloud offloading strategies are hindered by high\ncommunication costs. In this work, we introduce ORCA, a novel\nresource-efficient cloud-assisted environmental sound recognition system on\nbatteryless devices operating over the Low-Power Wide-Area Networks (LPWANs),\ntargeting wide-area audio sensing applications. We propose a cloud assistance\nstrategy that remedies the low accuracy of on-device inference while minimizing\nthe communication costs for cloud offloading. By leveraging a\nself-attention-based cloud sub-spectral feature selection method to facilitate\nefficient on-device inference, ORCA resolves three key challenges for\nresource-constrained cloud offloading over LPWANs: 1) high communication costs\nand low data rates, 2) dynamic wireless channel conditions, and 3) unreliable\noffloading. We implement ORCA on an energy-harvesting batteryless\nmicrocontroller and evaluate it in a real world urban sound testbed. Our\nresults show that ORCA outperforms state-of-the-art methods by up to $80\n\\times$ in energy savings and $220 \\times$ in latency reduction while\nmaintaining comparable accuracy."
                },
                "authors": [
                    {
                        "name": "Le Zhang"
                    },
                    {
                        "name": "Quanling Zhao"
                    },
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Shirley Bian"
                    },
                    {
                        "name": "Onat Gungor"
                    },
                    {
                        "name": "Flavio Ponzina"
                    },
                    {
                        "name": "Tajana Rosing"
                    }
                ],
                "author_detail": {
                    "name": "Tajana Rosing"
                },
                "author": "Tajana Rosing",
                "arxiv_comment": "Accepted by The 23rd ACM Conference on Embedded Networked Sensor\n  Systems (SenSys '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15285v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15285v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17040v1",
                "updated": "2025-03-21T10:53:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    53,
                    59,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:53:59Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    53,
                    59,
                    4,
                    80,
                    0
                ],
                "title": "Problem Framing in the AI era: a new model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem Framing in the AI era: a new model"
                },
                "summary": "Effective problem-solving in physics extends beyond the mere application of\nmathematical formulas; it necessitates an understanding of how mathematical\nconcepts connect to and reflect the physical world. A strong epistemological\nframework based on problem framing (PF) is essential for students, as it\nenables them to justify their mathematical decisions and recognize the\nrelationship between abstract mathematics and real-world physical phenomena.\nThis becomes increasingly important in the age of artificial intelligence (AI),\nwhere the use of Large Language Models (LLMs) in education is growing rapidly.\nThis paper explores the impact of AI, specifically LLMs like ChatGPT, on\nupper-level students' PF in physics education. Building on existing models, in\nthis exploratory theoretical paper, we propose a novel three-dimensional\nframework grounded in Situated Cognition Theory and Greeno's extended semantic\nmodel, aiming to elucidate how AI could influence students' epistemological\nframing during Cooperative Problem Solving activities (CPS). We advocate for\ninstructors to encourage AI-assisted CPS to foster critical thinking and\nenhance student engagement with real-world scenarios. Preliminary results\nsuggest that ChatGPT can aid in developing symbolic and visual languages within\nproblem framing, though further research is needed to confirm these findings\nand investigate the potential of AI-driven intelligent tutoring systems for\npersonalized learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective problem-solving in physics extends beyond the mere application of\nmathematical formulas; it necessitates an understanding of how mathematical\nconcepts connect to and reflect the physical world. A strong epistemological\nframework based on problem framing (PF) is essential for students, as it\nenables them to justify their mathematical decisions and recognize the\nrelationship between abstract mathematics and real-world physical phenomena.\nThis becomes increasingly important in the age of artificial intelligence (AI),\nwhere the use of Large Language Models (LLMs) in education is growing rapidly.\nThis paper explores the impact of AI, specifically LLMs like ChatGPT, on\nupper-level students' PF in physics education. Building on existing models, in\nthis exploratory theoretical paper, we propose a novel three-dimensional\nframework grounded in Situated Cognition Theory and Greeno's extended semantic\nmodel, aiming to elucidate how AI could influence students' epistemological\nframing during Cooperative Problem Solving activities (CPS). We advocate for\ninstructors to encourage AI-assisted CPS to foster critical thinking and\nenhance student engagement with real-world scenarios. Preliminary results\nsuggest that ChatGPT can aid in developing symbolic and visual languages within\nproblem framing, though further research is needed to confirm these findings\nand investigate the potential of AI-driven intelligent tutoring systems for\npersonalized learning."
                },
                "authors": [
                    {
                        "name": "Matteo Tuveri"
                    },
                    {
                        "name": "Arianna Steri"
                    },
                    {
                        "name": "Viviana Fanti"
                    }
                ],
                "author_detail": {
                    "name": "Viviana Fanti"
                },
                "author": "Viviana Fanti",
                "arxiv_comment": "18 pages, 6 figures, 1 table; submitted to European Journal of\n  Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12547v2",
                "updated": "2025-03-21T10:53:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    53,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-16T15:32:30Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    15,
                    32,
                    30,
                    6,
                    75,
                    0
                ],
                "title": "LLMSeR: Enhancing Sequential Recommendation via LLM-based Data\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSeR: Enhancing Sequential Recommendation via LLM-based Data\n  Augmentation"
                },
                "summary": "Sequential Recommender Systems (SRS) have become a cornerstone of online\nplatforms, leveraging users' historical interaction data to forecast their next\npotential engagement. Despite their widespread adoption, SRS often grapple with\nthe long-tail user dilemma, resulting in less effective recommendations for\nindividuals with limited interaction records. The advent of Large Language\nModels (LLMs), with their profound capability to discern semantic relationships\namong items, has opened new avenues for enhancing SRS through data\naugmentation. Nonetheless, current methodologies encounter obstacles, including\nthe absence of collaborative signals and the prevalence of hallucination\nphenomena. In this work, we present LLMSeR, an innovative framework that\nutilizes Large Language Models (LLMs) to generate pseudo-prior items, thereby\nimproving the efficacy of Sequential Recommender Systems (SRS). To alleviate\nthe challenge of insufficient collaborative signals, we introduce the Semantic\nInteraction Augmentor (SIA), a method that integrates both semantic and\ncollaborative information to comprehensively augment user interaction data.\nMoreover, to weaken the adverse effects of hallucination in SRS, we develop the\nAdaptive Reliability Validation (ARV), a validation technique designed to\nassess the reliability of the generated pseudo items. Complementing these\nadvancements, we also devise a Dual-Channel Training strategy, ensuring\nseamless integration of data augmentation into the SRS training\nprocess.Extensive experiments conducted with three widely-used SRS models\ndemonstrate the generalizability and efficacy of LLMSeR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Recommender Systems (SRS) have become a cornerstone of online\nplatforms, leveraging users' historical interaction data to forecast their next\npotential engagement. Despite their widespread adoption, SRS often grapple with\nthe long-tail user dilemma, resulting in less effective recommendations for\nindividuals with limited interaction records. The advent of Large Language\nModels (LLMs), with their profound capability to discern semantic relationships\namong items, has opened new avenues for enhancing SRS through data\naugmentation. Nonetheless, current methodologies encounter obstacles, including\nthe absence of collaborative signals and the prevalence of hallucination\nphenomena. In this work, we present LLMSeR, an innovative framework that\nutilizes Large Language Models (LLMs) to generate pseudo-prior items, thereby\nimproving the efficacy of Sequential Recommender Systems (SRS). To alleviate\nthe challenge of insufficient collaborative signals, we introduce the Semantic\nInteraction Augmentor (SIA), a method that integrates both semantic and\ncollaborative information to comprehensively augment user interaction data.\nMoreover, to weaken the adverse effects of hallucination in SRS, we develop the\nAdaptive Reliability Validation (ARV), a validation technique designed to\nassess the reliability of the generated pseudo items. Complementing these\nadvancements, we also devise a Dual-Channel Training strategy, ensuring\nseamless integration of data augmentation into the SRS training\nprocess.Extensive experiments conducted with three widely-used SRS models\ndemonstrate the generalizability and efficacy of LLMSeR."
                },
                "authors": [
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Haiping Zhu"
                    },
                    {
                        "name": "Feng Tian"
                    }
                ],
                "author_detail": {
                    "name": "Feng Tian"
                },
                "author": "Feng Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17039v1",
                "updated": "2025-03-21T10:52:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    52,
                    20,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:52:20Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    52,
                    20,
                    4,
                    80,
                    0
                ],
                "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and\n  LLM-Judges Correlate with Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and\n  LLM-Judges Correlate with Humans?"
                },
                "summary": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads."
                },
                "authors": [
                    {
                        "name": "Jeremy Barnes"
                    },
                    {
                        "name": "Naiara Perez"
                    },
                    {
                        "name": "Alba Bonet-Jover"
                    },
                    {
                        "name": "Begoña Altuna"
                    }
                ],
                "author_detail": {
                    "name": "Begoña Altuna"
                },
                "author": "Begoña Altuna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17777v2",
                "updated": "2025-03-21T10:51:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    51,
                    22,
                    4,
                    80,
                    0
                ],
                "published": "2024-07-25T05:10:48Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    5,
                    10,
                    48,
                    3,
                    207,
                    0
                ],
                "title": "Babel: A Scalable Pre-trained Model for Multi-Modal Sensing via\n  Expandable Modality Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Babel: A Scalable Pre-trained Model for Multi-Modal Sensing via\n  Expandable Modality Alignment"
                },
                "summary": "This paper presents Babel, the expandable modality alignment model, specially\ndesigned for multi-modal sensing. While there has been considerable work on\nmulti-modality alignment, they all struggle to effectively incorporate multiple\nsensing modalities due to the data scarcity constraints. How to utilize\nmulti-modal data with partial pairings in sensing remains an unresolved\nchallenge. Babel tackles this challenge by introducing the concept of\nexpandable modality alignment. The key idea involves transforming the\nN-modality alignment into a series of binary-modality alignments. Novel\ntechniques are also proposed to further mitigate data scarcity issue and\nbalance the contribution of the newly incorporated modality with the previously\nestablished modality alignment during the expandable alignment process. We\nprovide the comprehensive implementation. In the pre-training phase, Babel\ncurrently aligns 6 sensing modalities, namely Wi-Fi, mmWave, IMU, LiDAR, video,\nand depth. For the deployment phase, as a foundation model, any single or\ncombination of aligned modalities could be selected from Babel and applied to\ndownstream tasks. Evaluation demonstrates Babel's outstanding performance on\neight human activity recognition datasets, compared to a broad range of\nbaselines e.g., the SOTA single-modal sensing networks, multi-modal sensing\nframework, and multi-modal large language models. Babel not only improves the\nperformance of individual modality sensing (12% averaged accuracy improvement),\nbut also effectively fuses multiple available modalities (up to 22% accuracy\nincrease). Case studies also highlight emerging application scenarios empowered\nby Babel, including cross-modality retrieval (i.e., sensing imaging), and\nbridging LLM for sensing comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Babel, the expandable modality alignment model, specially\ndesigned for multi-modal sensing. While there has been considerable work on\nmulti-modality alignment, they all struggle to effectively incorporate multiple\nsensing modalities due to the data scarcity constraints. How to utilize\nmulti-modal data with partial pairings in sensing remains an unresolved\nchallenge. Babel tackles this challenge by introducing the concept of\nexpandable modality alignment. The key idea involves transforming the\nN-modality alignment into a series of binary-modality alignments. Novel\ntechniques are also proposed to further mitigate data scarcity issue and\nbalance the contribution of the newly incorporated modality with the previously\nestablished modality alignment during the expandable alignment process. We\nprovide the comprehensive implementation. In the pre-training phase, Babel\ncurrently aligns 6 sensing modalities, namely Wi-Fi, mmWave, IMU, LiDAR, video,\nand depth. For the deployment phase, as a foundation model, any single or\ncombination of aligned modalities could be selected from Babel and applied to\ndownstream tasks. Evaluation demonstrates Babel's outstanding performance on\neight human activity recognition datasets, compared to a broad range of\nbaselines e.g., the SOTA single-modal sensing networks, multi-modal sensing\nframework, and multi-modal large language models. Babel not only improves the\nperformance of individual modality sensing (12% averaged accuracy improvement),\nbut also effectively fuses multiple available modalities (up to 22% accuracy\nincrease). Case studies also highlight emerging application scenarios empowered\nby Babel, including cross-modality retrieval (i.e., sensing imaging), and\nbridging LLM for sensing comprehension."
                },
                "authors": [
                    {
                        "name": "Shenghong Dai"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Suman Banerjee"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted by SenSys'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22775v2",
                "updated": "2025-03-21T10:45:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    45,
                    28,
                    4,
                    80,
                    0
                ],
                "published": "2024-10-30T07:43:29Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    43,
                    29,
                    2,
                    304,
                    0
                ],
                "title": "Diffusion Beats Autoregressive: An Evaluation of Compositional\n  Generation in Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Beats Autoregressive: An Evaluation of Compositional\n  Generation in Text-to-Image Models"
                },
                "summary": "Text-to-image (T2I) generative models, such as Stable Diffusion and DALL-E,\nhave shown remarkable proficiency in producing high-quality, realistic, and\nnatural images from textual descriptions. However, these models sometimes fail\nto accurately capture all the details specified in the input prompts,\nparticularly concerning entities, attributes, and spatial relationships. This\nissue becomes more pronounced when the prompt contains novel or complex\ncompositions, leading to what are known as compositional generation failure\nmodes. Recently, a new open-source diffusion-based T2I model, FLUX, has been\nintroduced, demonstrating strong performance in high-quality image generation.\nAdditionally, autoregressive T2I models like LlamaGen have claimed competitive\nvisual quality performance compared to diffusion-based models. In this study,\nwe evaluate the compositional generation capabilities of these newly introduced\nmodels against established models using the T2I-CompBench benchmark. Our\nfindings reveal that LlamaGen, as a vanilla autoregressive model, is not yet on\npar with state-of-the-art diffusion models for compositional generation tasks\nunder the same criteria, such as model size and inference time. On the other\nhand, the open-source diffusion-based model FLUX exhibits compositional\ngeneration capabilities comparable to the state-of-the-art closed-source model\nDALL-E3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generative models, such as Stable Diffusion and DALL-E,\nhave shown remarkable proficiency in producing high-quality, realistic, and\nnatural images from textual descriptions. However, these models sometimes fail\nto accurately capture all the details specified in the input prompts,\nparticularly concerning entities, attributes, and spatial relationships. This\nissue becomes more pronounced when the prompt contains novel or complex\ncompositions, leading to what are known as compositional generation failure\nmodes. Recently, a new open-source diffusion-based T2I model, FLUX, has been\nintroduced, demonstrating strong performance in high-quality image generation.\nAdditionally, autoregressive T2I models like LlamaGen have claimed competitive\nvisual quality performance compared to diffusion-based models. In this study,\nwe evaluate the compositional generation capabilities of these newly introduced\nmodels against established models using the T2I-CompBench benchmark. Our\nfindings reveal that LlamaGen, as a vanilla autoregressive model, is not yet on\npar with state-of-the-art diffusion models for compositional generation tasks\nunder the same criteria, such as model size and inference time. On the other\nhand, the open-source diffusion-based model FLUX exhibits compositional\ngeneration capabilities comparable to the state-of-the-art closed-source model\nDALL-E3."
                },
                "authors": [
                    {
                        "name": "Arash Marioriyad"
                    },
                    {
                        "name": "Parham Rezaei"
                    },
                    {
                        "name": "Mahdieh Soleymani Baghshah"
                    },
                    {
                        "name": "Mohammad Hossein Rohban"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Hossein Rohban"
                },
                "author": "Mohammad Hossein Rohban",
                "arxiv_comment": "NeurIPS 2024 Workshop on Compositional Learning: Perspectives,\n  Methods, and Paths Forward",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16173v2",
                "updated": "2025-03-21T10:44:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    44,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-25T08:04:47Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    4,
                    47,
                    0,
                    330,
                    0
                ],
                "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis"
                },
                "summary": "Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences."
                },
                "authors": [
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Hosu Lee"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_comment": "Project page: https://ivy-lvlm.github.io/SALOVA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17035v1",
                "updated": "2025-03-21T10:42:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    42,
                    54,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:42:54Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    42,
                    54,
                    4,
                    80,
                    0
                ],
                "title": "Bayesian reconstruction of anisotropic flow fluctuations at fixed impact\n  parameter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian reconstruction of anisotropic flow fluctuations at fixed impact\n  parameter"
                },
                "summary": "The cumulants of the distribution of anisotropic flow are measured accurately\nin Pb+Pb collisions at the LHC as a function of centrality classifiers (charged\nmultiplicity and/or transverse energy). Using Bayesian inference, we\nreconstruct from these measurements the probability distribution of anisotropic\nflow in the ``theorists' frame'' where the impact parameter has a fixed\nmagnitude and orientation, up to $\\sim 70\\%$ centrality. The variation of flow\nfluctuations with impact parameter displays direct evidence of viscous damping,\nwhich is larger for higher Fourier harmonics, in line with expectations from\nhydrodynamics. We use intensive measures of non-Gaussian flow fluctuations,\nwhich have reduced dependence on centrality. We infer from ATLAS data the\nmagnitude of these intensive non-Gaussianities in each Fourier harmonic. They\nprovide data-driven estimates of response coefficients to initial anisotropies,\nwithout resorting to any specific microscopic model of initial conditions.\nThese estimates agree with viscous hydrodynamic calculations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cumulants of the distribution of anisotropic flow are measured accurately\nin Pb+Pb collisions at the LHC as a function of centrality classifiers (charged\nmultiplicity and/or transverse energy). Using Bayesian inference, we\nreconstruct from these measurements the probability distribution of anisotropic\nflow in the ``theorists' frame'' where the impact parameter has a fixed\nmagnitude and orientation, up to $\\sim 70\\%$ centrality. The variation of flow\nfluctuations with impact parameter displays direct evidence of viscous damping,\nwhich is larger for higher Fourier harmonics, in line with expectations from\nhydrodynamics. We use intensive measures of non-Gaussian flow fluctuations,\nwhich have reduced dependence on centrality. We infer from ATLAS data the\nmagnitude of these intensive non-Gaussianities in each Fourier harmonic. They\nprovide data-driven estimates of response coefficients to initial anisotropies,\nwithout resorting to any specific microscopic model of initial conditions.\nThese estimates agree with viscous hydrodynamic calculations."
                },
                "authors": [
                    {
                        "name": "Enak Roubertie"
                    },
                    {
                        "name": "Mathis Verdan"
                    },
                    {
                        "name": "Andreas Kirchner"
                    },
                    {
                        "name": "Jean-Yves Ollitrault"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Yves Ollitrault"
                },
                "author": "Jean-Yves Ollitrault",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.02738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.02738v2",
                "updated": "2025-03-21T10:42:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    42,
                    26,
                    4,
                    80,
                    0
                ],
                "published": "2023-08-04T23:13:49Z",
                "published_parsed": [
                    2023,
                    8,
                    4,
                    23,
                    13,
                    49,
                    4,
                    216,
                    0
                ],
                "title": "Exploring Part-Informed Visual-Language Learning for Person\n  Re-Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Part-Informed Visual-Language Learning for Person\n  Re-Identification"
                },
                "summary": "Recently, visual-language learning (VLL) has shown great potential in\nenhancing visual-based person re-identification (ReID). Existing VLL-based ReID\nmethods typically focus on image-text feature alignment at the whole-body\nlevel, while neglecting supervision on fine-grained part features, thus lacking\nconstraints for local feature semantic consistency. To this end, we propose\nPart-Informed Visual-language Learning ($\\pi$-VL) to enhance fine-grained\nvisual features with part-informed language supervisions for ReID tasks.\nSpecifically, $\\pi$-VL introduces a human parsing-guided prompt tuning strategy\nand a hierarchical visual-language alignment paradigm to ensure within-part\nfeature semantic consistency. The former combines both identity labels and\nhuman parsing maps to constitute pixel-level text prompts, and the latter fuses\nmulti-scale visual features with a light-weight auxiliary head to perform\nfine-grained image-text alignment. As a plug-and-play and inference-free\nsolution, our $\\pi$-VL achieves performance comparable to or better than\nstate-of-the-art methods on four commonly used ReID benchmarks. Notably, it\nreports 91.0% Rank-1 and 76.9% mAP on the challenging MSMT17 database, without\nbells and whistles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, visual-language learning (VLL) has shown great potential in\nenhancing visual-based person re-identification (ReID). Existing VLL-based ReID\nmethods typically focus on image-text feature alignment at the whole-body\nlevel, while neglecting supervision on fine-grained part features, thus lacking\nconstraints for local feature semantic consistency. To this end, we propose\nPart-Informed Visual-language Learning ($\\pi$-VL) to enhance fine-grained\nvisual features with part-informed language supervisions for ReID tasks.\nSpecifically, $\\pi$-VL introduces a human parsing-guided prompt tuning strategy\nand a hierarchical visual-language alignment paradigm to ensure within-part\nfeature semantic consistency. The former combines both identity labels and\nhuman parsing maps to constitute pixel-level text prompts, and the latter fuses\nmulti-scale visual features with a light-weight auxiliary head to perform\nfine-grained image-text alignment. As a plug-and-play and inference-free\nsolution, our $\\pi$-VL achieves performance comparable to or better than\nstate-of-the-art methods on four commonly used ReID benchmarks. Notably, it\nreports 91.0% Rank-1 and 76.9% mAP on the challenging MSMT17 database, without\nbells and whistles."
                },
                "authors": [
                    {
                        "name": "Yin Lin"
                    },
                    {
                        "name": "Yehansen Chen"
                    },
                    {
                        "name": "Baocai Yin"
                    },
                    {
                        "name": "Jinshui Hu"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Cong Liu"
                    },
                    {
                        "name": "Zengfu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zengfu Wang"
                },
                "author": "Zengfu Wang",
                "arxiv_comment": "6 pages, 4 figures, ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.02738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.02738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15463v2",
                "updated": "2025-03-21T10:33:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    33,
                    21,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-19T17:41:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    41,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment"
                },
                "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15722v2",
                "updated": "2025-03-21T10:20:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    20,
                    44,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-19T22:28:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    22,
                    28,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Leveraging MoE-based Large Language Model for Zero-Shot Multi-Task\n  Semantic Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging MoE-based Large Language Model for Zero-Shot Multi-Task\n  Semantic Communication"
                },
                "summary": "Multi-task semantic communication (SC) can reduce the computational resources\nin wireless systems since retraining is not required when switching between\ntasks. However, existing approaches typically rely on task-specific embeddings\nto identify the intended task, necessitating retraining the entire model when\ngiven a new task. Consequently, this drives the need for a multi-task SC system\nthat can handle new tasks without additional training, known as zero-shot\nlearning. Inspired by the superior zero-shot capabilities of large language\nmodels (LLMs), we leverage pre-trained instruction-tuned LLMs, referred to as\nfine-tuned language net (FLAN), to improve the generalization capability. We\nincorporate a mixture-of-experts (MoE) architecture in the FLAN model and\npropose MoE-FLAN-SC architecture for multi-task SC systems. Our proposed\nMoE-FLAN-SC architecture can further improve the performance of FLAN-T5 model\nwithout increasing the computational cost. Moreover, we design a multi-task\nfeature extraction module (FEM) which can adaptively extract relevant features\nacross various tasks given the provided features and signal-to-noise ratio\n(SNR). Simulation results show that our proposed MoE-FLAN-SC architecture\noutperforms three state-of-the-art models in terms of the average accuracy on\nfour different unseen tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task semantic communication (SC) can reduce the computational resources\nin wireless systems since retraining is not required when switching between\ntasks. However, existing approaches typically rely on task-specific embeddings\nto identify the intended task, necessitating retraining the entire model when\ngiven a new task. Consequently, this drives the need for a multi-task SC system\nthat can handle new tasks without additional training, known as zero-shot\nlearning. Inspired by the superior zero-shot capabilities of large language\nmodels (LLMs), we leverage pre-trained instruction-tuned LLMs, referred to as\nfine-tuned language net (FLAN), to improve the generalization capability. We\nincorporate a mixture-of-experts (MoE) architecture in the FLAN model and\npropose MoE-FLAN-SC architecture for multi-task SC systems. Our proposed\nMoE-FLAN-SC architecture can further improve the performance of FLAN-T5 model\nwithout increasing the computational cost. Moreover, we design a multi-task\nfeature extraction module (FEM) which can adaptively extract relevant features\nacross various tasks given the provided features and signal-to-noise ratio\n(SNR). Simulation results show that our proposed MoE-FLAN-SC architecture\noutperforms three state-of-the-art models in terms of the average accuracy on\nfour different unseen tasks."
                },
                "authors": [
                    {
                        "name": "Sin-Yu Huang"
                    },
                    {
                        "name": "Renjie Liao"
                    },
                    {
                        "name": "Vincent W. S. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Vincent W. S. Wong"
                },
                "author": "Vincent W. S. Wong",
                "arxiv_comment": "Accepted by IEEE International Conference on Communications (ICC),\n  June 2025, Montreal, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05935v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05935v3",
                "updated": "2025-03-21T10:19:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    19,
                    1,
                    4,
                    80,
                    0
                ],
                "published": "2024-02-08T18:59:48Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    18,
                    59,
                    48,
                    3,
                    39,
                    0
                ],
                "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large\n  Language Models"
                },
                "summary": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory"
                },
                "authors": [
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Longtian Qiu"
                    },
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Weifeng Lin"
                    },
                    {
                        "name": "Shitian Zhao"
                    },
                    {
                        "name": "Shijie Geng"
                    },
                    {
                        "name": "Ziyi Lin"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Hao Shao"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "arxiv_comment": "Accepted by ICML 2024. Code and models are released at\n  https://github.com/Alpha-VLLM/LLaMA2-Accessory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05935v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05935v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08376v2",
                "updated": "2025-03-21T10:18:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    18,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-11T13:36:18Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    36,
                    18,
                    2,
                    346,
                    0
                ],
                "title": "Reloc3r: Large-Scale Training of Relative Camera Pose Regression for\n  Generalizable, Fast, and Accurate Visual Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reloc3r: Large-Scale Training of Relative Camera Pose Regression for\n  Generalizable, Fast, and Accurate Visual Localization"
                },
                "summary": "Visual localization aims to determine the camera pose of a query image\nrelative to a database of posed images. In recent years, deep neural networks\nthat directly regress camera poses have gained popularity due to their fast\ninference capabilities. However, existing methods struggle to either generalize\nwell to new scenes or provide accurate camera pose estimates. To address these\nissues, we present Reloc3r, a simple yet effective visual localization\nframework. It consists of an elegantly designed relative pose regression\nnetwork, and a minimalist motion averaging module for absolute pose estimation.\nTrained on approximately eight million posed image pairs, Reloc3r achieves\nsurprisingly good performance and generalization ability. We conduct extensive\nexperiments on six public datasets, consistently demonstrating the\neffectiveness and efficiency of the proposed method. It provides high-quality\ncamera pose estimates in real time and generalizes to novel scenes. Code:\nhttps://github.com/ffrivera0/reloc3r.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual localization aims to determine the camera pose of a query image\nrelative to a database of posed images. In recent years, deep neural networks\nthat directly regress camera poses have gained popularity due to their fast\ninference capabilities. However, existing methods struggle to either generalize\nwell to new scenes or provide accurate camera pose estimates. To address these\nissues, we present Reloc3r, a simple yet effective visual localization\nframework. It consists of an elegantly designed relative pose regression\nnetwork, and a minimalist motion averaging module for absolute pose estimation.\nTrained on approximately eight million posed image pairs, Reloc3r achieves\nsurprisingly good performance and generalization ability. We conduct extensive\nexperiments on six public datasets, consistently demonstrating the\neffectiveness and efficiency of the proposed method. It provides high-quality\ncamera pose estimates in real time and generalizes to novel scenes. Code:\nhttps://github.com/ffrivera0/reloc3r."
                },
                "authors": [
                    {
                        "name": "Siyan Dong"
                    },
                    {
                        "name": "Shuzhe Wang"
                    },
                    {
                        "name": "Shaohui Liu"
                    },
                    {
                        "name": "Lulu Cai"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Juho Kannala"
                    },
                    {
                        "name": "Yanchao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yanchao Yang"
                },
                "author": "Yanchao Yang",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17004v1",
                "updated": "2025-03-21T10:09:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    34,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:09:34Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    34,
                    4,
                    80,
                    0
                ],
                "title": "Text2Model: Generating dynamic chemical reactor models using large\n  language models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Model: Generating dynamic chemical reactor models using large\n  language models (LLMs)"
                },
                "summary": "As large language models have shown remarkable capabilities in conversing via\nnatural language, the question arises as to how LLMs could potentially assist\nchemical engineers in research and industry with domain-specific tasks. We\ngenerate dynamic chemical reactor models in Modelica code format from textual\ndescriptions as user input. We fine-tune Llama 3.1 8B Instruct on synthetically\ngenerated Modelica code for different reactor scenarios. We compare the\nperformance of our fine-tuned model to the baseline Llama 3.1 8B Instruct model\nand GPT4o. We manually assess the models' predictions regarding the syntactic\nand semantic accuracy of the generated dynamic models. We find that\nconsiderable improvements are achieved by the fine-tuned model with respect to\nboth the semantic and the syntactic accuracy of the Modelica models. However,\nthe fine-tuned model lacks a satisfactory ability to generalize to unseen\nscenarios compared to GPT4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models have shown remarkable capabilities in conversing via\nnatural language, the question arises as to how LLMs could potentially assist\nchemical engineers in research and industry with domain-specific tasks. We\ngenerate dynamic chemical reactor models in Modelica code format from textual\ndescriptions as user input. We fine-tune Llama 3.1 8B Instruct on synthetically\ngenerated Modelica code for different reactor scenarios. We compare the\nperformance of our fine-tuned model to the baseline Llama 3.1 8B Instruct model\nand GPT4o. We manually assess the models' predictions regarding the syntactic\nand semantic accuracy of the generated dynamic models. We find that\nconsiderable improvements are achieved by the fine-tuned model with respect to\nboth the semantic and the syntactic accuracy of the Modelica models. However,\nthe fine-tuned model lacks a satisfactory ability to generalize to unseen\nscenarios compared to GPT4o."
                },
                "authors": [
                    {
                        "name": "Sophia Rupprecht"
                    },
                    {
                        "name": "Yassine Hounat"
                    },
                    {
                        "name": "Monisha Kumar"
                    },
                    {
                        "name": "Giacomo Lastrucci"
                    },
                    {
                        "name": "Artur M. Schweidtmann"
                    }
                ],
                "author_detail": {
                    "name": "Artur M. Schweidtmann"
                },
                "author": "Artur M. Schweidtmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17003v1",
                "updated": "2025-03-21T10:09:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    16,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:09:16Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    16,
                    4,
                    80,
                    0
                ],
                "title": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs."
                },
                "authors": [
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Chuanqi Cheng"
                    },
                    {
                        "name": "Wei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wu"
                },
                "author": "Wei Wu",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11623v2",
                "updated": "2025-03-21T09:59:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    59,
                    58,
                    4,
                    80,
                    0
                ],
                "published": "2024-03-18T09:55:22Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    9,
                    55,
                    22,
                    0,
                    78,
                    0
                ],
                "title": "Synthesizing multi-log grasp poses in cluttered environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing multi-log grasp poses in cluttered environments"
                },
                "summary": "Multi-object grasping is a challenging task. It is important for energy and\ncost-efficient operation of industrial crane manipulators, such as those used\nto collect tree logs from the forest floor and on forest machines. In this\nwork, we used synthetic data from physics simulations to explore how\ndata-driven modeling can be used to infer multi-object grasp poses from images.\nWe showed that convolutional neural networks can be trained specifically for\nsynthesizing multi-object grasps. Using RGB-Depth images and instance\nsegmentation masks as input, a U-Net model outputs grasp maps with the\ncorresponding grapple orientation and opening width. Given an observation of a\npile of logs, the model can be used to synthesize and rate the possible grasp\nposes and select the most suitable one, with the possibility to respect\nchanging operational constraints such as lift capacity and reach. When tested\non previously unseen data, the proposed model found successful grasp poses with\nan accuracy up to 96%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-object grasping is a challenging task. It is important for energy and\ncost-efficient operation of industrial crane manipulators, such as those used\nto collect tree logs from the forest floor and on forest machines. In this\nwork, we used synthetic data from physics simulations to explore how\ndata-driven modeling can be used to infer multi-object grasp poses from images.\nWe showed that convolutional neural networks can be trained specifically for\nsynthesizing multi-object grasps. Using RGB-Depth images and instance\nsegmentation masks as input, a U-Net model outputs grasp maps with the\ncorresponding grapple orientation and opening width. Given an observation of a\npile of logs, the model can be used to synthesize and rate the possible grasp\nposes and select the most suitable one, with the possibility to respect\nchanging operational constraints such as lift capacity and reach. When tested\non previously unseen data, the proposed model found successful grasp poses with\nan accuracy up to 96%."
                },
                "authors": [
                    {
                        "name": "Arvid Fälldin"
                    },
                    {
                        "name": "Tommy Löfstedt"
                    },
                    {
                        "name": "Tobias Semberg"
                    },
                    {
                        "name": "Erik Wallin"
                    },
                    {
                        "name": "Martin Servin"
                    }
                ],
                "author_detail": {
                    "name": "Martin Servin"
                },
                "author": "Martin Servin",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17397v2",
                "updated": "2025-03-21T09:56:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    56,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-25T22:14:34Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    22,
                    14,
                    34,
                    2,
                    269,
                    0
                ],
                "title": "Building Multilingual Datasets for Predicting Mental Health Severity\n  through LLMs: Prospects and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Multilingual Datasets for Predicting Mental Health Severity\n  through LLMs: Prospects and Challenges"
                },
                "summary": "Large Language Models (LLMs) are increasingly being integrated into various\nmedical fields, including mental health support systems. However, there is a\ngap in research regarding the effectiveness of LLMs in non-English mental\nhealth support applications. To address this problem, we present a novel\nmultilingual adaptation of widely-used mental health datasets, translated from\nEnglish into six languages (e.g., Greek, Turkish, French, Portuguese, German,\nand Finnish). This dataset enables a comprehensive evaluation of LLM\nperformance in detecting mental health conditions and assessing their severity\nacross multiple languages. By experimenting with GPT and Llama, we observe\nconsiderable variability in performance across languages, despite being\nevaluated on the same translated dataset. This inconsistency underscores the\ncomplexities inherent in multilingual mental health support, where\nlanguage-specific nuances and mental health data coverage can affect the\naccuracy of the models. Through comprehensive error analysis, we emphasize the\nrisks of relying exclusively on LLMs in medical settings (e.g., their potential\nto contribute to misdiagnoses). Moreover, our proposed approach offers\nsignificant cost savings for multilingual tasks, presenting a major advantage\nfor broad-scale implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being integrated into various\nmedical fields, including mental health support systems. However, there is a\ngap in research regarding the effectiveness of LLMs in non-English mental\nhealth support applications. To address this problem, we present a novel\nmultilingual adaptation of widely-used mental health datasets, translated from\nEnglish into six languages (e.g., Greek, Turkish, French, Portuguese, German,\nand Finnish). This dataset enables a comprehensive evaluation of LLM\nperformance in detecting mental health conditions and assessing their severity\nacross multiple languages. By experimenting with GPT and Llama, we observe\nconsiderable variability in performance across languages, despite being\nevaluated on the same translated dataset. This inconsistency underscores the\ncomplexities inherent in multilingual mental health support, where\nlanguage-specific nuances and mental health data coverage can affect the\naccuracy of the models. Through comprehensive error analysis, we emphasize the\nrisks of relying exclusively on LLMs in medical settings (e.g., their potential\nto contribute to misdiagnoses). Moreover, our proposed approach offers\nsignificant cost savings for multilingual tasks, presenting a major advantage\nfor broad-scale implementation."
                },
                "authors": [
                    {
                        "name": "Konstantinos Skianis"
                    },
                    {
                        "name": "John Pavlopoulos"
                    },
                    {
                        "name": "A. Seza Doğruöz"
                    }
                ],
                "author_detail": {
                    "name": "A. Seza Doğruöz"
                },
                "author": "A. Seza Doğruöz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16980v1",
                "updated": "2025-03-21T09:46:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    46,
                    31,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:46:31Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    46,
                    31,
                    4,
                    80,
                    0
                ],
                "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation\n  for Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation\n  for Video Large Language Models"
                },
                "summary": "Token-based video representation has emerged as a promising approach for\nenabling large language models to interpret video content. However, existing\ntoken reduction techniques, such as token pruning and token merging, often\ndisrupt essential spatial-temporal positional embeddings, failing to adequately\nbalance computational efficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their applicability in\nscenarios requiring extreme token compression, such as video large language\nmodels. In this paper, we introduce the novel task of extreme short token\nreduction, aiming to represent extensive video sequences with a minimal number\nof tokens. To address this challenge, we propose Token Dynamics, a new video\nrepresentation framework that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle video representations\nby separating visual embeddings from grid-level motion information, structuring\nthem into: 1. a concise token base, created by clustering tokens that describe\nobject-level content; 2. a token dynamics map, capturing detailed\nspatial-temporal motion patterns across grids. Furthermore, we introduce a\ncross-dynamics attention mechanism that integrates motion features into the\ntoken base without increasing token length, thereby maintaining compactness and\nspatial-temporal integrity. The experiments demonstrate a reduction of token\ncount to merely 0.07% of the original tokens, with only a minor performance\ndrop of 1.13%. Additionally, we propose two novel subtasks within extreme token\nreduction (fixed-length and adaptive-length compression), both effectively\nrepresenting long token sequences for video-language tasks. Our method offers\nsignificantly lower theoretical complexity, fewer tokens, and enhanced\nthroughput, thus providing an efficient solution for video LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-based video representation has emerged as a promising approach for\nenabling large language models to interpret video content. However, existing\ntoken reduction techniques, such as token pruning and token merging, often\ndisrupt essential spatial-temporal positional embeddings, failing to adequately\nbalance computational efficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their applicability in\nscenarios requiring extreme token compression, such as video large language\nmodels. In this paper, we introduce the novel task of extreme short token\nreduction, aiming to represent extensive video sequences with a minimal number\nof tokens. To address this challenge, we propose Token Dynamics, a new video\nrepresentation framework that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle video representations\nby separating visual embeddings from grid-level motion information, structuring\nthem into: 1. a concise token base, created by clustering tokens that describe\nobject-level content; 2. a token dynamics map, capturing detailed\nspatial-temporal motion patterns across grids. Furthermore, we introduce a\ncross-dynamics attention mechanism that integrates motion features into the\ntoken base without increasing token length, thereby maintaining compactness and\nspatial-temporal integrity. The experiments demonstrate a reduction of token\ncount to merely 0.07% of the original tokens, with only a minor performance\ndrop of 1.13%. Additionally, we propose two novel subtasks within extreme token\nreduction (fixed-length and adaptive-length compression), both effectively\nrepresenting long token sequences for video-language tasks. Our method offers\nsignificantly lower theoretical complexity, fewer tokens, and enhanced\nthroughput, thus providing an efficient solution for video LLMs."
                },
                "authors": [
                    {
                        "name": "Haichao Zhang"
                    },
                    {
                        "name": "Zhuowei Li"
                    },
                    {
                        "name": "Dimitris Metaxas"
                    },
                    {
                        "name": "Yun Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yun Fu"
                },
                "author": "Yun Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16979v1",
                "updated": "2025-03-21T09:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    46,
                    22,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:46:22Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    46,
                    22,
                    4,
                    80,
                    0
                ],
                "title": "Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic\n  Scene Reconstruction via Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic\n  Scene Reconstruction via Gaussian Splatting"
                },
                "summary": "Building Free-Viewpoint Videos in a streaming manner offers the advantage of\nrapid responsiveness compared to offline training methods, greatly enhancing\nuser experience. However, current streaming approaches face challenges of high\nper-frame reconstruction time (10s+) and error accumulation, limiting their\nbroader application. In this paper, we propose Instant Gaussian Stream (IGS), a\nfast and generalizable streaming framework, to address these issues. First, we\nintroduce a generalized Anchor-driven Gaussian Motion Network, which projects\nmulti-view 2D motion features into 3D space, using anchor points to drive the\nmotion of all Gaussians. This generalized Network generates the motion of\nGaussians for each target frame in the time required for a single inference.\nSecond, we propose a Key-frame-guided Streaming Strategy that refines each key\nframe, enabling accurate reconstruction of temporally complex scenes while\nmitigating error accumulation. We conducted extensive in-domain and\ncross-domain evaluations, demonstrating that our approach can achieve streaming\nwith a average per-frame reconstruction time of 2s+, alongside a enhancement in\nview synthesis quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Free-Viewpoint Videos in a streaming manner offers the advantage of\nrapid responsiveness compared to offline training methods, greatly enhancing\nuser experience. However, current streaming approaches face challenges of high\nper-frame reconstruction time (10s+) and error accumulation, limiting their\nbroader application. In this paper, we propose Instant Gaussian Stream (IGS), a\nfast and generalizable streaming framework, to address these issues. First, we\nintroduce a generalized Anchor-driven Gaussian Motion Network, which projects\nmulti-view 2D motion features into 3D space, using anchor points to drive the\nmotion of all Gaussians. This generalized Network generates the motion of\nGaussians for each target frame in the time required for a single inference.\nSecond, we propose a Key-frame-guided Streaming Strategy that refines each key\nframe, enabling accurate reconstruction of temporally complex scenes while\nmitigating error accumulation. We conducted extensive in-domain and\ncross-domain evaluations, demonstrating that our approach can achieve streaming\nwith a average per-frame reconstruction time of 2s+, alongside a enhancement in\nview synthesis quality."
                },
                "authors": [
                    {
                        "name": "Jinbo Yan"
                    },
                    {
                        "name": "Rui Peng"
                    },
                    {
                        "name": "Zhiyan Wang"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Jiayu Yang"
                    },
                    {
                        "name": "Jie Liang"
                    },
                    {
                        "name": "Jiahao Wu"
                    },
                    {
                        "name": "Ronggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ronggang Wang"
                },
                "author": "Ronggang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16978v1",
                "updated": "2025-03-21T09:45:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    45,
                    59,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:45:59Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    45,
                    59,
                    4,
                    80,
                    0
                ],
                "title": "Real-Time Diffusion Policies for Games: Enhancing Consistency Policies\n  with Q-Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Diffusion Policies for Games: Enhancing Consistency Policies\n  with Q-Ensembles"
                },
                "summary": "Diffusion models have shown impressive performance in capturing complex and\nmulti-modal action distributions for game agents, but their slow inference\nspeed prevents practical deployment in real-time game environments. While\nconsistency models offer a promising approach for one-step generation, they\noften suffer from training instability and performance degradation when applied\nto policy learning. In this paper, we present CPQE (Consistency Policy with\nQ-Ensembles), which combines consistency models with Q-ensembles to address\nthese challenges.CPQE leverages uncertainty estimation through Q-ensembles to\nprovide more reliable value function approximations, resulting in better\ntraining stability and improved performance compared to classic double\nQ-network methods. Our extensive experiments across multiple game scenarios\ndemonstrate that CPQE achieves inference speeds of up to 60 Hz -- a significant\nimprovement over state-of-the-art diffusion policies that operate at only 20 Hz\n-- while maintaining comparable performance to multi-step diffusion approaches.\nCPQE consistently outperforms state-of-the-art consistency model approaches,\nshowing both higher rewards and enhanced training stability throughout the\nlearning process. These results indicate that CPQE offers a practical solution\nfor deploying diffusion-based policies in games and other real-time\napplications where both multi-modal behavior modeling and rapid inference are\ncritical requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown impressive performance in capturing complex and\nmulti-modal action distributions for game agents, but their slow inference\nspeed prevents practical deployment in real-time game environments. While\nconsistency models offer a promising approach for one-step generation, they\noften suffer from training instability and performance degradation when applied\nto policy learning. In this paper, we present CPQE (Consistency Policy with\nQ-Ensembles), which combines consistency models with Q-ensembles to address\nthese challenges.CPQE leverages uncertainty estimation through Q-ensembles to\nprovide more reliable value function approximations, resulting in better\ntraining stability and improved performance compared to classic double\nQ-network methods. Our extensive experiments across multiple game scenarios\ndemonstrate that CPQE achieves inference speeds of up to 60 Hz -- a significant\nimprovement over state-of-the-art diffusion policies that operate at only 20 Hz\n-- while maintaining comparable performance to multi-step diffusion approaches.\nCPQE consistently outperforms state-of-the-art consistency model approaches,\nshowing both higher rewards and enhanced training stability throughout the\nlearning process. These results indicate that CPQE offers a practical solution\nfor deploying diffusion-based policies in games and other real-time\napplications where both multi-modal behavior modeling and rapid inference are\ncritical requirements."
                },
                "authors": [
                    {
                        "name": "Ruoqi Zhang"
                    },
                    {
                        "name": "Ziwei Luo"
                    },
                    {
                        "name": "Jens Sjölund"
                    },
                    {
                        "name": "Per Mattsson"
                    },
                    {
                        "name": "Linus Gisslén"
                    },
                    {
                        "name": "Alessandro Sestini"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Sestini"
                },
                "author": "Alessandro Sestini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16974v1",
                "updated": "2025-03-21T09:43:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    43,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:43:37Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    43,
                    37,
                    4,
                    80,
                    0
                ],
                "title": "Assessing Consistency and Reproducibility in the Outputs of Large\n  Language Models: Evidence Across Diverse Finance and Accounting Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Consistency and Reproducibility in the Outputs of Large\n  Language Models: Evidence Across Diverse Finance and Accounting Tasks"
                },
                "summary": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. Simulation analysis reveals that despite measurable\ninconsistency in LLM outputs, downstream statistical inferences remain\nremarkably robust. These findings address concerns about what we term\n\"G-hacking,\" the selective reporting of favorable outcomes from multiple\nGenerative AI runs, by demonstrating that such risks are relatively low for\nfinance and accounting tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. Simulation analysis reveals that despite measurable\ninconsistency in LLM outputs, downstream statistical inferences remain\nremarkably robust. These findings address concerns about what we term\n\"G-hacking,\" the selective reporting of favorable outcomes from multiple\nGenerative AI runs, by demonstrating that such risks are relatively low for\nfinance and accounting tasks."
                },
                "authors": [
                    {
                        "name": "Julian Junyan Wang"
                    },
                    {
                        "name": "Victor Xiaoqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Victor Xiaoqi Wang"
                },
                "author": "Victor Xiaoqi Wang",
                "arxiv_comment": "96 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12478v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12478v3",
                "updated": "2025-03-21T09:32:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    32,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T02:29:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    29,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on\n  Tibetan Script",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on\n  Tibetan Script"
                },
                "summary": "DNN-based language models perform excellently on various tasks, but even SOTA\nLLMs are susceptible to textual adversarial attacks. Adversarial texts play\ncrucial roles in multiple subfields of NLP. However, current research has the\nfollowing issues. (1) Most textual adversarial attack methods target\nrich-resourced languages. How do we generate adversarial texts for less-studied\nlanguages? (2) Most textual adversarial attack methods are prone to generating\ninvalid or ambiguous adversarial texts. How do we construct high-quality\nadversarial robustness benchmarks? (3) New language models may be immune to\npart of previously generated adversarial texts. How do we update adversarial\nrobustness benchmarks? To address the above issues, we introduce HITL-GAT, a\nsystem based on a general approach to human-in-the-loop generation of\nadversarial texts. HITL-GAT contains four stages in one pipeline: victim model\nconstruction, adversarial example generation, high-quality benchmark\nconstruction, and adversarial robustness evaluation. Additionally, we utilize\nHITL-GAT to make a case study on Tibetan script which can be a reference for\nthe adversarial research of other less-studied languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNN-based language models perform excellently on various tasks, but even SOTA\nLLMs are susceptible to textual adversarial attacks. Adversarial texts play\ncrucial roles in multiple subfields of NLP. However, current research has the\nfollowing issues. (1) Most textual adversarial attack methods target\nrich-resourced languages. How do we generate adversarial texts for less-studied\nlanguages? (2) Most textual adversarial attack methods are prone to generating\ninvalid or ambiguous adversarial texts. How do we construct high-quality\nadversarial robustness benchmarks? (3) New language models may be immune to\npart of previously generated adversarial texts. How do we update adversarial\nrobustness benchmarks? To address the above issues, we introduce HITL-GAT, a\nsystem based on a general approach to human-in-the-loop generation of\nadversarial texts. HITL-GAT contains four stages in one pipeline: victim model\nconstruction, adversarial example generation, high-quality benchmark\nconstruction, and adversarial robustness evaluation. Additionally, we utilize\nHITL-GAT to make a case study on Tibetan script which can be a reference for\nthe adversarial research of other less-studied languages."
                },
                "authors": [
                    {
                        "name": "Xi Cao"
                    },
                    {
                        "name": "Yuan Sun"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Quzong Gesang"
                    },
                    {
                        "name": "Nuo Qun"
                    },
                    {
                        "name": "Tashi Nyima"
                    }
                ],
                "author_detail": {
                    "name": "Tashi Nyima"
                },
                "author": "Tashi Nyima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12478v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12478v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16965v1",
                "updated": "2025-03-21T09:25:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    25,
                    23,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:25:23Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    25,
                    23,
                    4,
                    80,
                    0
                ],
                "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making"
                },
                "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms."
                },
                "authors": [
                    {
                        "name": "Zhe Hu"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Yu Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yu Yin"
                },
                "author": "Yu Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16963v1",
                "updated": "2025-03-21T09:21:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    21,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:21:37Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    21,
                    37,
                    4,
                    80,
                    0
                ],
                "title": "Center-guided Classifier for Semantic Segmentation of Remote Sensing\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Center-guided Classifier for Semantic Segmentation of Remote Sensing\n  Images"
                },
                "summary": "Compared with natural images, remote sensing images (RSIs) have the unique\ncharacteristic. i.e., larger intraclass variance, which makes semantic\nsegmentation for remote sensing images more challenging. Moreover, existing\nsemantic segmentation models for remote sensing images usually employ a vanilla\nsoftmax classifier, which has three drawbacks: (1) non-direct supervision for\nthe pixel representations during training; (2) inadequate modeling ability of\nparametric softmax classifiers under large intraclass variance; and (3) opaque\nprocess of classification decision. In this paper, we propose a novel\nclassifier (called CenterSeg) customized for RSI semantic segmentation, which\nsolves the abovementioned problems with multiple prototypes, direct supervision\nunder Grassmann manifold, and interpretability strategy. Specifically, for each\nclass, our CenterSeg obtains local class centers by aggregating corresponding\npixel features based on ground-truth masks, and generates multiple prototypes\nthrough hard attention assignment and momentum updating. In addition, we\nintroduce the Grassmann manifold and constrain the joint embedding space of\npixel features and prototypes based on two additional regularization terms.\nEspecially, during the inference, CenterSeg can further provide\ninterpretability to the model by restricting the prototype as a sample of the\ntraining set. Experimental results on three remote sensing segmentation\ndatasets validate the effectiveness of the model. Besides the superior\nperformance, CenterSeg has the advantages of simplicity, lightweight,\ncompatibility, and interpretability. Code is available at\nhttps://github.com/xwmaxwma/rssegmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared with natural images, remote sensing images (RSIs) have the unique\ncharacteristic. i.e., larger intraclass variance, which makes semantic\nsegmentation for remote sensing images more challenging. Moreover, existing\nsemantic segmentation models for remote sensing images usually employ a vanilla\nsoftmax classifier, which has three drawbacks: (1) non-direct supervision for\nthe pixel representations during training; (2) inadequate modeling ability of\nparametric softmax classifiers under large intraclass variance; and (3) opaque\nprocess of classification decision. In this paper, we propose a novel\nclassifier (called CenterSeg) customized for RSI semantic segmentation, which\nsolves the abovementioned problems with multiple prototypes, direct supervision\nunder Grassmann manifold, and interpretability strategy. Specifically, for each\nclass, our CenterSeg obtains local class centers by aggregating corresponding\npixel features based on ground-truth masks, and generates multiple prototypes\nthrough hard attention assignment and momentum updating. In addition, we\nintroduce the Grassmann manifold and constrain the joint embedding space of\npixel features and prototypes based on two additional regularization terms.\nEspecially, during the inference, CenterSeg can further provide\ninterpretability to the model by restricting the prototype as a sample of the\ntraining set. Experimental results on three remote sensing segmentation\ndatasets validate the effectiveness of the model. Besides the superior\nperformance, CenterSeg has the advantages of simplicity, lightweight,\ncompatibility, and interpretability. Code is available at\nhttps://github.com/xwmaxwma/rssegmentation."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Mengting Ma"
                    },
                    {
                        "name": "Yizhen Jiang"
                    },
                    {
                        "name": "Rongrong Lian"
                    },
                    {
                        "name": "Zhenkai Wu"
                    },
                    {
                        "name": "Kangning Cui"
                    },
                    {
                        "name": "Xiaowen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Ma"
                },
                "author": "Xiaowen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07871v2",
                "updated": "2025-03-21T09:17:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    17,
                    5,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-12T09:28:34Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    28,
                    34,
                    3,
                    256,
                    0
                ],
                "title": "Objection Overruled! Lay People can Distinguish Large Language Models\n  from Lawyers, but still Favour Advice from an LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objection Overruled! Lay People can Distinguish Large Language Models\n  from Lawyers, but still Favour Advice from an LLM"
                },
                "summary": "Large Language Models (LLMs) are seemingly infiltrating every domain, and the\nlegal context is no exception. In this paper, we present the results of three\nexperiments (total N = 288) that investigated lay people's willingness to act\nupon, and their ability to discriminate between, LLM- and lawyer-generated\nlegal advice. In Experiment 1, participants judged their willingness to act on\nlegal advice when the source of the advice was either known or unknown. When\nthe advice source was unknown, participants indicated that they were\nsignificantly more willing to act on the LLM-generated advice. The result of\nthe source unknown condition was replicated in Experiment 2. Intriguingly,\ndespite participants indicating higher willingness to act on LLM-generated\nadvice in Experiments 1 and 2, participants discriminated between the LLM- and\nlawyer-generated texts significantly above chance-level in Experiment 3.\nLastly, we discuss potential explanations and risks of our findings,\nlimitations and future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are seemingly infiltrating every domain, and the\nlegal context is no exception. In this paper, we present the results of three\nexperiments (total N = 288) that investigated lay people's willingness to act\nupon, and their ability to discriminate between, LLM- and lawyer-generated\nlegal advice. In Experiment 1, participants judged their willingness to act on\nlegal advice when the source of the advice was either known or unknown. When\nthe advice source was unknown, participants indicated that they were\nsignificantly more willing to act on the LLM-generated advice. The result of\nthe source unknown condition was replicated in Experiment 2. Intriguingly,\ndespite participants indicating higher willingness to act on LLM-generated\nadvice in Experiments 1 and 2, participants discriminated between the LLM- and\nlawyer-generated texts significantly above chance-level in Experiment 3.\nLastly, we discuss potential explanations and risks of our findings,\nlimitations and future work."
                },
                "authors": [
                    {
                        "name": "Eike Schneiders"
                    },
                    {
                        "name": "Tina Seabrooke"
                    },
                    {
                        "name": "Joshua Krook"
                    },
                    {
                        "name": "Richard Hyde"
                    },
                    {
                        "name": "Natalie Leesakul"
                    },
                    {
                        "name": "Jeremie Clos"
                    },
                    {
                        "name": "Joel Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Joel Fischer"
                },
                "author": "Joel Fischer",
                "arxiv_doi": "10.1145/3706598.3713470",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713470",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACMConference on Human Factors in Computing Systems (CHI'25)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16962v1",
                "updated": "2025-03-21T09:16:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    16,
                    46,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:16:46Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    16,
                    46,
                    4,
                    80,
                    0
                ],
                "title": "ATP requirements for growth reveal the bioenergetic impact of\n  mitochondrial symbiosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATP requirements for growth reveal the bioenergetic impact of\n  mitochondrial symbiosis"
                },
                "summary": "Studies by microbiologists from the 1970s provided robust estimates for the\nenergy supply and demand of a prokaryotic cell. The amount of ATP needed to\nsupport growth was calculated from the chemical composition of the cell and\nknown enzymatic pathways that synthesize its constituents from known substrates\nin culture. Starting in 2015, geneticists and evolutionary biologists began\ninvestigating the bioenergetic role of mitochondria at eukaryote origin and\nenergy in metazoan evolution using their own, widely trusted but hitherto\nunvetted model for the costs of growth in terms of ATP per cell. The more\nrecent model contains, however, a severe and previously unrecognized error that\nsystematically overestimates the ATP cost of amino acid synthesis up to 200\nfold. The error applies to all organisms studied by such models and leads to\nconspicuously false inferences, for example that the synthesis of an average\namino acid in humans requires 30 ATP, which no biochemistry textbook will\nconfirm. Their ATP cost calculations would require that Escherichia coli\nobtains roughly 100 ATP per glucose and that mammals obtain roughly 240 ATP per\nglucose, propositions that invalidate evolutionary inferences so based. By\ncontrast, established methods for estimating the ATP cost of microbial growth\nshow that the first mitochondrial endosymbionts could have easily doubled the\nhosts available ATP pool, provided that genes for growth on environmental amino\nacids were transferred from the mitochondrial symbiont to the archaeal host and\nthat the host for mitochondrial origin was an autotroph using the acetyl-CoA\npathway.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies by microbiologists from the 1970s provided robust estimates for the\nenergy supply and demand of a prokaryotic cell. The amount of ATP needed to\nsupport growth was calculated from the chemical composition of the cell and\nknown enzymatic pathways that synthesize its constituents from known substrates\nin culture. Starting in 2015, geneticists and evolutionary biologists began\ninvestigating the bioenergetic role of mitochondria at eukaryote origin and\nenergy in metazoan evolution using their own, widely trusted but hitherto\nunvetted model for the costs of growth in terms of ATP per cell. The more\nrecent model contains, however, a severe and previously unrecognized error that\nsystematically overestimates the ATP cost of amino acid synthesis up to 200\nfold. The error applies to all organisms studied by such models and leads to\nconspicuously false inferences, for example that the synthesis of an average\namino acid in humans requires 30 ATP, which no biochemistry textbook will\nconfirm. Their ATP cost calculations would require that Escherichia coli\nobtains roughly 100 ATP per glucose and that mammals obtain roughly 240 ATP per\nglucose, propositions that invalidate evolutionary inferences so based. By\ncontrast, established methods for estimating the ATP cost of microbial growth\nshow that the first mitochondrial endosymbionts could have easily doubled the\nhosts available ATP pool, provided that genes for growth on environmental amino\nacids were transferred from the mitochondrial symbiont to the archaeal host and\nthat the host for mitochondrial origin was an autotroph using the acetyl-CoA\npathway."
                },
                "authors": [
                    {
                        "name": "William F. Martin"
                    }
                ],
                "author_detail": {
                    "name": "William F. Martin"
                },
                "author": "William F. Martin",
                "arxiv_comment": "27 pages, 4 Tables, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10705v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10705v2",
                "updated": "2025-03-21T09:15:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    15,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-12T15:48:13Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    48,
                    13,
                    2,
                    71,
                    0
                ],
                "title": "Enhanced Continual Learning of Vision-Language Models with Model Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Continual Learning of Vision-Language Models with Model Fusion"
                },
                "summary": "Vision-Language Models (VLMs) represent a breakthrough in artificial\nintelligence by integrating visual and textual modalities to achieve impressive\nzero-shot capabilities. However, VLMs are susceptible to catastrophic\nforgetting when sequentially fine-tuned on multiple downstream tasks. Existing\ncontinual learning methods for VLMs often rely heavily on additional reference\ndatasets, compromise zero-shot performance, or are limited to\nparameter-efficient fine-tuning scenarios. In this paper, we propose Continual\nDecoupling-Unifying (ConDU), a novel approach, by introducing model fusion into\ncontinual learning for VLMs. ConDU maintains a unified model along with task\ntriggers and prototype sets, employing an iterative process of decoupling\ntask-specific models for previous tasks and unifying them with the model for\nthe newly learned task. Additionally, we introduce an inference strategy for\nzero-shot scenarios by aggregating predictions from multiple decoupled\ntask-specific models. Extensive experiments across various settings show that\nConDU achieves up to a 2\\% improvement in average performance across all seen\ntasks compared to state-of-the-art baselines, while also enhancing zero-shot\ncapabilities relative to the original VLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) represent a breakthrough in artificial\nintelligence by integrating visual and textual modalities to achieve impressive\nzero-shot capabilities. However, VLMs are susceptible to catastrophic\nforgetting when sequentially fine-tuned on multiple downstream tasks. Existing\ncontinual learning methods for VLMs often rely heavily on additional reference\ndatasets, compromise zero-shot performance, or are limited to\nparameter-efficient fine-tuning scenarios. In this paper, we propose Continual\nDecoupling-Unifying (ConDU), a novel approach, by introducing model fusion into\ncontinual learning for VLMs. ConDU maintains a unified model along with task\ntriggers and prototype sets, employing an iterative process of decoupling\ntask-specific models for previous tasks and unifying them with the model for\nthe newly learned task. Additionally, we introduce an inference strategy for\nzero-shot scenarios by aggregating predictions from multiple decoupled\ntask-specific models. Extensive experiments across various settings show that\nConDU achieves up to a 2\\% improvement in average performance across all seen\ntasks compared to state-of-the-art baselines, while also enhancing zero-shot\ncapabilities relative to the original VLM."
                },
                "authors": [
                    {
                        "name": "Haoyuan Gao"
                    },
                    {
                        "name": "Zicong Zhang"
                    },
                    {
                        "name": "Yuqi Wei"
                    },
                    {
                        "name": "Linglan Zhao"
                    },
                    {
                        "name": "Guilin Li"
                    },
                    {
                        "name": "Yexin Li"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Weiran Huang"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Huang"
                },
                "author": "Weiran Huang",
                "arxiv_comment": "Accepted by ICLR 2025 workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10705v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10705v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10298v2",
                "updated": "2025-03-21T08:58:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    58,
                    30,
                    4,
                    80,
                    0
                ],
                "published": "2024-08-19T18:00:00Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    18,
                    0,
                    0,
                    0,
                    232,
                    0
                ],
                "title": "Signature of hadron-quark crossover in binary-neutron-star mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signature of hadron-quark crossover in binary-neutron-star mergers"
                },
                "summary": "We study observational signatures of the hadron-quark crossover in\nbinary-neutron-star mergers by numerical-relativity simulations with various\nmass configurations. We employ two equations of state (EoSs) for matter\nconsistent with inference from the observational data. In the crossover\nscenario the EoS is softened in a density realized in binary-neutron-star\nmergers and is smoothly continued to quark matter. In the phase transition\nscenario without crossover, the EoS remains stiff and a first-order phase\ntransition takes place in a density out of reach of mergers. A GW170817-like\nsystem forms a remnant massive neutron star in both scenarios, and it collapses\ninto a black hole only in the crossover scenario due to the softening while\ngravitational-wave emission is strong. This difference is clearly reflected in\nthe sudden shutdown of gravitational waves. For a given EoS, the lifetime of\nthe merger remnant is determined primarily by the total mass of the system.\nIdentifying these features in a variety of future events with the next\ngeneration of ground-based gravitational-wave detectors will enable us to\nclarify details of hadron-quark transition. The mass of the accretion disk\nsurrounding the remnant black hole is affected not only by the lifetime of the\nremnant but also by the mass ratio of the system. Electromagnetic emission\nassociated with the disk outflow will also be useful for detailed investigation\nof the hadron-quark transition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study observational signatures of the hadron-quark crossover in\nbinary-neutron-star mergers by numerical-relativity simulations with various\nmass configurations. We employ two equations of state (EoSs) for matter\nconsistent with inference from the observational data. In the crossover\nscenario the EoS is softened in a density realized in binary-neutron-star\nmergers and is smoothly continued to quark matter. In the phase transition\nscenario without crossover, the EoS remains stiff and a first-order phase\ntransition takes place in a density out of reach of mergers. A GW170817-like\nsystem forms a remnant massive neutron star in both scenarios, and it collapses\ninto a black hole only in the crossover scenario due to the softening while\ngravitational-wave emission is strong. This difference is clearly reflected in\nthe sudden shutdown of gravitational waves. For a given EoS, the lifetime of\nthe merger remnant is determined primarily by the total mass of the system.\nIdentifying these features in a variety of future events with the next\ngeneration of ground-based gravitational-wave detectors will enable us to\nclarify details of hadron-quark transition. The mass of the accretion disk\nsurrounding the remnant black hole is affected not only by the lifetime of the\nremnant but also by the mass ratio of the system. Electromagnetic emission\nassociated with the disk outflow will also be useful for detailed investigation\nof the hadron-quark transition."
                },
                "authors": [
                    {
                        "name": "Yuki Fujimoto"
                    },
                    {
                        "name": "Kenji Fukushima"
                    },
                    {
                        "name": "Kenta Hotokezaka"
                    },
                    {
                        "name": "Koutarou Kyutoku"
                    }
                ],
                "author_detail": {
                    "name": "Koutarou Kyutoku"
                },
                "author": "Koutarou Kyutoku",
                "arxiv_doi": "10.1103/PhysRevD.111.063054",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.063054",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 063054 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16944v1",
                "updated": "2025-03-21T08:44:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    44,
                    27,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T08:44:27Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    44,
                    27,
                    4,
                    80,
                    0
                ],
                "title": "HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait\n  Synthesis"
                },
                "summary": "Personalized portrait synthesis, essential in domains like social\nentertainment, has recently made significant progress. Person-wise fine-tuning\nbased methods, such as LoRA and DreamBooth, can produce photorealistic outputs\nbut need training on individual samples, consuming time and resources and\nposing an unstable risk. Adapter based techniques such as IP-Adapter freeze the\nfoundational model parameters and employ a plug-in architecture to enable\nzero-shot inference, but they often exhibit a lack of naturalness and\nauthenticity, which are not to be overlooked in portrait synthesis tasks. In\nthis paper, we introduce a parameter-efficient adaptive generation method,\nnamely HyperLoRA, that uses an adaptive plug-in network to generate LoRA\nweights, merging the superior performance of LoRA with the zero-shot capability\nof adapter scheme. Through our carefully designed network structure and\ntraining strategy, we achieve zero-shot personalized portrait generation\n(supporting both single and multiple image inputs) with high photorealism,\nfidelity, and editability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized portrait synthesis, essential in domains like social\nentertainment, has recently made significant progress. Person-wise fine-tuning\nbased methods, such as LoRA and DreamBooth, can produce photorealistic outputs\nbut need training on individual samples, consuming time and resources and\nposing an unstable risk. Adapter based techniques such as IP-Adapter freeze the\nfoundational model parameters and employ a plug-in architecture to enable\nzero-shot inference, but they often exhibit a lack of naturalness and\nauthenticity, which are not to be overlooked in portrait synthesis tasks. In\nthis paper, we introduce a parameter-efficient adaptive generation method,\nnamely HyperLoRA, that uses an adaptive plug-in network to generate LoRA\nweights, merging the superior performance of LoRA with the zero-shot capability\nof adapter scheme. Through our carefully designed network structure and\ntraining strategy, we achieve zero-shot personalized portrait generation\n(supporting both single and multiple image inputs) with high photorealism,\nfidelity, and editability."
                },
                "authors": [
                    {
                        "name": "Mengtian Li"
                    },
                    {
                        "name": "Jinshu Chen"
                    },
                    {
                        "name": "Wanquan Feng"
                    },
                    {
                        "name": "Bingchuan Li"
                    },
                    {
                        "name": "Fei Dai"
                    },
                    {
                        "name": "Songtao Zhao"
                    },
                    {
                        "name": "Qian He"
                    }
                ],
                "author_detail": {
                    "name": "Qian He"
                },
                "author": "Qian He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16943v1",
                "updated": "2025-03-21T08:43:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    43,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T08:43:02Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    43,
                    2,
                    4,
                    80,
                    0
                ],
                "title": "Model-free front-to-end training of a large high performance laser\n  neural network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-free front-to-end training of a large high performance laser\n  neural network"
                },
                "summary": "Artificial neural networks (ANNs), have become ubiquitous and revolutionized\nmany applications ranging from computer vision to medical diagnoses. However,\nthey offer a fundamentally connectionist and distributed approach to computing,\nin stark contrast to classical computers that use the von Neumann architecture.\nThis distinction has sparked renewed interest in developing unconventional\nhardware to support more efficient implementations of ANNs, rather than merely\nemulating them on traditional systems. Photonics stands out as a particularly\npromising platform, providing scalability, high speed, energy efficiency, and\nthe ability for parallel information processing. However, fully realized\nautonomous optical neural networks (ONNs) with in-situ learning capabilities\nare still rare. In this work, we demonstrate a fully autonomous and parallel\nONN using a multimode vertical cavity surface emitting laser (VCSEL) using\noff-the-shelf components. Our ONN is highly efficient and is scalable both in\nnetwork size and inference bandwidth towards the GHz range. High performance\nhardware-compatible optimization algorithms are necessary in order to minimize\nreliance on external von Neumann computers to fully exploit the potential of\nONNs. As such we present and extensively study several algorithms which are\nbroadly compatible with a wide range of systems. We then apply these algorithms\nto optimize our ONN, and benchmark them using the MNIST dataset. We show that\nour ONN can achieve high accuracy and convergence efficiency, even under\nlimited hardware resources. Crucially, we compare these different algorithms in\nterms of scaling and optimization efficiency in term of convergence time which\nis crucial when working with limited external resources. Our work provides some\nguidance for the design of future ONNs as well as a simple and flexible way to\ntrain them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial neural networks (ANNs), have become ubiquitous and revolutionized\nmany applications ranging from computer vision to medical diagnoses. However,\nthey offer a fundamentally connectionist and distributed approach to computing,\nin stark contrast to classical computers that use the von Neumann architecture.\nThis distinction has sparked renewed interest in developing unconventional\nhardware to support more efficient implementations of ANNs, rather than merely\nemulating them on traditional systems. Photonics stands out as a particularly\npromising platform, providing scalability, high speed, energy efficiency, and\nthe ability for parallel information processing. However, fully realized\nautonomous optical neural networks (ONNs) with in-situ learning capabilities\nare still rare. In this work, we demonstrate a fully autonomous and parallel\nONN using a multimode vertical cavity surface emitting laser (VCSEL) using\noff-the-shelf components. Our ONN is highly efficient and is scalable both in\nnetwork size and inference bandwidth towards the GHz range. High performance\nhardware-compatible optimization algorithms are necessary in order to minimize\nreliance on external von Neumann computers to fully exploit the potential of\nONNs. As such we present and extensively study several algorithms which are\nbroadly compatible with a wide range of systems. We then apply these algorithms\nto optimize our ONN, and benchmark them using the MNIST dataset. We show that\nour ONN can achieve high accuracy and convergence efficiency, even under\nlimited hardware resources. Crucially, we compare these different algorithms in\nterms of scaling and optimization efficiency in term of convergence time which\nis crucial when working with limited external resources. Our work provides some\nguidance for the design of future ONNs as well as a simple and flexible way to\ntrain them."
                },
                "authors": [
                    {
                        "name": "Anas Skalli"
                    },
                    {
                        "name": "Satoshi Sunada"
                    },
                    {
                        "name": "Mirko Goldmann"
                    },
                    {
                        "name": "Marcin Gebski"
                    },
                    {
                        "name": "Stephan Reitzenstein"
                    },
                    {
                        "name": "James A. Lott"
                    },
                    {
                        "name": "Tomasz Czyszanowski"
                    },
                    {
                        "name": "Daniel Brunner"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Brunner"
                },
                "author": "Daniel Brunner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05933v2",
                "updated": "2025-03-21T08:41:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    41,
                    23,
                    4,
                    80,
                    0
                ],
                "published": "2025-01-10T12:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    12,
                    56,
                    18,
                    4,
                    10,
                    0
                ],
                "title": "Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact\n  Convolutional Transformers and SAM2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly Supervised Segmentation of Hyper-Reflective Foci with Compact\n  Convolutional Transformers and SAM2"
                },
                "summary": "Weakly supervised segmentation has the potential to greatly reduce the\nannotation effort for training segmentation models for small structures such as\nhyper-reflective foci (HRF) in optical coherence tomography (OCT). However,\nmost weakly supervised methods either involve a strong downsampling of input\nimages, or only achieve localization at a coarse resolution, both of which are\nunsatisfactory for small structures. We propose a novel framework that\nincreases the spatial resolution of a traditional attention-based Multiple\nInstance Learning (MIL) approach by using Layer-wise Relevance Propagation\n(LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with\niterative inference. Moreover, we demonstrate that replacing MIL with a Compact\nConvolutional Transformer (CCT), which adds a positional encoding, and permits\nan exchange of information between different regions of the OCT image, leads to\na further and substantial increase in segmentation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly supervised segmentation has the potential to greatly reduce the\nannotation effort for training segmentation models for small structures such as\nhyper-reflective foci (HRF) in optical coherence tomography (OCT). However,\nmost weakly supervised methods either involve a strong downsampling of input\nimages, or only achieve localization at a coarse resolution, both of which are\nunsatisfactory for small structures. We propose a novel framework that\nincreases the spatial resolution of a traditional attention-based Multiple\nInstance Learning (MIL) approach by using Layer-wise Relevance Propagation\n(LRP) to prompt the Segment Anything Model (SAM~2), and increases recall with\niterative inference. Moreover, we demonstrate that replacing MIL with a Compact\nConvolutional Transformer (CCT), which adds a positional encoding, and permits\nan exchange of information between different regions of the OCT image, leads to\na further and substantial increase in segmentation accuracy."
                },
                "authors": [
                    {
                        "name": "Olivier Morelle"
                    },
                    {
                        "name": "Justus Bisten"
                    },
                    {
                        "name": "Maximilian W. M. Wintergerst"
                    },
                    {
                        "name": "Robert P. Finger"
                    },
                    {
                        "name": "Thomas Schultz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Schultz"
                },
                "arxiv_affiliation": "Lamarr Institute for Machine Learning and Artificial Intelligence",
                "author": "Thomas Schultz",
                "arxiv_doi": "10.1007/978-3-658-47422-5_23",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_23",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 1 figure, accepted at German Conference on Medical Image\n  Computing 2025",
                "arxiv_journal_ref": "Bildverarbeitung fuer die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.17363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17363v1",
                "updated": "2025-03-21T17:59:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    59,
                    55,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:59:55Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    59,
                    55,
                    4,
                    80,
                    0
                ],
                "title": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural\n  Language Self-Critique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural\n  Language Self-Critique"
                },
                "summary": "Enhancing the reasoning capabilities of large language models (LLMs),\nparticularly for complex tasks requiring multi-step logical deductions, remains\na significant challenge. Traditional inference time scaling methods utilize\nscalar reward signals from process reward models to evaluate candidate\nreasoning steps, but these scalar rewards lack the nuanced qualitative\ninformation essential for understanding and justifying each step. In this\npaper, we propose a novel inference-time scaling approach -- stepwise natural\nlanguage self-critique (PANEL), which employs self-generated natural language\ncritiques as feedback to guide the step-level search process. By generating\nrich, human-readable critiques for each candidate reasoning step, PANEL retains\nessential qualitative information, facilitating better-informed decision-making\nduring inference. This approach bypasses the need for task-specific verifiers\nand the associated training overhead, making it broadly applicable across\ndiverse tasks. Experimental results on challenging reasoning benchmarks,\nincluding AIME and GPQA, demonstrate that PANEL significantly enhances\nreasoning performance, outperforming traditional scalar reward-based methods.\nOur code is available at https://github.com/puddingyeah/PANEL to support and\nencourage future research in this promising field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the reasoning capabilities of large language models (LLMs),\nparticularly for complex tasks requiring multi-step logical deductions, remains\na significant challenge. Traditional inference time scaling methods utilize\nscalar reward signals from process reward models to evaluate candidate\nreasoning steps, but these scalar rewards lack the nuanced qualitative\ninformation essential for understanding and justifying each step. In this\npaper, we propose a novel inference-time scaling approach -- stepwise natural\nlanguage self-critique (PANEL), which employs self-generated natural language\ncritiques as feedback to guide the step-level search process. By generating\nrich, human-readable critiques for each candidate reasoning step, PANEL retains\nessential qualitative information, facilitating better-informed decision-making\nduring inference. This approach bypasses the need for task-specific verifiers\nand the associated training overhead, making it broadly applicable across\ndiverse tasks. Experimental results on challenging reasoning benchmarks,\nincluding AIME and GPQA, demonstrate that PANEL significantly enhances\nreasoning performance, outperforming traditional scalar reward-based methods.\nOur code is available at https://github.com/puddingyeah/PANEL to support and\nencourage future research in this promising field."
                },
                "authors": [
                    {
                        "name": "Yansi Li"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_doi": "10.13140/RG.2.2.27912.33289",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.27912.33289",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17352v1",
                "updated": "2025-03-21T17:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    52,
                    43,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    52,
                    43,
                    4,
                    80,
                    0
                ],
                "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement"
                },
                "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker."
                },
                "authors": [
                    {
                        "name": "Yihe Deng"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Fan Yin"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "arxiv_comment": "23 pages, 11 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14649v2",
                "updated": "2025-03-21T17:51:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    51,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-18T18:58:13Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    58,
                    13,
                    1,
                    77,
                    0
                ],
                "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented\n  Generation Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGO: Systematic Performance Optimization for Retrieval-Augmented\n  Generation Serving"
                },
                "summary": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions."
                },
                "authors": [
                    {
                        "name": "Wenqi Jiang"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Cat Graves"
                    },
                    {
                        "name": "Gustavo Alonso"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Vidushi Dadu"
                    }
                ],
                "author_detail": {
                    "name": "Vidushi Dadu"
                },
                "author": "Vidushi Dadu",
                "arxiv_comment": "16 pages, 19 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; C.4; H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17343v1",
                "updated": "2025-03-21T17:45:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    45,
                    44,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:45:44Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    45,
                    44,
                    4,
                    80,
                    0
                ],
                "title": "Commercial Dishes Can Be My Ladder: Sustainable and Collaborative Data\n  Offloading in LEO Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commercial Dishes Can Be My Ladder: Sustainable and Collaborative Data\n  Offloading in LEO Satellite Networks"
                },
                "summary": "Low Earth Orbit (LEO) satellite networks, characterized by their high data\nthroughput and low latency, have gained significant interest from both industry\nand academia. Routing data efficiently within these networks is essential for\nmaintaining a high quality of service. However, current routing strategies,\nsuch as bent-pipe and inter-satellite link (ISL) routing, have their unique\nchallenges. The bent-pipe strategy requires a dense deployment of dedicated\nground stations, while the ISL-based strategy can negatively impact satellite\nbattery lifespan due to increased traffic load, leading to sustainability\nissues.\n  In this paper, we propose sustainable collaborative offloading, a framework\nthat orchestrates groups of existing commercial resources like ground stations\nand 5G base stations for data offloading. This orchestration enhances total\ncapacity, overcoming the limitations of a single resource. We propose the\ncollaborator group set construction algorithm to construct candidate groups and\nthe collaborator selection and total payment algorithm to select offloading\ntargets and determine payments no less than the costs. Extensive\nreal-world-based simulations show that our solution significantly improves\nenergy consumption, satellite service life, and end-to-end latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit (LEO) satellite networks, characterized by their high data\nthroughput and low latency, have gained significant interest from both industry\nand academia. Routing data efficiently within these networks is essential for\nmaintaining a high quality of service. However, current routing strategies,\nsuch as bent-pipe and inter-satellite link (ISL) routing, have their unique\nchallenges. The bent-pipe strategy requires a dense deployment of dedicated\nground stations, while the ISL-based strategy can negatively impact satellite\nbattery lifespan due to increased traffic load, leading to sustainability\nissues.\n  In this paper, we propose sustainable collaborative offloading, a framework\nthat orchestrates groups of existing commercial resources like ground stations\nand 5G base stations for data offloading. This orchestration enhances total\ncapacity, overcoming the limitations of a single resource. We propose the\ncollaborator group set construction algorithm to construct candidate groups and\nthe collaborator selection and total payment algorithm to select offloading\ntargets and determine payments no less than the costs. Extensive\nreal-world-based simulations show that our solution significantly improves\nenergy consumption, satellite service life, and end-to-end latency."
                },
                "authors": [
                    {
                        "name": "Yi Ching Chou"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Hengzhi Wang"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Hao Fang"
                    },
                    {
                        "name": "Haoyuan Zhao"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Xiaoyi Fan"
                    },
                    {
                        "name": "Jiangchuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangchuan Liu"
                },
                "author": "Jiangchuan Liu",
                "arxiv_comment": "This is a preliminary extended version of the paper accepted to\n  INFOCOM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16047v2",
                "updated": "2025-03-21T17:40:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    40,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T11:31:45Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    31,
                    45,
                    3,
                    79,
                    0
                ],
                "title": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in\n  Network Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in\n  Network Traffic"
                },
                "summary": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications."
                },
                "authors": [
                    {
                        "name": "Bisola Faith Kayode"
                    },
                    {
                        "name": "Akinyemi Sadeeq Akintola"
                    },
                    {
                        "name": "Oluwole Fagbohun"
                    },
                    {
                        "name": "Egonna Anaesiuba-Bristol"
                    },
                    {
                        "name": "Onyekachukwu Ojumah"
                    },
                    {
                        "name": "Oluwagbade Odimayo"
                    },
                    {
                        "name": "Toyese Oloyede"
                    },
                    {
                        "name": "Aniema Inyang"
                    },
                    {
                        "name": "Teslim Kazeem"
                    },
                    {
                        "name": "Habeeb Alli"
                    },
                    {
                        "name": "Udodirim Ibem Offia"
                    },
                    {
                        "name": "Prisca Chinazor Amajuoyi"
                    }
                ],
                "author_detail": {
                    "name": "Prisca Chinazor Amajuoyi"
                },
                "author": "Prisca Chinazor Amajuoyi",
                "arxiv_comment": "19 Pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17336v1",
                "updated": "2025-03-21T17:34:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    34,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:34:37Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    34,
                    37,
                    4,
                    80,
                    0
                ],
                "title": "Efficient Intent-Based Filtering for Multi-Party Conversations Using\n  Knowledge Distillation from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Intent-Based Filtering for Multi-Party Conversations Using\n  Knowledge Distillation from LLMs"
                },
                "summary": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments."
                },
                "authors": [
                    {
                        "name": "Reem Gody"
                    },
                    {
                        "name": "Mohamed Abdelghaffar"
                    },
                    {
                        "name": "Mohammed Jabreel"
                    },
                    {
                        "name": "Ahmed Tawfik"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Tawfik"
                },
                "author": "Ahmed Tawfik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16041v2",
                "updated": "2025-03-21T17:33:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T11:19:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    11,
                    19,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis\n  and Automated Report Generation"
                },
                "summary": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes"
                },
                "authors": [
                    {
                        "name": "Oluwole Fagbohun"
                    },
                    {
                        "name": "Sai Yashwanth"
                    },
                    {
                        "name": "Akinyemi Sadeeq Akintola"
                    },
                    {
                        "name": "Ifeoluwa Wurola"
                    },
                    {
                        "name": "Lanre Shittu"
                    },
                    {
                        "name": "Aniema Inyang"
                    },
                    {
                        "name": "Oluwatimilehin Odubola"
                    },
                    {
                        "name": "Udodirim Offia"
                    },
                    {
                        "name": "Said Olanrewaju"
                    },
                    {
                        "name": "Ogidan Toluwaleke"
                    },
                    {
                        "name": "Ilemona Abutu"
                    },
                    {
                        "name": "Taiwo Akinbolaji"
                    }
                ],
                "author_detail": {
                    "name": "Taiwo Akinbolaji"
                },
                "author": "Taiwo Akinbolaji",
                "arxiv_comment": "12 Pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v1",
                "updated": "2025-03-21T17:33:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17332v1",
                "updated": "2025-03-21T17:32:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    32,
                    32,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:32:32Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    32,
                    32,
                    4,
                    80,
                    0
                ],
                "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web\n  Application Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web\n  Application Vulnerabilities"
                },
                "summary": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Antony Kellermann"
                    },
                    {
                        "name": "Dylan Bowman"
                    },
                    {
                        "name": "Philip Li"
                    },
                    {
                        "name": "Akul Gupta"
                    },
                    {
                        "name": "Adarsh Danda"
                    },
                    {
                        "name": "Richard Fang"
                    },
                    {
                        "name": "Conner Jensen"
                    },
                    {
                        "name": "Eric Ihli"
                    },
                    {
                        "name": "Jason Benn"
                    },
                    {
                        "name": "Jet Geronimo"
                    },
                    {
                        "name": "Avi Dhir"
                    },
                    {
                        "name": "Sudhit Rao"
                    },
                    {
                        "name": "Kaicheng Yu"
                    },
                    {
                        "name": "Twm Stone"
                    },
                    {
                        "name": "Daniel Kang"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kang"
                },
                "author": "Daniel Kang",
                "arxiv_comment": "15 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17309v1",
                "updated": "2025-03-21T17:04:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    4,
                    1,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T17:04:01Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    4,
                    1,
                    4,
                    80,
                    0
                ],
                "title": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and\n  Planning Domain Definition Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM+MAP: Bimanual Robot Task Planning using Large Language Models and\n  Planning Domain Definition Language"
                },
                "summary": "Bimanual robotic manipulation provides significant versatility, but also\npresents an inherent challenge due to the complexity involved in the spatial\nand temporal coordination between two hands. Existing works predominantly focus\non attaining human-level manipulation skills for robotic hands, yet little\nattention has been paid to task planning on long-horizon timescales. With their\noutstanding in-context learning and zero-shot generation abilities, Large\nLanguage Models (LLMs) have been applied and grounded in diverse robotic\nembodiments to facilitate task planning. However, LLMs still suffer from errors\nin long-horizon reasoning and from hallucinations in complex robotic tasks,\nlacking a guarantee of logical correctness when generating the plan. Previous\nworks, such as LLM+P, extended LLMs with symbolic planners. However, none have\nbeen successfully applied to bimanual robots. New challenges inevitably arise\nin bimanual manipulation, necessitating not only effective task decomposition\nbut also efficient task allocation. To address these challenges, this paper\nintroduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning\nand multi-agent planning, automating effective and efficient bimanual task\nplanning. We conduct simulated experiments on various long-horizon manipulation\ntasks of differing complexity. Our method is built using GPT-4o as the backend,\nand we compare its performance against plans generated directly by LLMs,\nincluding GPT-4o, V3 and also recent strong reasoning models o1 and R1. By\nanalyzing metrics such as planning time, success rate, group debits, and\nplanning-step reduction rate, we demonstrate the superior performance of\nLLM+MAP, while also providing insights into robotic reasoning. Code is\navailable at https://github.com/Kchu/LLM-MAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bimanual robotic manipulation provides significant versatility, but also\npresents an inherent challenge due to the complexity involved in the spatial\nand temporal coordination between two hands. Existing works predominantly focus\non attaining human-level manipulation skills for robotic hands, yet little\nattention has been paid to task planning on long-horizon timescales. With their\noutstanding in-context learning and zero-shot generation abilities, Large\nLanguage Models (LLMs) have been applied and grounded in diverse robotic\nembodiments to facilitate task planning. However, LLMs still suffer from errors\nin long-horizon reasoning and from hallucinations in complex robotic tasks,\nlacking a guarantee of logical correctness when generating the plan. Previous\nworks, such as LLM+P, extended LLMs with symbolic planners. However, none have\nbeen successfully applied to bimanual robots. New challenges inevitably arise\nin bimanual manipulation, necessitating not only effective task decomposition\nbut also efficient task allocation. To address these challenges, this paper\nintroduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning\nand multi-agent planning, automating effective and efficient bimanual task\nplanning. We conduct simulated experiments on various long-horizon manipulation\ntasks of differing complexity. Our method is built using GPT-4o as the backend,\nand we compare its performance against plans generated directly by LLMs,\nincluding GPT-4o, V3 and also recent strong reasoning models o1 and R1. By\nanalyzing metrics such as planning time, success rate, group debits, and\nplanning-step reduction rate, we demonstrate the superior performance of\nLLM+MAP, while also providing insights into robotic reasoning. Code is\navailable at https://github.com/Kchu/LLM-MAP."
                },
                "authors": [
                    {
                        "name": "Kun Chu"
                    },
                    {
                        "name": "Xufeng Zhao"
                    },
                    {
                        "name": "Cornelius Weber"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Code and video are available at https://github.com/Kchu/LLM-MAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17302v1",
                "updated": "2025-03-21T16:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    52,
                    3,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T16:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    52,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests"
                },
                "summary": "As software systems grow increasingly complex, ensuring security during\ndevelopment poses significant challenges. Traditional manual code audits are\noften expensive, time-intensive, and ill-suited for fast-paced workflows, while\nautomated tools frequently suffer from high false-positive rates, limiting\ntheir reliability. To address these issues, we introduce Bugdar, an\nAI-augmented code review system that integrates seamlessly into GitHub pull\nrequests, providing near real-time, context-aware vulnerability analysis.\nBugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAGs) to deliver project-specific, actionable feedback\nthat aligns with each codebase's unique requirements and developer practices.\nSupporting multiple programming languages, including Solidity, Move, Rust, and\nPython, Bugdar demonstrates exceptional efficiency, processing an average of\n56.4 seconds per pull request or 30 lines of code per second. This is\nsignificantly faster than manual reviews, which could take hours per pull\nrequest. By facilitating a proactive approach to secure coding, Bugdar reduces\nthe reliance on manual reviews, accelerates development cycles, and enhances\nthe security posture of software systems without compromising productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software systems grow increasingly complex, ensuring security during\ndevelopment poses significant challenges. Traditional manual code audits are\noften expensive, time-intensive, and ill-suited for fast-paced workflows, while\nautomated tools frequently suffer from high false-positive rates, limiting\ntheir reliability. To address these issues, we introduce Bugdar, an\nAI-augmented code review system that integrates seamlessly into GitHub pull\nrequests, providing near real-time, context-aware vulnerability analysis.\nBugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAGs) to deliver project-specific, actionable feedback\nthat aligns with each codebase's unique requirements and developer practices.\nSupporting multiple programming languages, including Solidity, Move, Rust, and\nPython, Bugdar demonstrates exceptional efficiency, processing an average of\n56.4 seconds per pull request or 30 lines of code per second. This is\nsignificantly faster than manual reviews, which could take hours per pull\nrequest. By facilitating a proactive approach to secure coding, Bugdar reduces\nthe reliance on manual reviews, accelerates development cycles, and enhances\nthe security posture of software systems without compromising productivity."
                },
                "authors": [
                    {
                        "name": "John Naulty"
                    },
                    {
                        "name": "Eason Chen"
                    },
                    {
                        "name": "Joy Wang"
                    },
                    {
                        "name": "George Digkas"
                    },
                    {
                        "name": "Kostas Chalkias"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Chalkias"
                },
                "author": "Kostas Chalkias",
                "arxiv_comment": "4 pages, 1 figure, accepted at IEEE Conference on Artificial\n  Intelligence (CAI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16304v2",
                "updated": "2025-03-21T16:34:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    34,
                    40,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T16:25:24Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    25,
                    24,
                    3,
                    79,
                    0
                ],
                "title": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Technology and Humanities: Evaluating the Impact of Large\n  Language Models on Social Sciences Research with DeepSeek-R1"
                },
                "summary": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications."
                },
                "authors": [
                    {
                        "name": "Peiran Gu"
                    },
                    {
                        "name": "Fuhao Duan"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Bochen Xu"
                    },
                    {
                        "name": "Ying Cai"
                    },
                    {
                        "name": "Teng Yao"
                    },
                    {
                        "name": "Chenxun Zhuo"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Bao Ge"
                    }
                ],
                "author_detail": {
                    "name": "Bao Ge"
                },
                "author": "Bao Ge",
                "arxiv_comment": "52 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17283v1",
                "updated": "2025-03-21T16:30:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    30,
                    22,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T16:30:22Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    30,
                    22,
                    4,
                    80,
                    0
                ],
                "title": "Energy Efficiency trends in HPC: what high-energy and astrophysicists\n  need to know",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Efficiency trends in HPC: what high-energy and astrophysicists\n  need to know"
                },
                "summary": "The growing energy demands of HPC systems have made energy efficiency a\ncritical concern for system developers and operators. However, HPC users are\ngenerally less aware of how these energy concerns influence the design,\ndeployment, and operation of supercomputers even though they experience the\nconsequences. This paper examines the implications of HPC's energy consumption,\nproviding an overview of current trends aimed at improving energy efficiency.\nWe describe how hardware innovations such as energy-efficient processors, novel\nsystem architectures, power management techniques, and advanced scheduling\npolicies do have a direct impact on how applications need to be programmed and\nexecuted on HPC systems. For application developers, understanding how these\nnew systems work and how to analyse and report the performances of their own\nsoftware is critical in the dialog with HPC system designers and\nadministrators. The paper aims to raise awareness about energy efficiency among\nusers, particularly in the high energy physics and astrophysics domains,\noffering practical advice on how to analyse and optimise applications to reduce\ntheir energy consumption without compromising on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing energy demands of HPC systems have made energy efficiency a\ncritical concern for system developers and operators. However, HPC users are\ngenerally less aware of how these energy concerns influence the design,\ndeployment, and operation of supercomputers even though they experience the\nconsequences. This paper examines the implications of HPC's energy consumption,\nproviding an overview of current trends aimed at improving energy efficiency.\nWe describe how hardware innovations such as energy-efficient processors, novel\nsystem architectures, power management techniques, and advanced scheduling\npolicies do have a direct impact on how applications need to be programmed and\nexecuted on HPC systems. For application developers, understanding how these\nnew systems work and how to analyse and report the performances of their own\nsoftware is critical in the dialog with HPC system designers and\nadministrators. The paper aims to raise awareness about energy efficiency among\nusers, particularly in the high energy physics and astrophysics domains,\noffering practical advice on how to analyse and optimise applications to reduce\ntheir energy consumption without compromising on performance."
                },
                "authors": [
                    {
                        "name": "Estela Suarez"
                    },
                    {
                        "name": "Jorge Amaya"
                    },
                    {
                        "name": "Martin Frank"
                    },
                    {
                        "name": "Oliver Freyermuth"
                    },
                    {
                        "name": "Maria Girone"
                    },
                    {
                        "name": "Bartosz Kostrzewa"
                    },
                    {
                        "name": "Susanne Pfalzner"
                    }
                ],
                "author_detail": {
                    "name": "Susanne Pfalzner"
                },
                "author": "Susanne Pfalzner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17279v1",
                "updated": "2025-03-21T16:27:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    27,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T16:27:12Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    27,
                    12,
                    4,
                    80,
                    0
                ],
                "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic\n  Textual Similarity Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic\n  Textual Similarity Measurement"
                },
                "summary": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance."
                },
                "authors": [
                    {
                        "name": "Gaifan Zhang"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Danushka Bollegala"
                    }
                ],
                "author_detail": {
                    "name": "Danushka Bollegala"
                },
                "author": "Danushka Bollegala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11702v2",
                "updated": "2025-03-21T16:17:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    17,
                    59,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-12T09:32:43Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    32,
                    43,
                    2,
                    71,
                    0
                ],
                "title": "Toward a method for LLM-enabled Indoor Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a method for LLM-enabled Indoor Navigation"
                },
                "summary": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 52% correct indications and a maximum of 62%. The results do not\nappear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor navigation presents unique challenges due to complex layouts, lack of\nGPS signals, and accessibility concerns. Existing solutions often struggle with\nreal-time adaptability and user-specific needs. In this work, we explore the\npotential of a Large Language Model (LLM), i.e., ChatGPT, to generate natural,\ncontext-aware navigation instructions from indoor map images. We design and\nevaluate test cases across different real-world environments, analyzing the\neffectiveness of LLMs in interpreting spatial layouts, handling user\nconstraints, and planning efficient routes. Our findings demonstrate the\npotential of LLMs for supporting personalized indoor navigation, with an\naverage of 52% correct indications and a maximum of 62%. The results do not\nappear to depend on the complexity of the layout or the complexity of the\nexpected path, but rather on the number of points of interest and the abundance\nof visual information, which negatively affect the performance."
                },
                "authors": [
                    {
                        "name": "Alberto Coffrini"
                    },
                    {
                        "name": "Mohammad Amin Zadenoori"
                    },
                    {
                        "name": "Paolo Barsocchi"
                    },
                    {
                        "name": "Francesco Furfari"
                    },
                    {
                        "name": "Antonino Crivello"
                    },
                    {
                        "name": "Alessio Ferrari"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Ferrari"
                },
                "author": "Alessio Ferrari",
                "arxiv_comment": "7 pages, 3 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17257v1",
                "updated": "2025-03-21T16:00:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    0,
                    23,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T16:00:23Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    0,
                    23,
                    4,
                    80,
                    0
                ],
                "title": "The EnviroMapper Toolkit: an Input Physicalisation that Captures the\n  Situated Experience of Environmental Comfort in Offices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The EnviroMapper Toolkit: an Input Physicalisation that Captures the\n  Situated Experience of Environmental Comfort in Offices"
                },
                "summary": "The environmental comfort in offices is traditionally captured by surveying\nan entire workforce simultaneously, which yet fails to capture the situatedness\nof the different personal experiences. To address this limitation, we developed\nthe EnviroMapper Toolkit, a data physicalisation toolkit that allows individual\noffice workers to record their personal experiences of environmental comfort by\nmapping the actual moments and locations these occurred. By analysing two\nin-the-wild studies in existing open-plan office environments (N=14), we\ndemonstrate how this toolkit acts like a situated input visualisation that can\nbe interpreted by domain experts who were not present during its construction.\nThis study therefore offers four key contributions: (1) the iterative design\nprocess of the physicalisation toolkit; (2) its preliminary deployment in two\nreal-world office contexts; (3) the decoding of the resulting artefacts by\ndomain experts; and (4) design considerations to support future input\nphysicalisation and visualisation constructions that capture and synthesise\ndata from multiple individuals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The environmental comfort in offices is traditionally captured by surveying\nan entire workforce simultaneously, which yet fails to capture the situatedness\nof the different personal experiences. To address this limitation, we developed\nthe EnviroMapper Toolkit, a data physicalisation toolkit that allows individual\noffice workers to record their personal experiences of environmental comfort by\nmapping the actual moments and locations these occurred. By analysing two\nin-the-wild studies in existing open-plan office environments (N=14), we\ndemonstrate how this toolkit acts like a situated input visualisation that can\nbe interpreted by domain experts who were not present during its construction.\nThis study therefore offers four key contributions: (1) the iterative design\nprocess of the physicalisation toolkit; (2) its preliminary deployment in two\nreal-world office contexts; (3) the decoding of the resulting artefacts by\ndomain experts; and (4) design considerations to support future input\nphysicalisation and visualisation constructions that capture and synthesise\ndata from multiple individuals."
                },
                "authors": [
                    {
                        "name": "Silvia Cazacu"
                    },
                    {
                        "name": "Stien Poncelet"
                    },
                    {
                        "name": "Emma Feijtraij"
                    },
                    {
                        "name": "Andrew Vande Moere"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Vande Moere"
                },
                "author": "Andrew Vande Moere",
                "arxiv_doi": "10.1145/3706599.3720084",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3720084",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The first two listed authors contributed equally to this research. In\n  CHI EA'25, ACM (2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10602v2",
                "updated": "2025-03-21T15:58:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    58,
                    26,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-13T17:46:06Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    46,
                    6,
                    3,
                    72,
                    0
                ],
                "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention"
                },
                "summary": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object Hallucination (OH) has been acknowledged as one of the major\ntrustworthy challenges in Large Vision-Language Models (LVLMs). Recent\nadvancements in Large Language Models (LLMs) indicate that internal states,\nsuch as hidden states, encode the \"overall truthfulness\" of generated\nresponses. However, it remains under-explored how internal states in LVLMs\nfunction and whether they could serve as \"per-token\" hallucination indicators,\nwhich is essential for mitigating OH. In this paper, we first conduct an\nin-depth exploration of LVLM internal states in relation to OH issues and\ndiscover that (1) LVLM internal states are high-specificity per-token\nindicators of hallucination behaviors. Moreover, (2) different LVLMs encode\nuniversal patterns of hallucinations in common latent subspaces, indicating\nthat there exist \"generic truthful directions\" shared by various LVLMs. Based\non these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt)\nthat first learns the truthful direction of LVLM decoding and then applies\ntruthful-guided inference-time intervention during LVLM decoding. We further\npropose ComnHallu to enhance both cross-LVLM and cross-data hallucination\ndetection transferability by constructing and aligning hallucination latent\nsubspaces. We evaluate TruthPrInt in extensive experimental settings, including\nin-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks.\nExperimental results indicate that TruthPrInt significantly outperforms\nstate-of-the-art methods. Codes will be available at\nhttps://github.com/jinhaoduan/TruthPrInt."
                },
                "authors": [
                    {
                        "name": "Jinhao Duan"
                    },
                    {
                        "name": "Fei Kong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "James Diffenderfer"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Xiaoshuang Shi"
                    },
                    {
                        "name": "Kaidi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kaidi Xu"
                },
                "author": "Kaidi Xu",
                "arxiv_comment": "15 pages, 9 figures, the first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17239v1",
                "updated": "2025-03-21T15:44:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    44,
                    9,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T15:44:09Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    44,
                    9,
                    4,
                    80,
                    0
                ],
                "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language\n  Models via Selective Layer-Wise Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language\n  Models via Selective Layer-Wise Model Merging"
                },
                "summary": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses."
                },
                "authors": [
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Swanand Ravindra Kadhe"
                    },
                    {
                        "name": "Farhan Ahmed"
                    },
                    {
                        "name": "Syed Zawad"
                    },
                    {
                        "name": "Holger Boche"
                    }
                ],
                "author_detail": {
                    "name": "Holger Boche"
                },
                "author": "Holger Boche",
                "arxiv_journal_ref": "ICLR 2025 Workshop on Building Trust in Language Models and\n  Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06796v2",
                "updated": "2025-03-21T15:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    40,
                    38,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-11T08:50:24Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    50,
                    24,
                    0,
                    316,
                    0
                ],
                "title": "Write Your Own CodeChecker: An Automated Test-Driven Checker Development\n  Approach with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write Your Own CodeChecker: An Automated Test-Driven Checker Development\n  Approach with LLMs"
                },
                "summary": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic and the complex API usage of large-scale\nchecker frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we propose AutoChecker, an innovative LLM-powered approach that can\nwrite code checkers automatically based on only a rule description and a test\nsuite. To achieve comprehensive checking logic, AutoChecker incrementally\nupdates the checker's logic by focusing on solving one selected case each time.\nTo obtain precise API knowledge, during each iteration, it leverages\nfine-grained logic-guided API-context retrieval, where it first decomposes the\nchecking logic into a series of sub-operations and then retrieves\nchecker-related API-contexts for each sub-operation. For evaluation, we apply\nAutoChecker, five baselines, and three ablation methods using multiple LLMs to\ngenerate checkers for 20 randomly selected PMD rules. Experimental results show\nthat AutoChecker significantly outperforms others across all effectiveness\nmetrics, with an average test pass rate of 82.28%. Additionally, the checkers\ngenerated by AutoChecker can be successfully applied to real-world projects,\nmatching the performance of official checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic and the complex API usage of large-scale\nchecker frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we propose AutoChecker, an innovative LLM-powered approach that can\nwrite code checkers automatically based on only a rule description and a test\nsuite. To achieve comprehensive checking logic, AutoChecker incrementally\nupdates the checker's logic by focusing on solving one selected case each time.\nTo obtain precise API knowledge, during each iteration, it leverages\nfine-grained logic-guided API-context retrieval, where it first decomposes the\nchecking logic into a series of sub-operations and then retrieves\nchecker-related API-contexts for each sub-operation. For evaluation, we apply\nAutoChecker, five baselines, and three ablation methods using multiple LLMs to\ngenerate checkers for 20 randomly selected PMD rules. Experimental results show\nthat AutoChecker significantly outperforms others across all effectiveness\nmetrics, with an average test pass rate of 82.28%. Additionally, the checkers\ngenerated by AutoChecker can be successfully applied to real-world projects,\nmatching the performance of official checkers."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Xie"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Jiwei Yan"
                    },
                    {
                        "name": "Jinhao Huang"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17229v1",
                "updated": "2025-03-21T15:32:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    32,
                    24,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T15:32:24Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    32,
                    24,
                    4,
                    80,
                    0
                ],
                "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs"
                },
                "summary": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content."
                },
                "authors": [
                    {
                        "name": "Albert Sawczyn"
                    },
                    {
                        "name": "Jakub Binkowski"
                    },
                    {
                        "name": "Denis Janiak"
                    },
                    {
                        "name": "Bogdan Gabrys"
                    },
                    {
                        "name": "Tomasz Kajdanowicz"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kajdanowicz"
                },
                "author": "Tomasz Kajdanowicz",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12262v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12262v4",
                "updated": "2025-03-21T15:32:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    32,
                    7,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-18T18:47:58Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    18,
                    47,
                    58,
                    2,
                    262,
                    0
                ],
                "title": "Bootstrapping Object-level Planning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Object-level Planning with Large Language Models"
                },
                "summary": "We introduce a new method that extracts knowledge from a large language model\n(LLM) to produce object-level plans, which describe high-level changes to\nobject state, and uses them to bootstrap task and motion planning (TAMP).\nExisting work uses LLMs to directly output task plans or generate goals in\nrepresentations like PDDL. However, these methods fall short because they rely\non the LLM to do the actual planning or output a hard-to-satisfy goal. Our\napproach instead extracts knowledge from an LLM in the form of plan schemas as\nan object-level representation called functional object-oriented networks\n(FOON), from which we automatically generate PDDL subgoals. Our method markedly\noutperforms alternative planning strategies in completing several\npick-and-place tasks in simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new method that extracts knowledge from a large language model\n(LLM) to produce object-level plans, which describe high-level changes to\nobject state, and uses them to bootstrap task and motion planning (TAMP).\nExisting work uses LLMs to directly output task plans or generate goals in\nrepresentations like PDDL. However, these methods fall short because they rely\non the LLM to do the actual planning or output a hard-to-satisfy goal. Our\napproach instead extracts knowledge from an LLM in the form of plan schemas as\nan object-level representation called functional object-oriented networks\n(FOON), from which we automatically generate PDDL subgoals. Our method markedly\noutperforms alternative planning strategies in completing several\npick-and-place tasks in simulation."
                },
                "authors": [
                    {
                        "name": "David Paulius"
                    },
                    {
                        "name": "Alejandro Agostini"
                    },
                    {
                        "name": "Benedict Quartey"
                    },
                    {
                        "name": "George Konidaris"
                    }
                ],
                "author_detail": {
                    "name": "George Konidaris"
                },
                "author": "George Konidaris",
                "arxiv_comment": "Accepted to ICRA 2025; 11 pages (6 pages + 1 page references + 4\n  pages appendix); for demo videos, please see\n  https://davidpaulius.github.io/olp_llm/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12262v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12262v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17222v1",
                "updated": "2025-03-21T15:25:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    25,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T15:25:53Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    25,
                    53,
                    4,
                    80,
                    0
                ],
                "title": "Automating Adjudication of Cardiovascular Events Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Adjudication of Cardiovascular Events Using Large Language\n  Models"
                },
                "summary": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies."
                },
                "authors": [
                    {
                        "name": "Sonish Sivarajkumar"
                    },
                    {
                        "name": "Kimia Ameri"
                    },
                    {
                        "name": "Chuqin Li"
                    },
                    {
                        "name": "Yanshan Wang"
                    },
                    {
                        "name": "Min Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Min Jiang"
                },
                "author": "Min Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11006v2",
                "updated": "2025-03-21T15:07:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    7,
                    55,
                    4,
                    80,
                    0
                ],
                "published": "2025-01-19T10:44:03Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    10,
                    44,
                    3,
                    6,
                    19,
                    0
                ],
                "title": "GREEN-CODE: Learning to Optimize Energy Efficiency in LLM-based Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GREEN-CODE: Learning to Optimize Energy Efficiency in LLM-based Code\n  Generation"
                },
                "summary": "Large Language Models (LLMs) are becoming integral to daily life, showcasing\ntheir vast potential across various Natural Language Processing (NLP) tasks.\nBeyond NLP, LLMs are increasingly used in software development tasks, such as\ncode completion, modification, bug fixing, and code translation. Software\nengineers widely use tools like GitHub Copilot and Amazon Q, streamlining\nworkflows and automating tasks with high accuracy. While the resource and\nenergy intensity of LLM training is often highlighted, inference can be even\nmore resource-intensive over time, as it's a continuous process with a high\nnumber of invocations. Therefore, developing resource-efficient alternatives\nfor LLM inference is crucial for sustainability. This work proposes GREEN-CODE,\na framework for energy-aware code generation in LLMs. GREEN-CODE performs\ndynamic early exit during LLM inference. We train a Reinforcement Learning (RL)\nagent that learns to balance the trade-offs between accuracy, latency, and\nenergy consumption. Our approach is evaluated on two open-source LLMs, Llama\n3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that\nour method reduces the energy consumption between 23-50 % on average for code\ngeneration tasks without significantly affecting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming integral to daily life, showcasing\ntheir vast potential across various Natural Language Processing (NLP) tasks.\nBeyond NLP, LLMs are increasingly used in software development tasks, such as\ncode completion, modification, bug fixing, and code translation. Software\nengineers widely use tools like GitHub Copilot and Amazon Q, streamlining\nworkflows and automating tasks with high accuracy. While the resource and\nenergy intensity of LLM training is often highlighted, inference can be even\nmore resource-intensive over time, as it's a continuous process with a high\nnumber of invocations. Therefore, developing resource-efficient alternatives\nfor LLM inference is crucial for sustainability. This work proposes GREEN-CODE,\na framework for energy-aware code generation in LLMs. GREEN-CODE performs\ndynamic early exit during LLM inference. We train a Reinforcement Learning (RL)\nagent that learns to balance the trade-offs between accuracy, latency, and\nenergy consumption. Our approach is evaluated on two open-source LLMs, Llama\n3.2 3B and OPT 2.7B, using the JavaCorpus and PY150 datasets. Results show that\nour method reduces the energy consumption between 23-50 % on average for code\ngeneration tasks without significantly affecting accuracy."
                },
                "authors": [
                    {
                        "name": "Shashikant Ilager"
                    },
                    {
                        "name": "Lukas Florian Briem"
                    },
                    {
                        "name": "Ivona Brandic"
                    }
                ],
                "author_detail": {
                    "name": "Ivona Brandic"
                },
                "author": "Ivona Brandic",
                "arxiv_comment": "Under submission in ACM/IEEE conference, 11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; D.0; E.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17205v1",
                "updated": "2025-03-21T15:03:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    3,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T15:03:02Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    3,
                    2,
                    4,
                    80,
                    0
                ],
                "title": "Minimum Mean Squared Error Holographic Beamforming for Sum-Rate\n  Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum Mean Squared Error Holographic Beamforming for Sum-Rate\n  Maximization"
                },
                "summary": "This paper studies the problem of hybrid holographic beamforming for sum-rate\nmaximization in a communication system assisted by a reconfigurable holographic\nsurface. Existing methodologies predominantly rely on gradient-based or\napproximation techniques necessitating iterative optimization for each update\nof the holographic response, which imposes substantial computational overhead.\nTo address these limitations, we establish a mathematical relationship between\nthe mean squared error (MSE) criterion and the holographic response of the RHS\nto enable alternating optimization based on the minimum MSE (MMSE). Our\nanalysis demonstrates that this relationship exhibits a quadratic dependency on\neach element of the holographic beamformer. Exploiting this property, we derive\nclosed-form optimal expressions for updating the holographic beamforming\nweights. Our complexity analysis indicates that the proposed approach exhibits\nonly linear complexity in terms of the RHS size, thus, ensuring scalability for\nlarge-scale deployments. The presented simulation results validate the\neffectiveness of our MMSE-based holographic approach, providing useful\ninsights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the problem of hybrid holographic beamforming for sum-rate\nmaximization in a communication system assisted by a reconfigurable holographic\nsurface. Existing methodologies predominantly rely on gradient-based or\napproximation techniques necessitating iterative optimization for each update\nof the holographic response, which imposes substantial computational overhead.\nTo address these limitations, we establish a mathematical relationship between\nthe mean squared error (MSE) criterion and the holographic response of the RHS\nto enable alternating optimization based on the minimum MSE (MMSE). Our\nanalysis demonstrates that this relationship exhibits a quadratic dependency on\neach element of the holographic beamformer. Exploiting this property, we derive\nclosed-form optimal expressions for updating the holographic beamforming\nweights. Our complexity analysis indicates that the proposed approach exhibits\nonly linear complexity in terms of the RHS size, thus, ensuring scalability for\nlarge-scale deployments. The presented simulation results validate the\neffectiveness of our MMSE-based holographic approach, providing useful\ninsights."
                },
                "authors": [
                    {
                        "name": "Chandan Kumar Sheemar"
                    },
                    {
                        "name": "Wali Ullah Khan"
                    },
                    {
                        "name": "George C. Alexandropoulos"
                    },
                    {
                        "name": "Manzoor Ahmed"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    }
                ],
                "author_detail": {
                    "name": "Symeon Chatzinotas"
                },
                "author": "Symeon Chatzinotas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15249v2",
                "updated": "2025-03-21T14:56:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    56,
                    58,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-15T01:12:26Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    1,
                    12,
                    26,
                    6,
                    350,
                    0
                ],
                "title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LitLLMs, LLMs for Literature Review: Are we there yet?"
                },
                "summary": "Literature reviews are an essential component of scientific research, but\nthey remain time-intensive and challenging to write, especially due to the\nrecent influx of research papers. This paper explores the zero-shot abilities\nof recent Large Language Models (LLMs) in assisting with the writing of\nliterature reviews based on an abstract. We decompose the task into two\ncomponents: 1. Retrieving related works given a query abstract, and 2. Writing\na literature review based on the retrieved results. We analyze how effective\nLLMs are for both components. For retrieval, we introduce a novel two-step\nsearch strategy that first uses an LLM to extract meaningful keywords from the\nabstract of a paper and then retrieves potentially relevant papers by querying\nan external knowledge base. Additionally, we study a prompting-based re-ranking\nmechanism with attribution and show that re-ranking doubles the normalized\nrecall compared to naive search methods, while providing insights into the\nLLM's decision-making process. In the generation phase, we propose a two-step\napproach that first outlines a plan for the review and then executes steps in\nthe plan to generate the actual review. To evaluate different LLM-based\nliterature review methods, we create test sets from arXiv papers using a\nprotocol designed for rolling use with newly released LLMs to avoid test set\ncontamination in zero-shot evaluations. We release this evaluation protocol to\npromote additional research and development in this regard. Our empirical\nresults suggest that LLMs show promising potential for writing literature\nreviews when the task is decomposed into smaller components of retrieval and\nplanning. Our project page including a demonstration system and toolkit can be\naccessed here: https://litllm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature reviews are an essential component of scientific research, but\nthey remain time-intensive and challenging to write, especially due to the\nrecent influx of research papers. This paper explores the zero-shot abilities\nof recent Large Language Models (LLMs) in assisting with the writing of\nliterature reviews based on an abstract. We decompose the task into two\ncomponents: 1. Retrieving related works given a query abstract, and 2. Writing\na literature review based on the retrieved results. We analyze how effective\nLLMs are for both components. For retrieval, we introduce a novel two-step\nsearch strategy that first uses an LLM to extract meaningful keywords from the\nabstract of a paper and then retrieves potentially relevant papers by querying\nan external knowledge base. Additionally, we study a prompting-based re-ranking\nmechanism with attribution and show that re-ranking doubles the normalized\nrecall compared to naive search methods, while providing insights into the\nLLM's decision-making process. In the generation phase, we propose a two-step\napproach that first outlines a plan for the review and then executes steps in\nthe plan to generate the actual review. To evaluate different LLM-based\nliterature review methods, we create test sets from arXiv papers using a\nprotocol designed for rolling use with newly released LLMs to avoid test set\ncontamination in zero-shot evaluations. We release this evaluation protocol to\npromote additional research and development in this regard. Our empirical\nresults suggest that LLMs show promising potential for writing literature\nreviews when the task is decomposed into smaller components of retrieval and\nplanning. Our project page including a demonstration system and toolkit can be\naccessed here: https://litllm.github.io."
                },
                "authors": [
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Gaurav Sahu"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Issam H. Laradji"
                    },
                    {
                        "name": "Krishnamurthy DJ Dvijotham"
                    },
                    {
                        "name": "Jason Stanley"
                    },
                    {
                        "name": "Laurent Charlin"
                    },
                    {
                        "name": "Christopher Pal"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Pal"
                },
                "author": "Christopher Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01788v2",
                "updated": "2025-03-21T14:49:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    49,
                    10,
                    4,
                    80,
                    0
                ],
                "published": "2024-02-02T02:41:28Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    2,
                    41,
                    28,
                    4,
                    33,
                    0
                ],
                "title": "LitLLM: A Toolkit for Scientific Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LitLLM: A Toolkit for Scientific Literature Review"
                },
                "summary": "Conducting literature reviews for scientific papers is essential for\nunderstanding research, its limitations, and building on existing work. It is a\ntedious task which makes an automatic literature review generator appealing.\nUnfortunately, many existing works that generate such reviews using Large\nLanguage Models (LLMs) have significant limitations. They tend to\nhallucinate-generate non-factual information-and ignore the latest research\nthey have not been trained on. To address these limitations, we propose a\ntoolkit that operates on Retrieval Augmented Generation (RAG) principles,\nspecialized prompting and instructing techniques with the help of LLMs. Our\nsystem first initiates a web search to retrieve relevant papers by summarizing\nuser-provided abstracts into keywords using an off-the-shelf LLM. Authors can\nenhance the search by supplementing it with relevant papers or keywords,\ncontributing to a tailored retrieval process. Second, the system re-ranks the\nretrieved papers based on the user-provided abstract. Finally, the related work\nsection is generated based on the re-ranked results and the abstract. There is\na substantial reduction in time and effort for literature review compared to\ntraditional methods, establishing our toolkit as an efficient alternative. Our\nproject page including the demo and toolkit can be accessed here:\nhttps://litllm.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conducting literature reviews for scientific papers is essential for\nunderstanding research, its limitations, and building on existing work. It is a\ntedious task which makes an automatic literature review generator appealing.\nUnfortunately, many existing works that generate such reviews using Large\nLanguage Models (LLMs) have significant limitations. They tend to\nhallucinate-generate non-factual information-and ignore the latest research\nthey have not been trained on. To address these limitations, we propose a\ntoolkit that operates on Retrieval Augmented Generation (RAG) principles,\nspecialized prompting and instructing techniques with the help of LLMs. Our\nsystem first initiates a web search to retrieve relevant papers by summarizing\nuser-provided abstracts into keywords using an off-the-shelf LLM. Authors can\nenhance the search by supplementing it with relevant papers or keywords,\ncontributing to a tailored retrieval process. Second, the system re-ranks the\nretrieved papers based on the user-provided abstract. Finally, the related work\nsection is generated based on the re-ranked results and the abstract. There is\na substantial reduction in time and effort for literature review compared to\ntraditional methods, establishing our toolkit as an efficient alternative. Our\nproject page including the demo and toolkit can be accessed here:\nhttps://litllm.github.io"
                },
                "authors": [
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Gaurav Sahu"
                    },
                    {
                        "name": "Abhay Puri"
                    },
                    {
                        "name": "Issam H. Laradji"
                    },
                    {
                        "name": "Krishnamurthy DJ Dvijotham"
                    },
                    {
                        "name": "Jason Stanley"
                    },
                    {
                        "name": "Laurent Charlin"
                    },
                    {
                        "name": "Christopher Pal"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Pal"
                },
                "author": "Christopher Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17195v1",
                "updated": "2025-03-21T14:43:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    43,
                    23,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T14:43:23Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    43,
                    23,
                    4,
                    80,
                    0
                ],
                "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided\n  Subspace Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided\n  Subspace Partitioning"
                },
                "summary": "Model customization requires high-quality and diverse datasets, but acquiring\nsuch data remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited\nseed data, model bias and insufficient control over the generation process,\nresulting in limited diversity and biased distribution with the increase of\ndata scales. To tackle this challenge, we present TreeSynth, a tree-guided\nsubspace-based data synthesis framework that recursively partitions the entire\ndata space into hierar-chical subspaces, enabling comprehensive and diverse\nscaling of data synthesis. Briefly, given a task-specific description, we\nconstruct a data space partitioning tree by iteratively executing criteria\ndetermination and subspace coverage steps. This hierarchically divides the\nwhole space (i.e., root node) into mutually exclusive and complementary atomic\nsubspaces (i.e., leaf nodes). By collecting synthesized data according to the\nattributes of each leaf node, we obtain a diverse dataset that fully covers the\ndata space. Empirically, our extensive experiments demonstrate that TreeSynth\nsurpasses both human-designed datasets and the state-of-the-art data synthesis\nbaselines, achieving maximum improvements of 45.2% in data diversity and 17.6%\nin downstream task performance across various models and tasks. Hopefully,\nTreeSynth provides a scalable solution to synthesize diverse and comprehensive\ndatasets from scratch without human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model customization requires high-quality and diverse datasets, but acquiring\nsuch data remains challenging and costly. Although large language models (LLMs)\ncan synthesize training data, current approaches are constrained by limited\nseed data, model bias and insufficient control over the generation process,\nresulting in limited diversity and biased distribution with the increase of\ndata scales. To tackle this challenge, we present TreeSynth, a tree-guided\nsubspace-based data synthesis framework that recursively partitions the entire\ndata space into hierar-chical subspaces, enabling comprehensive and diverse\nscaling of data synthesis. Briefly, given a task-specific description, we\nconstruct a data space partitioning tree by iteratively executing criteria\ndetermination and subspace coverage steps. This hierarchically divides the\nwhole space (i.e., root node) into mutually exclusive and complementary atomic\nsubspaces (i.e., leaf nodes). By collecting synthesized data according to the\nattributes of each leaf node, we obtain a diverse dataset that fully covers the\ndata space. Empirically, our extensive experiments demonstrate that TreeSynth\nsurpasses both human-designed datasets and the state-of-the-art data synthesis\nbaselines, achieving maximum improvements of 45.2% in data diversity and 17.6%\nin downstream task performance across various models and tasks. Hopefully,\nTreeSynth provides a scalable solution to synthesize diverse and comprehensive\ndatasets from scratch without human intervention."
                },
                "authors": [
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Jingqi Zhou"
                    },
                    {
                        "name": "Qintong Li"
                    },
                    {
                        "name": "Jingwei Dong"
                    },
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17181v1",
                "updated": "2025-03-21T14:29:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    29,
                    35,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T14:29:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    29,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and\n  Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Love Python: A Study of LLMs' Bias for Programming Languages and\n  Libraries"
                },
                "summary": "Programming language and library choices are crucial to software reliability\nand security. Poor or inconsistent choices can lead to increased technical\ndebt, security vulnerabilities, and even catastrophic failures in\nsafety-critical systems. As Large Language Models (LLMs) play an increasing\nrole in code generation, it is essential to understand how they make these\ndecisions. However, little is known about their preferences when selecting\nprogramming languages and libraries for different coding tasks. To fill this\ngap, this study provides the first in-depth investigation into LLM preferences\nfor programming languages and libraries used when generating code. We assess\nthe preferences of eight diverse LLMs by prompting them to complete various\ncoding tasks, including widely-studied benchmarks and the more practical task\nof generating the initial structural code for new projects (a crucial step that\noften determines a project's language or library choices).\n  Our findings reveal that LLMs heavily favour Python when solving\nlanguage-agnostic problems, using it in 90%-97% of cases for benchmark tasks.\nEven when generating initial project code where Python is not a suitable\nlanguage, it remains the most-used language in 58% of instances. Moreover, LLMs\ncontradict their own language recommendations in 83% of project initialisation\ntasks, raising concerns about their reliability in guiding language selection.\nSimilar biases toward well-established libraries further create serious\ndiscoverability challenges for newer open-source projects. These results\nhighlight the need to improve LLMs' adaptability to diverse programming\ncontexts and to develop mechanisms for mitigating programming language and\nlibrary bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming language and library choices are crucial to software reliability\nand security. Poor or inconsistent choices can lead to increased technical\ndebt, security vulnerabilities, and even catastrophic failures in\nsafety-critical systems. As Large Language Models (LLMs) play an increasing\nrole in code generation, it is essential to understand how they make these\ndecisions. However, little is known about their preferences when selecting\nprogramming languages and libraries for different coding tasks. To fill this\ngap, this study provides the first in-depth investigation into LLM preferences\nfor programming languages and libraries used when generating code. We assess\nthe preferences of eight diverse LLMs by prompting them to complete various\ncoding tasks, including widely-studied benchmarks and the more practical task\nof generating the initial structural code for new projects (a crucial step that\noften determines a project's language or library choices).\n  Our findings reveal that LLMs heavily favour Python when solving\nlanguage-agnostic problems, using it in 90%-97% of cases for benchmark tasks.\nEven when generating initial project code where Python is not a suitable\nlanguage, it remains the most-used language in 58% of instances. Moreover, LLMs\ncontradict their own language recommendations in 83% of project initialisation\ntasks, raising concerns about their reliability in guiding language selection.\nSimilar biases toward well-established libraries further create serious\ndiscoverability challenges for newer open-source projects. These results\nhighlight the need to improve LLMs' adaptability to diverse programming\ncontexts and to develop mechanisms for mitigating programming language and\nlibrary bias."
                },
                "authors": [
                    {
                        "name": "Lukas Twist"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Don Syme"
                    },
                    {
                        "name": "Joost Noppen"
                    },
                    {
                        "name": "Detlef Nauck"
                    }
                ],
                "author_detail": {
                    "name": "Detlef Nauck"
                },
                "author": "Detlef Nauck",
                "arxiv_comment": "12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12739v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12739v3",
                "updated": "2025-03-21T14:17:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    17,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-19T13:02:54Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    13,
                    2,
                    54,
                    3,
                    263,
                    0
                ],
                "title": "Edu-Values: Towards Evaluating the Chinese Education Values of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edu-Values: Towards Evaluating the Chinese Education Values of Large\n  Language Models"
                },
                "summary": "In this paper, we present Edu-Values, the first Chinese education values\nevaluation benchmark that includes seven core values: professional philosophy,\nteachers' professional ethics, education laws and regulations, cultural\nliteracy, educational knowledge and skills, basic competencies and subject\nknowledge. We meticulously design 1,418 questions, covering multiple-choice,\nmulti-modal question answering, subjective analysis, adversarial prompts, and\nChinese traditional culture (short answer) questions. We conduct human feedback\nbased automatic evaluation over 21 state-of-the-art (SoTA) LLMs, and highlight\nthree main findings: (1) due to differences in educational culture, Chinese\nLLMs outperform English LLMs, with Qwen 2 ranking the first with a score of\n81.37; (2) LLMs often struggle with teachers' professional ethics and\nprofessional philosophy; (3) leveraging Edu-Values to build an external\nknowledge repository for RAG significantly improves LLMs' alignment. This\ndemonstrates the effectiveness of the proposed benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Edu-Values, the first Chinese education values\nevaluation benchmark that includes seven core values: professional philosophy,\nteachers' professional ethics, education laws and regulations, cultural\nliteracy, educational knowledge and skills, basic competencies and subject\nknowledge. We meticulously design 1,418 questions, covering multiple-choice,\nmulti-modal question answering, subjective analysis, adversarial prompts, and\nChinese traditional culture (short answer) questions. We conduct human feedback\nbased automatic evaluation over 21 state-of-the-art (SoTA) LLMs, and highlight\nthree main findings: (1) due to differences in educational culture, Chinese\nLLMs outperform English LLMs, with Qwen 2 ranking the first with a score of\n81.37; (2) LLMs often struggle with teachers' professional ethics and\nprofessional philosophy; (3) leveraging Edu-Values to build an external\nknowledge repository for RAG significantly improves LLMs' alignment. This\ndemonstrates the effectiveness of the proposed benchmark."
                },
                "authors": [
                    {
                        "name": "Peiyi Zhang"
                    },
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Lu Rong"
                    },
                    {
                        "name": "Prayag Tiwari"
                    },
                    {
                        "name": "Jing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qin"
                },
                "author": "Jing Qin",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12739v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12739v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16021v2",
                "updated": "2025-03-21T13:35:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    35,
                    52,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T10:37:29Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    37,
                    29,
                    3,
                    79,
                    0
                ],
                "title": "Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems"
                },
                "summary": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's impact on the\ndiversity and democratic value of information ecosystems. We introduce a\nlarge-scale simulation framework to examine AI-based imitation within news, a\ncontext crucial for public discourse. By systematically testing two distinct\nimitation strategies across a range of information environments varying in\ninitial diversity, we demonstrate that AI-generated articles do not uniformly\nhomogenize content. Instead, AI's influence is strongly context-dependent:\nAI-generated content can introduce valuable diversity in originally homogeneous\nnews environments but diminish diversity in initially heterogeneous contexts.\nThese results illustrate that the initial diversity of an information\nenvironment critically shapes AI's impact, challenging assumptions that\nAI-driven imitation uniformly threatens diversity. Instead, when information is\ninitially homogeneous, AI-driven imitation can expand perspectives, styles, and\ntopics. This is especially important in news contexts, where information\ndiversity fosters richer public debate by exposing citizens to alternative\nviewpoints, challenging biases, and preventing narrative monopolies, which is\nessential for a resilient democracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's impact on the\ndiversity and democratic value of information ecosystems. We introduce a\nlarge-scale simulation framework to examine AI-based imitation within news, a\ncontext crucial for public discourse. By systematically testing two distinct\nimitation strategies across a range of information environments varying in\ninitial diversity, we demonstrate that AI-generated articles do not uniformly\nhomogenize content. Instead, AI's influence is strongly context-dependent:\nAI-generated content can introduce valuable diversity in originally homogeneous\nnews environments but diminish diversity in initially heterogeneous contexts.\nThese results illustrate that the initial diversity of an information\nenvironment critically shapes AI's impact, challenging assumptions that\nAI-driven imitation uniformly threatens diversity. Instead, when information is\ninitially homogeneous, AI-driven imitation can expand perspectives, styles, and\ntopics. This is especially important in news contexts, where information\ndiversity fosters richer public debate by exposing citizens to alternative\nviewpoints, challenging biases, and preventing narrative monopolies, which is\nessential for a resilient democracy."
                },
                "authors": [
                    {
                        "name": "Emil Bakkensen Johansen"
                    },
                    {
                        "name": "Oliver Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Baumann"
                },
                "author": "Oliver Baumann",
                "arxiv_comment": "35 pages, 10 figures, 4 tables; v2: corrected typographical errors,\n  streamlined language, updated abstract, added supplementary information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17126v1",
                "updated": "2025-03-21T13:21:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    21,
                    45,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T13:21:45Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    21,
                    45,
                    4,
                    80,
                    0
                ],
                "title": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modifying Large Language Model Post-Training for Diverse Creative\n  Writing"
                },
                "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO."
                },
                "authors": [
                    {
                        "name": "John Joon Young Chung"
                    },
                    {
                        "name": "Vishakh Padmakumar"
                    },
                    {
                        "name": "Melissa Roemmele"
                    },
                    {
                        "name": "Yuqian Sun"
                    },
                    {
                        "name": "Max Kreminski"
                    }
                ],
                "author_detail": {
                    "name": "Max Kreminski"
                },
                "author": "Max Kreminski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11302v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11302v3",
                "updated": "2025-03-21T13:15:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    15,
                    27,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-14T11:11:03Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    11,
                    3,
                    4,
                    73,
                    0
                ],
                "title": "Are formal and functional linguistic mechanisms dissociated in language\n  models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are formal and functional linguistic mechanisms dissociated in language\n  models?"
                },
                "summary": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist."
                },
                "authors": [
                    {
                        "name": "Michael Hanna"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pezzelle"
                },
                "author": "Sandro Pezzelle",
                "arxiv_comment": "35 pages, 10 figures, 3 tables. Code available at\n  https://github.com/hannamw/formal-functional-dissociation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11302v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11302v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15289v2",
                "updated": "2025-03-21T13:00:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    0,
                    44,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-19T05:57:37Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    5,
                    57,
                    37,
                    3,
                    354,
                    0
                ],
                "title": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage"
                },
                "summary": "Large language models (LLMs) have made significant advancements across\nvarious tasks, but their safety alignment remain a major concern. Exploring\njailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure\nthem. Existing methods primarily design sophisticated instructions for the LLM\nto follow, or rely on multiple iterations, which could hinder the performance\nand efficiency of jailbreaks. In this work, we propose a novel jailbreak\nparadigm, Simple Assistive Task Linkage (SATA), which can effectively\ncircumvent LLM safeguards and elicit harmful responses. Specifically, SATA\nfirst masks harmful keywords within a malicious query to generate a relatively\nbenign query containing one or multiple [MASK] special tokens. It then employs\na simple assistive task such as a masked language model task or an element\nlookup by position task to encode the semantics of the masked keywords.\nFinally, SATA links the assistive task with the masked query to jointly perform\nthe jailbreak. Extensive experiments show that SATA achieves state-of-the-art\nperformance and outperforms baselines by a large margin. Specifically, on\nAdvBench dataset, with mask language model (MLM) assistive task, SATA achieves\nan overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and\nwith element lookup by position (ELP) assistive task, SATA attains an overall\nASR of 76% and HS of 4.43.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advancements across\nvarious tasks, but their safety alignment remain a major concern. Exploring\njailbreak prompts can expose LLMs' vulnerabilities and guide efforts to secure\nthem. Existing methods primarily design sophisticated instructions for the LLM\nto follow, or rely on multiple iterations, which could hinder the performance\nand efficiency of jailbreaks. In this work, we propose a novel jailbreak\nparadigm, Simple Assistive Task Linkage (SATA), which can effectively\ncircumvent LLM safeguards and elicit harmful responses. Specifically, SATA\nfirst masks harmful keywords within a malicious query to generate a relatively\nbenign query containing one or multiple [MASK] special tokens. It then employs\na simple assistive task such as a masked language model task or an element\nlookup by position task to encode the semantics of the masked keywords.\nFinally, SATA links the assistive task with the masked query to jointly perform\nthe jailbreak. Extensive experiments show that SATA achieves state-of-the-art\nperformance and outperforms baselines by a large margin. Specifically, on\nAdvBench dataset, with mask language model (MLM) assistive task, SATA achieves\nan overall attack success rate (ASR) of 85% and harmful score (HS) of 4.57, and\nwith element lookup by position (ELP) assistive task, SATA attains an overall\nASR of 76% and HS of 4.43."
                },
                "authors": [
                    {
                        "name": "Xiaoning Dong"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18008v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18008v5",
                "updated": "2025-03-21T12:53:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    53,
                    4,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-25T09:12:07Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    12,
                    7,
                    1,
                    56,
                    0
                ],
                "title": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NotaGen: Advancing Musicality in Symbolic Music Generation with Large\n  Language Model Training Paradigms"
                },
                "summary": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NotaGen, a symbolic music generation model aiming to explore the\npotential of producing high-quality classical sheet music. Inspired by the\nsuccess of Large Language Models (LLMs), NotaGen adopts pre-training,\nfine-tuning, and reinforcement learning paradigms (henceforth referred to as\nthe LLM training paradigms). It is pre-trained on 1.6M pieces of music in ABC\nnotation, and then fine-tuned on approximately 9K high-quality classical\ncompositions conditioned on \"period-composer-instrumentation\" prompts. For\nreinforcement learning, we propose the CLaMP-DPO method, which further enhances\ngeneration quality and controllability without requiring human annotations or\npredefined rewards. Our experiments demonstrate the efficacy of CLaMP-DPO in\nsymbolic music generation models with different architectures and encoding\nschemes. Furthermore, subjective A/B tests show that NotaGen outperforms\nbaseline models against human compositions, greatly advancing musical\naesthetics in symbolic music generation."
                },
                "authors": [
                    {
                        "name": "Yashan Wang"
                    },
                    {
                        "name": "Shangda Wu"
                    },
                    {
                        "name": "Jianhuai Hu"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Yueqi Peng"
                    },
                    {
                        "name": "Yongxin Huang"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Xiaobing Li"
                    },
                    {
                        "name": "Feng Yu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18008v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18008v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00363v2",
                "updated": "2025-03-21T12:52:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    52,
                    57,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-31T09:29:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    29,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "SPDZCoder: Combining Expert Knowledge with LLMs for Generating\n  Privacy-Computing Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPDZCoder: Combining Expert Knowledge with LLMs for Generating\n  Privacy-Computing Code"
                },
                "summary": "Privacy computing receives increasing attention but writing privacy computing\ncode remains challenging for developers due to limited library functions,\nnecessitating function implementation from scratch, and data-oblivious\nrequirement, contradicting intuitive thinking and usual practices of\nprogrammers. Automating the generation of privacy computing code with Large\nLanguage Models can streamline development effort and lower the barrier to\nusing privacy computing frameworks. However, existing LLMs still encounter\nchallenges in code translation for privacy-preserving computation, such as\ntranslating Python to MP-SPDZ, due to the scarcity of MP-SPDZ data required for\neffective pre-training or fine-tuning. Moreover, the lack of a benchmark\nfurther complicates the evaluation of translation quality. To address the\nlimitations, this work proposes SPDZCoder, a rule-based framework that combines\nLLMs with expert knowledge for generating privacy-computing code without\nrequiring additional training data. Specifically, SPDZCoder employ a rigorous\nprocedure for collecting high-quality expert knowledge to represent the\nsemantic-expressing differences between Python and MP-SPDZ, and to derive\ntransformation rules for translating Python to MP-SPDZ based on these\nknowledge. Then, SPDZCoder progressively converts Python code into MP-SPDZ code\nusing transformation rules in a three stage pipeline. To evaluate SPDZCoder, we\nmanually constructed a benchmark dataset, SPDZEval, which comprises six data\nsplits, each representing a distinct class of challenging tasks in MP-SPDZ\nimplementation. Extensive experiments show that SPDZCoder achieves superior\nperformance, significantly surpassing baselines in pass@1 and pass@2.\nSpecifically, SPDZCoder attains an overall correctness of 85.94% and 92.01% in\npass@1 and pass@2, respectively, whereas the best-performing baseline achieves\n63.58% and 76.36%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy computing receives increasing attention but writing privacy computing\ncode remains challenging for developers due to limited library functions,\nnecessitating function implementation from scratch, and data-oblivious\nrequirement, contradicting intuitive thinking and usual practices of\nprogrammers. Automating the generation of privacy computing code with Large\nLanguage Models can streamline development effort and lower the barrier to\nusing privacy computing frameworks. However, existing LLMs still encounter\nchallenges in code translation for privacy-preserving computation, such as\ntranslating Python to MP-SPDZ, due to the scarcity of MP-SPDZ data required for\neffective pre-training or fine-tuning. Moreover, the lack of a benchmark\nfurther complicates the evaluation of translation quality. To address the\nlimitations, this work proposes SPDZCoder, a rule-based framework that combines\nLLMs with expert knowledge for generating privacy-computing code without\nrequiring additional training data. Specifically, SPDZCoder employ a rigorous\nprocedure for collecting high-quality expert knowledge to represent the\nsemantic-expressing differences between Python and MP-SPDZ, and to derive\ntransformation rules for translating Python to MP-SPDZ based on these\nknowledge. Then, SPDZCoder progressively converts Python code into MP-SPDZ code\nusing transformation rules in a three stage pipeline. To evaluate SPDZCoder, we\nmanually constructed a benchmark dataset, SPDZEval, which comprises six data\nsplits, each representing a distinct class of challenging tasks in MP-SPDZ\nimplementation. Extensive experiments show that SPDZCoder achieves superior\nperformance, significantly surpassing baselines in pass@1 and pass@2.\nSpecifically, SPDZCoder attains an overall correctness of 85.94% and 92.01% in\npass@1 and pass@2, respectively, whereas the best-performing baseline achieves\n63.58% and 76.36%, respectively."
                },
                "authors": [
                    {
                        "name": "Xiaoning Dong"
                    },
                    {
                        "name": "Peilin Xin"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10332v3",
                "updated": "2025-03-21T12:40:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    40,
                    26,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-15T16:32:34Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    32,
                    34,
                    4,
                    320,
                    0
                ],
                "title": "Number it: Temporal Grounding Videos like Flipping Manga",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number it: Temporal Grounding Videos like Flipping Manga"
                },
                "summary": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in\ncomprehending video content for QA dialogue. However, they struggle to extend\nthis visual understanding to tasks requiring precise temporal localization,\nknown as Video Temporal Grounding (VTG). To address this gap, we introduce\nNumber-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual\ncomprehension with temporal grounding by adding unique numerical identifiers to\neach video frame. Treating a video as a sequence of numbered frame images,\nNumPro transforms VTG into an intuitive process: flipping through manga panels\nin sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking\nvisual content with corresponding temporal information. Our experiments\ndemonstrate that NumPro significantly boosts VTG performance of top-tier\nVid-LLMs without additional computational cost. Furthermore, fine-tuning on a\nNumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing\nprevious top-performing methods by up to 6.9\\% in mIoU for moment retrieval and\n8.5\\% in mAP for highlight detection. The code will be available at\nhttps://github.com/yongliang-wu/NumPro."
                },
                "authors": [
                    {
                        "name": "Yongliang Wu"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Yuyang Sun"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Wenbo Zhu"
                    },
                    {
                        "name": "Fengyun Rao"
                    },
                    {
                        "name": "Bernt Schiele"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17101v1",
                "updated": "2025-03-21T12:39:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    39,
                    16,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T12:39:16Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    39,
                    16,
                    4,
                    80,
                    0
                ],
                "title": "Large Language Model Compression via the Nested Activation-Aware\n  Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Compression via the Nested Activation-Aware\n  Decomposition"
                },
                "summary": "In this paper, we tackle the critical challenge of compressing large language\nmodels (LLMs) to facilitate their practical deployment and broader adoption. We\nintroduce a novel post-training compression paradigm that focuses on low-rank\ndecomposition of LLM weights. Our analysis identifies two main challenges in\nthis task: the variability in LLM activation distributions and handling unseen\nactivations from different datasets and models.\n  To address these challenges, we propose a nested activation-aware framework\n(NSVD) for LLMs, a training-free approach designed to enhance the accuracy of\nlow-rank decompositions by managing activation outliers through transforming\nthe weight matrix based on activation distribution and the original weight\nmatrix. This method allows for the absorption of outliers into the transformed\nweight matrix, improving decomposition accuracy. Our comprehensive evaluation\nacross eight datasets and six models from three distinct LLM families\ndemonstrates the superiority of NSVD over current state-of-the-art methods,\nespecially at medium to large compression ratios or in multilingual and\nmultitask settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we tackle the critical challenge of compressing large language\nmodels (LLMs) to facilitate their practical deployment and broader adoption. We\nintroduce a novel post-training compression paradigm that focuses on low-rank\ndecomposition of LLM weights. Our analysis identifies two main challenges in\nthis task: the variability in LLM activation distributions and handling unseen\nactivations from different datasets and models.\n  To address these challenges, we propose a nested activation-aware framework\n(NSVD) for LLMs, a training-free approach designed to enhance the accuracy of\nlow-rank decompositions by managing activation outliers through transforming\nthe weight matrix based on activation distribution and the original weight\nmatrix. This method allows for the absorption of outliers into the transformed\nweight matrix, improving decomposition accuracy. Our comprehensive evaluation\nacross eight datasets and six models from three distinct LLM families\ndemonstrates the superiority of NSVD over current state-of-the-art methods,\nespecially at medium to large compression ratios or in multilingual and\nmultitask settings."
                },
                "authors": [
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Tianyi Xu"
                    },
                    {
                        "name": "Bill Ding"
                    },
                    {
                        "name": "David Li"
                    },
                    {
                        "name": "Yu Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Kang"
                },
                "author": "Yu Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16801v2",
                "updated": "2025-03-21T12:34:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    34,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-10-22T08:27:23Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    8,
                    27,
                    23,
                    1,
                    296,
                    0
                ],
                "title": "Controlled Low-Rank Adaptation with Subspace Regularization for\n  Continued Training on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlled Low-Rank Adaptation with Subspace Regularization for\n  Continued Training on Large Language Models"
                },
                "summary": "Large language models (LLMs) exhibit remarkable capabilities in natural\nlanguage processing but face catastrophic forgetting when learning new tasks,\nwhere adaptation to a new domain leads to a substantial decline in performance\non previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a\nsub-space regularization method on LoRA structure. Aiming to reduce the scale\nof output change while introduce minimal constraint on model capacity, CLoRA\nimposes constraint on the direction of updating matrix's null space.\nExperimental results on one-stage LLM finetuning tasks and continual learning\nsettings highlight the superority of CLoRA as a effective parameter efficient\nfinetuning method with catastrophic forgetting mitigating.Further investigation\nfor model parameters indicates that CLoRA effectively balances the trade-off\nbetween model capacity and degree of forgetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable capabilities in natural\nlanguage processing but face catastrophic forgetting when learning new tasks,\nwhere adaptation to a new domain leads to a substantial decline in performance\non previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a\nsub-space regularization method on LoRA structure. Aiming to reduce the scale\nof output change while introduce minimal constraint on model capacity, CLoRA\nimposes constraint on the direction of updating matrix's null space.\nExperimental results on one-stage LLM finetuning tasks and continual learning\nsettings highlight the superority of CLoRA as a effective parameter efficient\nfinetuning method with catastrophic forgetting mitigating.Further investigation\nfor model parameters indicates that CLoRA effectively balances the trade-off\nbetween model capacity and degree of forgetting."
                },
                "authors": [
                    {
                        "name": "Yuheng Lu"
                    },
                    {
                        "name": "Bingshuo Qian"
                    },
                    {
                        "name": "Caixia Yuan"
                    },
                    {
                        "name": "Huixing Jiang"
                    },
                    {
                        "name": "Xiaojie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojie Wang"
                },
                "author": "Xiaojie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17073v1",
                "updated": "2025-03-21T11:56:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    11,
                    56,
                    17,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T11:56:17Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    11,
                    56,
                    17,
                    4,
                    80,
                    0
                ],
                "title": "A Study into Investigating Temporal Robustness of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study into Investigating Temporal Robustness of LLMs"
                },
                "summary": "Large Language Models (LLMs) encapsulate a surprising amount of factual world\nknowledge. However, their performance on temporal questions and historical\nknowledge is limited because they often cannot understand temporal scope and\norientation or neglect the temporal aspect altogether. In this study, we aim to\nmeasure precisely how robust LLMs are for question answering based on their\nability to process temporal information and perform tasks requiring temporal\nreasoning and temporal factual knowledge. Specifically, we design eight\ntime-sensitive robustness tests for factual information to check the\nsensitivity of six popular LLMs in the zero-shot setting. Overall, we find LLMs\nlacking temporal robustness, especially to temporal reformulations and the use\nof different granularities of temporal references. We show how a selection of\nthese eight tests can be used automatically to judge a model's temporal\nrobustness for user questions on the fly. Finally, we apply the findings of\nthis study to improve the temporal QA performance by up to 55 percent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encapsulate a surprising amount of factual world\nknowledge. However, their performance on temporal questions and historical\nknowledge is limited because they often cannot understand temporal scope and\norientation or neglect the temporal aspect altogether. In this study, we aim to\nmeasure precisely how robust LLMs are for question answering based on their\nability to process temporal information and perform tasks requiring temporal\nreasoning and temporal factual knowledge. Specifically, we design eight\ntime-sensitive robustness tests for factual information to check the\nsensitivity of six popular LLMs in the zero-shot setting. Overall, we find LLMs\nlacking temporal robustness, especially to temporal reformulations and the use\nof different granularities of temporal references. We show how a selection of\nthese eight tests can be used automatically to judge a model's temporal\nrobustness for user questions on the fly. Finally, we apply the findings of\nthis study to improve the temporal QA performance by up to 55 percent."
                },
                "authors": [
                    {
                        "name": "Jonas Wallat"
                    },
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Adam Jatowt"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16193v2",
                "updated": "2025-03-21T11:50:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    11,
                    50,
                    8,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T14:40:48Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    40,
                    48,
                    3,
                    79,
                    0
                ],
                "title": "Affective Polarization Amongst Swedish Politicians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affective Polarization Amongst Swedish Politicians"
                },
                "summary": "This study investigates affective polarization among Swedish politicians on\nTwitter from 2021 to 2023, including the September 2022 parliamentary election.\nAnalyzing over 25,000 tweets and employing large language models (LLMs) for\nsentiment and political classification, we distinguish between positive\npartisanship (support of allies) and negative partisanship (criticism of\nopponents).\n  Our findings are contingent on the definition of the in-group. When political\nin-groups are defined at the ideological bloc level, negative and positive\npartisanship occur at similar rates. However, when the in-group is defined at\nthe party level, negative partisanship becomes significantly more dominant and\nis 1.51 times more likely (1.45, 1.58). This effect is even stronger among\nextreme politicians, who engage in negativity more than their moderate\ncounterparts. Negative partisanship also proves to be a strategic choice for\nonline visibility, attracting 3.18 more likes and 1.69 more retweets on\naverage.\n  By adapting methods developed for two-party systems and leveraging LLMs for\nSwedish-language analysis, we provide novel insights into how multiparty\npolitics shapes polarizing discourse. Our results underscore both the strategic\nappeal of negativity in digital spaces and the growing potential of LLMs for\nlarge-scale, non-English political research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates affective polarization among Swedish politicians on\nTwitter from 2021 to 2023, including the September 2022 parliamentary election.\nAnalyzing over 25,000 tweets and employing large language models (LLMs) for\nsentiment and political classification, we distinguish between positive\npartisanship (support of allies) and negative partisanship (criticism of\nopponents).\n  Our findings are contingent on the definition of the in-group. When political\nin-groups are defined at the ideological bloc level, negative and positive\npartisanship occur at similar rates. However, when the in-group is defined at\nthe party level, negative partisanship becomes significantly more dominant and\nis 1.51 times more likely (1.45, 1.58). This effect is even stronger among\nextreme politicians, who engage in negativity more than their moderate\ncounterparts. Negative partisanship also proves to be a strategic choice for\nonline visibility, attracting 3.18 more likes and 1.69 more retweets on\naverage.\n  By adapting methods developed for two-party systems and leveraging LLMs for\nSwedish-language analysis, we provide novel insights into how multiparty\npolitics shapes polarizing discourse. Our results underscore both the strategic\nappeal of negativity in digital spaces and the growing potential of LLMs for\nlarge-scale, non-English political research."
                },
                "authors": [
                    {
                        "name": "François t'Serstevens"
                    },
                    {
                        "name": "Roberto Cerina"
                    },
                    {
                        "name": "Gustav Peper"
                    }
                ],
                "author_detail": {
                    "name": "Gustav Peper"
                },
                "author": "Gustav Peper",
                "arxiv_comment": "5 figures, 4 tables, 26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17042v1",
                "updated": "2025-03-21T10:56:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    56,
                    38,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:56:38Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    56,
                    38,
                    4,
                    80,
                    0
                ],
                "title": "FPA Beamforming for Alignment-Tolerant FSO QKD Links",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FPA Beamforming for Alignment-Tolerant FSO QKD Links"
                },
                "summary": "We demonstrate focal plane array beamforming for semi-blind deployments of\nfree-space optical QKD links. We accomplish a secure-key rate of 1.2 kb/s at a\nQBER of 9.1% over a 63-m out-door link during full sunshine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate focal plane array beamforming for semi-blind deployments of\nfree-space optical QKD links. We accomplish a secure-key rate of 1.2 kb/s at a\nQBER of 9.1% over a 63-m out-door link during full sunshine."
                },
                "authors": [
                    {
                        "name": "Florian Honz"
                    },
                    {
                        "name": "Winfried Boxleitner"
                    },
                    {
                        "name": "Michael Hentschel"
                    },
                    {
                        "name": "Philip Walther"
                    },
                    {
                        "name": "Hannes Hübel"
                    },
                    {
                        "name": "Bernhard Schrenk"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schrenk"
                },
                "arxiv_affiliation": "AIT Austrian Institute of Technology",
                "author": "Bernhard Schrenk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17040v1",
                "updated": "2025-03-21T10:53:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    53,
                    59,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:53:59Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    53,
                    59,
                    4,
                    80,
                    0
                ],
                "title": "Problem Framing in the AI era: a new model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem Framing in the AI era: a new model"
                },
                "summary": "Effective problem-solving in physics extends beyond the mere application of\nmathematical formulas; it necessitates an understanding of how mathematical\nconcepts connect to and reflect the physical world. A strong epistemological\nframework based on problem framing (PF) is essential for students, as it\nenables them to justify their mathematical decisions and recognize the\nrelationship between abstract mathematics and real-world physical phenomena.\nThis becomes increasingly important in the age of artificial intelligence (AI),\nwhere the use of Large Language Models (LLMs) in education is growing rapidly.\nThis paper explores the impact of AI, specifically LLMs like ChatGPT, on\nupper-level students' PF in physics education. Building on existing models, in\nthis exploratory theoretical paper, we propose a novel three-dimensional\nframework grounded in Situated Cognition Theory and Greeno's extended semantic\nmodel, aiming to elucidate how AI could influence students' epistemological\nframing during Cooperative Problem Solving activities (CPS). We advocate for\ninstructors to encourage AI-assisted CPS to foster critical thinking and\nenhance student engagement with real-world scenarios. Preliminary results\nsuggest that ChatGPT can aid in developing symbolic and visual languages within\nproblem framing, though further research is needed to confirm these findings\nand investigate the potential of AI-driven intelligent tutoring systems for\npersonalized learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective problem-solving in physics extends beyond the mere application of\nmathematical formulas; it necessitates an understanding of how mathematical\nconcepts connect to and reflect the physical world. A strong epistemological\nframework based on problem framing (PF) is essential for students, as it\nenables them to justify their mathematical decisions and recognize the\nrelationship between abstract mathematics and real-world physical phenomena.\nThis becomes increasingly important in the age of artificial intelligence (AI),\nwhere the use of Large Language Models (LLMs) in education is growing rapidly.\nThis paper explores the impact of AI, specifically LLMs like ChatGPT, on\nupper-level students' PF in physics education. Building on existing models, in\nthis exploratory theoretical paper, we propose a novel three-dimensional\nframework grounded in Situated Cognition Theory and Greeno's extended semantic\nmodel, aiming to elucidate how AI could influence students' epistemological\nframing during Cooperative Problem Solving activities (CPS). We advocate for\ninstructors to encourage AI-assisted CPS to foster critical thinking and\nenhance student engagement with real-world scenarios. Preliminary results\nsuggest that ChatGPT can aid in developing symbolic and visual languages within\nproblem framing, though further research is needed to confirm these findings\nand investigate the potential of AI-driven intelligent tutoring systems for\npersonalized learning."
                },
                "authors": [
                    {
                        "name": "Matteo Tuveri"
                    },
                    {
                        "name": "Arianna Steri"
                    },
                    {
                        "name": "Viviana Fanti"
                    }
                ],
                "author_detail": {
                    "name": "Viviana Fanti"
                },
                "author": "Viviana Fanti",
                "arxiv_comment": "18 pages, 6 figures, 1 table; submitted to European Journal of\n  Physics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12547v2",
                "updated": "2025-03-21T10:53:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    53,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-16T15:32:30Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    15,
                    32,
                    30,
                    6,
                    75,
                    0
                ],
                "title": "LLMSeR: Enhancing Sequential Recommendation via LLM-based Data\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSeR: Enhancing Sequential Recommendation via LLM-based Data\n  Augmentation"
                },
                "summary": "Sequential Recommender Systems (SRS) have become a cornerstone of online\nplatforms, leveraging users' historical interaction data to forecast their next\npotential engagement. Despite their widespread adoption, SRS often grapple with\nthe long-tail user dilemma, resulting in less effective recommendations for\nindividuals with limited interaction records. The advent of Large Language\nModels (LLMs), with their profound capability to discern semantic relationships\namong items, has opened new avenues for enhancing SRS through data\naugmentation. Nonetheless, current methodologies encounter obstacles, including\nthe absence of collaborative signals and the prevalence of hallucination\nphenomena. In this work, we present LLMSeR, an innovative framework that\nutilizes Large Language Models (LLMs) to generate pseudo-prior items, thereby\nimproving the efficacy of Sequential Recommender Systems (SRS). To alleviate\nthe challenge of insufficient collaborative signals, we introduce the Semantic\nInteraction Augmentor (SIA), a method that integrates both semantic and\ncollaborative information to comprehensively augment user interaction data.\nMoreover, to weaken the adverse effects of hallucination in SRS, we develop the\nAdaptive Reliability Validation (ARV), a validation technique designed to\nassess the reliability of the generated pseudo items. Complementing these\nadvancements, we also devise a Dual-Channel Training strategy, ensuring\nseamless integration of data augmentation into the SRS training\nprocess.Extensive experiments conducted with three widely-used SRS models\ndemonstrate the generalizability and efficacy of LLMSeR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Recommender Systems (SRS) have become a cornerstone of online\nplatforms, leveraging users' historical interaction data to forecast their next\npotential engagement. Despite their widespread adoption, SRS often grapple with\nthe long-tail user dilemma, resulting in less effective recommendations for\nindividuals with limited interaction records. The advent of Large Language\nModels (LLMs), with their profound capability to discern semantic relationships\namong items, has opened new avenues for enhancing SRS through data\naugmentation. Nonetheless, current methodologies encounter obstacles, including\nthe absence of collaborative signals and the prevalence of hallucination\nphenomena. In this work, we present LLMSeR, an innovative framework that\nutilizes Large Language Models (LLMs) to generate pseudo-prior items, thereby\nimproving the efficacy of Sequential Recommender Systems (SRS). To alleviate\nthe challenge of insufficient collaborative signals, we introduce the Semantic\nInteraction Augmentor (SIA), a method that integrates both semantic and\ncollaborative information to comprehensively augment user interaction data.\nMoreover, to weaken the adverse effects of hallucination in SRS, we develop the\nAdaptive Reliability Validation (ARV), a validation technique designed to\nassess the reliability of the generated pseudo items. Complementing these\nadvancements, we also devise a Dual-Channel Training strategy, ensuring\nseamless integration of data augmentation into the SRS training\nprocess.Extensive experiments conducted with three widely-used SRS models\ndemonstrate the generalizability and efficacy of LLMSeR."
                },
                "authors": [
                    {
                        "name": "Yuqi Sun"
                    },
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Haiping Zhu"
                    },
                    {
                        "name": "Feng Tian"
                    }
                ],
                "author_detail": {
                    "name": "Feng Tian"
                },
                "author": "Feng Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17039v1",
                "updated": "2025-03-21T10:52:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    52,
                    20,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:52:20Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    52,
                    20,
                    4,
                    80,
                    0
                ],
                "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and\n  LLM-Judges Correlate with Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and\n  LLM-Judges Correlate with Humans?"
                },
                "summary": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads."
                },
                "authors": [
                    {
                        "name": "Jeremy Barnes"
                    },
                    {
                        "name": "Naiara Perez"
                    },
                    {
                        "name": "Alba Bonet-Jover"
                    },
                    {
                        "name": "Begoña Altuna"
                    }
                ],
                "author_detail": {
                    "name": "Begoña Altuna"
                },
                "author": "Begoña Altuna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17777v2",
                "updated": "2025-03-21T10:51:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    51,
                    22,
                    4,
                    80,
                    0
                ],
                "published": "2024-07-25T05:10:48Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    5,
                    10,
                    48,
                    3,
                    207,
                    0
                ],
                "title": "Babel: A Scalable Pre-trained Model for Multi-Modal Sensing via\n  Expandable Modality Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Babel: A Scalable Pre-trained Model for Multi-Modal Sensing via\n  Expandable Modality Alignment"
                },
                "summary": "This paper presents Babel, the expandable modality alignment model, specially\ndesigned for multi-modal sensing. While there has been considerable work on\nmulti-modality alignment, they all struggle to effectively incorporate multiple\nsensing modalities due to the data scarcity constraints. How to utilize\nmulti-modal data with partial pairings in sensing remains an unresolved\nchallenge. Babel tackles this challenge by introducing the concept of\nexpandable modality alignment. The key idea involves transforming the\nN-modality alignment into a series of binary-modality alignments. Novel\ntechniques are also proposed to further mitigate data scarcity issue and\nbalance the contribution of the newly incorporated modality with the previously\nestablished modality alignment during the expandable alignment process. We\nprovide the comprehensive implementation. In the pre-training phase, Babel\ncurrently aligns 6 sensing modalities, namely Wi-Fi, mmWave, IMU, LiDAR, video,\nand depth. For the deployment phase, as a foundation model, any single or\ncombination of aligned modalities could be selected from Babel and applied to\ndownstream tasks. Evaluation demonstrates Babel's outstanding performance on\neight human activity recognition datasets, compared to a broad range of\nbaselines e.g., the SOTA single-modal sensing networks, multi-modal sensing\nframework, and multi-modal large language models. Babel not only improves the\nperformance of individual modality sensing (12% averaged accuracy improvement),\nbut also effectively fuses multiple available modalities (up to 22% accuracy\nincrease). Case studies also highlight emerging application scenarios empowered\nby Babel, including cross-modality retrieval (i.e., sensing imaging), and\nbridging LLM for sensing comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Babel, the expandable modality alignment model, specially\ndesigned for multi-modal sensing. While there has been considerable work on\nmulti-modality alignment, they all struggle to effectively incorporate multiple\nsensing modalities due to the data scarcity constraints. How to utilize\nmulti-modal data with partial pairings in sensing remains an unresolved\nchallenge. Babel tackles this challenge by introducing the concept of\nexpandable modality alignment. The key idea involves transforming the\nN-modality alignment into a series of binary-modality alignments. Novel\ntechniques are also proposed to further mitigate data scarcity issue and\nbalance the contribution of the newly incorporated modality with the previously\nestablished modality alignment during the expandable alignment process. We\nprovide the comprehensive implementation. In the pre-training phase, Babel\ncurrently aligns 6 sensing modalities, namely Wi-Fi, mmWave, IMU, LiDAR, video,\nand depth. For the deployment phase, as a foundation model, any single or\ncombination of aligned modalities could be selected from Babel and applied to\ndownstream tasks. Evaluation demonstrates Babel's outstanding performance on\neight human activity recognition datasets, compared to a broad range of\nbaselines e.g., the SOTA single-modal sensing networks, multi-modal sensing\nframework, and multi-modal large language models. Babel not only improves the\nperformance of individual modality sensing (12% averaged accuracy improvement),\nbut also effectively fuses multiple available modalities (up to 22% accuracy\nincrease). Case studies also highlight emerging application scenarios empowered\nby Babel, including cross-modality retrieval (i.e., sensing imaging), and\nbridging LLM for sensing comprehension."
                },
                "authors": [
                    {
                        "name": "Shenghong Dai"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Suman Banerjee"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted by SenSys'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v1",
                "updated": "2025-03-21T10:48:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16173v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16173v2",
                "updated": "2025-03-21T10:44:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    44,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-25T08:04:47Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    4,
                    47,
                    0,
                    330,
                    0
                ],
                "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis"
                },
                "summary": "Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences."
                },
                "authors": [
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Hosu Lee"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_comment": "Project page: https://ivy-lvlm.github.io/SALOVA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16173v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16173v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15463v2",
                "updated": "2025-03-21T10:33:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    33,
                    21,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-19T17:41:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    41,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment"
                },
                "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15722v2",
                "updated": "2025-03-21T10:20:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    20,
                    44,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-19T22:28:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    22,
                    28,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Leveraging MoE-based Large Language Model for Zero-Shot Multi-Task\n  Semantic Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging MoE-based Large Language Model for Zero-Shot Multi-Task\n  Semantic Communication"
                },
                "summary": "Multi-task semantic communication (SC) can reduce the computational resources\nin wireless systems since retraining is not required when switching between\ntasks. However, existing approaches typically rely on task-specific embeddings\nto identify the intended task, necessitating retraining the entire model when\ngiven a new task. Consequently, this drives the need for a multi-task SC system\nthat can handle new tasks without additional training, known as zero-shot\nlearning. Inspired by the superior zero-shot capabilities of large language\nmodels (LLMs), we leverage pre-trained instruction-tuned LLMs, referred to as\nfine-tuned language net (FLAN), to improve the generalization capability. We\nincorporate a mixture-of-experts (MoE) architecture in the FLAN model and\npropose MoE-FLAN-SC architecture for multi-task SC systems. Our proposed\nMoE-FLAN-SC architecture can further improve the performance of FLAN-T5 model\nwithout increasing the computational cost. Moreover, we design a multi-task\nfeature extraction module (FEM) which can adaptively extract relevant features\nacross various tasks given the provided features and signal-to-noise ratio\n(SNR). Simulation results show that our proposed MoE-FLAN-SC architecture\noutperforms three state-of-the-art models in terms of the average accuracy on\nfour different unseen tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task semantic communication (SC) can reduce the computational resources\nin wireless systems since retraining is not required when switching between\ntasks. However, existing approaches typically rely on task-specific embeddings\nto identify the intended task, necessitating retraining the entire model when\ngiven a new task. Consequently, this drives the need for a multi-task SC system\nthat can handle new tasks without additional training, known as zero-shot\nlearning. Inspired by the superior zero-shot capabilities of large language\nmodels (LLMs), we leverage pre-trained instruction-tuned LLMs, referred to as\nfine-tuned language net (FLAN), to improve the generalization capability. We\nincorporate a mixture-of-experts (MoE) architecture in the FLAN model and\npropose MoE-FLAN-SC architecture for multi-task SC systems. Our proposed\nMoE-FLAN-SC architecture can further improve the performance of FLAN-T5 model\nwithout increasing the computational cost. Moreover, we design a multi-task\nfeature extraction module (FEM) which can adaptively extract relevant features\nacross various tasks given the provided features and signal-to-noise ratio\n(SNR). Simulation results show that our proposed MoE-FLAN-SC architecture\noutperforms three state-of-the-art models in terms of the average accuracy on\nfour different unseen tasks."
                },
                "authors": [
                    {
                        "name": "Sin-Yu Huang"
                    },
                    {
                        "name": "Renjie Liao"
                    },
                    {
                        "name": "Vincent W. S. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Vincent W. S. Wong"
                },
                "author": "Vincent W. S. Wong",
                "arxiv_comment": "Accepted by IEEE International Conference on Communications (ICC),\n  June 2025, Montreal, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05935v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05935v3",
                "updated": "2025-03-21T10:19:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    19,
                    1,
                    4,
                    80,
                    0
                ],
                "published": "2024-02-08T18:59:48Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    18,
                    59,
                    48,
                    3,
                    39,
                    0
                ],
                "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large\n  Language Models"
                },
                "summary": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory"
                },
                "authors": [
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Longtian Qiu"
                    },
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Weifeng Lin"
                    },
                    {
                        "name": "Shitian Zhao"
                    },
                    {
                        "name": "Shijie Geng"
                    },
                    {
                        "name": "Ziyi Lin"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Hao Shao"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "arxiv_comment": "Accepted by ICML 2024. Code and models are released at\n  https://github.com/Alpha-VLLM/LLaMA2-Accessory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05935v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05935v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17004v1",
                "updated": "2025-03-21T10:09:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    34,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:09:34Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    34,
                    4,
                    80,
                    0
                ],
                "title": "Text2Model: Generating dynamic chemical reactor models using large\n  language models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Model: Generating dynamic chemical reactor models using large\n  language models (LLMs)"
                },
                "summary": "As large language models have shown remarkable capabilities in conversing via\nnatural language, the question arises as to how LLMs could potentially assist\nchemical engineers in research and industry with domain-specific tasks. We\ngenerate dynamic chemical reactor models in Modelica code format from textual\ndescriptions as user input. We fine-tune Llama 3.1 8B Instruct on synthetically\ngenerated Modelica code for different reactor scenarios. We compare the\nperformance of our fine-tuned model to the baseline Llama 3.1 8B Instruct model\nand GPT4o. We manually assess the models' predictions regarding the syntactic\nand semantic accuracy of the generated dynamic models. We find that\nconsiderable improvements are achieved by the fine-tuned model with respect to\nboth the semantic and the syntactic accuracy of the Modelica models. However,\nthe fine-tuned model lacks a satisfactory ability to generalize to unseen\nscenarios compared to GPT4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models have shown remarkable capabilities in conversing via\nnatural language, the question arises as to how LLMs could potentially assist\nchemical engineers in research and industry with domain-specific tasks. We\ngenerate dynamic chemical reactor models in Modelica code format from textual\ndescriptions as user input. We fine-tune Llama 3.1 8B Instruct on synthetically\ngenerated Modelica code for different reactor scenarios. We compare the\nperformance of our fine-tuned model to the baseline Llama 3.1 8B Instruct model\nand GPT4o. We manually assess the models' predictions regarding the syntactic\nand semantic accuracy of the generated dynamic models. We find that\nconsiderable improvements are achieved by the fine-tuned model with respect to\nboth the semantic and the syntactic accuracy of the Modelica models. However,\nthe fine-tuned model lacks a satisfactory ability to generalize to unseen\nscenarios compared to GPT4o."
                },
                "authors": [
                    {
                        "name": "Sophia Rupprecht"
                    },
                    {
                        "name": "Yassine Hounat"
                    },
                    {
                        "name": "Monisha Kumar"
                    },
                    {
                        "name": "Giacomo Lastrucci"
                    },
                    {
                        "name": "Artur M. Schweidtmann"
                    }
                ],
                "author_detail": {
                    "name": "Artur M. Schweidtmann"
                },
                "author": "Artur M. Schweidtmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17003v1",
                "updated": "2025-03-21T10:09:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    16,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T10:09:16Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    9,
                    16,
                    4,
                    80,
                    0
                ],
                "title": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Personalized Alignment -- The Missing Piece for Large\n  Language Models in Real-World Applications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs."
                },
                "authors": [
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Chuanqi Cheng"
                    },
                    {
                        "name": "Wei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wu"
                },
                "author": "Wei Wu",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17397v2",
                "updated": "2025-03-21T09:56:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    56,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-25T22:14:34Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    22,
                    14,
                    34,
                    2,
                    269,
                    0
                ],
                "title": "Building Multilingual Datasets for Predicting Mental Health Severity\n  through LLMs: Prospects and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Multilingual Datasets for Predicting Mental Health Severity\n  through LLMs: Prospects and Challenges"
                },
                "summary": "Large Language Models (LLMs) are increasingly being integrated into various\nmedical fields, including mental health support systems. However, there is a\ngap in research regarding the effectiveness of LLMs in non-English mental\nhealth support applications. To address this problem, we present a novel\nmultilingual adaptation of widely-used mental health datasets, translated from\nEnglish into six languages (e.g., Greek, Turkish, French, Portuguese, German,\nand Finnish). This dataset enables a comprehensive evaluation of LLM\nperformance in detecting mental health conditions and assessing their severity\nacross multiple languages. By experimenting with GPT and Llama, we observe\nconsiderable variability in performance across languages, despite being\nevaluated on the same translated dataset. This inconsistency underscores the\ncomplexities inherent in multilingual mental health support, where\nlanguage-specific nuances and mental health data coverage can affect the\naccuracy of the models. Through comprehensive error analysis, we emphasize the\nrisks of relying exclusively on LLMs in medical settings (e.g., their potential\nto contribute to misdiagnoses). Moreover, our proposed approach offers\nsignificant cost savings for multilingual tasks, presenting a major advantage\nfor broad-scale implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being integrated into various\nmedical fields, including mental health support systems. However, there is a\ngap in research regarding the effectiveness of LLMs in non-English mental\nhealth support applications. To address this problem, we present a novel\nmultilingual adaptation of widely-used mental health datasets, translated from\nEnglish into six languages (e.g., Greek, Turkish, French, Portuguese, German,\nand Finnish). This dataset enables a comprehensive evaluation of LLM\nperformance in detecting mental health conditions and assessing their severity\nacross multiple languages. By experimenting with GPT and Llama, we observe\nconsiderable variability in performance across languages, despite being\nevaluated on the same translated dataset. This inconsistency underscores the\ncomplexities inherent in multilingual mental health support, where\nlanguage-specific nuances and mental health data coverage can affect the\naccuracy of the models. Through comprehensive error analysis, we emphasize the\nrisks of relying exclusively on LLMs in medical settings (e.g., their potential\nto contribute to misdiagnoses). Moreover, our proposed approach offers\nsignificant cost savings for multilingual tasks, presenting a major advantage\nfor broad-scale implementation."
                },
                "authors": [
                    {
                        "name": "Konstantinos Skianis"
                    },
                    {
                        "name": "John Pavlopoulos"
                    },
                    {
                        "name": "A. Seza Doğruöz"
                    }
                ],
                "author_detail": {
                    "name": "A. Seza Doğruöz"
                },
                "author": "A. Seza Doğruöz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17964v2",
                "updated": "2025-03-21T09:48:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    48,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2025-01-29T19:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    19,
                    57,
                    1,
                    2,
                    29,
                    0
                ],
                "title": "Packet Level Resilience for the User Plane in 5G Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Packet Level Resilience for the User Plane in 5G Networks"
                },
                "summary": "The growing demands of ultra-reliable and low-latency communication (URLLC)\nin 5G networks necessitate enhanced resilience mechanisms to address user plane\nfailures caused by outages, hardware defects, or software bugs. An important\naspect for achieving ultra-reliable communication is the redundant transmission\nof packets, as also highlighted in 3GPP Release 18. This paper explores\nleveraging the Packet Replication, Elimination, and Ordering Function (PREOF)\nto achieve 1+1 path protection within private 5G environments. By extending\nexisting 5G components with mechanisms for packet level redundancy and\noffloading the reordering mechanism to external servers, the proposed approach\nensures minimal packet loss in case of a failure. A conceptual integration of\nredundant paths and programmable elements is presented, with considerations for\ndeployment in existing 5G infrastructures and the trade-offs of latency versus\nenhanced traffic engineering. Future work aims to evaluate practical\nimplementations using an open source 5G core, P4-based hardware and offloading\ntechnologies like DPDK and eBPF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demands of ultra-reliable and low-latency communication (URLLC)\nin 5G networks necessitate enhanced resilience mechanisms to address user plane\nfailures caused by outages, hardware defects, or software bugs. An important\naspect for achieving ultra-reliable communication is the redundant transmission\nof packets, as also highlighted in 3GPP Release 18. This paper explores\nleveraging the Packet Replication, Elimination, and Ordering Function (PREOF)\nto achieve 1+1 path protection within private 5G environments. By extending\nexisting 5G components with mechanisms for packet level redundancy and\noffloading the reordering mechanism to external servers, the proposed approach\nensures minimal packet loss in case of a failure. A conceptual integration of\nredundant paths and programmable elements is presented, with considerations for\ndeployment in existing 5G infrastructures and the trade-offs of latency versus\nenhanced traffic engineering. Future work aims to evaluate practical\nimplementations using an open source 5G core, P4-based hardware and offloading\ntechnologies like DPDK and eBPF."
                },
                "authors": [
                    {
                        "name": "Fabian Ihle"
                    },
                    {
                        "name": "Tobias Meuser"
                    },
                    {
                        "name": "Michael Menth"
                    },
                    {
                        "name": "Björn Scheuermann"
                    }
                ],
                "author_detail": {
                    "name": "Björn Scheuermann"
                },
                "author": "Björn Scheuermann",
                "arxiv_comment": "Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16980v1",
                "updated": "2025-03-21T09:46:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    46,
                    31,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:46:31Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    46,
                    31,
                    4,
                    80,
                    0
                ],
                "title": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation\n  for Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Dynamics: Towards Efficient and Dynamic Video Token Representation\n  for Video Large Language Models"
                },
                "summary": "Token-based video representation has emerged as a promising approach for\nenabling large language models to interpret video content. However, existing\ntoken reduction techniques, such as token pruning and token merging, often\ndisrupt essential spatial-temporal positional embeddings, failing to adequately\nbalance computational efficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their applicability in\nscenarios requiring extreme token compression, such as video large language\nmodels. In this paper, we introduce the novel task of extreme short token\nreduction, aiming to represent extensive video sequences with a minimal number\nof tokens. To address this challenge, we propose Token Dynamics, a new video\nrepresentation framework that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle video representations\nby separating visual embeddings from grid-level motion information, structuring\nthem into: 1. a concise token base, created by clustering tokens that describe\nobject-level content; 2. a token dynamics map, capturing detailed\nspatial-temporal motion patterns across grids. Furthermore, we introduce a\ncross-dynamics attention mechanism that integrates motion features into the\ntoken base without increasing token length, thereby maintaining compactness and\nspatial-temporal integrity. The experiments demonstrate a reduction of token\ncount to merely 0.07% of the original tokens, with only a minor performance\ndrop of 1.13%. Additionally, we propose two novel subtasks within extreme token\nreduction (fixed-length and adaptive-length compression), both effectively\nrepresenting long token sequences for video-language tasks. Our method offers\nsignificantly lower theoretical complexity, fewer tokens, and enhanced\nthroughput, thus providing an efficient solution for video LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-based video representation has emerged as a promising approach for\nenabling large language models to interpret video content. However, existing\ntoken reduction techniques, such as token pruning and token merging, often\ndisrupt essential spatial-temporal positional embeddings, failing to adequately\nbalance computational efficiency with fewer tokens. Consequently, these methods\nresult in relatively lengthy token sequences, limiting their applicability in\nscenarios requiring extreme token compression, such as video large language\nmodels. In this paper, we introduce the novel task of extreme short token\nreduction, aiming to represent extensive video sequences with a minimal number\nof tokens. To address this challenge, we propose Token Dynamics, a new video\nrepresentation framework that dynamically reduces token count while preserving\nspatial-temporal coherence. Specifically, we disentangle video representations\nby separating visual embeddings from grid-level motion information, structuring\nthem into: 1. a concise token base, created by clustering tokens that describe\nobject-level content; 2. a token dynamics map, capturing detailed\nspatial-temporal motion patterns across grids. Furthermore, we introduce a\ncross-dynamics attention mechanism that integrates motion features into the\ntoken base without increasing token length, thereby maintaining compactness and\nspatial-temporal integrity. The experiments demonstrate a reduction of token\ncount to merely 0.07% of the original tokens, with only a minor performance\ndrop of 1.13%. Additionally, we propose two novel subtasks within extreme token\nreduction (fixed-length and adaptive-length compression), both effectively\nrepresenting long token sequences for video-language tasks. Our method offers\nsignificantly lower theoretical complexity, fewer tokens, and enhanced\nthroughput, thus providing an efficient solution for video LLMs."
                },
                "authors": [
                    {
                        "name": "Haichao Zhang"
                    },
                    {
                        "name": "Zhuowei Li"
                    },
                    {
                        "name": "Dimitris Metaxas"
                    },
                    {
                        "name": "Yun Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yun Fu"
                },
                "author": "Yun Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16978v1",
                "updated": "2025-03-21T09:45:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    45,
                    59,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:45:59Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    45,
                    59,
                    4,
                    80,
                    0
                ],
                "title": "Real-Time Diffusion Policies for Games: Enhancing Consistency Policies\n  with Q-Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Diffusion Policies for Games: Enhancing Consistency Policies\n  with Q-Ensembles"
                },
                "summary": "Diffusion models have shown impressive performance in capturing complex and\nmulti-modal action distributions for game agents, but their slow inference\nspeed prevents practical deployment in real-time game environments. While\nconsistency models offer a promising approach for one-step generation, they\noften suffer from training instability and performance degradation when applied\nto policy learning. In this paper, we present CPQE (Consistency Policy with\nQ-Ensembles), which combines consistency models with Q-ensembles to address\nthese challenges.CPQE leverages uncertainty estimation through Q-ensembles to\nprovide more reliable value function approximations, resulting in better\ntraining stability and improved performance compared to classic double\nQ-network methods. Our extensive experiments across multiple game scenarios\ndemonstrate that CPQE achieves inference speeds of up to 60 Hz -- a significant\nimprovement over state-of-the-art diffusion policies that operate at only 20 Hz\n-- while maintaining comparable performance to multi-step diffusion approaches.\nCPQE consistently outperforms state-of-the-art consistency model approaches,\nshowing both higher rewards and enhanced training stability throughout the\nlearning process. These results indicate that CPQE offers a practical solution\nfor deploying diffusion-based policies in games and other real-time\napplications where both multi-modal behavior modeling and rapid inference are\ncritical requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown impressive performance in capturing complex and\nmulti-modal action distributions for game agents, but their slow inference\nspeed prevents practical deployment in real-time game environments. While\nconsistency models offer a promising approach for one-step generation, they\noften suffer from training instability and performance degradation when applied\nto policy learning. In this paper, we present CPQE (Consistency Policy with\nQ-Ensembles), which combines consistency models with Q-ensembles to address\nthese challenges.CPQE leverages uncertainty estimation through Q-ensembles to\nprovide more reliable value function approximations, resulting in better\ntraining stability and improved performance compared to classic double\nQ-network methods. Our extensive experiments across multiple game scenarios\ndemonstrate that CPQE achieves inference speeds of up to 60 Hz -- a significant\nimprovement over state-of-the-art diffusion policies that operate at only 20 Hz\n-- while maintaining comparable performance to multi-step diffusion approaches.\nCPQE consistently outperforms state-of-the-art consistency model approaches,\nshowing both higher rewards and enhanced training stability throughout the\nlearning process. These results indicate that CPQE offers a practical solution\nfor deploying diffusion-based policies in games and other real-time\napplications where both multi-modal behavior modeling and rapid inference are\ncritical requirements."
                },
                "authors": [
                    {
                        "name": "Ruoqi Zhang"
                    },
                    {
                        "name": "Ziwei Luo"
                    },
                    {
                        "name": "Jens Sjölund"
                    },
                    {
                        "name": "Per Mattsson"
                    },
                    {
                        "name": "Linus Gisslén"
                    },
                    {
                        "name": "Alessandro Sestini"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Sestini"
                },
                "author": "Alessandro Sestini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16975v1",
                "updated": "2025-03-21T09:43:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    43,
                    42,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:43:42Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    43,
                    42,
                    4,
                    80,
                    0
                ],
                "title": "EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and\n  Generalized Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and\n  Generalized Vision"
                },
                "summary": "Deep neural networks (DNNs) has shown great promise in computer vision tasks.\nHowever, machine vision achieved by DNNs cannot be as robust as human\nperception. Adversarial attacks and data distribution shifts have been known as\ntwo major scenarios which degrade machine performance and obstacle the wide\ndeployment of machines \"in the wild\". In order to break these obstructions and\nfacilitate the research of model robustness, we develop EasyRobust, a\ncomprehensive and easy-to-use toolkit for training, evaluation and analysis of\nrobust vision models. EasyRobust targets at two types of robustness: 1)\nAdversarial robustness enables the model to defense against malicious inputs\ncrafted by worst-case perturbations, also known as adversarial examples; 2)\nNon-adversarial robustness enhances the model performance on natural test\nimages with corruptions or distribution shifts. Thorough benchmarks on image\nclassification enable EasyRobust to provide an accurate robustness evaluation\non vision models. We wish our EasyRobust can help for training\npractically-robust models and promote academic and industrial progress in\nclosing the gap between human and machine vision. Codes and models of\nEasyRobust have been open-sourced in https://github.com/alibaba/easyrobust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) has shown great promise in computer vision tasks.\nHowever, machine vision achieved by DNNs cannot be as robust as human\nperception. Adversarial attacks and data distribution shifts have been known as\ntwo major scenarios which degrade machine performance and obstacle the wide\ndeployment of machines \"in the wild\". In order to break these obstructions and\nfacilitate the research of model robustness, we develop EasyRobust, a\ncomprehensive and easy-to-use toolkit for training, evaluation and analysis of\nrobust vision models. EasyRobust targets at two types of robustness: 1)\nAdversarial robustness enables the model to defense against malicious inputs\ncrafted by worst-case perturbations, also known as adversarial examples; 2)\nNon-adversarial robustness enhances the model performance on natural test\nimages with corruptions or distribution shifts. Thorough benchmarks on image\nclassification enable EasyRobust to provide an accurate robustness evaluation\non vision models. We wish our EasyRobust can help for training\npractically-robust models and promote academic and industrial progress in\nclosing the gap between human and machine vision. Codes and models of\nEasyRobust have been open-sourced in https://github.com/alibaba/easyrobust."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Yuefeng Chen"
                    },
                    {
                        "name": "Rong Zhang"
                    },
                    {
                        "name": "Hui Xue"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Hang Su"
                    }
                ],
                "author_detail": {
                    "name": "Hang Su"
                },
                "author": "Hang Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16974v1",
                "updated": "2025-03-21T09:43:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    43,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:43:37Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    43,
                    37,
                    4,
                    80,
                    0
                ],
                "title": "Assessing Consistency and Reproducibility in the Outputs of Large\n  Language Models: Evidence Across Diverse Finance and Accounting Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Consistency and Reproducibility in the Outputs of Large\n  Language Models: Evidence Across Diverse Finance and Accounting Tasks"
                },
                "summary": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. Simulation analysis reveals that despite measurable\ninconsistency in LLM outputs, downstream statistical inferences remain\nremarkably robust. These findings address concerns about what we term\n\"G-hacking,\" the selective reporting of favorable outcomes from multiple\nGenerative AI runs, by demonstrating that such risks are relatively low for\nfinance and accounting tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. Simulation analysis reveals that despite measurable\ninconsistency in LLM outputs, downstream statistical inferences remain\nremarkably robust. These findings address concerns about what we term\n\"G-hacking,\" the selective reporting of favorable outcomes from multiple\nGenerative AI runs, by demonstrating that such risks are relatively low for\nfinance and accounting tasks."
                },
                "authors": [
                    {
                        "name": "Julian Junyan Wang"
                    },
                    {
                        "name": "Victor Xiaoqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Victor Xiaoqi Wang"
                },
                "author": "Victor Xiaoqi Wang",
                "arxiv_comment": "96 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16972v1",
                "updated": "2025-03-21T09:41:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    41,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:41:12Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    41,
                    12,
                    4,
                    80,
                    0
                ],
                "title": "Governance of Ledger-Anchored Decentralized Identifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Governance of Ledger-Anchored Decentralized Identifiers"
                },
                "summary": "A Decentralized Identifier (DID) empowers an entity to prove control over a\nunique and self-issued identifier without relying on any identity provider. The\npublic key material for the proof is encoded into an associated DID document\n(DDO). This is preferable shared via a distributed ledger because it guarantees\nalgorithmically that everyone has access to the latest state of any\ntamper-proof DDO but only the entities in control of a DID are able to update\ntheirs. Yet, it is possible to grant deputies the authority to update the DDO\non behalf of the DID owner. However, the DID specification leaves largely open\non how authorizations over a DDO are managed and enforced among multiple\ndeputies. This article investigates what it means to govern a DID and discusses\nvarious forms of how a DID can be controlled by potentially more than one\nentity. It also presents a prototype of a DID-conform identifier management\nsystem where a selected set of governance policies are deployed as Smart\nContracts. The article highlights the critical role of governance for the\ntrustworthy and flexible deployment of ledger-anchored DIDs across various\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Decentralized Identifier (DID) empowers an entity to prove control over a\nunique and self-issued identifier without relying on any identity provider. The\npublic key material for the proof is encoded into an associated DID document\n(DDO). This is preferable shared via a distributed ledger because it guarantees\nalgorithmically that everyone has access to the latest state of any\ntamper-proof DDO but only the entities in control of a DID are able to update\ntheirs. Yet, it is possible to grant deputies the authority to update the DDO\non behalf of the DID owner. However, the DID specification leaves largely open\non how authorizations over a DDO are managed and enforced among multiple\ndeputies. This article investigates what it means to govern a DID and discusses\nvarious forms of how a DID can be controlled by potentially more than one\nentity. It also presents a prototype of a DID-conform identifier management\nsystem where a selected set of governance policies are deployed as Smart\nContracts. The article highlights the critical role of governance for the\ntrustworthy and flexible deployment of ledger-anchored DIDs across various\ndomains."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Carlo Segat"
                    },
                    {
                        "name": "Axel Küpper"
                    }
                ],
                "author_detail": {
                    "name": "Axel Küpper"
                },
                "author": "Axel Küpper",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12478v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12478v3",
                "updated": "2025-03-21T09:32:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    32,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T02:29:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    29,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on\n  Tibetan Script",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on\n  Tibetan Script"
                },
                "summary": "DNN-based language models perform excellently on various tasks, but even SOTA\nLLMs are susceptible to textual adversarial attacks. Adversarial texts play\ncrucial roles in multiple subfields of NLP. However, current research has the\nfollowing issues. (1) Most textual adversarial attack methods target\nrich-resourced languages. How do we generate adversarial texts for less-studied\nlanguages? (2) Most textual adversarial attack methods are prone to generating\ninvalid or ambiguous adversarial texts. How do we construct high-quality\nadversarial robustness benchmarks? (3) New language models may be immune to\npart of previously generated adversarial texts. How do we update adversarial\nrobustness benchmarks? To address the above issues, we introduce HITL-GAT, a\nsystem based on a general approach to human-in-the-loop generation of\nadversarial texts. HITL-GAT contains four stages in one pipeline: victim model\nconstruction, adversarial example generation, high-quality benchmark\nconstruction, and adversarial robustness evaluation. Additionally, we utilize\nHITL-GAT to make a case study on Tibetan script which can be a reference for\nthe adversarial research of other less-studied languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNN-based language models perform excellently on various tasks, but even SOTA\nLLMs are susceptible to textual adversarial attacks. Adversarial texts play\ncrucial roles in multiple subfields of NLP. However, current research has the\nfollowing issues. (1) Most textual adversarial attack methods target\nrich-resourced languages. How do we generate adversarial texts for less-studied\nlanguages? (2) Most textual adversarial attack methods are prone to generating\ninvalid or ambiguous adversarial texts. How do we construct high-quality\nadversarial robustness benchmarks? (3) New language models may be immune to\npart of previously generated adversarial texts. How do we update adversarial\nrobustness benchmarks? To address the above issues, we introduce HITL-GAT, a\nsystem based on a general approach to human-in-the-loop generation of\nadversarial texts. HITL-GAT contains four stages in one pipeline: victim model\nconstruction, adversarial example generation, high-quality benchmark\nconstruction, and adversarial robustness evaluation. Additionally, we utilize\nHITL-GAT to make a case study on Tibetan script which can be a reference for\nthe adversarial research of other less-studied languages."
                },
                "authors": [
                    {
                        "name": "Xi Cao"
                    },
                    {
                        "name": "Yuan Sun"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Quzong Gesang"
                    },
                    {
                        "name": "Nuo Qun"
                    },
                    {
                        "name": "Tashi Nyima"
                    }
                ],
                "author_detail": {
                    "name": "Tashi Nyima"
                },
                "author": "Tashi Nyima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12478v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12478v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16965v1",
                "updated": "2025-03-21T09:25:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    25,
                    23,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T09:25:23Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    25,
                    23,
                    4,
                    80,
                    0
                ],
                "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making"
                },
                "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms."
                },
                "authors": [
                    {
                        "name": "Zhe Hu"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Yu Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yu Yin"
                },
                "author": "Yu Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07871v2",
                "updated": "2025-03-21T09:17:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    17,
                    5,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-12T09:28:34Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    28,
                    34,
                    3,
                    256,
                    0
                ],
                "title": "Objection Overruled! Lay People can Distinguish Large Language Models\n  from Lawyers, but still Favour Advice from an LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objection Overruled! Lay People can Distinguish Large Language Models\n  from Lawyers, but still Favour Advice from an LLM"
                },
                "summary": "Large Language Models (LLMs) are seemingly infiltrating every domain, and the\nlegal context is no exception. In this paper, we present the results of three\nexperiments (total N = 288) that investigated lay people's willingness to act\nupon, and their ability to discriminate between, LLM- and lawyer-generated\nlegal advice. In Experiment 1, participants judged their willingness to act on\nlegal advice when the source of the advice was either known or unknown. When\nthe advice source was unknown, participants indicated that they were\nsignificantly more willing to act on the LLM-generated advice. The result of\nthe source unknown condition was replicated in Experiment 2. Intriguingly,\ndespite participants indicating higher willingness to act on LLM-generated\nadvice in Experiments 1 and 2, participants discriminated between the LLM- and\nlawyer-generated texts significantly above chance-level in Experiment 3.\nLastly, we discuss potential explanations and risks of our findings,\nlimitations and future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are seemingly infiltrating every domain, and the\nlegal context is no exception. In this paper, we present the results of three\nexperiments (total N = 288) that investigated lay people's willingness to act\nupon, and their ability to discriminate between, LLM- and lawyer-generated\nlegal advice. In Experiment 1, participants judged their willingness to act on\nlegal advice when the source of the advice was either known or unknown. When\nthe advice source was unknown, participants indicated that they were\nsignificantly more willing to act on the LLM-generated advice. The result of\nthe source unknown condition was replicated in Experiment 2. Intriguingly,\ndespite participants indicating higher willingness to act on LLM-generated\nadvice in Experiments 1 and 2, participants discriminated between the LLM- and\nlawyer-generated texts significantly above chance-level in Experiment 3.\nLastly, we discuss potential explanations and risks of our findings,\nlimitations and future work."
                },
                "authors": [
                    {
                        "name": "Eike Schneiders"
                    },
                    {
                        "name": "Tina Seabrooke"
                    },
                    {
                        "name": "Joshua Krook"
                    },
                    {
                        "name": "Richard Hyde"
                    },
                    {
                        "name": "Natalie Leesakul"
                    },
                    {
                        "name": "Jeremie Clos"
                    },
                    {
                        "name": "Joel Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Joel Fischer"
                },
                "author": "Joel Fischer",
                "arxiv_doi": "10.1145/3706598.3713470",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713470",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACMConference on Human Factors in Computing Systems (CHI'25)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11068v2",
                "updated": "2025-03-21T08:25:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    25,
                    11,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-14T04:23:59Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    23,
                    59,
                    4,
                    73,
                    0
                ],
                "title": "DeepSeek Powered Solid Dosage Formulation Design and Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek Powered Solid Dosage Formulation Design and Development"
                },
                "summary": "Pharmaceutical process design and development for generic, innovative, or\npersonalized drugs have always been a time-consuming, costly, rigorous process,\nthat involves multi-stage evaluation for better quality control and assurance.\nLarge language models (LLMs), a type of generative artificial intelligence\nsystem, can augment laboratory research in the pharmaceutical engineering\nprocess by helping scientists to extract knowledge from literature, design\nparameters, and collect and interpret experimental data ultimately accelerating\nscientific discovery. LLMs with prompt engineering technologies change the\nresearchers thinking protocol from traditional empirical knowledge to\nstreamlined thinking that connects the performance and structured parameters\ntogether. In this work, we investigate and evaluate how prompt engineering\ntechnologies can enhance the drug design process from different strategies such\nas zero-shot, few-shot, chain-of-thought, etc. The dissolution profile for\nspecific drugs is predicted and suggested from the LLMs model. Furthermore, the\nfundamental physical properties such as PSD, aspect ratio, and specific surface\narea could be inversely designed from the LLMs model. Finally, all the results\nare evaluated and validated by real-world cases to prove the reliability of\nprompt engineering techniques. Initial evaluations show an MSE of 23.61 and R2\nof 0.97 in zero-shot, an MSE of 114.89 and R2 of 0.90 in zero-shot-CoT, an MSE\nof 57.0 and R2 of 0.92 in few-shot, a MSE of 22.56 and R2 of 0.97 in\nfew-shot-CoT and a MSE of 10.56 and R2 of 0.99 with the involvement of RAG.\nThis work breaks down any barriers in developing a systematic framework where\nLLMs assist in formulation design, process control, and decision-making.\nFinally, we conclude the work by discussing open challenges and future research\ndirections in pharmaceutical processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pharmaceutical process design and development for generic, innovative, or\npersonalized drugs have always been a time-consuming, costly, rigorous process,\nthat involves multi-stage evaluation for better quality control and assurance.\nLarge language models (LLMs), a type of generative artificial intelligence\nsystem, can augment laboratory research in the pharmaceutical engineering\nprocess by helping scientists to extract knowledge from literature, design\nparameters, and collect and interpret experimental data ultimately accelerating\nscientific discovery. LLMs with prompt engineering technologies change the\nresearchers thinking protocol from traditional empirical knowledge to\nstreamlined thinking that connects the performance and structured parameters\ntogether. In this work, we investigate and evaluate how prompt engineering\ntechnologies can enhance the drug design process from different strategies such\nas zero-shot, few-shot, chain-of-thought, etc. The dissolution profile for\nspecific drugs is predicted and suggested from the LLMs model. Furthermore, the\nfundamental physical properties such as PSD, aspect ratio, and specific surface\narea could be inversely designed from the LLMs model. Finally, all the results\nare evaluated and validated by real-world cases to prove the reliability of\nprompt engineering techniques. Initial evaluations show an MSE of 23.61 and R2\nof 0.97 in zero-shot, an MSE of 114.89 and R2 of 0.90 in zero-shot-CoT, an MSE\nof 57.0 and R2 of 0.92 in few-shot, a MSE of 22.56 and R2 of 0.97 in\nfew-shot-CoT and a MSE of 10.56 and R2 of 0.99 with the involvement of RAG.\nThis work breaks down any barriers in developing a systematic framework where\nLLMs assist in formulation design, process control, and decision-making.\nFinally, we conclude the work by discussing open challenges and future research\ndirections in pharmaceutical processes."
                },
                "authors": [
                    {
                        "name": "Leqi Lin"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Kaiyuan Yang"
                    },
                    {
                        "name": "Xizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xizhong Chen"
                },
                "author": "Xizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12923v2",
                "updated": "2025-03-21T08:23:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    23,
                    56,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-18T15:03:17Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    3,
                    17,
                    1,
                    49,
                    0
                ],
                "title": "On-Device LLMs for Home Assistant: Dual Role in Intent Detection and\n  Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device LLMs for Home Assistant: Dual Role in Intent Detection and\n  Response Generation"
                },
                "summary": "This paper investigates whether Large Language Models (LLMs), fine-tuned on\nsynthetic but domain-representative data, can perform the twofold task of (i)\nslot and intent detection and (ii) natural language response generation for a\nsmart home assistant, while running solely on resource-limited, CPU-only edge\nhardware. We fine-tune LLMs to produce both JSON action calls and text\nresponses. Our experiments show that 16-bit and 8-bit quantized variants\npreserve high accuracy on slot and intent detection and maintain strong\nsemantic coherence in generated text, while the 4-bit model, while retaining\ngenerative fluency, suffers a noticeable drop in device-service classification\naccuracy. Further evaluations on noisy human (non-synthetic) prompts and\nout-of-domain intents confirm the models' generalization ability, obtaining\naround 80--86\\% accuracy. While the average inference time is 5--6 seconds per\nquery -- acceptable for one-shot commands but suboptimal for multi-turn\ndialogue -- our results affirm that an on-device LLM can effectively unify\ncommand interpretation and flexible response generation for home automation\nwithout relying on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates whether Large Language Models (LLMs), fine-tuned on\nsynthetic but domain-representative data, can perform the twofold task of (i)\nslot and intent detection and (ii) natural language response generation for a\nsmart home assistant, while running solely on resource-limited, CPU-only edge\nhardware. We fine-tune LLMs to produce both JSON action calls and text\nresponses. Our experiments show that 16-bit and 8-bit quantized variants\npreserve high accuracy on slot and intent detection and maintain strong\nsemantic coherence in generated text, while the 4-bit model, while retaining\ngenerative fluency, suffers a noticeable drop in device-service classification\naccuracy. Further evaluations on noisy human (non-synthetic) prompts and\nout-of-domain intents confirm the models' generalization ability, obtaining\naround 80--86\\% accuracy. While the average inference time is 5--6 seconds per\nquery -- acceptable for one-shot commands but suboptimal for multi-turn\ndialogue -- our results affirm that an on-device LLM can effectively unify\ncommand interpretation and flexible response generation for home automation\nwithout relying on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Rune Birkmose"
                    },
                    {
                        "name": "Nathan Mørkeberg Reece"
                    },
                    {
                        "name": "Esben Hofstedt Norvin"
                    },
                    {
                        "name": "Johannes Bjerva"
                    },
                    {
                        "name": "Mike Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zhang"
                },
                "author": "Mike Zhang",
                "arxiv_comment": "WNUT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07115v2",
                "updated": "2025-03-21T08:12:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    12,
                    7,
                    4,
                    80,
                    0
                ],
                "published": "2024-06-11T10:00:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    10,
                    0,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "Advancing Tool-Augmented Large Language Models: Integrating Insights\n  from Errors in Inference Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Tool-Augmented Large Language Models: Integrating Insights\n  from Errors in Inference Trees"
                },
                "summary": "Tool-augmented large language models (LLMs) leverage tools, often in the form\nof APIs, to improve their reasoning capabilities on complex tasks. This enables\nthem to act as intelligent agents interacting with the real world. The recently\nintroduced ToolLLaMA model by Qin et al. [2023] utilizes the depth-first\nsearch-based decision tree (DFSDT) mechanism for multi-step reasoning with\n$16000+$ real-world APIs, effectively enhancing the performance of\ntool-augmented LLMs compared to traditional chain reasoning mechanisms.\nHowever, their approach only employs successful paths from decision trees (also\ncalled inference trees) for supervised fine-tuning (SFT), missing out on the\npotential learning opportunities from failed paths. Inspired by this, we\npropose an inference trajectory optimization framework based on preference\nlearning to address this limitation. We first introduce a novel method for\nconstructing step-wise preference data from tree-like expert trajectories,\nwhich leverages the previously ignored failed explorations in the decision\ntrees. In the subsequent training phase, we first fine-tune the LLM with\nsuccessful tool-usage expert trajectories and then apply direct preference\noptimization (DPO) with the preference data to update the LLM's policy,\nresulting in our ToolPrefer-LLaMA (TP-LLaMA) model. This approach not only\nenhances the utilization of original expert data but also broadens the learning\nspace of the model. Our experiments demonstrate that by obtaining insights from\nerrors in inference trees, TP-LLaMA significantly outperforms the baselines\nacross almost all test scenarios by a large margin and exhibits better\ngeneralization capabilities with unseen APIs. At the same time, TP-LLaMA has\nalso demonstrated superior reasoning efficiency compared to the baselines,\nmaking it more suitable for complex tool-usage reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented large language models (LLMs) leverage tools, often in the form\nof APIs, to improve their reasoning capabilities on complex tasks. This enables\nthem to act as intelligent agents interacting with the real world. The recently\nintroduced ToolLLaMA model by Qin et al. [2023] utilizes the depth-first\nsearch-based decision tree (DFSDT) mechanism for multi-step reasoning with\n$16000+$ real-world APIs, effectively enhancing the performance of\ntool-augmented LLMs compared to traditional chain reasoning mechanisms.\nHowever, their approach only employs successful paths from decision trees (also\ncalled inference trees) for supervised fine-tuning (SFT), missing out on the\npotential learning opportunities from failed paths. Inspired by this, we\npropose an inference trajectory optimization framework based on preference\nlearning to address this limitation. We first introduce a novel method for\nconstructing step-wise preference data from tree-like expert trajectories,\nwhich leverages the previously ignored failed explorations in the decision\ntrees. In the subsequent training phase, we first fine-tune the LLM with\nsuccessful tool-usage expert trajectories and then apply direct preference\noptimization (DPO) with the preference data to update the LLM's policy,\nresulting in our ToolPrefer-LLaMA (TP-LLaMA) model. This approach not only\nenhances the utilization of original expert data but also broadens the learning\nspace of the model. Our experiments demonstrate that by obtaining insights from\nerrors in inference trees, TP-LLaMA significantly outperforms the baselines\nacross almost all test scenarios by a large margin and exhibits better\ngeneralization capabilities with unseen APIs. At the same time, TP-LLaMA has\nalso demonstrated superior reasoning efficiency compared to the baselines,\nmaking it more suitable for complex tool-usage reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Sijia Chen"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Yi-Feng Wu"
                    },
                    {
                        "name": "Qing-Guo Chen"
                    },
                    {
                        "name": "Zhao Xu"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    },
                    {
                        "name": "Lijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Zhang"
                },
                "author": "Lijun Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16931v1",
                "updated": "2025-03-21T08:08:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    8,
                    44,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T08:08:44Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    8,
                    44,
                    4,
                    80,
                    0
                ],
                "title": "Efficient Deployment of Deep MIMO Detection Using Learngene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Deployment of Deep MIMO Detection Using Learngene"
                },
                "summary": "Deep learning (DL) has introduced a new paradigm in multiple-input\nmultiple-output (MIMO) detection, balancing performance and complexity.\nHowever, the practical deployment of DL-based detectors is hindered by poor\ngeneralization, necessitating costly retraining for different devices and\nscenarios. To address this challenge, this paper presents a novel knowledge\ntransfer technique, termed learngene, for the design of a DL-based MIMO\ndetector and proposes an efficient deployment framework. The proposed detector,\nSDNet, leverages zero-forcing detection outputs and least squares-estimated\nchannel state information (CSI) as inputs. It is further optimized through a\ncollective-individual paradigm to enhance knowledge transfer. In this paradigm,\nlearngene, a reusable neural network (NN) segment, encapsulates detection\nmeta-knowledge acquired from large-scale collective models trained by\nmanufacturers. This segment can then be distributed to device-specific teams.\nBy integrating learngene into different lightweight individual models,\ndetection meta-knowledge is efficiently transferred across heterogeneous NNs,\nenabling adaptation to diverse devices and scenarios. Simulation results\ndemonstrate that the proposed scheme enhances performance, enables rapid\nadaptation, and ensures high scalability, with transferred parameters\ncomprising only 10.8% of the total model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) has introduced a new paradigm in multiple-input\nmultiple-output (MIMO) detection, balancing performance and complexity.\nHowever, the practical deployment of DL-based detectors is hindered by poor\ngeneralization, necessitating costly retraining for different devices and\nscenarios. To address this challenge, this paper presents a novel knowledge\ntransfer technique, termed learngene, for the design of a DL-based MIMO\ndetector and proposes an efficient deployment framework. The proposed detector,\nSDNet, leverages zero-forcing detection outputs and least squares-estimated\nchannel state information (CSI) as inputs. It is further optimized through a\ncollective-individual paradigm to enhance knowledge transfer. In this paradigm,\nlearngene, a reusable neural network (NN) segment, encapsulates detection\nmeta-knowledge acquired from large-scale collective models trained by\nmanufacturers. This segment can then be distributed to device-specific teams.\nBy integrating learngene into different lightweight individual models,\ndetection meta-knowledge is efficiently transferred across heterogeneous NNs,\nenabling adaptation to diverse devices and scenarios. Simulation results\ndemonstrate that the proposed scheme enhances performance, enables rapid\nadaptation, and ensures high scalability, with transferred parameters\ncomprising only 10.8% of the total model size."
                },
                "authors": [
                    {
                        "name": "Jinya Zhang"
                    },
                    {
                        "name": "Jiajia Guo"
                    },
                    {
                        "name": "Xiangyi Li"
                    },
                    {
                        "name": "Chao-Kai Wen"
                    },
                    {
                        "name": "Xin Geng"
                    },
                    {
                        "name": "Shi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shi Jin"
                },
                "author": "Shi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16929v1",
                "updated": "2025-03-21T08:00:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    0,
                    29,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T08:00:29Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    8,
                    0,
                    29,
                    4,
                    80,
                    0
                ],
                "title": "TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty\n  Scheduling and Pre-SFT Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty\n  Scheduling and Pre-SFT Alignment"
                },
                "summary": "Video Large Language Models (Video LLMs) have achieved significant success by\nleveraging a two-stage paradigm: pretraining on large-scale video-text data for\nvision-language alignment, followed by supervised fine-tuning (SFT) for\ntask-specific capabilities. However, existing approaches struggle with temporal\nreasoning due to weak temporal correspondence in the data and reliance on the\nnext-token prediction paradigm during training. To address these limitations,\nwe propose TEMPO (TEMporal Preference Optimization), a systematic framework\nthat enhances Video LLMs' temporal reasoning capabilities through Direct\nPreference Optimization (DPO). To facilitate this, we introduce an automated\npreference data generation pipeline that systematically constructs preference\npairs by selecting videos that are rich in temporal information, designing\nvideo-specific perturbation strategies, and finally evaluating model responses\non clean and perturbed video inputs. Our temporal alignment features two key\ninnovations: curriculum learning which that progressively increases\nperturbation difficulty to improve model robustness and adaptability; and\n``Pre-SFT Alignment'', applying preference optimization before instruction\ntuning to prioritize fine-grained temporal comprehension. Extensive experiments\ndemonstrate that our approach consistently improves Video LLM performance\nacross multiple benchmarks with a relatively small set of self-generated DPO\ndata. We further analyze the transferability of DPO data across architectures\nand the role of difficulty scheduling in optimization. Our findings highlight\nour TEMPO as a scalable and efficient complement to SFT-based methods, paving\nthe way for developing reliable Video LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Video LLMs) have achieved significant success by\nleveraging a two-stage paradigm: pretraining on large-scale video-text data for\nvision-language alignment, followed by supervised fine-tuning (SFT) for\ntask-specific capabilities. However, existing approaches struggle with temporal\nreasoning due to weak temporal correspondence in the data and reliance on the\nnext-token prediction paradigm during training. To address these limitations,\nwe propose TEMPO (TEMporal Preference Optimization), a systematic framework\nthat enhances Video LLMs' temporal reasoning capabilities through Direct\nPreference Optimization (DPO). To facilitate this, we introduce an automated\npreference data generation pipeline that systematically constructs preference\npairs by selecting videos that are rich in temporal information, designing\nvideo-specific perturbation strategies, and finally evaluating model responses\non clean and perturbed video inputs. Our temporal alignment features two key\ninnovations: curriculum learning which that progressively increases\nperturbation difficulty to improve model robustness and adaptability; and\n``Pre-SFT Alignment'', applying preference optimization before instruction\ntuning to prioritize fine-grained temporal comprehension. Extensive experiments\ndemonstrate that our approach consistently improves Video LLM performance\nacross multiple benchmarks with a relatively small set of self-generated DPO\ndata. We further analyze the transferability of DPO data across architectures\nand the role of difficulty scheduling in optimization. Our findings highlight\nour TEMPO as a scalable and efficient complement to SFT-based methods, paving\nthe way for developing reliable Video LLMs."
                },
                "authors": [
                    {
                        "name": "Shicheng Li"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Kun Ouyang"
                    },
                    {
                        "name": "Shuhuai Ren"
                    },
                    {
                        "name": "Yuanxin Liu"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Xu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xu Sun"
                },
                "author": "Xu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12785v2",
                "updated": "2025-03-21T07:53:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    7,
                    53,
                    51,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T10:44:47Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    10,
                    44,
                    47,
                    1,
                    352,
                    0
                ],
                "title": "Activating Distributed Visual Region within LLMs for Efficient and\n  Effective Vision-Language Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activating Distributed Visual Region within LLMs for Efficient and\n  Effective Vision-Language Training and Inference"
                },
                "summary": "Large Vision-Language Models (LVLMs) typically learn visual capacity through\nvisual instruction tuning, involving updates to both a projector and their LLM\nbackbones. Inspired by the concept of a visual region in the human brain, we\ninvestigate the existence of an analogous \\textit{visual region} within LLMs\nthat functions as a cognitive core, and explore the potential of efficient\ntraining of LVLMs via selective layers tuning. Using Bunny-Llama-3-8B-V for\ndetailed analysis and other three LVLMs for validation across diverse visual\nand textual tasks, we find that selectively updating 25\\% of LLMs layers, when\nsparsely and uniformly distributed, can preserve nearly 99\\% of visual\nperformance and maintain or improve textual task results, while effectively\nreducing training time. Based on this targeted training approach, we further\npropose a novel visual region-based pruning paradigm, removing non-critical\nlayers outside the visual region, which can achieve minimal performance loss.\nThis study offers an effective and efficient strategy for LVLM training and\ninference by activating a layer-wise visual region within LLMs, which proves\nconsistently effective across different models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) typically learn visual capacity through\nvisual instruction tuning, involving updates to both a projector and their LLM\nbackbones. Inspired by the concept of a visual region in the human brain, we\ninvestigate the existence of an analogous \\textit{visual region} within LLMs\nthat functions as a cognitive core, and explore the potential of efficient\ntraining of LVLMs via selective layers tuning. Using Bunny-Llama-3-8B-V for\ndetailed analysis and other three LVLMs for validation across diverse visual\nand textual tasks, we find that selectively updating 25\\% of LLMs layers, when\nsparsely and uniformly distributed, can preserve nearly 99\\% of visual\nperformance and maintain or improve textual task results, while effectively\nreducing training time. Based on this targeted training approach, we further\npropose a novel visual region-based pruning paradigm, removing non-critical\nlayers outside the visual region, which can achieve minimal performance loss.\nThis study offers an effective and efficient strategy for LVLM training and\ninference by activating a layer-wise visual region within LLMs, which proves\nconsistently effective across different models."
                },
                "authors": [
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Dianyi Wang"
                    },
                    {
                        "name": "Chengxing Zhou"
                    },
                    {
                        "name": "Zejun Li"
                    },
                    {
                        "name": "Zhihao Fan"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13879v2",
                "updated": "2025-03-21T07:36:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    7,
                    36,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-18T04:13:11Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    13,
                    11,
                    1,
                    77,
                    0
                ],
                "title": "Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review\n  Generation via Cognitive Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review\n  Generation via Cognitive Alignment"
                },
                "summary": "The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of scholarly submissions has overwhelmed traditional peer\nreview systems, driving the need for intelligent automation to preserve\nscientific rigor. While large language models (LLMs) show promise in automating\nmanuscript critiques, their ability to synthesize high-stakes meta-reviews,\nwhich require conflict-aware reasoning and consensus derivation, remains\nunderdeveloped. Existing methods fail to effectively handle conflicting\nviewpoints within differing opinions, and often introduce additional cognitive\nbiases, such as anchoring effects and conformity bias.To overcome these\nlimitations, we propose the Cognitive Alignment Framework (CAF), a dual-process\narchitecture that transforms LLMs into adaptive scientific arbitrators. By\noperationalizing Kahneman's dual-process theory, CAF introduces a three-step\ncognitive pipeline: review initialization, incremental integration, and\ncognitive alignment.Empirical validation shows that CAF outperforms existing\nLLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and\ncontent consistency improving by as much as 12.95\\%."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Han Ding"
                    },
                    {
                        "name": "Meng Yuan"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Deqing Wang"
                    },
                    {
                        "name": "Fuzhen Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Fuzhen Zhuang"
                },
                "author": "Fuzhen Zhuang",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16922v1",
                "updated": "2025-03-21T07:33:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    7,
                    33,
                    59,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T07:33:59Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    7,
                    33,
                    59,
                    4,
                    80,
                    0
                ],
                "title": "RustEvo^2: An Evolving Benchmark for API Evolution in LLM-based Rust\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RustEvo^2: An Evolving Benchmark for API Evolution in LLM-based Rust\n  Code Generation"
                },
                "summary": "Large Language Models (LLMs) have become pivotal tools for automating code\ngeneration in software development. However, these models face significant\nchallenges in producing version-aware code for rapidly evolving languages like\nRust, where frequent Application Programming Interfaces (API) changes across\nversions lead to compatibility issues and correctness errors. Existing\nbenchmarks lack systematic evaluation of how models navigate API transitions,\nrelying on labor-intensive manual curation and offering limited\nversion-specific insights. To address this gap, we present RustEvo, a novel\nframework for constructing dynamic benchmarks that evaluate the ability of LLMs\nto adapt to evolving Rust APIs. RustEvo automates dataset creation by\nsynthesizing 588 API changes (380 from Rust standard libraries, 208 from 15\nthird-party crates) into programming tasks mirroring real-world challenges.\nThese tasks cover four API evolution categories: Stabilizations, Signature\nChanges, Behavioral Changes, and Deprecations, reflecting their actual\ndistribution in the Rust ecosystem.\n  Experiments on state-of-the-art (SOTA) LLMs reveal significant performance\nvariations: models achieve a 65.8% average success rate on stabilized APIs but\nonly 38.0% on behavioral changes, highlighting difficulties in detecting\nsemantic shifts without signature alterations. Knowledge cutoff dates strongly\ninfluence performance, with models scoring 56.1% on before-cutoff APIs versus\n32.5% on after-cutoff tasks. Retrieval-Augmented Generation (RAG) mitigates\nthis gap, improving success rates by 13.5% on average for APIs released after\nmodel training. Our findings underscore the necessity of our evolution-aware\nbenchmarks to advance the adaptability of LLMs in fast-paced software\necosystems. The framework and the benchmarks are publicly released at\nhttps://github.com/SYSUSELab/RustEvo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become pivotal tools for automating code\ngeneration in software development. However, these models face significant\nchallenges in producing version-aware code for rapidly evolving languages like\nRust, where frequent Application Programming Interfaces (API) changes across\nversions lead to compatibility issues and correctness errors. Existing\nbenchmarks lack systematic evaluation of how models navigate API transitions,\nrelying on labor-intensive manual curation and offering limited\nversion-specific insights. To address this gap, we present RustEvo, a novel\nframework for constructing dynamic benchmarks that evaluate the ability of LLMs\nto adapt to evolving Rust APIs. RustEvo automates dataset creation by\nsynthesizing 588 API changes (380 from Rust standard libraries, 208 from 15\nthird-party crates) into programming tasks mirroring real-world challenges.\nThese tasks cover four API evolution categories: Stabilizations, Signature\nChanges, Behavioral Changes, and Deprecations, reflecting their actual\ndistribution in the Rust ecosystem.\n  Experiments on state-of-the-art (SOTA) LLMs reveal significant performance\nvariations: models achieve a 65.8% average success rate on stabilized APIs but\nonly 38.0% on behavioral changes, highlighting difficulties in detecting\nsemantic shifts without signature alterations. Knowledge cutoff dates strongly\ninfluence performance, with models scoring 56.1% on before-cutoff APIs versus\n32.5% on after-cutoff tasks. Retrieval-Augmented Generation (RAG) mitigates\nthis gap, improving success rates by 13.5% on average for APIs released after\nmodel training. Our findings underscore the necessity of our evolution-aware\nbenchmarks to advance the adaptability of LLMs in fast-paced software\necosystems. The framework and the benchmarks are publicly released at\nhttps://github.com/SYSUSELab/RustEvo."
                },
                "authors": [
                    {
                        "name": "Linxi Liang"
                    },
                    {
                        "name": "Jing Gong"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16913v1",
                "updated": "2025-03-21T07:23:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    7,
                    23,
                    26,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T07:23:26Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    7,
                    23,
                    26,
                    4,
                    80,
                    0
                ],
                "title": "FAIT: Fault-Aware Fine-Tuning for Better Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAIT: Fault-Aware Fine-Tuning for Better Code Generation"
                },
                "summary": "Modern instruction-tuned large language models (LLMs) have made remarkable\nprogress in code generation. However, these LLMs fine-tuned with standard\nsupervised fine-tuning (SFT) sometimes generate plausible-looking but\nfunctionally incorrect code variants. This issue likely stems from the\nlimitation of standard SFT, which treats all tokens equally during optimization\nand fails to emphasize the error-sensitive segments-specific code differences\nbetween correct implementations and similar incorrect variants. To address this\nproblem, we propose Fault-Aware Fine-Tuning (FAIT), a novel fine-tuning\ntechnique that enhances LLMs' code generation by (1) extracting\nmulti-granularity (line/token-level) differences between correct and incorrect\nyet similar implementations to identify error-sensitive segments, and (2)\ndynamically prioritizing those segments during training via dynamic loss\nweighting. Through extensive experiments on seven LLMs across three widely-used\nbenchmarks, our method achieves an average relative improvement of 6.9% on\npass@1 with just one epoch of training, with some enhanced 6.7B LLMs\noutperforming closed-source models, e.g., GPT-3.5-Turbo. Furthermore, our\nfine-tuning technique demonstrates strong generalization with performance\nimprovements ranging from 3.8% to 19.1% across diverse instruction-tuned LLMs,\nand our ablation studies confirm the contributions of different granularities\nof differences and loss function components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern instruction-tuned large language models (LLMs) have made remarkable\nprogress in code generation. However, these LLMs fine-tuned with standard\nsupervised fine-tuning (SFT) sometimes generate plausible-looking but\nfunctionally incorrect code variants. This issue likely stems from the\nlimitation of standard SFT, which treats all tokens equally during optimization\nand fails to emphasize the error-sensitive segments-specific code differences\nbetween correct implementations and similar incorrect variants. To address this\nproblem, we propose Fault-Aware Fine-Tuning (FAIT), a novel fine-tuning\ntechnique that enhances LLMs' code generation by (1) extracting\nmulti-granularity (line/token-level) differences between correct and incorrect\nyet similar implementations to identify error-sensitive segments, and (2)\ndynamically prioritizing those segments during training via dynamic loss\nweighting. Through extensive experiments on seven LLMs across three widely-used\nbenchmarks, our method achieves an average relative improvement of 6.9% on\npass@1 with just one epoch of training, with some enhanced 6.7B LLMs\noutperforming closed-source models, e.g., GPT-3.5-Turbo. Furthermore, our\nfine-tuning technique demonstrates strong generalization with performance\nimprovements ranging from 3.8% to 19.1% across diverse instruction-tuned LLMs,\nand our ablation studies confirm the contributions of different granularities\nof differences and loss function components."
                },
                "authors": [
                    {
                        "name": "Lishui Fan"
                    },
                    {
                        "name": "Zhongxin Liu"
                    },
                    {
                        "name": "Haoye Wang"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Shanping Li"
                    }
                ],
                "author_detail": {
                    "name": "Shanping Li"
                },
                "author": "Shanping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16893v1",
                "updated": "2025-03-21T06:56:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    6,
                    56,
                    35,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T06:56:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    6,
                    56,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Improving the End-to-End Efficiency of Offline Inference for Multi-LLM\n  Applications Based on Sampling and Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the End-to-End Efficiency of Offline Inference for Multi-LLM\n  Applications Based on Sampling and Simulation"
                },
                "summary": "As large language models (LLMs) have shown great success in many tasks, they\nare used in various applications. While a lot of works have focused on the\nefficiency of single-LLM application (e.g., offloading, request scheduling,\nparallelism strategy selection), multi-LLM applications receive less attention,\nparticularly in offline inference scenarios. In this work, we aim to improve\nthe offline end-to-end inference efficiency of multi-LLM applications in the\nsingle-node multi-GPU environment. The problem involves two key decisions: (1)\ndetermining which LLMs to run concurrently each time (we may not run all the\nmodels at the same time), and (2) selecting a parallelism strategy to use for\neach LLM. This problem is NP-hard. Naive solutions may not work well because\nthe running time for a model to complete a set of requests depends on the\nrequest workload and the selected parallelism strategy, and they lack an\naccurate model of the running time. As the LLM output lengths are unknown\nbefore running, to estimate the model running time, we propose a\nsampling-then-simulation method which first estimates the output lengths by\nsampling from an empirical cumulative function we obtained from a large dataset\nin advance, and then simulates the LLM inference process accordingly. Based on\nthe simulation, we estimate the per-iteration latencys to get the total\nlatency. A greedy method is proposed to optimize the scheduling of the LLMs in\nthe application across the GPUs. We then propose a framework SamuLLM which\ncontains two phases: planning, which calls the greedy method for an application\nand running, which runs the application and dynamically adjust the model\nscheduling based on the runtime information. Experiments on 3 applications and\na mixed application show that SamuLLM can achieve 1.0-2.4$\\times$ end-to-end\nspeedups compared to the competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) have shown great success in many tasks, they\nare used in various applications. While a lot of works have focused on the\nefficiency of single-LLM application (e.g., offloading, request scheduling,\nparallelism strategy selection), multi-LLM applications receive less attention,\nparticularly in offline inference scenarios. In this work, we aim to improve\nthe offline end-to-end inference efficiency of multi-LLM applications in the\nsingle-node multi-GPU environment. The problem involves two key decisions: (1)\ndetermining which LLMs to run concurrently each time (we may not run all the\nmodels at the same time), and (2) selecting a parallelism strategy to use for\neach LLM. This problem is NP-hard. Naive solutions may not work well because\nthe running time for a model to complete a set of requests depends on the\nrequest workload and the selected parallelism strategy, and they lack an\naccurate model of the running time. As the LLM output lengths are unknown\nbefore running, to estimate the model running time, we propose a\nsampling-then-simulation method which first estimates the output lengths by\nsampling from an empirical cumulative function we obtained from a large dataset\nin advance, and then simulates the LLM inference process accordingly. Based on\nthe simulation, we estimate the per-iteration latencys to get the total\nlatency. A greedy method is proposed to optimize the scheduling of the LLMs in\nthe application across the GPUs. We then propose a framework SamuLLM which\ncontains two phases: planning, which calls the greedy method for an application\nand running, which runs the application and dynamically adjust the model\nscheduling based on the runtime information. Experiments on 3 applications and\na mixed application show that SamuLLM can achieve 1.0-2.4$\\times$ end-to-end\nspeedups compared to the competitors."
                },
                "authors": [
                    {
                        "name": "Jingzhi Fang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01827v3",
                "updated": "2025-03-21T06:52:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    6,
                    52,
                    25,
                    4,
                    80,
                    0
                ],
                "published": "2024-03-04T08:22:29Z",
                "published_parsed": [
                    2024,
                    3,
                    4,
                    8,
                    22,
                    29,
                    0,
                    64,
                    0
                ],
                "title": "Analysis and Fully Memristor-based Reservoir Computing for Temporal Data\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Fully Memristor-based Reservoir Computing for Temporal Data\n  Classification"
                },
                "summary": "Reservoir computing (RC) offers a neuromorphic framework that is particularly\neffective for processing spatiotemporal signals. Known for its temporal\nprocessing prowess, RC significantly lowers training costs compared to\nconventional recurrent neural networks. A key component in its hardware\ndeployment is the ability to generate dynamic reservoir states. Our research\nintroduces a novel dual-memory RC system, integrating a short-term memory via a\nWOx-based memristor, capable of achieving 16 distinct states encoded over 4\nbits, and a long-term memory component using a TiOx-based memristor within the\nreadout layer. We thoroughly examine both memristor types and leverage the RC\nsystem to process temporal data sets. The performance of the proposed RC system\nis validated through two benchmark tasks: isolated spoken digit recognition\nwith incomplete inputs and Mackey-Glass time series prediction. The system\ndelivered an impressive 98.84% accuracy in digit recognition and sustained a\nlow normalized root mean square error (NRMSE) of 0.036 in the time series\nprediction task, underscoring its capability. This study illuminates the\nadeptness of memristor-based RC systems in managing intricate temporal\nchallenges, laying the groundwork for further innovations in neuromorphic\ncomputing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reservoir computing (RC) offers a neuromorphic framework that is particularly\neffective for processing spatiotemporal signals. Known for its temporal\nprocessing prowess, RC significantly lowers training costs compared to\nconventional recurrent neural networks. A key component in its hardware\ndeployment is the ability to generate dynamic reservoir states. Our research\nintroduces a novel dual-memory RC system, integrating a short-term memory via a\nWOx-based memristor, capable of achieving 16 distinct states encoded over 4\nbits, and a long-term memory component using a TiOx-based memristor within the\nreadout layer. We thoroughly examine both memristor types and leverage the RC\nsystem to process temporal data sets. The performance of the proposed RC system\nis validated through two benchmark tasks: isolated spoken digit recognition\nwith incomplete inputs and Mackey-Glass time series prediction. The system\ndelivered an impressive 98.84% accuracy in digit recognition and sustained a\nlow normalized root mean square error (NRMSE) of 0.036 in the time series\nprediction task, underscoring its capability. This study illuminates the\nadeptness of memristor-based RC systems in managing intricate temporal\nchallenges, laying the groundwork for further innovations in neuromorphic\ncomputing."
                },
                "authors": [
                    {
                        "name": "Ankur Singh"
                    },
                    {
                        "name": "Sanghyeon Choi"
                    },
                    {
                        "name": "Gunuk Wang"
                    },
                    {
                        "name": "Maryaradhiya Daimari"
                    },
                    {
                        "name": "Byung-Geun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Geun Lee"
                },
                "author": "Byung-Geun Lee",
                "arxiv_doi": "10.1016/j.neunet.2024.106925",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.neunet.2024.106925",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.01827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, 20 figures, Journal, Typo corrected and updated reference",
                "arxiv_journal_ref": "Neural Networks, 2024",
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16454v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16454v3",
                "updated": "2025-03-21T06:37:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    6,
                    37,
                    37,
                    4,
                    80,
                    0
                ],
                "published": "2024-10-21T19:28:37Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    19,
                    28,
                    37,
                    0,
                    295,
                    0
                ],
                "title": "Catastrophic Failure of LLM Unlearning via Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catastrophic Failure of LLM Unlearning via Quantization"
                },
                "summary": "Large language models (LLMs) have shown remarkable proficiency in generating\ntext, benefiting from extensive training on vast textual corpora. However, LLMs\nmay also acquire unwanted behaviors from the diverse and sensitive nature of\ntheir training data, which can include copyrighted and private content. Machine\nunlearning has been introduced as a viable solution to remove the influence of\nsuch problematic content without the need for costly and time-consuming\nretraining. This process aims to erase specific knowledge from LLMs while\npreserving as much model utility as possible. Despite the effectiveness of\ncurrent unlearning methods, little attention has been given to whether existing\nunlearning methods for LLMs truly achieve forgetting or merely hide the\nknowledge, which current unlearning benchmarks fail to detect. This paper\nreveals that applying quantization to models that have undergone unlearning can\nrestore the \"forgotten\" information. To thoroughly evaluate this phenomenon, we\nconduct comprehensive experiments using various quantization techniques across\nmultiple precision levels. We find that for unlearning methods with utility\nconstraints, the unlearned model retains an average of 21\\% of the intended\nforgotten knowledge in full precision, which significantly increases to 83\\%\nafter 4-bit quantization. ... Our code is available at:\n\\href{https://github.com/zzwjames/FailureLLMUnlearning}{https://github.com/zzwjames/FailureLLMUnlearning}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable proficiency in generating\ntext, benefiting from extensive training on vast textual corpora. However, LLMs\nmay also acquire unwanted behaviors from the diverse and sensitive nature of\ntheir training data, which can include copyrighted and private content. Machine\nunlearning has been introduced as a viable solution to remove the influence of\nsuch problematic content without the need for costly and time-consuming\nretraining. This process aims to erase specific knowledge from LLMs while\npreserving as much model utility as possible. Despite the effectiveness of\ncurrent unlearning methods, little attention has been given to whether existing\nunlearning methods for LLMs truly achieve forgetting or merely hide the\nknowledge, which current unlearning benchmarks fail to detect. This paper\nreveals that applying quantization to models that have undergone unlearning can\nrestore the \"forgotten\" information. To thoroughly evaluate this phenomenon, we\nconduct comprehensive experiments using various quantization techniques across\nmultiple precision levels. We find that for unlearning methods with utility\nconstraints, the unlearned model retains an average of 21\\% of the intended\nforgotten knowledge in full precision, which significantly increases to 83\\%\nafter 4-bit quantization. ... Our code is available at:\n\\href{https://github.com/zzwjames/FailureLLMUnlearning}{https://github.com/zzwjames/FailureLLMUnlearning}."
                },
                "authors": [
                    {
                        "name": "Zhiwei Zhang"
                    },
                    {
                        "name": "Fali Wang"
                    },
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Zongyu Wu"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Suhang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Suhang Wang"
                },
                "author": "Suhang Wang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16454v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16454v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14345v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14345v4",
                "updated": "2025-03-21T06:36:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    6,
                    36,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2023-09-03T07:14:49Z",
                "published_parsed": [
                    2023,
                    9,
                    3,
                    7,
                    14,
                    49,
                    6,
                    246,
                    0
                ],
                "title": "Bias Testing and Mitigation in LLM-based Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Testing and Mitigation in LLM-based Code Generation"
                },
                "summary": "As the adoption of LLMs becomes more widespread in software coding\necosystems, a pressing issue has emerged: does the generated code contain\nsocial bias and unfairness, such as those related to age, gender, and race?\nThis issue concerns the integrity, fairness, and ethical foundation of software\napplications that depend on the code generated by these models but are\nunderexplored in the literature. This paper presents a novel bias testing\nframework that is specifically designed for code generation tasks. Based on\nthis framework, we conduct an extensive empirical study on the biases in code\ngenerated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison,\nClaude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal\nthat biases are prevalent. For example, 13.47% to 49.10% of the codes generated\nby these LLMs have biased behaviors towards gender. Moreover, we study five\nbias mitigation prompt strategies that are commonly used in current code\ngeneration scenarios, i.e., zero-shot, one-shot, few-shot, and two\nChain-of-Thought (CoT) prompts, with and without provided feedback-driven\nrefinement. Our evaluation results illustrate that using direct prompt\nengineering strategies has limited effectiveness in mitigating bias, but our\ntest execution feedback can help to reduce the ratio of code biases to a large\nextent (e.g., from 59.88% to 4.79% for GPT-4).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the adoption of LLMs becomes more widespread in software coding\necosystems, a pressing issue has emerged: does the generated code contain\nsocial bias and unfairness, such as those related to age, gender, and race?\nThis issue concerns the integrity, fairness, and ethical foundation of software\napplications that depend on the code generated by these models but are\nunderexplored in the literature. This paper presents a novel bias testing\nframework that is specifically designed for code generation tasks. Based on\nthis framework, we conduct an extensive empirical study on the biases in code\ngenerated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison,\nClaude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal\nthat biases are prevalent. For example, 13.47% to 49.10% of the codes generated\nby these LLMs have biased behaviors towards gender. Moreover, we study five\nbias mitigation prompt strategies that are commonly used in current code\ngeneration scenarios, i.e., zero-shot, one-shot, few-shot, and two\nChain-of-Thought (CoT) prompts, with and without provided feedback-driven\nrefinement. Our evaluation results illustrate that using direct prompt\nengineering strategies has limited effectiveness in mitigating bias, but our\ntest execution feedback can help to reduce the ratio of code biases to a large\nextent (e.g., from 59.88% to 4.79% for GPT-4)."
                },
                "authors": [
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Heming Cui"
                    }
                ],
                "author_detail": {
                    "name": "Heming Cui"
                },
                "author": "Heming Cui",
                "arxiv_comment": "Accepted by TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14345v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14345v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16883v1",
                "updated": "2025-03-21T06:35:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    6,
                    35,
                    49,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T06:35:49Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    6,
                    35,
                    49,
                    4,
                    80,
                    0
                ],
                "title": "Assessing the Reliability and Validity of GPT-4 in Annotating Emotion\n  Appraisal Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Reliability and Validity of GPT-4 in Annotating Emotion\n  Appraisal Ratings"
                },
                "summary": "Appraisal theories suggest that emotions arise from subjective evaluations of\nevents, referred to as appraisals. The taxonomy of appraisals is quite diverse,\nand they are usually given ratings on a Likert scale to be annotated in an\nexperiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as\na reader-annotator of 21 specific appraisal ratings in different prompt\nsettings, aiming to evaluate and improve its performance compared to human\nannotators. We found that GPT-4 is an effective reader-annotator that performs\nclose to or even slightly better than human annotators, and its results can be\nsignificantly improved by using a majority voting of five completions. GPT-4\nalso effectively predicts appraisal ratings and emotion labels using a single\nprompt, but adding instruction complexity results in poorer performance. We\nalso found that longer event descriptions lead to more accurate annotations for\nboth model and human annotator ratings. This work contributes to the growing\nusage of LLMs in psychology and the strategies for improving GPT-4 performance\nin annotating appraisals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Appraisal theories suggest that emotions arise from subjective evaluations of\nevents, referred to as appraisals. The taxonomy of appraisals is quite diverse,\nand they are usually given ratings on a Likert scale to be annotated in an\nexperiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as\na reader-annotator of 21 specific appraisal ratings in different prompt\nsettings, aiming to evaluate and improve its performance compared to human\nannotators. We found that GPT-4 is an effective reader-annotator that performs\nclose to or even slightly better than human annotators, and its results can be\nsignificantly improved by using a majority voting of five completions. GPT-4\nalso effectively predicts appraisal ratings and emotion labels using a single\nprompt, but adding instruction complexity results in poorer performance. We\nalso found that longer event descriptions lead to more accurate annotations for\nboth model and human annotator ratings. This work contributes to the growing\nusage of LLMs in psychology and the strategies for improving GPT-4 performance\nin annotating appraisals."
                },
                "authors": [
                    {
                        "name": "Deniss Ruder"
                    },
                    {
                        "name": "Andero Uusberg"
                    },
                    {
                        "name": "Kairit Sirts"
                    }
                ],
                "author_detail": {
                    "name": "Kairit Sirts"
                },
                "author": "Kairit Sirts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19509v2",
                "updated": "2025-03-21T06:01:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    6,
                    1,
                    23,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-27T07:55:36Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    55,
                    36,
                    4,
                    362,
                    0
                ],
                "title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) have enabled a variety of real-world\napplications. The large parameter size of VLMs brings large memory and\ncomputation overhead which poses significant challenges for deployment.\nPost-Training Quantization (PTQ) is an effective technique to reduce the memory\nand computation overhead. Existing PTQ methods mainly focus on large language\nmodels (LLMs), without considering the differences across other modalities. In\nthis paper, we discover that there is a significant difference in sensitivity\nbetween language and vision tokens in large VLMs. Therefore, treating tokens\nfrom different modalities equally, as in existing PTQ methods, may\nover-emphasize the insensitive modalities, leading to significant accuracy\nloss. To deal with the above issue, we propose a simple yet effective method,\nModality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ\nincorporates the different sensitivities across modalities during the\ncalibration process to minimize the reconstruction loss for better quantization\nparameters. Extensive experiments show that MBQ can significantly improve task\naccuracy by up to 4.4% and 11.6% under W3 and W4A8 quantization for 7B to 70B\nVLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel\nthat fuses the dequantization and GEMV operators, achieving a 1.4x speedup on\nLLaVA-onevision-7B on the RTX 4090. The code is available at\nhttps://github.com/thu-nics/MBQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have enabled a variety of real-world\napplications. The large parameter size of VLMs brings large memory and\ncomputation overhead which poses significant challenges for deployment.\nPost-Training Quantization (PTQ) is an effective technique to reduce the memory\nand computation overhead. Existing PTQ methods mainly focus on large language\nmodels (LLMs), without considering the differences across other modalities. In\nthis paper, we discover that there is a significant difference in sensitivity\nbetween language and vision tokens in large VLMs. Therefore, treating tokens\nfrom different modalities equally, as in existing PTQ methods, may\nover-emphasize the insensitive modalities, leading to significant accuracy\nloss. To deal with the above issue, we propose a simple yet effective method,\nModality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ\nincorporates the different sensitivities across modalities during the\ncalibration process to minimize the reconstruction loss for better quantization\nparameters. Extensive experiments show that MBQ can significantly improve task\naccuracy by up to 4.4% and 11.6% under W3 and W4A8 quantization for 7B to 70B\nVLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel\nthat fuses the dequantization and GEMV operators, achieving a 1.4x speedup on\nLLaVA-onevision-7B on the RTX 4090. The code is available at\nhttps://github.com/thu-nics/MBQ."
                },
                "authors": [
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Yingchun Hu"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiaotao Jia"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yaqi Yan"
                    },
                    {
                        "name": "Pei Ran"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16867v1",
                "updated": "2025-03-21T05:52:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    52,
                    50,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:52:50Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    52,
                    50,
                    4,
                    80,
                    0
                ],
                "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering"
                },
                "summary": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation."
                },
                "authors": [
                    {
                        "name": "Kaisi Guan"
                    },
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Yuchong Sun"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Ruihua Song"
                    }
                ],
                "author_detail": {
                    "name": "Ruihua Song"
                },
                "author": "Ruihua Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16861v1",
                "updated": "2025-03-21T05:09:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    9,
                    46,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:09:46Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    9,
                    46,
                    4,
                    80,
                    0
                ],
                "title": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw\n  Disclosure for General-Purpose AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw\n  Disclosure for General-Purpose AI"
                },
                "summary": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of general-purpose AI (GPAI) systems introduces\nsignificant new risks. Yet the infrastructure, practices, and norms for\nreporting flaws in GPAI systems remain seriously underdeveloped, lagging far\nbehind more established fields like software security. Based on a collaboration\nbetween experts from the fields of software security, machine learning, law,\nsocial science, and policy, we identify key gaps in the evaluation and\nreporting of flaws in GPAI systems. We call for three interventions to advance\nsystem safety. First, we propose using standardized AI flaw reports and rules\nof engagement for researchers in order to ease the process of submitting,\nreproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system\nproviders adopt broadly-scoped flaw disclosure programs, borrowing from bug\nbounties, with legal safe harbors to protect researchers. Third, we advocate\nfor the development of improved infrastructure to coordinate distribution of\nflaw reports across the many stakeholders who may be impacted. These\ninterventions are increasingly urgent, as evidenced by the prevalence of\njailbreaks and other flaws that can transfer across different providers' GPAI\nsystems. By promoting robust reporting and coordination in the AI ecosystem,\nthese proposals could significantly improve the safety, security, and\naccountability of GPAI systems."
                },
                "authors": [
                    {
                        "name": "Shayne Longpre"
                    },
                    {
                        "name": "Kevin Klyman"
                    },
                    {
                        "name": "Ruth E. Appel"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Michelle Sahar"
                    },
                    {
                        "name": "Sean McGregor"
                    },
                    {
                        "name": "Avijit Ghosh"
                    },
                    {
                        "name": "Borhane Blili-Hamelin"
                    },
                    {
                        "name": "Nathan Butters"
                    },
                    {
                        "name": "Alondra Nelson"
                    },
                    {
                        "name": "Amit Elazari"
                    },
                    {
                        "name": "Andrew Sellars"
                    },
                    {
                        "name": "Casey John Ellis"
                    },
                    {
                        "name": "Dane Sherrets"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Harley Geiger"
                    },
                    {
                        "name": "Ilona Cohen"
                    },
                    {
                        "name": "Lauren McIlvenny"
                    },
                    {
                        "name": "Madhulika Srikumar"
                    },
                    {
                        "name": "Mark M. Jaycox"
                    },
                    {
                        "name": "Markus Anderljung"
                    },
                    {
                        "name": "Nadine Farid Johnson"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Nicolas Miailhe"
                    },
                    {
                        "name": "Nik Marda"
                    },
                    {
                        "name": "Peter Henderson"
                    },
                    {
                        "name": "Rebecca S. Portnoff"
                    },
                    {
                        "name": "Rebecca Weiss"
                    },
                    {
                        "name": "Victoria Westerhoff"
                    },
                    {
                        "name": "Yacine Jernite"
                    },
                    {
                        "name": "Rumman Chowdhury"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Arvind Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Narayanan"
                },
                "author": "Arvind Narayanan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16858v1",
                "updated": "2025-03-21T05:04:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    4,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:04:53Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    4,
                    53,
                    4,
                    80,
                    0
                ],
                "title": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and\n  Question Answering"
                },
                "summary": "Understanding the relationship between textual news and time-series evolution\nis a critical yet under-explored challenge in applied data science. While\nmultimodal learning has gained traction, existing multimodal time-series\ndatasets fall short in evaluating cross-modal reasoning and complex question\nanswering, which are essential for capturing complex interactions between\nnarrative information and temporal patterns. To bridge this gap, we introduce\nMultimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to\nevaluate large language models (LLMs) on time series and text understanding\nacross financial and weather domains. MTbench comprises paired time series and\ntextual data, including financial news with corresponding stock price movements\nand weather reports aligned with historical temperature records. Unlike\nexisting benchmarks that focus on isolated modalities, MTbench provides a\ncomprehensive testbed for models to jointly reason over structured numerical\ntrends and unstructured textual narratives. The richness of MTbench enables\nformulation of diverse tasks that require a deep understanding of both text and\ntime-series data, including time-series forecasting, semantic and technical\ntrend analysis, and news-driven question answering (QA). These tasks target the\nmodel's ability to capture temporal dependencies, extract key insights from\ntextual context, and integrate cross-modal information. We evaluate\nstate-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the\ncomplex relationships between news narratives and temporal patterns. Our\nfindings reveal significant challenges in current models, including\ndifficulties in capturing long-term dependencies, interpreting causality in\nfinancial and weather trends, and effectively fusing multimodal information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the relationship between textual news and time-series evolution\nis a critical yet under-explored challenge in applied data science. While\nmultimodal learning has gained traction, existing multimodal time-series\ndatasets fall short in evaluating cross-modal reasoning and complex question\nanswering, which are essential for capturing complex interactions between\nnarrative information and temporal patterns. To bridge this gap, we introduce\nMultimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to\nevaluate large language models (LLMs) on time series and text understanding\nacross financial and weather domains. MTbench comprises paired time series and\ntextual data, including financial news with corresponding stock price movements\nand weather reports aligned with historical temperature records. Unlike\nexisting benchmarks that focus on isolated modalities, MTbench provides a\ncomprehensive testbed for models to jointly reason over structured numerical\ntrends and unstructured textual narratives. The richness of MTbench enables\nformulation of diverse tasks that require a deep understanding of both text and\ntime-series data, including time-series forecasting, semantic and technical\ntrend analysis, and news-driven question answering (QA). These tasks target the\nmodel's ability to capture temporal dependencies, extract key insights from\ntextual context, and integrate cross-modal information. We evaluate\nstate-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the\ncomplex relationships between news narratives and temporal patterns. Our\nfindings reveal significant challenges in current models, including\ndifficulties in capturing long-term dependencies, interpreting causality in\nfinancial and weather trends, and effectively fusing multimodal information."
                },
                "authors": [
                    {
                        "name": "Jialin Chen"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Juan Garza"
                    },
                    {
                        "name": "Gaukhar Nurbek"
                    },
                    {
                        "name": "Cheng Qin"
                    },
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09606v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09606v3",
                "updated": "2025-03-21T04:57:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    4,
                    57,
                    45,
                    4,
                    80,
                    0
                ],
                "published": "2024-03-14T17:47:20Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    47,
                    20,
                    3,
                    74,
                    0
                ],
                "title": "Large Language Models and Causal Inference in Collaboration: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Causal Inference in Collaboration: A Survey"
                },
                "summary": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Paiheng Xu"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Jiaxin Yuan"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Fuxiao Liu"
                    },
                    {
                        "name": "Tianrui Guan"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Wei Ai"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_comment": "Findings of the Association for Computational Linguistics: NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09606v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09606v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16851v1",
                "updated": "2025-03-21T04:50:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    4,
                    50,
                    25,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T04:50:25Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    4,
                    50,
                    25,
                    4,
                    80,
                    0
                ],
                "title": "Towards LLM Guardrails via Sparse Representation Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLM Guardrails via Sparse Representation Steering"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nnatural language generation tasks, yet their uncontrolled outputs pose\nsignificant ethical and safety risks. Recently, representation engineering\nmethods have shown promising results in steering model behavior by modifying\nthe rich semantic information encoded in activation vectors. However, due to\nthe difficulty of precisely disentangling semantic directions within\nhigh-dimensional representation space, existing approaches suffer from three\nmajor limitations: lack of fine-grained control, quality degradation of\ngenerated content, and poor interpretability. To address these challenges, we\npropose a sparse encoding-based representation engineering method, named SRE,\nwhich decomposes polysemantic activations into a structured, monosemantic\nfeature space. By leveraging sparse autoencoding, our approach isolates and\nadjusts only task-specific sparse feature dimensions, enabling precise and\ninterpretable steering of model behavior while preserving content quality. We\nvalidate our method on three critical domains, i.e., safety, fairness, and\ntruthfulness using the open-source LLM Gemma-2-2B-it. Experimental results show\nthat SRE achieves superior controllability while maintaining the overall\nquality of generated content (i.e., controllability and quality), demonstrating\nits effectiveness as a fine-grained and interpretable activation steering\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in\nnatural language generation tasks, yet their uncontrolled outputs pose\nsignificant ethical and safety risks. Recently, representation engineering\nmethods have shown promising results in steering model behavior by modifying\nthe rich semantic information encoded in activation vectors. However, due to\nthe difficulty of precisely disentangling semantic directions within\nhigh-dimensional representation space, existing approaches suffer from three\nmajor limitations: lack of fine-grained control, quality degradation of\ngenerated content, and poor interpretability. To address these challenges, we\npropose a sparse encoding-based representation engineering method, named SRE,\nwhich decomposes polysemantic activations into a structured, monosemantic\nfeature space. By leveraging sparse autoencoding, our approach isolates and\nadjusts only task-specific sparse feature dimensions, enabling precise and\ninterpretable steering of model behavior while preserving content quality. We\nvalidate our method on three critical domains, i.e., safety, fairness, and\ntruthfulness using the open-source LLM Gemma-2-2B-it. Experimental results show\nthat SRE achieves superior controllability while maintaining the overall\nquality of generated content (i.e., controllability and quality), demonstrating\nits effectiveness as a fine-grained and interpretable activation steering\nframework."
                },
                "authors": [
                    {
                        "name": "Zeqing He"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Huiyu Xu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16849v1",
                "updated": "2025-03-21T04:40:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    4,
                    40,
                    4,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T04:40:04Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    4,
                    40,
                    4,
                    4,
                    80,
                    0
                ],
                "title": "Safe On-Orbit Dislodging of Deployable Structures via Robust Adaptive\n  MPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe On-Orbit Dislodging of Deployable Structures via Robust Adaptive\n  MPC"
                },
                "summary": "This paper proposes a novel robust adaptive model predictive controller for\non-orbit dislodging. We consider the scenario where a servicer, equipped with a\nrobot arm, must dislodge a client, a time-varying system composed of an\nunderpowered jammed solar panel with a hybrid hinge system on a space station.\nOur approach leverages online set-membership identification to reduce the\nuncertainty to provide robust safety guarantees during dislodging despite\nbounded disturbances while balancing exploration and exploitation effectively\nin the parameter space. The feasibility of the developed robust adaptive MPC\nmethod is also examined through dislodging simulations and hardware experiments\nin zero-gravity and gravity environments, respectively. In addition, the\nadvantages of our method are shown through comparison experiments with several\nstate-of-the-art control schemes for both accuracy of parameter estimation and\ncontrol performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel robust adaptive model predictive controller for\non-orbit dislodging. We consider the scenario where a servicer, equipped with a\nrobot arm, must dislodge a client, a time-varying system composed of an\nunderpowered jammed solar panel with a hybrid hinge system on a space station.\nOur approach leverages online set-membership identification to reduce the\nuncertainty to provide robust safety guarantees during dislodging despite\nbounded disturbances while balancing exploration and exploitation effectively\nin the parameter space. The feasibility of the developed robust adaptive MPC\nmethod is also examined through dislodging simulations and hardware experiments\nin zero-gravity and gravity environments, respectively. In addition, the\nadvantages of our method are shown through comparison experiments with several\nstate-of-the-art control schemes for both accuracy of parameter estimation and\ncontrol performance."
                },
                "authors": [
                    {
                        "name": "Longsen Gao"
                    },
                    {
                        "name": "Claus Danielson"
                    },
                    {
                        "name": "Andrew Kwas"
                    },
                    {
                        "name": "Rafael Fierro"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Fierro"
                },
                "author": "Rafael Fierro",
                "arxiv_comment": "This paper has been submitted to IEEE Transactions on Control Systems\n  Technology and is being reviewed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16833v1",
                "updated": "2025-03-21T04:03:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    4,
                    3,
                    59,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T04:03:59Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    4,
                    3,
                    59,
                    4,
                    80,
                    0
                ],
                "title": "The Deployment of End-to-End Audio Language Models Should Take into\n  Account the Principle of Least Privilege",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deployment of End-to-End Audio Language Models Should Take into\n  Account the Principle of Least Privilege"
                },
                "summary": "We are at a turning point for language models that accept audio input. The\nlatest end-to-end audio language models (Audio LMs) process speech directly\ninstead of relying on a separate transcription step. This shift preserves\ndetailed information, such as intonation or the presence of multiple speakers,\nthat would otherwise be lost in transcription. However, it also introduces new\nsafety risks, including the potential misuse of speaker identity cues and other\nsensitive vocal attributes, which could have legal implications. In this\nposition paper, we urge a closer examination of how these models are built and\ndeployed. We argue that the principle of least privilege should guide decisions\non whether to deploy cascaded or end-to-end models. Specifically, evaluations\nshould assess (1) whether end-to-end modeling is necessary for a given\napplication; and (2), the appropriate scope of information access. Finally, We\nhighlight related gaps in current audio LM benchmarks and identify key open\nresearch questions, both technical and policy-related, that must be addressed\nto enable the responsible deployment of end-to-end Audio LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are at a turning point for language models that accept audio input. The\nlatest end-to-end audio language models (Audio LMs) process speech directly\ninstead of relying on a separate transcription step. This shift preserves\ndetailed information, such as intonation or the presence of multiple speakers,\nthat would otherwise be lost in transcription. However, it also introduces new\nsafety risks, including the potential misuse of speaker identity cues and other\nsensitive vocal attributes, which could have legal implications. In this\nposition paper, we urge a closer examination of how these models are built and\ndeployed. We argue that the principle of least privilege should guide decisions\non whether to deploy cascaded or end-to-end models. Specifically, evaluations\nshould assess (1) whether end-to-end modeling is necessary for a given\napplication; and (2), the appropriate scope of information access. Finally, We\nhighlight related gaps in current audio LM benchmarks and identify key open\nresearch questions, both technical and policy-related, that must be addressed\nto enable the responsible deployment of end-to-end Audio LMs."
                },
                "authors": [
                    {
                        "name": "Luxi He"
                    },
                    {
                        "name": "Xiangyu Qi"
                    },
                    {
                        "name": "Michel Liao"
                    },
                    {
                        "name": "Inyoung Cheong"
                    },
                    {
                        "name": "Prateek Mittal"
                    },
                    {
                        "name": "Danqi Chen"
                    },
                    {
                        "name": "Peter Henderson"
                    }
                ],
                "author_detail": {
                    "name": "Peter Henderson"
                },
                "author": "Peter Henderson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09396v5",
                "updated": "2025-03-21T03:42:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    3,
                    42,
                    27,
                    4,
                    80,
                    0
                ],
                "published": "2024-06-13T17:59:16Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    17,
                    59,
                    16,
                    3,
                    165,
                    0
                ],
                "title": "Too Many Frames, Not All Useful: Efficient Strategies for Long-Form\n  Video QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Too Many Frames, Not All Useful: Efficient Strategies for Long-Form\n  Video QA"
                },
                "summary": "Long-form videos that span across wide temporal intervals are highly\ninformation redundant and contain multiple distinct events or entities that are\noften loosely related. Therefore, when performing long-form video question\nanswering (LVQA), all information necessary to generate a correct response can\noften be contained within a small subset of frames. Recent literature explore\nuse of large language models (LLMs) in LVQA benchmarks, achieving exceptional\nperformance, while relying on vision language models (VLMs) to convert all\nvisual content within videos into natural language. Such VLMs often\nindependently caption a large number of frames uniformly sampled from long\nvideos, which is not efficient and can mostly be redundant. Questioning these\ndecision choices, we explore optimal strategies for key-frame selection that\ncan significantly reduce these redundancies, namely Hierarchical Keyframe\nSelector. Our proposed framework, LVNet, achieves state-of-the-art performance\nat a comparable caption scale across three benchmark LVQA datasets: EgoSchema,\nNExT-QA, and IntentQA, while also demonstrating a strong performance on videos\nup to an hour long in VideoMME. Our code will be released publicly. The code\ncan be found at https://github.com/jongwoopark7978/LVNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form videos that span across wide temporal intervals are highly\ninformation redundant and contain multiple distinct events or entities that are\noften loosely related. Therefore, when performing long-form video question\nanswering (LVQA), all information necessary to generate a correct response can\noften be contained within a small subset of frames. Recent literature explore\nuse of large language models (LLMs) in LVQA benchmarks, achieving exceptional\nperformance, while relying on vision language models (VLMs) to convert all\nvisual content within videos into natural language. Such VLMs often\nindependently caption a large number of frames uniformly sampled from long\nvideos, which is not efficient and can mostly be redundant. Questioning these\ndecision choices, we explore optimal strategies for key-frame selection that\ncan significantly reduce these redundancies, namely Hierarchical Keyframe\nSelector. Our proposed framework, LVNet, achieves state-of-the-art performance\nat a comparable caption scale across three benchmark LVQA datasets: EgoSchema,\nNExT-QA, and IntentQA, while also demonstrating a strong performance on videos\nup to an hour long in VideoMME. Our code will be released publicly. The code\ncan be found at https://github.com/jongwoopark7978/LVNet."
                },
                "authors": [
                    {
                        "name": "Jongwoo Park"
                    },
                    {
                        "name": "Kanchana Ranasinghe"
                    },
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Wonjeong Ryu"
                    },
                    {
                        "name": "Donghyun Kim"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    }
                ],
                "author_detail": {
                    "name": "Michael S. Ryoo"
                },
                "author": "Michael S. Ryoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00212v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00212v4",
                "updated": "2025-03-21T03:27:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    3,
                    27,
                    55,
                    4,
                    80,
                    0
                ],
                "published": "2025-01-31T23:01:48Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    23,
                    1,
                    48,
                    4,
                    31,
                    0
                ],
                "title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and\n  Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and\n  Proving"
                },
                "summary": "A fundamental challenge in formal theorem proving by LLMs is the lack of\nhigh-quality training data. Although reinforcement learning or expert iteration\npartially mitigates this issue by alternating between LLM generating proofs and\nfinetuning them on correctly generated ones, performance quickly plateaus due\nto the scarcity of correct proofs (sparse rewards). To keep improving the\nmodels with limited data, we draw inspiration from mathematicians, who\ncontinuously develop new results, partly by proposing novel conjectures or\nexercises (which are often variants of known results) and attempting to solve\nthem. We design the Self-play Theorem Prover (STP) that simultaneously takes on\ntwo roles, conjecturer and prover, each providing training signals to the\nother. The conjecturer is trained iteratively on previously generated\nconjectures that are barely provable by the current prover, which incentivizes\nit to generate increasingly challenging conjectures over time. The prover\nattempts to prove the conjectures with standard expert iteration. We evaluate\nSTP with both Lean and Isabelle formal versifiers. With 51.3 billion tokens\ngenerated during the training in Lean, STP proves 28.5% of the statements in\nthe LeanWorkbook dataset, doubling the previous best result of 13.2% achieved\nthrough expert iteration. The final model achieves state-of-the-art performance\namong whole-proof generation methods on miniF2F-test (65.0%, pass@3200),\nProofnet-test (23.9%, pass@3200) and PutnamBench (8/644, pass@3200). We release\nour code, model, and dataset in this URL: https://github.com/kfdong/STP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in formal theorem proving by LLMs is the lack of\nhigh-quality training data. Although reinforcement learning or expert iteration\npartially mitigates this issue by alternating between LLM generating proofs and\nfinetuning them on correctly generated ones, performance quickly plateaus due\nto the scarcity of correct proofs (sparse rewards). To keep improving the\nmodels with limited data, we draw inspiration from mathematicians, who\ncontinuously develop new results, partly by proposing novel conjectures or\nexercises (which are often variants of known results) and attempting to solve\nthem. We design the Self-play Theorem Prover (STP) that simultaneously takes on\ntwo roles, conjecturer and prover, each providing training signals to the\nother. The conjecturer is trained iteratively on previously generated\nconjectures that are barely provable by the current prover, which incentivizes\nit to generate increasingly challenging conjectures over time. The prover\nattempts to prove the conjectures with standard expert iteration. We evaluate\nSTP with both Lean and Isabelle formal versifiers. With 51.3 billion tokens\ngenerated during the training in Lean, STP proves 28.5% of the statements in\nthe LeanWorkbook dataset, doubling the previous best result of 13.2% achieved\nthrough expert iteration. The final model achieves state-of-the-art performance\namong whole-proof generation methods on miniF2F-test (65.0%, pass@3200),\nProofnet-test (23.9%, pass@3200) and PutnamBench (8/644, pass@3200). We release\nour code, model, and dataset in this URL: https://github.com/kfdong/STP."
                },
                "authors": [
                    {
                        "name": "Kefan Dong"
                    },
                    {
                        "name": "Tengyu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Tengyu Ma"
                },
                "author": "Tengyu Ma",
                "arxiv_comment": "25 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00212v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00212v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15937v2",
                "updated": "2025-03-21T03:19:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    3,
                    19,
                    57,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T08:25:00Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    25,
                    0,
                    3,
                    79,
                    0
                ],
                "title": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment"
                },
                "summary": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities."
                },
                "authors": [
                    {
                        "name": "Gaole Dai"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Rui Tan"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "14 pages, 4 iterations, refine figs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04832v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04832v3",
                "updated": "2025-03-21T03:12:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    3,
                    12,
                    4,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-06T07:56:14Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    7,
                    56,
                    14,
                    4,
                    341,
                    0
                ],
                "title": "Neural Representation for Wireless Radiation Field Reconstruction: A 3D\n  Gaussian Splatting Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Representation for Wireless Radiation Field Reconstruction: A 3D\n  Gaussian Splatting Approach"
                },
                "summary": "Wireless channel modeling plays a pivotal role in designing, analyzing, and\noptimizing wireless communication systems. Nevertheless, developing an\neffective channel modeling approach has been a long-standing challenge. This\nissue has been escalated due to denser network deployment, larger antenna\narrays, and broader bandwidth in next-generation networks. To address this\nchallenge, we put forth WRF-GS, a novel framework for channel modeling based on\nwireless radiation field (WRF) reconstruction using 3D Gaussian splatting\n(3D-GS). WRF-GS employs 3D Gaussian primitives and neural networks to capture\nthe interactions between the environment and radio signals, enabling efficient\nWRF reconstruction and visualization of the propagation characteristics. The\nreconstructed WRF can then be used to synthesize the spatial spectrum for\ncomprehensive wireless channel characterization. While WRF-GS demonstrates\nremarkable effectiveness, it faces limitations in capturing high-frequency\nsignal variations caused by complex multipath effects. To overcome these\nlimitations, we propose WRF-GS+, an enhanced framework that integrates\nelectromagnetic wave physics into the neural network design. WRF-GS+ leverages\ndeformable 3D Gaussians to model both static and dynamic components of the WRF,\nsignificantly improving its ability to characterize signal variations. In\naddition, WRF-GS+ enhances the splatting process by simplifying the 3D-GS\nmodeling process and improving computational efficiency. Experimental results\ndemonstrate that both WRF-GS and WRF-GS+ outperform baselines for spatial\nspectrum synthesis, including ray tracing and other deep-learning approaches.\nNotably, WRF-GS+ achieves state-of-the-art performance in the received signal\nstrength indication (RSSI) and channel state information (CSI) prediction\ntasks, surpassing existing methods by more than 0.7 dB and 3.36 dB,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless channel modeling plays a pivotal role in designing, analyzing, and\noptimizing wireless communication systems. Nevertheless, developing an\neffective channel modeling approach has been a long-standing challenge. This\nissue has been escalated due to denser network deployment, larger antenna\narrays, and broader bandwidth in next-generation networks. To address this\nchallenge, we put forth WRF-GS, a novel framework for channel modeling based on\nwireless radiation field (WRF) reconstruction using 3D Gaussian splatting\n(3D-GS). WRF-GS employs 3D Gaussian primitives and neural networks to capture\nthe interactions between the environment and radio signals, enabling efficient\nWRF reconstruction and visualization of the propagation characteristics. The\nreconstructed WRF can then be used to synthesize the spatial spectrum for\ncomprehensive wireless channel characterization. While WRF-GS demonstrates\nremarkable effectiveness, it faces limitations in capturing high-frequency\nsignal variations caused by complex multipath effects. To overcome these\nlimitations, we propose WRF-GS+, an enhanced framework that integrates\nelectromagnetic wave physics into the neural network design. WRF-GS+ leverages\ndeformable 3D Gaussians to model both static and dynamic components of the WRF,\nsignificantly improving its ability to characterize signal variations. In\naddition, WRF-GS+ enhances the splatting process by simplifying the 3D-GS\nmodeling process and improving computational efficiency. Experimental results\ndemonstrate that both WRF-GS and WRF-GS+ outperform baselines for spatial\nspectrum synthesis, including ray tracing and other deep-learning approaches.\nNotably, WRF-GS+ achieves state-of-the-art performance in the received signal\nstrength indication (RSSI) and channel state information (CSI) prediction\ntasks, surpassing existing methods by more than 0.7 dB and 3.36 dB,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Chaozheng Wen"
                    },
                    {
                        "name": "Jingwen Tong"
                    },
                    {
                        "name": "Yingdong Hu"
                    },
                    {
                        "name": "Zehong Lin"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "This work has been submitted to the IEEE journals for possible\n  publication. The code is available at\n  https://github.com/wenchaozheng/WRF-GSplus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04832v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04832v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16029v2",
                "updated": "2025-03-21T03:03:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    3,
                    3,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T10:52:50Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    52,
                    50,
                    3,
                    79,
                    0
                ],
                "title": "A Controllable and Realistic Framework for Evaluating Microservice\n  Scheduling in Cloud-Edge Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Controllable and Realistic Framework for Evaluating Microservice\n  Scheduling in Cloud-Edge Continuum"
                },
                "summary": "The transition from traditional architectures to containerized microservices\nwithin the cloud-edge computing continuum introduces significant challenges,\nparticularly in the efficient scheduling of microservices under dynamic\nconditions. Complex and fluctuating call-graph dependencies, varying cross-node\ncommunication latencies, and unpredictable bandwidth conditions substantially\nimpact the performance and reliability of deployed microservices. Consequently,\naccurately evaluating scheduling policies in such dynamic environments remains\nessential yet challenging due to the lack of realistic and controllable\nevaluation frameworks.\n  In this paper, we propose iDynamics, a novel evaluation framework designed\nexplicitly to address these challenges. iDynamics provides comprehensive and\ncontrollable evaluation capabilities by emulating realistic dynamics, including\nconfigurable call-graph topologies, cross-node communication delays, and\nbandwidth variability. The framework is composed of modular components, such as\nthe Graph Dynamics Analyzer, Networking Dynamics Manager, and Scheduling Policy\nExtender, enabling fine-grained environmental control and facilitating\nsystematic comparisons of different scheduling strategies. Extensive\nexperiments on a real cloud-edge testbed demonstrate that iDynamics effectively\ncaptures diverse dynamic scenarios encountered in microservice deployments,\noffering a robust solution for evaluating and optimizing policy performance\nunder realistic and controllable conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition from traditional architectures to containerized microservices\nwithin the cloud-edge computing continuum introduces significant challenges,\nparticularly in the efficient scheduling of microservices under dynamic\nconditions. Complex and fluctuating call-graph dependencies, varying cross-node\ncommunication latencies, and unpredictable bandwidth conditions substantially\nimpact the performance and reliability of deployed microservices. Consequently,\naccurately evaluating scheduling policies in such dynamic environments remains\nessential yet challenging due to the lack of realistic and controllable\nevaluation frameworks.\n  In this paper, we propose iDynamics, a novel evaluation framework designed\nexplicitly to address these challenges. iDynamics provides comprehensive and\ncontrollable evaluation capabilities by emulating realistic dynamics, including\nconfigurable call-graph topologies, cross-node communication delays, and\nbandwidth variability. The framework is composed of modular components, such as\nthe Graph Dynamics Analyzer, Networking Dynamics Manager, and Scheduling Policy\nExtender, enabling fine-grained environmental control and facilitating\nsystematic comparisons of different scheduling strategies. Extensive\nexperiments on a real cloud-edge testbed demonstrate that iDynamics effectively\ncaptures diverse dynamic scenarios encountered in microservice deployments,\noffering a robust solution for evaluating and optimizing policy performance\nunder realistic and controllable conditions."
                },
                "authors": [
                    {
                        "name": "Ming Chen"
                    },
                    {
                        "name": "Muhammed Tawfiqul Islam"
                    },
                    {
                        "name": "Maria Rodriguez Read"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "arxiv_comment": "14 pages, 10 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11727v2",
                "updated": "2025-03-21T02:55:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    2,
                    55,
                    51,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-14T06:06:18Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    6,
                    18,
                    4,
                    73,
                    0
                ],
                "title": "Survey of City-Wide Homelessness Detection Through Environmental Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey of City-Wide Homelessness Detection Through Environmental Sensing"
                },
                "summary": "The growing homelessness crisis in the U.S. presents complex social,\neconomic, and public health challenges, straining shelters, healthcare, and\nsocial services while limiting effective interventions. Traditional assessment\nmethods struggle to capture its dynamic, dispersed nature, highlighting the\nneed for scalable, data-driven detection. This survey explores computational\napproaches across four domains: (1) computer vision and deep learning to\nidentify encampments and urban indicators of homelessness, (2) air quality\nsensing via fixed, mobile, and crowdsourced deployments to assess environmental\nrisks, (3) IoT and edge computing for real-time urban monitoring, and (4)\npedestrian behavior analysis to understand mobility patterns and interactions.\nDespite advancements, challenges persist in computational constraints, data\nprivacy, accurate environmental measurement, and adaptability. This survey\nsynthesizes recent research, identifies key gaps, and highlights opportunities\nto enhance homelessness detection, optimize resource allocation, and improve\nurban planning and social support systems for equitable aid distribution and\nbetter neighborhood conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing homelessness crisis in the U.S. presents complex social,\neconomic, and public health challenges, straining shelters, healthcare, and\nsocial services while limiting effective interventions. Traditional assessment\nmethods struggle to capture its dynamic, dispersed nature, highlighting the\nneed for scalable, data-driven detection. This survey explores computational\napproaches across four domains: (1) computer vision and deep learning to\nidentify encampments and urban indicators of homelessness, (2) air quality\nsensing via fixed, mobile, and crowdsourced deployments to assess environmental\nrisks, (3) IoT and edge computing for real-time urban monitoring, and (4)\npedestrian behavior analysis to understand mobility patterns and interactions.\nDespite advancements, challenges persist in computational constraints, data\nprivacy, accurate environmental measurement, and adaptability. This survey\nsynthesizes recent research, identifies key gaps, and highlights opportunities\nto enhance homelessness detection, optimize resource allocation, and improve\nurban planning and social support systems for equitable aid distribution and\nbetter neighborhood conditions."
                },
                "authors": [
                    {
                        "name": "Julia Gersey"
                    },
                    {
                        "name": "Rose Allegrette"
                    },
                    {
                        "name": "Joshua Lian"
                    },
                    {
                        "name": "Zawad Munshi"
                    },
                    {
                        "name": "Aarti Phatke"
                    }
                ],
                "author_detail": {
                    "name": "Aarti Phatke"
                },
                "author": "Aarti Phatke",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15886v2",
                "updated": "2025-03-21T02:55:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    2,
                    55,
                    26,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T06:20:13Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    6,
                    20,
                    13,
                    3,
                    79,
                    0
                ],
                "title": "Enhancing Zero-Shot Image Recognition in Vision-Language Models through\n  Human-like Concept Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Zero-Shot Image Recognition in Vision-Language Models through\n  Human-like Concept Guidance"
                },
                "summary": "In zero-shot image recognition tasks, humans demonstrate remarkable\nflexibility in classifying unseen categories by composing known simpler\nconcepts. However, existing vision-language models (VLMs), despite achieving\nsignificant progress through large-scale natural language supervision, often\nunderperform in real-world applications because of sub-optimal prompt\nengineering and the inability to adapt effectively to target classes. To\naddress these issues, we propose a Concept-guided Human-like Bayesian Reasoning\n(CHBR) framework. Grounded in Bayes' theorem, CHBR models the concept used in\nhuman image recognition as latent variables and formulates this task by summing\nacross potential concepts, weighted by a prior distribution and a likelihood\nfunction. To tackle the intractable computation over an infinite concept space,\nwe introduce an importance sampling algorithm that iteratively prompts large\nlanguage models (LLMs) to generate discriminative concepts, emphasizing\ninter-class differences. We further propose three heuristic approaches\ninvolving Average Likelihood, Confidence Likelihood, and Test Time Augmentation\n(TTA) Likelihood, which dynamically refine the combination of concepts based on\nthe test image. Extensive evaluations across fifteen datasets demonstrate that\nCHBR consistently outperforms existing state-of-the-art zero-shot\ngeneralization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot image recognition tasks, humans demonstrate remarkable\nflexibility in classifying unseen categories by composing known simpler\nconcepts. However, existing vision-language models (VLMs), despite achieving\nsignificant progress through large-scale natural language supervision, often\nunderperform in real-world applications because of sub-optimal prompt\nengineering and the inability to adapt effectively to target classes. To\naddress these issues, we propose a Concept-guided Human-like Bayesian Reasoning\n(CHBR) framework. Grounded in Bayes' theorem, CHBR models the concept used in\nhuman image recognition as latent variables and formulates this task by summing\nacross potential concepts, weighted by a prior distribution and a likelihood\nfunction. To tackle the intractable computation over an infinite concept space,\nwe introduce an importance sampling algorithm that iteratively prompts large\nlanguage models (LLMs) to generate discriminative concepts, emphasizing\ninter-class differences. We further propose three heuristic approaches\ninvolving Average Likelihood, Confidence Likelihood, and Test Time Augmentation\n(TTA) Likelihood, which dynamically refine the combination of concepts based on\nthe test image. Extensive evaluations across fifteen datasets demonstrate that\nCHBR consistently outperforms existing state-of-the-art zero-shot\ngeneralization methods."
                },
                "authors": [
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Wenya Wang"
                    },
                    {
                        "name": "Kecheng Chen"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Yibing Liu"
                    },
                    {
                        "name": "Tiexin Qin"
                    },
                    {
                        "name": "Peisong He"
                    },
                    {
                        "name": "Xinghao Jiang"
                    },
                    {
                        "name": "Haoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoliang Li"
                },
                "author": "Haoliang Li",
                "arxiv_comment": "21 pages, 7 figures 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16814v1",
                "updated": "2025-03-21T02:51:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    2,
                    51,
                    30,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T02:51:30Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    2,
                    51,
                    30,
                    4,
                    80,
                    0
                ],
                "title": "When Debate Fails: Bias Reinforcement in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Debate Fails: Bias Reinforcement in Large Language Models"
                },
                "summary": "Large Language Models $($LLMs$)$ solve complex problems using training-free\nmethods like prompt engineering and in-context learning, yet ensuring reasoning\ncorrectness remains challenging. While self-correction methods such as\nself-consistency and self-refinement aim to improve reliability, they often\nreinforce biases due to the lack of effective feedback mechanisms. Multi-Agent\nDebate $($MAD$)$ has emerged as an alternative, but we identify two key\nlimitations: bias reinforcement, where debate amplifies model biases instead of\ncorrecting them, and lack of perspective diversity, as all agents share the\nsame model and reasoning patterns, limiting true debate effectiveness. To\nsystematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a\nbenchmark designed to assess LLMs in adversarial strategic decision-making,\nwhere dynamic interactions influence optimal decisions. To overcome MAD's\nlimitations, we propose $\\textbf{DReaMAD}$ $($$\\textbf{D}$iverse\n$\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate\nwith Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic\nprior knowledge to improve reasoning quality and $(2)$ promotes diverse\nviewpoints within a single model by systematically modifying prompts, reducing\nbias. Empirical results show that $\\textbf{DReaMAD}$ significantly improves\ndecision accuracy, reasoning diversity, and bias mitigation across multiple\nstrategic tasks, establishing it as a more effective approach for LLM-based\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models $($LLMs$)$ solve complex problems using training-free\nmethods like prompt engineering and in-context learning, yet ensuring reasoning\ncorrectness remains challenging. While self-correction methods such as\nself-consistency and self-refinement aim to improve reliability, they often\nreinforce biases due to the lack of effective feedback mechanisms. Multi-Agent\nDebate $($MAD$)$ has emerged as an alternative, but we identify two key\nlimitations: bias reinforcement, where debate amplifies model biases instead of\ncorrecting them, and lack of perspective diversity, as all agents share the\nsame model and reasoning patterns, limiting true debate effectiveness. To\nsystematically evaluate these issues, we introduce $\\textit{MetaNIM Arena}$, a\nbenchmark designed to assess LLMs in adversarial strategic decision-making,\nwhere dynamic interactions influence optimal decisions. To overcome MAD's\nlimitations, we propose $\\textbf{DReaMAD}$ $($$\\textbf{D}$iverse\n$\\textbf{Rea}$soning via $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{D}$ebate\nwith Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic\nprior knowledge to improve reasoning quality and $(2)$ promotes diverse\nviewpoints within a single model by systematically modifying prompts, reducing\nbias. Empirical results show that $\\textbf{DReaMAD}$ significantly improves\ndecision accuracy, reasoning diversity, and bias mitigation across multiple\nstrategic tasks, establishing it as a more effective approach for LLM-based\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Jihwan Oh"
                    },
                    {
                        "name": "Minchan Jeong"
                    },
                    {
                        "name": "Jongwoo Ko"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs.\n  First two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16789v1",
                "updated": "2025-03-21T02:01:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    2,
                    1,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T02:01:02Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    2,
                    1,
                    2,
                    4,
                    80,
                    0
                ],
                "title": "Conversational User-AI Intervention: A Study on Prompt Rewriting for\n  Improved LLM Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational User-AI Intervention: A Study on Prompt Rewriting for\n  Improved LLM Response Generation"
                },
                "summary": "Human-LLM conversations are increasingly becoming more pervasive in peoples'\nprofessional and personal lives, yet many users still struggle to elicit\nhelpful responses from LLM Chatbots. One of the reasons for this issue is\nusers' lack of understanding in crafting effective prompts that accurately\nconvey their information needs. Meanwhile, the existence of real-world\nconversational datasets on the one hand, and the text understanding faculties\nof LLMs on the other, present a unique opportunity to study this problem, and\nits potential solutions at scale. Thus, in this paper we present the first\nLLM-centric study of real human-AI chatbot conversations, focused on\ninvestigating aspects in which user queries fall short of expressing\ninformation needs, and the potential of using LLMs to rewrite suboptimal user\nprompts. Our findings demonstrate that rephrasing ineffective prompts can\nelicit better responses from a conversational system, while preserving the\nuser's original intent. Notably, the performance of rewrites improves in longer\nconversations, where contextual inferences about user needs can be made more\naccurately. Additionally, we observe that LLMs often need to -- and inherently\ndo -- make \\emph{plausible} assumptions about a user's intentions and goals\nwhen interpreting prompts. Our findings largely hold true across conversational\ndomains, user intents, and LLMs of varying sizes and families, indicating the\npromise of using prompt rewriting as a solution for better human-AI\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-LLM conversations are increasingly becoming more pervasive in peoples'\nprofessional and personal lives, yet many users still struggle to elicit\nhelpful responses from LLM Chatbots. One of the reasons for this issue is\nusers' lack of understanding in crafting effective prompts that accurately\nconvey their information needs. Meanwhile, the existence of real-world\nconversational datasets on the one hand, and the text understanding faculties\nof LLMs on the other, present a unique opportunity to study this problem, and\nits potential solutions at scale. Thus, in this paper we present the first\nLLM-centric study of real human-AI chatbot conversations, focused on\ninvestigating aspects in which user queries fall short of expressing\ninformation needs, and the potential of using LLMs to rewrite suboptimal user\nprompts. Our findings demonstrate that rephrasing ineffective prompts can\nelicit better responses from a conversational system, while preserving the\nuser's original intent. Notably, the performance of rewrites improves in longer\nconversations, where contextual inferences about user needs can be made more\naccurately. Additionally, we observe that LLMs often need to -- and inherently\ndo -- make \\emph{plausible} assumptions about a user's intentions and goals\nwhen interpreting prompts. Our findings largely hold true across conversational\ndomains, user intents, and LLMs of varying sizes and families, indicating the\npromise of using prompt rewriting as a solution for better human-AI\ninteractions."
                },
                "authors": [
                    {
                        "name": "Rupak Sarkar"
                    },
                    {
                        "name": "Bahareh Sarrafzadeh"
                    },
                    {
                        "name": "Nirupama Chandrasekaran"
                    },
                    {
                        "name": "Nagu Rangan"
                    },
                    {
                        "name": "Philip Resnik"
                    },
                    {
                        "name": "Longqi Yang"
                    },
                    {
                        "name": "Sujay Kumar Jauhar"
                    }
                ],
                "author_detail": {
                    "name": "Sujay Kumar Jauhar"
                },
                "author": "Sujay Kumar Jauhar",
                "arxiv_comment": "8 pages, ACL style",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14908v2",
                "updated": "2025-03-21T01:58:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    58,
                    0,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-23T11:02:46Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    11,
                    2,
                    46,
                    0,
                    267,
                    0
                ],
                "title": "KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory\n  Systems"
                },
                "summary": "Embodied AI agents responsible for executing interconnected, long-sequence\nhousehold tasks often face difficulties with in-context memory, leading to\ninefficiencies and errors in task execution. To address this issue, we\nintroduce KARMA, an innovative memory system that integrates long-term and\nshort-term memory modules, enhancing large language models (LLMs) for planning\nin embodied agents through memory-augmented prompting. KARMA distinguishes\nbetween long-term and short-term memory, with long-term memory capturing\ncomprehensive 3D scene graphs as representations of the environment, while\nshort-term memory dynamically records changes in objects' positions and states.\nThis dual-memory structure allows agents to retrieve relevant past scene\nexperiences, thereby improving the accuracy and efficiency of task planning.\nShort-term memory employs strategies for effective and adaptive memory\nreplacement, ensuring the retention of critical information while discarding\nless pertinent data. Compared to state-of-the-art embodied agents enhanced with\nmemory, our memory-augmented embodied AI agent improves success rates by 1.3x\nand 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator,\nrespectively, and enhances task execution efficiency by 3.4x and 62.7x.\nFurthermore, we demonstrate that KARMA's plug-and-play capability allows for\nseamless deployment on real-world robotic systems, such as mobile manipulation\nplatforms.Through this plug-and-play memory system, KARMA significantly\nenhances the ability of embodied agents to generate coherent and contextually\nappropriate plans, making the execution of complex household tasks more\nefficient. The experimental videos from the work can be found at\nhttps://youtu.be/4BT7fnw9ehs. Our code is available at\nhttps://github.com/WZX0Swarm0Robotics/KARMA/tree/master.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI agents responsible for executing interconnected, long-sequence\nhousehold tasks often face difficulties with in-context memory, leading to\ninefficiencies and errors in task execution. To address this issue, we\nintroduce KARMA, an innovative memory system that integrates long-term and\nshort-term memory modules, enhancing large language models (LLMs) for planning\nin embodied agents through memory-augmented prompting. KARMA distinguishes\nbetween long-term and short-term memory, with long-term memory capturing\ncomprehensive 3D scene graphs as representations of the environment, while\nshort-term memory dynamically records changes in objects' positions and states.\nThis dual-memory structure allows agents to retrieve relevant past scene\nexperiences, thereby improving the accuracy and efficiency of task planning.\nShort-term memory employs strategies for effective and adaptive memory\nreplacement, ensuring the retention of critical information while discarding\nless pertinent data. Compared to state-of-the-art embodied agents enhanced with\nmemory, our memory-augmented embodied AI agent improves success rates by 1.3x\nand 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator,\nrespectively, and enhances task execution efficiency by 3.4x and 62.7x.\nFurthermore, we demonstrate that KARMA's plug-and-play capability allows for\nseamless deployment on real-world robotic systems, such as mobile manipulation\nplatforms.Through this plug-and-play memory system, KARMA significantly\nenhances the ability of embodied agents to generate coherent and contextually\nappropriate plans, making the execution of complex household tasks more\nefficient. The experimental videos from the work can be found at\nhttps://youtu.be/4BT7fnw9ehs. Our code is available at\nhttps://github.com/WZX0Swarm0Robotics/KARMA/tree/master."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Junzhe Zhao"
                    },
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Sai Hou"
                    },
                    {
                        "name": "Shuai Liang"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Yiming Gan"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Gan"
                },
                "author": "Yiming Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16252v2",
                "updated": "2025-03-21T01:57:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    57,
                    58,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T15:46:18Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    46,
                    18,
                    3,
                    79,
                    0
                ],
                "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning"
                },
                "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1."
                },
                "authors": [
                    {
                        "name": "Zhaowei Liu"
                    },
                    {
                        "name": "Xin Guo"
                    },
                    {
                        "name": "Fangqi Lou"
                    },
                    {
                        "name": "Lingfeng Zeng"
                    },
                    {
                        "name": "Jinyi Niu"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jiajie Xu"
                    },
                    {
                        "name": "Weige Cai"
                    },
                    {
                        "name": "Ziwei Yang"
                    },
                    {
                        "name": "Xueqian Zhao"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Sheng Xu"
                    },
                    {
                        "name": "Dezhi Chen"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Zuo Bai"
                    },
                    {
                        "name": "Liwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liwen Zhang"
                },
                "author": "Liwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03035v3",
                "updated": "2025-03-21T01:34:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    34,
                    48,
                    4,
                    80,
                    0
                ],
                "published": "2024-10-03T22:41:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    41,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "SPINE: Online Semantic Planning for Missions with Incomplete Natural\n  Language Specifications in Unstructured Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPINE: Online Semantic Planning for Missions with Incomplete Natural\n  Language Specifications in Unstructured Environments"
                },
                "summary": "As robots become increasingly capable, users will want to describe high-level\nmissions and have robots infer the relevant details. Because pre-built maps are\ndifficult to obtain in many realistic settings, accomplishing such missions\nwill require the robot to map and plan online. While many semantic planning\nmethods operate online, they are typically designed for well specified missions\nsuch as object search or exploration. Recently, Large Language Models (LLMs)\nhave demonstrated powerful contextual reasoning abilities over a range of\nrobotic tasks described in natural language. However, existing LLM-enabled\nplanners typically do not consider online planning or complex missions; rather,\nrelevant subtasks and semantics are provided by a pre-built map or a user. We\naddress these limitations via SPINE, an online planner for missions with\nincomplete mission specifications provided in natural language. The planner\nuses an LLM to reason about subtasks implied by the mission specification and\nthen realizes these subtasks in a receding horizon framework. Tasks are\nautomatically validated for safety and refined online with new map\nobservations. We evaluate SPINE in simulation and real-world settings with\nmissions that require multiple steps of semantic reasoning and exploration in\ncluttered outdoor environments of over 20,000m$^2$. Compared to baselines that\nuse existing LLM-enabled planning approaches, our method is over twice as\nefficient in terms of time and distance, requires less user interactions, and\ndoes not require a full map. Additional resources are provided at\nhttps://zacravichandran.github.io/SPINE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robots become increasingly capable, users will want to describe high-level\nmissions and have robots infer the relevant details. Because pre-built maps are\ndifficult to obtain in many realistic settings, accomplishing such missions\nwill require the robot to map and plan online. While many semantic planning\nmethods operate online, they are typically designed for well specified missions\nsuch as object search or exploration. Recently, Large Language Models (LLMs)\nhave demonstrated powerful contextual reasoning abilities over a range of\nrobotic tasks described in natural language. However, existing LLM-enabled\nplanners typically do not consider online planning or complex missions; rather,\nrelevant subtasks and semantics are provided by a pre-built map or a user. We\naddress these limitations via SPINE, an online planner for missions with\nincomplete mission specifications provided in natural language. The planner\nuses an LLM to reason about subtasks implied by the mission specification and\nthen realizes these subtasks in a receding horizon framework. Tasks are\nautomatically validated for safety and refined online with new map\nobservations. We evaluate SPINE in simulation and real-world settings with\nmissions that require multiple steps of semantic reasoning and exploration in\ncluttered outdoor environments of over 20,000m$^2$. Compared to baselines that\nuse existing LLM-enabled planning approaches, our method is over twice as\nefficient in terms of time and distance, requires less user interactions, and\ndoes not require a full map. Additional resources are provided at\nhttps://zacravichandran.github.io/SPINE."
                },
                "authors": [
                    {
                        "name": "Zachary Ravichandran"
                    },
                    {
                        "name": "Varun Murali"
                    },
                    {
                        "name": "Mariliza Tzes"
                    },
                    {
                        "name": "George J. Pappas"
                    },
                    {
                        "name": "Vijay Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Kumar"
                },
                "author": "Vijay Kumar",
                "arxiv_comment": "Accepted to the International Conference on Robotics and Automation\n  (ICRA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]